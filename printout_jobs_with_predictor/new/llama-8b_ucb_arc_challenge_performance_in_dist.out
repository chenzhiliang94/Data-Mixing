2026-01-01 05:33:56.278214: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 05:33:56.309391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 05:33:56.309450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 05:33:56.310470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 05:33:56.315680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 05:33:57.244704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'arc_challenge', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_arc_challenge_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['arc_challenge']
evaluation tasks and weights:  {'arc_challenge': (1.0, 'acc,none')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/arc_challenge/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 17, 1, 0, 1, 1, 0, 59, 0.00445603671922955, 41, 0]
Checking history sample input_X_between_0_1:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 0.53125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4609375, 0.044560367192295496, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.47
Checking history sample input_X:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 22, 1, 1, 1, 0, 0, 61, 0.08406775506720655, 5, 1]
Checking history sample input_X_between_0_1:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 0.6875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4765625, 0.8406775506720655, 0.10416666666666667, 1.0]
Checking history sample performance at 625 steps:  0.47
Checking history sample input_X:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 15, 1, 1, 1, 1, 0, 24, 0.05173325143341287, 28, 0]
Checking history sample input_X_between_0_1:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 0.46875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 0.5173325143341286, 0.5833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 1, 0, 1, 1, 0, 0, 114, 0.021033555465481826, 23, 1]
Checking history sample input_X_between_0_1:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 0.03125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.890625, 0.21033555465481824, 0.4791666666666667, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 27, 1, 1, 1, 1, 1, 70, 0.005963494977880357, 46, 1]
Checking history sample input_X_between_0_1:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.05963494977880357, 0.9583333333333334, 1.0]
Checking history sample performance at 625 steps:  0.46
Checking history sample input_X:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 12, 1, 0, 1, 0, 0, 3, 0.017954470577765235, 12, 1]
Checking history sample input_X_between_0_1:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0234375, 0.17954470577765233, 0.25, 1.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 18, 1, 0, 0, 0, 1, 5, 0.028541773579282805, 35, 0]
Checking history sample input_X_between_0_1:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 0.5625, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0390625, 0.28541773579282803, 0.7291666666666666, 0.0]
Checking history sample performance at 625 steps:  0.4
Checking history sample input_X:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 24, 0, 1, 0, 1, 0, 55, 0.08420649661865921, 22, 0]
Checking history sample input_X_between_0_1:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 0.75, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4296875, 0.8420649661865921, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 30, 1, 0, 1, 1, 0, 5, 0.09059974254780842, 29, 1]
Checking history sample input_X_between_0_1:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 0.9375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0390625, 0.9059974254780842, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.44
Checking history sample input_X:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 20, 1, 1, 0, 1, 0, 27, 0.07188580123073206, 22, 0]
Checking history sample input_X_between_0_1:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2109375, 0.7188580123073205, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 8, 0, 0, 0, 1, 1, 31, 0.06288146497812035, 13, 1]
Checking history sample input_X_between_0_1:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2421875, 0.6288146497812034, 0.2708333333333333, 1.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 32, 1, 1, 0, 1, 1, 114, 0.08252011949389138, 13, 0]
Checking history sample input_X_between_0_1:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.890625, 0.8252011949389138, 0.2708333333333333, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 11, 0, 1, 0, 0, 1, 100, 0.09203832421630076, 16, 1]
Checking history sample input_X_between_0_1:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 0.34375, 0.0, 1.0, 0.0, 0.0, 1.0, 0.78125, 0.9203832421630076, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 31, 1, 0, 1, 1, 1, 60, 0.008025447056787238, 26, 0]
Checking history sample input_X_between_0_1:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 0.96875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.46875, 0.08025447056787237, 0.5416666666666666, 0.0]
Checking history sample performance at 625 steps:  0.57
Checking history sample input_X:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 7, 0, 0, 1, 0, 0, 10, 0.06890300584266877, 3, 1]
Checking history sample input_X_between_0_1:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.078125, 0.6890300584266876, 0.0625, 1.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 1, 1, 1, 0, 1, 0, 28, 0.0879909943648135, 16, 0]
Checking history sample input_X_between_0_1:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 0.03125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21875, 0.879909943648135, 0.3333333333333333, 0.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 1, 1, 0, 0, 1, 0, 47, 0.04979780998318395, 37, 0]
Checking history sample input_X_between_0_1:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 0.03125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3671875, 0.4979780998318395, 0.7708333333333334, 0.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 30, 1, 0, 1, 0, 0, 86, 0.08085387005284786, 6, 1]
Checking history sample input_X_between_0_1:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 0.9375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.671875, 0.8085387005284785, 0.125, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 31, 0, 1, 1, 1, 1, 84, 0.015776955723827136, 41, 1]
Checking history sample input_X_between_0_1:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 0.96875, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.15776955723827135, 0.8541666666666666, 1.0]
Checking history sample performance at 625 steps:  0.43
Checking history sample input_X:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 24, 0, 1, 1, 0, 1, 1, 0.07052967196513893, 22, 0]
Checking history sample input_X_between_0_1:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.7052967196513893, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.46
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.6457 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.21284466981887817, 0.6387652158737183, 0.17035681009292603, 0.9559035301208496, 0.9607769250869751, 0.6517424583435059, 0.1394832730293274, 0.41211336851119995, 0.8426974415779114, 0.5913131237030029, 0.049931883811950684, 0.9973477721214294, 0.9850741624832153, 0.22613710165023804, 0.2523707151412964, 0.730124831199646, 0.2981664538383484, 0.7997794151306152, 0.8386002779006958]  ‚Üí  acq = 0.6033038872608796
X = [0.5501054525375366, 0.2628321051597595, 0.41607600450515747, 0.8941196799278259, 0.12184029817581177, 0.1427212357521057, 0.284953773021698, 0.16947495937347412, 0.955461859703064, 0.939895510673523, 0.7312613725662231, 0.16400116682052612, 0.6947259306907654, 0.9718984365463257, 0.11714828014373779, 0.9569462537765503, 0.2389141321182251, 0.141102135181427, 0.04895317554473877]  ‚Üí  acq = 0.6031767900606152
X = [0.6433089375495911, 0.46062707901000977, 0.16426414251327515, 0.5767724514007568, 0.06681394577026367, 0.7250623106956482, 0.46319055557250977, 0.18715763092041016, 0.7539563775062561, 0.7758975625038147, 0.16784071922302246, 0.35990846157073975, 0.5000655651092529, 0.9118218421936035, 0.5728258490562439, 0.0791480541229248, 0.6589025259017944, 0.8285610675811768, 0.4409768581390381]  ‚Üí  acq = 0.6020609323319261
X = [0.23919904232025146, 0.7948135137557983, 0.8842803239822388, 0.6406307220458984, 0.1640486717224121, 0.26854515075683594, 0.6649753451347351, 0.5322253108024597, 0.7760331034660339, 0.11757059395313263, 0.05975759029388428, 0.12152522802352905, 0.28443610668182373, 0.6696209907531738, 0.9802253842353821, 0.9204164743423462, 0.3942437171936035, 0.47339531779289246, 0.26674360036849976]  ‚Üí  acq = 0.6033736650702807
X = [0.8790185451507568, 0.26736336946487427, 0.435327410697937, 0.8526361584663391, 0.7878652811050415, 0.8344386219978333, 0.182719886302948, 0.6324243545532227, 0.7360810041427612, 0.6895639896392822, 0.04820406436920166, 0.5818954706192017, 0.24858999252319336, 0.2500644326210022, 0.3016858696937561, 0.5182454586029053, 0.9588213562965393, 0.4821402430534363, 0.2866358160972595]  ‚Üí  acq = 0.6033733815877047
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, 0, tensor(0.3486, dtype=torch.float64), tensor(0.2282, dtype=torch.float64), tensor(0.1200, dtype=torch.float64), tensor(0.1610, dtype=torch.float64), tensor(0.0943, dtype=torch.float64), tensor(0.0479, dtype=torch.float64), 0, 30, 1, 0, 1, 1, 1, 32, 0.0091781622496234, 16.47830600605779, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(2.2236e-18, dtype=torch.float64), tensor(1.3671e-18, dtype=torch.float64), tensor(0.3486, dtype=torch.float64), tensor(0.2282, dtype=torch.float64), tensor(0.1200, dtype=torch.float64), tensor(0.1610, dtype=torch.float64), tensor(0.0943, dtype=torch.float64), tensor(0.0479, dtype=torch.float64), tensor(5.6709e-19, dtype=torch.float64), tensor(0.9496, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2499, dtype=torch.float64), tensor(0.0918, dtype=torch.float64), tensor(0.3433, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.349
  sciq: 0.228
  triviaqa: 0.12
  truthfulqa_gen: 0.161
  wikitext: 0.094
  mmlu: 0.048
  arc_challenge: 0

LoRA Parameters:
  lora_r: (32,)
  lora_dropout: (0.0091781622496234,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (16.47830600605779,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  32
lora dropout:  0.0091781622496234
lora alpha:  16.47830600605779
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 60,948,480 || all params: 8,091,209,728 || trainable%: 0.7533
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_054259-ofzqks7d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/ofzqks7d
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 50.01it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 82.11it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 93.98it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 101.41it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:03, 107.24it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 118.79it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:01<00:02, 119.25it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 128.15it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 128.87it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 129.80it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 131.73it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 139.26it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 142.30it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 143.08it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 145.67it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 145.84it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 150.50it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 163.00it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 164.92it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 171.56it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 174.57it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 183.66it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 142.89it/s]
Evaluation performance at step 25: 0.51
{'loss': 3.5936, 'grad_norm': 1.0959491729736328, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.51}
{'eval_loss': 2.245576858520508, 'eval_runtime': 10.6849, 'eval_samples_per_second': 93.497, 'eval_steps_per_second': 5.896, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.31it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 89.61it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 100.77it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.35it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.09it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.23it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.59it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.01it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.03it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 137.56it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.27it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 145.88it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 149.18it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.10it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 154.56it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.00it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 159.18it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 171.34it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 180.56it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 184.83it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 195.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.11it/s]
Evaluation performance at step 50: 0.42
{'loss': 1.8546, 'grad_norm': 0.6060585379600525, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.42}
{'eval_loss': 1.6311277151107788, 'eval_runtime': 10.6894, 'eval_samples_per_second': 93.457, 'eval_steps_per_second': 5.894, 'epoch': 0.08}
{'loss': 1.6806, 'grad_norm': 0.245231494307518, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5507524013519287, 'eval_runtime': 10.7079, 'eval_samples_per_second': 93.295, 'eval_steps_per_second': 5.883, 'epoch': 0.12}
{'loss': 1.5657, 'grad_norm': 0.2897748351097107, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5196467638015747, 'eval_runtime': 10.7824, 'eval_samples_per_second': 92.651, 'eval_steps_per_second': 5.843, 'epoch': 0.16}
{'loss': 1.5516, 'grad_norm': 0.23694702982902527, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5030180215835571, 'eval_runtime': 10.774, 'eval_samples_per_second': 92.723, 'eval_steps_per_second': 5.847, 'epoch': 0.2}
{'loss': 1.5108, 'grad_norm': 0.23786379396915436, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4918299913406372, 'eval_runtime': 10.7525, 'eval_samples_per_second': 92.909, 'eval_steps_per_second': 5.859, 'epoch': 0.24}
{'loss': 1.4709, 'grad_norm': 0.21299560368061066, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.472619891166687, 'eval_runtime': 10.7086, 'eval_samples_per_second': 93.289, 'eval_steps_per_second': 5.883, 'epoch': 0.28}
{'loss': 1.5385, 'grad_norm': 0.22224438190460205, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.459612250328064, 'eval_runtime': 10.6842, 'eval_samples_per_second': 93.503, 'eval_steps_per_second': 5.897, 'epoch': 0.32}
{'loss': 1.5154, 'grad_norm': 0.30361735820770264, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4505655765533447, 'eval_runtime': 10.7235, 'eval_samples_per_second': 93.16, 'eval_steps_per_second': 5.875, 'epoch': 0.36}
{'loss': 1.5114, 'grad_norm': 0.2177026867866516, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4407674074172974, 'eval_runtime': 10.7436, 'eval_samples_per_second': 92.985, 'eval_steps_per_second': 5.864, 'epoch': 0.4}
{'loss': 1.4977, 'grad_norm': 0.23794586956501007, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4322319030761719, 'eval_runtime': 10.7466, 'eval_samples_per_second': 92.96, 'eval_steps_per_second': 5.862, 'epoch': 0.44}
{'loss': 1.4824, 'grad_norm': 0.24197813868522644, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.42727792263031, 'eval_runtime': 10.7976, 'eval_samples_per_second': 92.52, 'eval_steps_per_second': 5.835, 'epoch': 0.48}
{'loss': 1.4894, 'grad_norm': 0.23653437197208405, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4175703525543213, 'eval_runtime': 10.7542, 'eval_samples_per_second': 92.894, 'eval_steps_per_second': 5.858, 'epoch': 0.52}
{'loss': 1.4456, 'grad_norm': 0.26823195815086365, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4097703695297241, 'eval_runtime': 10.7569, 'eval_samples_per_second': 92.871, 'eval_steps_per_second': 5.857, 'epoch': 0.56}
{'loss': 1.4652, 'grad_norm': 0.2688368558883667, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4015543460845947, 'eval_runtime': 10.7493, 'eval_samples_per_second': 92.936, 'eval_steps_per_second': 5.861, 'epoch': 0.6}
{'loss': 1.4621, 'grad_norm': 0.262111634016037, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3964625597000122, 'eval_runtime': 10.7606, 'eval_samples_per_second': 92.839, 'eval_steps_per_second': 5.855, 'epoch': 0.64}
{'loss': 1.448, 'grad_norm': 0.25134697556495667, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3919885158538818, 'eval_runtime': 10.759, 'eval_samples_per_second': 92.852, 'eval_steps_per_second': 5.856, 'epoch': 0.68}
{'loss': 1.4721, 'grad_norm': 0.24634236097335815, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.386452317237854, 'eval_runtime': 10.7363, 'eval_samples_per_second': 93.048, 'eval_steps_per_second': 5.868, 'epoch': 0.72}
{'loss': 1.4511, 'grad_norm': 0.23194162547588348, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.378700852394104, 'eval_runtime': 10.6769, 'eval_samples_per_second': 93.567, 'eval_steps_per_second': 5.901, 'epoch': 0.76}
{'loss': 1.4082, 'grad_norm': 0.23231017589569092, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.374139428138733, 'eval_runtime': 10.6757, 'eval_samples_per_second': 93.577, 'eval_steps_per_second': 5.901, 'epoch': 0.8}
{'loss': 1.4347, 'grad_norm': 0.21655036509037018, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3708672523498535, 'eval_runtime': 10.6819, 'eval_samples_per_second': 93.523, 'eval_steps_per_second': 5.898, 'epoch': 0.84}
{'loss': 1.4374, 'grad_norm': 0.27757760882377625, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3648254871368408, 'eval_runtime': 10.6845, 'eval_samples_per_second': 93.5, 'eval_steps_per_second': 5.896, 'epoch': 0.88}
{'loss': 1.469, 'grad_norm': 0.25258609652519226, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3629953861236572, 'eval_runtime': 10.6908, 'eval_samples_per_second': 93.445, 'eval_steps_per_second': 5.893, 'epoch': 0.92}
{'loss': 1.4245, 'grad_norm': 0.2203216701745987, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3603596687316895, 'eval_runtime': 10.6902, 'eval_samples_per_second': 93.45, 'eval_steps_per_second': 5.893, 'epoch': 0.96}
{'loss': 1.4915, 'grad_norm': 0.29860952496528625, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3598212003707886, 'eval_runtime': 10.6728, 'eval_samples_per_second': 93.603, 'eval_steps_per_second': 5.903, 'epoch': 1.0}
{'train_runtime': 554.6404, 'train_samples_per_second': 18.026, 'train_steps_per_second': 1.127, 'train_loss': 1.5868720397949219, 'epoch': 1.0}
train_results:  {'eval_loss': [2.245576858520508, 1.6311277151107788, 1.5507524013519287, 1.5196467638015747, 1.5030180215835571, 1.4918299913406372, 1.472619891166687, 1.459612250328064, 1.4505655765533447, 1.4407674074172974, 1.4322319030761719, 1.42727792263031, 1.4175703525543213, 1.4097703695297241, 1.4015543460845947, 1.3964625597000122, 1.3919885158538818, 1.386452317237854, 1.378700852394104, 1.374139428138733, 1.3708672523498535, 1.3648254871368408, 1.3629953861236572, 1.3603596687316895, 1.3598212003707886], 'performance': [0.51, 0.42]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:29,  4.44it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.13it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 65.18it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 72.86it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 78.48it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 83.11it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 89.00it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 96.79it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 97.73it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 98.99it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 102.01it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 108.45it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 118.13it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 115.35it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 120.09it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 119.04it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 118.71it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 120.61it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 122.78it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 129.91it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 129.06it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 129.47it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 139.06it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 109.90it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.51, 0.42]
current iteration observed (possibly low-fid or predicted) performance:  0.7225359678268433
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.2918 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 0.7453987754933469
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.7452596742322933
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 0.7453987669244558
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 0.745403663372497
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 0.7453989239066541
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4619, dtype=torch.float64), tensor(0.3916, dtype=torch.float64), 0, tensor(0.1465, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 1, 0, 55, 0.03663973508542471, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4619, dtype=torch.float64), tensor(0.3916, dtype=torch.float64), tensor(3.9248e-18, dtype=torch.float64), tensor(0.1465, dtype=torch.float64), tensor(6.2041e-18, dtype=torch.float64), tensor(1.9223e-17, dtype=torch.float64), tensor(5.9927e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4319, dtype=torch.float64), tensor(0.3664, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.462
  sciq: 0.392
  triviaqa: 0
  truthfulqa_gen: 0.147
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (55,)
  lora_dropout: (0.03663973508542471,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  55
lora dropout:  0.03663973508542471
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 55,869,440 || all params: 8,086,130,688 || trainable%: 0.6909
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.00it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.14it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.85it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 110.67it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 115.47it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.81it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 129.19it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 138.75it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 138.64it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 140.03it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 141.61it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 148.32it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 151.89it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 152.70it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 157.24it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.91it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 165.44it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 176.82it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 184.02it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 197.23it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 202.44it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 155.21it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3617, 'grad_norm': 0.4785611927509308, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.583813190460205, 'eval_runtime': 10.4815, 'eval_samples_per_second': 95.311, 'eval_steps_per_second': 6.011, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.56it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.72it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 103.33it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 111.21it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 115.97it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 128.18it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 129.61it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 139.40it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 139.34it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 140.66it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 142.04it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 148.68it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 152.21it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 153.12it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 157.74it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 158.30it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 165.82it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 177.22it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 184.43it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 197.87it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 203.22it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 155.77it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.7795, 'grad_norm': 0.37986287474632263, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.1474897861480713, 'eval_runtime': 10.4497, 'eval_samples_per_second': 95.601, 'eval_steps_per_second': 6.029, 'epoch': 0.08}
{'loss': 1.9042, 'grad_norm': 0.09169293940067291, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7413947582244873, 'eval_runtime': 10.443, 'eval_samples_per_second': 95.662, 'eval_steps_per_second': 6.033, 'epoch': 0.12}
{'loss': 1.629, 'grad_norm': 0.1271616816520691, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.621720552444458, 'eval_runtime': 10.4758, 'eval_samples_per_second': 95.363, 'eval_steps_per_second': 6.014, 'epoch': 0.16}
{'loss': 1.559, 'grad_norm': 0.08310338109731674, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5777090787887573, 'eval_runtime': 10.4936, 'eval_samples_per_second': 95.201, 'eval_steps_per_second': 6.004, 'epoch': 0.2}
{'loss': 1.5727, 'grad_norm': 0.10218705981969833, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.560441017150879, 'eval_runtime': 10.4974, 'eval_samples_per_second': 95.166, 'eval_steps_per_second': 6.001, 'epoch': 0.24}
{'loss': 1.5367, 'grad_norm': 0.08447729796171188, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.548940658569336, 'eval_runtime': 10.5014, 'eval_samples_per_second': 95.13, 'eval_steps_per_second': 5.999, 'epoch': 0.28}
{'loss': 1.5425, 'grad_norm': 0.0883818045258522, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5394608974456787, 'eval_runtime': 10.5009, 'eval_samples_per_second': 95.135, 'eval_steps_per_second': 5.999, 'epoch': 0.32}
{'loss': 1.5799, 'grad_norm': 0.13427101075649261, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5326406955718994, 'eval_runtime': 10.5032, 'eval_samples_per_second': 95.114, 'eval_steps_per_second': 5.998, 'epoch': 0.36}
{'loss': 1.5637, 'grad_norm': 0.08604282885789871, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5266938209533691, 'eval_runtime': 10.4967, 'eval_samples_per_second': 95.173, 'eval_steps_per_second': 6.002, 'epoch': 0.4}
{'loss': 1.495, 'grad_norm': 0.09730829298496246, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5242009162902832, 'eval_runtime': 10.4977, 'eval_samples_per_second': 95.164, 'eval_steps_per_second': 6.001, 'epoch': 0.44}
{'loss': 1.5425, 'grad_norm': 0.12693339586257935, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5183357000350952, 'eval_runtime': 10.4905, 'eval_samples_per_second': 95.229, 'eval_steps_per_second': 6.005, 'epoch': 0.48}
{'loss': 1.4748, 'grad_norm': 0.09352754056453705, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5152966976165771, 'eval_runtime': 10.4926, 'eval_samples_per_second': 95.21, 'eval_steps_per_second': 6.004, 'epoch': 0.52}
{'loss': 1.4481, 'grad_norm': 0.10271718353033066, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5123112201690674, 'eval_runtime': 10.5611, 'eval_samples_per_second': 94.592, 'eval_steps_per_second': 5.965, 'epoch': 0.56}
{'loss': 1.5435, 'grad_norm': 0.10747192800045013, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5096739530563354, 'eval_runtime': 10.5813, 'eval_samples_per_second': 94.412, 'eval_steps_per_second': 5.954, 'epoch': 0.6}
{'loss': 1.5178, 'grad_norm': 0.11372976750135422, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5065224170684814, 'eval_runtime': 10.6048, 'eval_samples_per_second': 94.202, 'eval_steps_per_second': 5.941, 'epoch': 0.64}
{'loss': 1.5071, 'grad_norm': 0.09988820552825928, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5051186084747314, 'eval_runtime': 10.5714, 'eval_samples_per_second': 94.5, 'eval_steps_per_second': 5.959, 'epoch': 0.68}
{'loss': 1.5075, 'grad_norm': 0.08633080869913101, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5039043426513672, 'eval_runtime': 10.5632, 'eval_samples_per_second': 94.573, 'eval_steps_per_second': 5.964, 'epoch': 0.72}
{'loss': 1.5197, 'grad_norm': 0.10395555198192596, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5017155408859253, 'eval_runtime': 10.564, 'eval_samples_per_second': 94.567, 'eval_steps_per_second': 5.964, 'epoch': 0.76}
{'loss': 1.5078, 'grad_norm': 0.08176161348819733, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.498964786529541, 'eval_runtime': 10.5507, 'eval_samples_per_second': 94.685, 'eval_steps_per_second': 5.971, 'epoch': 0.8}
{'loss': 1.4679, 'grad_norm': 0.09629655629396439, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4977490901947021, 'eval_runtime': 10.4953, 'eval_samples_per_second': 95.185, 'eval_steps_per_second': 6.003, 'epoch': 0.84}
{'loss': 1.4852, 'grad_norm': 0.11228027194738388, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4962760210037231, 'eval_runtime': 10.4899, 'eval_samples_per_second': 95.234, 'eval_steps_per_second': 6.006, 'epoch': 0.88}
{'loss': 1.4814, 'grad_norm': 0.09336806833744049, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4956421852111816, 'eval_runtime': 10.4908, 'eval_samples_per_second': 95.226, 'eval_steps_per_second': 6.005, 'epoch': 0.92}
{'loss': 1.4914, 'grad_norm': 0.09787364304065704, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.494879126548767, 'eval_runtime': 10.5042, 'eval_samples_per_second': 95.105, 'eval_steps_per_second': 5.998, 'epoch': 0.96}
{'loss': 1.4982, 'grad_norm': 0.11218714714050293, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4948002099990845, 'eval_runtime': 10.5066, 'eval_samples_per_second': 95.083, 'eval_steps_per_second': 5.996, 'epoch': 1.0}
{'train_runtime': 541.1762, 'train_samples_per_second': 18.475, 'train_steps_per_second': 1.155, 'train_loss': 1.7006730407714843, 'epoch': 1.0}
train_results:  {'eval_loss': [3.583813190460205, 2.1474897861480713, 1.7413947582244873, 1.621720552444458, 1.5777090787887573, 1.560441017150879, 1.548940658569336, 1.5394608974456787, 1.5326406955718994, 1.5266938209533691, 1.5242009162902832, 1.5183357000350952, 1.5152966976165771, 1.5123112201690674, 1.5096739530563354, 1.5065224170684814, 1.5051186084747314, 1.5039043426513672, 1.5017155408859253, 1.498964786529541, 1.4977490901947021, 1.4962760210037231, 1.4956421852111816, 1.494879126548767, 1.4948002099990845], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:50,  3.62it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:09, 41.60it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 61.20it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 70.66it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 77.72it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 83.63it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 89.91it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 98.18it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 99.15it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 100.79it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 104.50it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 111.35it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 121.54it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 119.56it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 124.67it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 123.23it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 122.40it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 124.88it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 127.29it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 134.81it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 134.74it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 135.27it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 145.51it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 111.41it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  0.9564425945281982
current iteration best possible performance (full train run):  0.504
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0149 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.0132985701175747
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 1.0280229286304912
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 0.9623010933091745
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.9022435916577647
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.8761171230303209
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.6844, dtype=torch.float64), tensor(0.3156, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 1, 0, 1, 0, 3, 0.0, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(9.4557e-18, dtype=torch.float64), tensor(3.7760e-17, dtype=torch.float64), tensor(0.6844, dtype=torch.float64), tensor(0.3156, dtype=torch.float64), tensor(5.6274e-19, dtype=torch.float64), tensor(3.6794e-18, dtype=torch.float64), tensor(1.3291e-19, dtype=torch.float64), tensor(1.0822e-18, dtype=torch.float64), tensor(3.6280e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0210, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.684
  sciq: 0.316
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (3,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  3
lora dropout:  0.0
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,047,424 || all params: 8,033,308,672 || trainable%: 0.0379
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 57.25it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 93.14it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 104.42it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 112.36it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 117.37it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 129.81it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 131.35it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 140.76it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 140.64it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 142.06it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 143.77it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 150.54it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 154.20it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 155.07it/s]Running loglikelihood requests:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 249/400 [00:01<00:00, 173.64it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 156.75it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 172.62it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 175.64it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 189.66it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 198.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 157.50it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.2217, 'grad_norm': 1.5839169025421143, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.5062127113342285, 'eval_runtime': 10.3426, 'eval_samples_per_second': 96.59, 'eval_steps_per_second': 6.091, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 57.66it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 93.32it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 104.67it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 112.33it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 117.11it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 129.46it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 130.94it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 140.62it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 139.74it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 141.15it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 142.81it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 149.83it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 153.63it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 154.63it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 251/400 [00:01<00:00, 179.01it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 270/400 [00:01<00:00, 160.10it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 169.68it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 173.70it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 187.83it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 196.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 157.04it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.7379, 'grad_norm': 1.0705448389053345, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.1348519325256348, 'eval_runtime': 10.3907, 'eval_samples_per_second': 96.143, 'eval_steps_per_second': 6.063, 'epoch': 0.08}
{'loss': 1.8966, 'grad_norm': 0.6170288920402527, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8246535062789917, 'eval_runtime': 10.4152, 'eval_samples_per_second': 95.918, 'eval_steps_per_second': 6.049, 'epoch': 0.12}
{'loss': 1.7349, 'grad_norm': 0.4272424280643463, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7350236177444458, 'eval_runtime': 10.3865, 'eval_samples_per_second': 96.183, 'eval_steps_per_second': 6.066, 'epoch': 0.16}
{'loss': 1.7088, 'grad_norm': 0.31514737010002136, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7055860757827759, 'eval_runtime': 10.3761, 'eval_samples_per_second': 96.279, 'eval_steps_per_second': 6.072, 'epoch': 0.2}
{'loss': 1.6762, 'grad_norm': 0.3427780270576477, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6935981512069702, 'eval_runtime': 10.394, 'eval_samples_per_second': 96.113, 'eval_steps_per_second': 6.061, 'epoch': 0.24}
{'loss': 1.6626, 'grad_norm': 0.39170777797698975, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6828700304031372, 'eval_runtime': 10.3894, 'eval_samples_per_second': 96.155, 'eval_steps_per_second': 6.064, 'epoch': 0.28}
{'loss': 1.6361, 'grad_norm': 0.40113362669944763, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.676031470298767, 'eval_runtime': 10.384, 'eval_samples_per_second': 96.206, 'eval_steps_per_second': 6.067, 'epoch': 0.32}
{'loss': 1.6334, 'grad_norm': 0.37702691555023193, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.670292854309082, 'eval_runtime': 10.3813, 'eval_samples_per_second': 96.231, 'eval_steps_per_second': 6.069, 'epoch': 0.36}
{'loss': 1.6488, 'grad_norm': 0.35115790367126465, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6646400690078735, 'eval_runtime': 10.4092, 'eval_samples_per_second': 95.973, 'eval_steps_per_second': 6.052, 'epoch': 0.4}
{'loss': 1.631, 'grad_norm': 0.36654049158096313, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6597776412963867, 'eval_runtime': 10.4681, 'eval_samples_per_second': 95.433, 'eval_steps_per_second': 6.018, 'epoch': 0.44}
{'loss': 1.6031, 'grad_norm': 0.3827177584171295, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6563242673873901, 'eval_runtime': 10.4536, 'eval_samples_per_second': 95.565, 'eval_steps_per_second': 6.027, 'epoch': 0.48}
{'loss': 1.6635, 'grad_norm': 0.3996039927005768, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6528675556182861, 'eval_runtime': 10.454, 'eval_samples_per_second': 95.562, 'eval_steps_per_second': 6.026, 'epoch': 0.52}
{'loss': 1.6447, 'grad_norm': 0.3372054100036621, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6508527994155884, 'eval_runtime': 10.4574, 'eval_samples_per_second': 95.531, 'eval_steps_per_second': 6.024, 'epoch': 0.56}
{'loss': 1.6238, 'grad_norm': 0.39213234186172485, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6480584144592285, 'eval_runtime': 10.4655, 'eval_samples_per_second': 95.457, 'eval_steps_per_second': 6.02, 'epoch': 0.6}
{'loss': 1.6399, 'grad_norm': 0.3749798834323883, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6454508304595947, 'eval_runtime': 10.4354, 'eval_samples_per_second': 95.732, 'eval_steps_per_second': 6.037, 'epoch': 0.64}
{'loss': 1.642, 'grad_norm': 0.369080513715744, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6438300609588623, 'eval_runtime': 10.3797, 'eval_samples_per_second': 96.246, 'eval_steps_per_second': 6.07, 'epoch': 0.68}
{'loss': 1.6201, 'grad_norm': 0.36093440651893616, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.641353726387024, 'eval_runtime': 10.4025, 'eval_samples_per_second': 96.035, 'eval_steps_per_second': 6.056, 'epoch': 0.72}
{'loss': 1.6117, 'grad_norm': 0.34636226296424866, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6402442455291748, 'eval_runtime': 10.3757, 'eval_samples_per_second': 96.283, 'eval_steps_per_second': 6.072, 'epoch': 0.76}
{'loss': 1.6612, 'grad_norm': 0.3747451901435852, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6386311054229736, 'eval_runtime': 10.3672, 'eval_samples_per_second': 96.361, 'eval_steps_per_second': 6.077, 'epoch': 0.8}
{'loss': 1.6265, 'grad_norm': 0.42522814869880676, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.636907696723938, 'eval_runtime': 10.3678, 'eval_samples_per_second': 96.356, 'eval_steps_per_second': 6.076, 'epoch': 0.84}
{'loss': 1.6322, 'grad_norm': 0.36929890513420105, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.635878324508667, 'eval_runtime': 10.4398, 'eval_samples_per_second': 95.691, 'eval_steps_per_second': 6.035, 'epoch': 0.88}
{'loss': 1.618, 'grad_norm': 0.3887973427772522, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6351145505905151, 'eval_runtime': 10.4555, 'eval_samples_per_second': 95.548, 'eval_steps_per_second': 6.026, 'epoch': 0.92}
{'loss': 1.6566, 'grad_norm': 0.36489245295524597, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6344181299209595, 'eval_runtime': 10.5252, 'eval_samples_per_second': 94.915, 'eval_steps_per_second': 5.986, 'epoch': 0.96}
{'loss': 1.6235, 'grad_norm': 0.34984371066093445, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6342118978500366, 'eval_runtime': 10.521, 'eval_samples_per_second': 94.953, 'eval_steps_per_second': 5.988, 'epoch': 1.0}
{'train_runtime': 533.4494, 'train_samples_per_second': 18.744, 'train_steps_per_second': 1.172, 'train_loss': 1.802193035888672, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5062127113342285, 2.1348519325256348, 1.8246535062789917, 1.7350236177444458, 1.7055860757827759, 1.6935981512069702, 1.6828700304031372, 1.676031470298767, 1.670292854309082, 1.6646400690078735, 1.6597776412963867, 1.6563242673873901, 1.6528675556182861, 1.6508527994155884, 1.6480584144592285, 1.6454508304595947, 1.6438300609588623, 1.641353726387024, 1.6402442455291748, 1.6386311054229736, 1.636907696723938, 1.635878324508667, 1.6351145505905151, 1.6344181299209595, 1.6342118978500366], 'performance': [0.48, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:56,  3.41it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:09, 42.31it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 61.96it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.32it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 78.26it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 83.98it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 90.53it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 99.05it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 100.60it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 102.16it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 105.48it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 112.63it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 123.00it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 121.12it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 126.32it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 124.88it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 124.30it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 126.66it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 129.25it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 136.62it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 136.36it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 136.39it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 146.66it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 112.45it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  0.6223999857902527
current iteration best possible performance (full train run):  0.462
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6229 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.8575475045056601
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 0.8507275514207768
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.8408406967433348
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 0.8535169263234477
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 0.8443555350920002
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4348, dtype=torch.float64), tensor(0.5652, dtype=torch.float64), 0, 0, 0, 0, 0, 3, 1, 1, 0, 1, 0, 59, 0.010144099497573148, 1.4800000190734877, 0]
normalized proposed parameters for next round by BO: [tensor(3.5136e-18, dtype=torch.float64), tensor(1.3812e-18, dtype=torch.float64), tensor(0.4348, dtype=torch.float64), tensor(0.5652, dtype=torch.float64), tensor(9.9589e-18, dtype=torch.float64), tensor(5.5463e-18, dtype=torch.float64), tensor(6.5364e-18, dtype=torch.float64), tensor(1.6983e-17, dtype=torch.float64), tensor(1.2003e-17, dtype=torch.float64), tensor(0.0951, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4635, dtype=torch.float64), tensor(0.1014, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.435
  sciq: 0.565
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (59,)
  lora_dropout: (0.010144099497573148,)
  num_layers_to_apply: (3,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734877,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  3
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  59
lora dropout:  0.010144099497573148
lora alpha:  1.4800000190734877
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 5,618,688 || all params: 8,035,879,936 || trainable%: 0.0699
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 67.69it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 109.93it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 123.85it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 133.29it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 139.14it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 153.30it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 155.24it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 167.07it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 169.29it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 172.23it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 182.64it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 185.38it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 190.44it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 198.67it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 211.37it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 220.72it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 237.29it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 188.83it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.6276, 'grad_norm': 0.15556974709033966, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 4.465718746185303, 'eval_runtime': 8.9121, 'eval_samples_per_second': 112.095, 'eval_steps_per_second': 7.069, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.24it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 110.20it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 123.97it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 133.34it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 139.09it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 154.11it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 155.43it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 167.16it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 169.11it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 172.02it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 182.51it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 185.14it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 190.89it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 198.27it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 211.09it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 220.42it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 237.00it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 188.65it/s]
Evaluation performance at step 50: 0.5
{'loss': 3.7829, 'grad_norm': 0.3041219115257263, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 3.1169843673706055, 'eval_runtime': 8.8755, 'eval_samples_per_second': 112.557, 'eval_steps_per_second': 7.098, 'epoch': 0.08}
{'loss': 2.7633, 'grad_norm': 0.17776983976364136, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.460969924926758, 'eval_runtime': 8.9191, 'eval_samples_per_second': 112.006, 'eval_steps_per_second': 7.063, 'epoch': 0.12}
{'loss': 2.3022, 'grad_norm': 0.09988448768854141, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1371278762817383, 'eval_runtime': 8.9484, 'eval_samples_per_second': 111.64, 'eval_steps_per_second': 7.04, 'epoch': 0.16}
{'loss': 2.0041, 'grad_norm': 0.08017803728580475, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9715090990066528, 'eval_runtime': 8.9638, 'eval_samples_per_second': 111.448, 'eval_steps_per_second': 7.028, 'epoch': 0.2}
{'loss': 1.9074, 'grad_norm': 0.07901135087013245, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8875210285186768, 'eval_runtime': 8.9961, 'eval_samples_per_second': 111.048, 'eval_steps_per_second': 7.003, 'epoch': 0.24}
{'loss': 1.8307, 'grad_norm': 0.07829263806343079, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8389570713043213, 'eval_runtime': 9.0052, 'eval_samples_per_second': 110.936, 'eval_steps_per_second': 6.996, 'epoch': 0.28}
{'loss': 1.8176, 'grad_norm': 0.09799662977457047, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.808983325958252, 'eval_runtime': 9.0014, 'eval_samples_per_second': 110.983, 'eval_steps_per_second': 6.999, 'epoch': 0.32}
{'loss': 1.8152, 'grad_norm': 0.07782979309558868, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7900390625, 'eval_runtime': 9.0144, 'eval_samples_per_second': 110.823, 'eval_steps_per_second': 6.989, 'epoch': 0.36}
{'loss': 1.7563, 'grad_norm': 0.1004967913031578, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7751750946044922, 'eval_runtime': 9.0107, 'eval_samples_per_second': 110.868, 'eval_steps_per_second': 6.992, 'epoch': 0.4}
{'loss': 1.7978, 'grad_norm': 0.0724838450551033, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7626994848251343, 'eval_runtime': 9.0161, 'eval_samples_per_second': 110.802, 'eval_steps_per_second': 6.988, 'epoch': 0.44}
{'loss': 1.7208, 'grad_norm': 0.08496644347906113, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7550793886184692, 'eval_runtime': 9.0158, 'eval_samples_per_second': 110.805, 'eval_steps_per_second': 6.988, 'epoch': 0.48}
{'loss': 1.7528, 'grad_norm': 0.08290965110063553, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.747370958328247, 'eval_runtime': 9.012, 'eval_samples_per_second': 110.853, 'eval_steps_per_second': 6.991, 'epoch': 0.52}
{'loss': 1.7607, 'grad_norm': 0.08324450254440308, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7403055429458618, 'eval_runtime': 9.0224, 'eval_samples_per_second': 110.724, 'eval_steps_per_second': 6.983, 'epoch': 0.56}
{'loss': 1.7437, 'grad_norm': 0.08501724898815155, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7356152534484863, 'eval_runtime': 9.0577, 'eval_samples_per_second': 110.293, 'eval_steps_per_second': 6.955, 'epoch': 0.6}
{'loss': 1.751, 'grad_norm': 0.07661356776952744, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7317146062850952, 'eval_runtime': 9.0517, 'eval_samples_per_second': 110.366, 'eval_steps_per_second': 6.96, 'epoch': 0.64}
{'loss': 1.7199, 'grad_norm': 0.10719947516918182, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7283748388290405, 'eval_runtime': 9.0576, 'eval_samples_per_second': 110.294, 'eval_steps_per_second': 6.955, 'epoch': 0.68}
{'loss': 1.7366, 'grad_norm': 0.09570653736591339, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7251594066619873, 'eval_runtime': 9.0563, 'eval_samples_per_second': 110.31, 'eval_steps_per_second': 6.956, 'epoch': 0.72}
{'loss': 1.774, 'grad_norm': 0.10261281579732895, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.722269892692566, 'eval_runtime': 9.0138, 'eval_samples_per_second': 110.831, 'eval_steps_per_second': 6.989, 'epoch': 0.76}
{'loss': 1.7266, 'grad_norm': 0.11231914162635803, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7203854322433472, 'eval_runtime': 9.0153, 'eval_samples_per_second': 110.811, 'eval_steps_per_second': 6.988, 'epoch': 0.8}
{'loss': 1.7556, 'grad_norm': 0.08052639663219452, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.718526840209961, 'eval_runtime': 8.9982, 'eval_samples_per_second': 111.023, 'eval_steps_per_second': 7.001, 'epoch': 0.84}
{'loss': 1.7308, 'grad_norm': 0.08743990957736969, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7173024415969849, 'eval_runtime': 9.0046, 'eval_samples_per_second': 110.943, 'eval_steps_per_second': 6.996, 'epoch': 0.88}
{'loss': 1.7061, 'grad_norm': 0.09220568090677261, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7163760662078857, 'eval_runtime': 9.0116, 'eval_samples_per_second': 110.857, 'eval_steps_per_second': 6.991, 'epoch': 0.92}
{'loss': 1.7627, 'grad_norm': 0.07478488981723785, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7156617641448975, 'eval_runtime': 9.0841, 'eval_samples_per_second': 109.972, 'eval_steps_per_second': 6.935, 'epoch': 0.96}
{'loss': 1.7651, 'grad_norm': 0.0811576172709465, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7154078483581543, 'eval_runtime': 9.0766, 'eval_samples_per_second': 110.063, 'eval_steps_per_second': 6.941, 'epoch': 1.0}
{'train_runtime': 369.4116, 'train_samples_per_second': 27.067, 'train_steps_per_second': 1.692, 'train_loss': 2.032465539550781, 'epoch': 1.0}
train_results:  {'eval_loss': [4.465718746185303, 3.1169843673706055, 2.460969924926758, 2.1371278762817383, 1.9715090990066528, 1.8875210285186768, 1.8389570713043213, 1.808983325958252, 1.7900390625, 1.7751750946044922, 1.7626994848251343, 1.7550793886184692, 1.747370958328247, 1.7403055429458618, 1.7356152534484863, 1.7317146062850952, 1.7283748388290405, 1.7251594066619873, 1.722269892692566, 1.7203854322433472, 1.718526840209961, 1.7173024415969849, 1.7163760662078857, 1.7156617641448975, 1.7154078483581543], 'performance': [0.48, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:11,  5.61it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:12, 30.27it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 52.38it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 66.57it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 77.66it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 87.37it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 97.22it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 108.94it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 112.29it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 114.92it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 119.91it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 128.57it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 140.77it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 138.66it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 144.79it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 143.73it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 143.24it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 145.84it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 148.62it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 157.08it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 156.79it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 157.50it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 192.07it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 122.35it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  0.8614088296890259
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4654 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 0.7846082562526097
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.8333000493705482
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 0.8312491296369491
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.8032760246658561
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.6792561473109855
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4999, dtype=torch.float64), 0, 0, tensor(0.5001, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 1, 0, 128, 0.09999999999999985, 1.4800000190735152, 0]
normalized proposed parameters for next round by BO: [tensor(8.1744e-18, dtype=torch.float64), tensor(3.1782e-18, dtype=torch.float64), tensor(0.4999, dtype=torch.float64), tensor(2.5474e-18, dtype=torch.float64), tensor(1.7019e-18, dtype=torch.float64), tensor(0.5001, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.5
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.5
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999985,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190735152,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.09999999999999985
lora alpha:  1.4800000190735152
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.75it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.83it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 100.00it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 107.13it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 112.86it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.00it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.24it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.98it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.61it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.80it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.20it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.56it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.52it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 149.19it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 153.33it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 153.11it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 162.16it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 172.92it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 176.89it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 187.43it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 189.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 150.89it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.3004, 'grad_norm': 0.35395607352256775, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.5955207347869873, 'eval_runtime': 9.9389, 'eval_samples_per_second': 100.515, 'eval_steps_per_second': 6.339, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 59.95it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 96.83it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 108.74it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 116.70it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 121.47it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 134.73it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 135.98it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 146.35it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 146.60it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 148.18it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 149.95it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 157.08it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 160.77it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 162.28it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 166.86it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 173.63it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 184.47it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 191.84it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 205.54it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 211.36it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 163.87it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.725, 'grad_norm': 0.22540588676929474, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.1543548107147217, 'eval_runtime': 9.9352, 'eval_samples_per_second': 100.552, 'eval_steps_per_second': 6.341, 'epoch': 0.08}
{'loss': 1.8938, 'grad_norm': 0.05948012322187424, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7550501823425293, 'eval_runtime': 9.9161, 'eval_samples_per_second': 100.745, 'eval_steps_per_second': 6.353, 'epoch': 0.12}
{'loss': 1.6483, 'grad_norm': 0.08543717116117477, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6353733539581299, 'eval_runtime': 10.0056, 'eval_samples_per_second': 99.844, 'eval_steps_per_second': 6.296, 'epoch': 0.16}
{'loss': 1.5906, 'grad_norm': 0.07522634416818619, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5952072143554688, 'eval_runtime': 10.0228, 'eval_samples_per_second': 99.673, 'eval_steps_per_second': 6.286, 'epoch': 0.2}
{'loss': 1.5926, 'grad_norm': 0.05425019562244415, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5778337717056274, 'eval_runtime': 10.0526, 'eval_samples_per_second': 99.377, 'eval_steps_per_second': 6.267, 'epoch': 0.24}
{'loss': 1.5298, 'grad_norm': 0.06577318906784058, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5610662698745728, 'eval_runtime': 10.0739, 'eval_samples_per_second': 99.167, 'eval_steps_per_second': 6.254, 'epoch': 0.28}
{'loss': 1.5556, 'grad_norm': 0.07145076990127563, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5515363216400146, 'eval_runtime': 10.0487, 'eval_samples_per_second': 99.416, 'eval_steps_per_second': 6.269, 'epoch': 0.32}
{'loss': 1.553, 'grad_norm': 0.056125544011592865, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.549149751663208, 'eval_runtime': 10.0597, 'eval_samples_per_second': 99.307, 'eval_steps_per_second': 6.263, 'epoch': 0.36}
{'loss': 1.546, 'grad_norm': 0.07090874016284943, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5384745597839355, 'eval_runtime': 10.0712, 'eval_samples_per_second': 99.194, 'eval_steps_per_second': 6.255, 'epoch': 0.4}
{'loss': 1.5408, 'grad_norm': 0.06283362954854965, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.532047152519226, 'eval_runtime': 10.0788, 'eval_samples_per_second': 99.119, 'eval_steps_per_second': 6.251, 'epoch': 0.44}
{'loss': 1.4839, 'grad_norm': 0.06525061279535294, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5269893407821655, 'eval_runtime': 10.0888, 'eval_samples_per_second': 99.021, 'eval_steps_per_second': 6.245, 'epoch': 0.48}
{'loss': 1.51, 'grad_norm': 0.06103156507015228, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5240482091903687, 'eval_runtime': 10.0848, 'eval_samples_per_second': 99.06, 'eval_steps_per_second': 6.247, 'epoch': 0.52}
{'loss': 1.5165, 'grad_norm': 0.056139230728149414, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5197162628173828, 'eval_runtime': 10.0646, 'eval_samples_per_second': 99.259, 'eval_steps_per_second': 6.26, 'epoch': 0.56}
{'loss': 1.5122, 'grad_norm': 0.060142748057842255, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5151164531707764, 'eval_runtime': 10.0161, 'eval_samples_per_second': 99.739, 'eval_steps_per_second': 6.29, 'epoch': 0.6}
{'loss': 1.5472, 'grad_norm': 0.06566794216632843, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5118318796157837, 'eval_runtime': 10.0447, 'eval_samples_per_second': 99.456, 'eval_steps_per_second': 6.272, 'epoch': 0.64}
{'loss': 1.4928, 'grad_norm': 0.0751759484410286, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5089393854141235, 'eval_runtime': 10.0517, 'eval_samples_per_second': 99.387, 'eval_steps_per_second': 6.268, 'epoch': 0.68}
{'loss': 1.5045, 'grad_norm': 0.07968038320541382, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5059808492660522, 'eval_runtime': 10.1566, 'eval_samples_per_second': 98.36, 'eval_steps_per_second': 6.203, 'epoch': 0.72}
{'loss': 1.4823, 'grad_norm': 0.07645292580127716, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.50411057472229, 'eval_runtime': 10.1285, 'eval_samples_per_second': 98.633, 'eval_steps_per_second': 6.22, 'epoch': 0.76}
{'loss': 1.476, 'grad_norm': 0.06883657723665237, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5010899305343628, 'eval_runtime': 10.087, 'eval_samples_per_second': 99.038, 'eval_steps_per_second': 6.246, 'epoch': 0.8}
{'loss': 1.5104, 'grad_norm': 0.06800194829702377, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4993300437927246, 'eval_runtime': 10.0765, 'eval_samples_per_second': 99.142, 'eval_steps_per_second': 6.252, 'epoch': 0.84}
{'loss': 1.5005, 'grad_norm': 0.07053306698799133, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4979870319366455, 'eval_runtime': 10.0725, 'eval_samples_per_second': 99.181, 'eval_steps_per_second': 6.255, 'epoch': 0.88}
{'loss': 1.505, 'grad_norm': 0.07192757725715637, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.497370958328247, 'eval_runtime': 10.0826, 'eval_samples_per_second': 99.081, 'eval_steps_per_second': 6.248, 'epoch': 0.92}
{'loss': 1.5171, 'grad_norm': 0.07244180887937546, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4963018894195557, 'eval_runtime': 10.0804, 'eval_samples_per_second': 99.103, 'eval_steps_per_second': 6.25, 'epoch': 0.96}
{'loss': 1.502, 'grad_norm': 0.07631655037403107, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4958624839782715, 'eval_runtime': 10.0872, 'eval_samples_per_second': 99.036, 'eval_steps_per_second': 6.246, 'epoch': 1.0}
{'train_runtime': 527.017, 'train_samples_per_second': 18.973, 'train_steps_per_second': 1.186, 'train_loss': 1.7014553588867187, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5955207347869873, 2.1543548107147217, 1.7550501823425293, 1.6353733539581299, 1.5952072143554688, 1.5778337717056274, 1.5610662698745728, 1.5515363216400146, 1.549149751663208, 1.5384745597839355, 1.532047152519226, 1.5269893407821655, 1.5240482091903687, 1.5197162628173828, 1.5151164531707764, 1.5118318796157837, 1.5089393854141235, 1.5059808492660522, 1.50411057472229, 1.5010899305343628, 1.4993300437927246, 1.4979870319366455, 1.497370958328247, 1.4963018894195557, 1.4958624839782715], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<10:19,  1.55s/it]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:33, 11.48it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:02<00:15, 23.70it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:02<00:09, 35.17it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:02<00:07, 46.49it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:02<00:05, 57.52it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:04, 68.94it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 81.73it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:03<00:03, 87.79it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:03<00:02, 93.10it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:03<00:02, 99.36it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:03<00:01, 108.51it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:03<00:01, 119.91it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 119.53it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 125.31it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:04<00:01, 123.98it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:04<00:01, 124.38it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:04<00:00, 127.26it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:04<00:00, 129.97it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:04<00:00, 137.30it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 137.74it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 138.46it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 148.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 80.75it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4104418754577637
current iteration best possible performance (full train run):  0.42000000000000004
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0486 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.2476490136513583
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 1.2365623236541134
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 1.0607445013031211
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 1.1782537880704518
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.2115613597686894
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4128, dtype=torch.float64), 0, 0, tensor(0.5872, dtype=torch.float64), 0, 0, 0, 1, 1, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(3.9634e-16, dtype=torch.float64), tensor(1.1161e-16, dtype=torch.float64), tensor(0.4128, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5872, dtype=torch.float64), tensor(7.1266e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.413
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.587
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.68it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 112.29it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 126.11it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 135.49it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.70it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñé       | 94/400 [00:00<00:01, 160.32it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 158.74it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 170.23it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 173.31it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 181.16it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 185.43it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 191.55it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 194.40it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 210.55it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 214.74it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 231.35it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 245.11it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 192.71it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.6144, 'grad_norm': 0.07790440320968628, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 4.533422470092773, 'eval_runtime': 8.7465, 'eval_samples_per_second': 114.217, 'eval_steps_per_second': 7.203, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.06it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.43it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.00it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 135.05it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.09it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.33it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 159.05it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 170.47it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 173.45it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 181.04it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 185.32it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 190.88it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 193.71it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 209.75it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 213.97it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 230.89it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 243.68it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.94it/s]
Evaluation performance at step 50: 0.49
{'loss': 4.1502, 'grad_norm': 0.22844856977462769, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 3.423790454864502, 'eval_runtime': 8.7948, 'eval_samples_per_second': 113.59, 'eval_steps_per_second': 7.163, 'epoch': 0.08}
{'loss': 3.0308, 'grad_norm': 0.10614630579948425, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.770817518234253, 'eval_runtime': 8.7881, 'eval_samples_per_second': 113.677, 'eval_steps_per_second': 7.169, 'epoch': 0.12}
{'loss': 2.5795, 'grad_norm': 0.09000502526760101, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.440720558166504, 'eval_runtime': 8.8269, 'eval_samples_per_second': 113.176, 'eval_steps_per_second': 7.137, 'epoch': 0.16}
{'loss': 2.2946, 'grad_norm': 0.08475373685359955, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.2475998401641846, 'eval_runtime': 8.8361, 'eval_samples_per_second': 113.059, 'eval_steps_per_second': 7.13, 'epoch': 0.2}
{'loss': 2.1476, 'grad_norm': 0.06474076956510544, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1343705654144287, 'eval_runtime': 8.85, 'eval_samples_per_second': 112.881, 'eval_steps_per_second': 7.119, 'epoch': 0.24}
{'loss': 2.0205, 'grad_norm': 0.07094328105449677, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.055417776107788, 'eval_runtime': 8.8729, 'eval_samples_per_second': 112.589, 'eval_steps_per_second': 7.1, 'epoch': 0.28}
{'loss': 2.0034, 'grad_norm': 0.07440675795078278, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9944312572479248, 'eval_runtime': 8.8844, 'eval_samples_per_second': 112.444, 'eval_steps_per_second': 7.091, 'epoch': 0.32}
{'loss': 1.9301, 'grad_norm': 0.06884496659040451, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9466668367385864, 'eval_runtime': 8.921, 'eval_samples_per_second': 111.983, 'eval_steps_per_second': 7.062, 'epoch': 0.36}
{'loss': 1.8937, 'grad_norm': 0.07216516882181168, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9103829860687256, 'eval_runtime': 8.9185, 'eval_samples_per_second': 112.014, 'eval_steps_per_second': 7.064, 'epoch': 0.4}
{'loss': 1.9293, 'grad_norm': 0.09334611147642136, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8832746744155884, 'eval_runtime': 8.9042, 'eval_samples_per_second': 112.194, 'eval_steps_per_second': 7.075, 'epoch': 0.44}
{'loss': 1.8465, 'grad_norm': 0.05408233404159546, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8626224994659424, 'eval_runtime': 8.9109, 'eval_samples_per_second': 112.109, 'eval_steps_per_second': 7.07, 'epoch': 0.48}
{'loss': 1.8486, 'grad_norm': 0.06593739986419678, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8471260070800781, 'eval_runtime': 8.897, 'eval_samples_per_second': 112.285, 'eval_steps_per_second': 7.081, 'epoch': 0.52}
{'loss': 1.8194, 'grad_norm': 0.06594505906105042, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8335539102554321, 'eval_runtime': 8.916, 'eval_samples_per_second': 112.045, 'eval_steps_per_second': 7.066, 'epoch': 0.56}
{'loss': 1.822, 'grad_norm': 0.0700901672244072, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.823791265487671, 'eval_runtime': 8.95, 'eval_samples_per_second': 111.62, 'eval_steps_per_second': 7.039, 'epoch': 0.6}
{'loss': 1.8483, 'grad_norm': 0.05608438700437546, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8150733709335327, 'eval_runtime': 8.9114, 'eval_samples_per_second': 112.104, 'eval_steps_per_second': 7.07, 'epoch': 0.64}
{'loss': 1.7988, 'grad_norm': 0.07705243676900864, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8082334995269775, 'eval_runtime': 8.9058, 'eval_samples_per_second': 112.174, 'eval_steps_per_second': 7.074, 'epoch': 0.68}
{'loss': 1.8047, 'grad_norm': 0.06927408277988434, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8018312454223633, 'eval_runtime': 8.9063, 'eval_samples_per_second': 112.168, 'eval_steps_per_second': 7.074, 'epoch': 0.72}
{'loss': 1.8144, 'grad_norm': 0.05529762804508209, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.796830177307129, 'eval_runtime': 8.8858, 'eval_samples_per_second': 112.426, 'eval_steps_per_second': 7.09, 'epoch': 0.76}
{'loss': 1.8052, 'grad_norm': 0.07402078062295914, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7929387092590332, 'eval_runtime': 8.8922, 'eval_samples_per_second': 112.346, 'eval_steps_per_second': 7.085, 'epoch': 0.8}
{'loss': 1.7964, 'grad_norm': 0.05721611529588699, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7897446155548096, 'eval_runtime': 8.8975, 'eval_samples_per_second': 112.279, 'eval_steps_per_second': 7.081, 'epoch': 0.84}
{'loss': 1.7687, 'grad_norm': 0.0821717381477356, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7875231504440308, 'eval_runtime': 8.9023, 'eval_samples_per_second': 112.218, 'eval_steps_per_second': 7.077, 'epoch': 0.88}
{'loss': 1.7595, 'grad_norm': 0.06498789042234421, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7861992120742798, 'eval_runtime': 8.8904, 'eval_samples_per_second': 112.368, 'eval_steps_per_second': 7.086, 'epoch': 0.92}
{'loss': 1.8365, 'grad_norm': 0.06937949359416962, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.784645438194275, 'eval_runtime': 8.8843, 'eval_samples_per_second': 112.446, 'eval_steps_per_second': 7.091, 'epoch': 0.96}
{'loss': 1.7937, 'grad_norm': 0.10517840087413788, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7843714952468872, 'eval_runtime': 8.8831, 'eval_samples_per_second': 112.46, 'eval_steps_per_second': 7.092, 'epoch': 1.0}
{'train_runtime': 357.4601, 'train_samples_per_second': 27.972, 'train_steps_per_second': 1.748, 'train_loss': 2.158281756591797, 'epoch': 1.0}
train_results:  {'eval_loss': [4.533422470092773, 3.423790454864502, 2.770817518234253, 2.440720558166504, 2.2475998401641846, 2.1343705654144287, 2.055417776107788, 1.9944312572479248, 1.9466668367385864, 1.9103829860687256, 1.8832746744155884, 1.8626224994659424, 1.8471260070800781, 1.8335539102554321, 1.823791265487671, 1.8150733709335327, 1.8082334995269775, 1.8018312454223633, 1.796830177307129, 1.7929387092590332, 1.7897446155548096, 1.7875231504440308, 1.7861992120742798, 1.784645438194275, 1.7843714952468872], 'performance': [0.49, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:53,  2.30it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 28.27it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 50.18it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 64.88it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 76.49it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 87.01it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 97.47it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 109.44it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 112.76it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 116.30it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 120.98it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 130.23it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 143.11it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 141.38it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 147.58it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 146.32it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 145.77it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 148.70it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 150.99it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 160.04it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 344/400 [00:03<00:00, 162.63it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 377/400 [00:03<00:00, 210.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 399/400 [00:03<00:00, 194.25it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 119.86it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.2963961362838745
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9974 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.2761171144395167
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 1.1844237110583542
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.411971398786466
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 1.0070806623539044
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.230574937476116
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5331, dtype=torch.float64), 0, 0, tensor(0.4669, dtype=torch.float64), 0, 0, 0, 1, 1, 1, 0, 1, 0, 128, 0.0999999999999997, 47.99999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(7.9335e-18, dtype=torch.float64), tensor(0.5331, dtype=torch.float64), tensor(1.6861e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4669, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.7623e-16, dtype=torch.float64), tensor(4.4576e-18, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.533
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.467
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0999999999999997,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (47.99999999999986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0999999999999997
lora alpha:  47.99999999999986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.49it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 112.33it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.98it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 135.04it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.25it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 95/400 [00:00<00:01, 163.15it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 158.27it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 170.31it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 173.37it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 181.33it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 186.52it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 192.21it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 195.03it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 210.95it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 214.85it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 231.41it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 244.83it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 192.90it/s]
Evaluation performance at step 25: 0.51
{'loss': 4.2298, 'grad_norm': 1.1243113279342651, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.51}
{'eval_loss': 3.314324140548706, 'eval_runtime': 8.8463, 'eval_samples_per_second': 112.929, 'eval_steps_per_second': 7.122, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.59it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.80it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.07it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.59it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.41it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 154.93it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 156.21it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 167.91it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 170.04it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.11it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.78it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 186.35it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 192.08it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 200.68it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 213.54it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 223.33it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 240.20it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 190.58it/s]
Evaluation performance at step 50: 0.49
{'loss': 2.6859, 'grad_norm': 0.6478283405303955, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 2.252399444580078, 'eval_runtime': 8.8442, 'eval_samples_per_second': 112.955, 'eval_steps_per_second': 7.123, 'epoch': 0.08}
{'loss': 2.0512, 'grad_norm': 0.3359381854534149, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9554376602172852, 'eval_runtime': 8.831, 'eval_samples_per_second': 113.124, 'eval_steps_per_second': 7.134, 'epoch': 0.12}
{'loss': 1.9034, 'grad_norm': 0.29297909140586853, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.876733660697937, 'eval_runtime': 8.885, 'eval_samples_per_second': 112.437, 'eval_steps_per_second': 7.091, 'epoch': 0.16}
{'loss': 1.7967, 'grad_norm': 0.3385082185268402, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8371880054473877, 'eval_runtime': 8.9305, 'eval_samples_per_second': 111.863, 'eval_steps_per_second': 7.054, 'epoch': 0.2}
{'loss': 1.8167, 'grad_norm': 0.368143230676651, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.810331106185913, 'eval_runtime': 8.9521, 'eval_samples_per_second': 111.594, 'eval_steps_per_second': 7.037, 'epoch': 0.24}
{'loss': 1.8141, 'grad_norm': 0.3885432481765747, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.794681429862976, 'eval_runtime': 8.9507, 'eval_samples_per_second': 111.611, 'eval_steps_per_second': 7.039, 'epoch': 0.28}
{'loss': 1.7755, 'grad_norm': 0.3208059072494507, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7809364795684814, 'eval_runtime': 8.9591, 'eval_samples_per_second': 111.507, 'eval_steps_per_second': 7.032, 'epoch': 0.32}
{'loss': 1.7926, 'grad_norm': 0.35669511556625366, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.772722840309143, 'eval_runtime': 8.9651, 'eval_samples_per_second': 111.432, 'eval_steps_per_second': 7.027, 'epoch': 0.36}
{'loss': 1.7247, 'grad_norm': 0.36168816685676575, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7591902017593384, 'eval_runtime': 8.9669, 'eval_samples_per_second': 111.41, 'eval_steps_per_second': 7.026, 'epoch': 0.4}
{'loss': 1.7588, 'grad_norm': 0.32627609372138977, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7539020776748657, 'eval_runtime': 8.9734, 'eval_samples_per_second': 111.329, 'eval_steps_per_second': 7.021, 'epoch': 0.44}
{'loss': 1.7808, 'grad_norm': 0.39746934175491333, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.748231053352356, 'eval_runtime': 9.0121, 'eval_samples_per_second': 110.851, 'eval_steps_per_second': 6.991, 'epoch': 0.48}
{'loss': 1.7713, 'grad_norm': 0.402763694524765, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.741839051246643, 'eval_runtime': 8.9834, 'eval_samples_per_second': 111.205, 'eval_steps_per_second': 7.013, 'epoch': 0.52}
{'loss': 1.7559, 'grad_norm': 0.3537127673625946, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7365248203277588, 'eval_runtime': 8.9731, 'eval_samples_per_second': 111.332, 'eval_steps_per_second': 7.021, 'epoch': 0.56}
{'loss': 1.7705, 'grad_norm': 0.35770493745803833, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.731881022453308, 'eval_runtime': 8.9552, 'eval_samples_per_second': 111.556, 'eval_steps_per_second': 7.035, 'epoch': 0.6}
{'loss': 1.7686, 'grad_norm': 0.35364824533462524, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7264227867126465, 'eval_runtime': 8.9176, 'eval_samples_per_second': 112.026, 'eval_steps_per_second': 7.065, 'epoch': 0.64}
{'loss': 1.7276, 'grad_norm': 0.36408868432044983, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7243905067443848, 'eval_runtime': 8.9265, 'eval_samples_per_second': 111.913, 'eval_steps_per_second': 7.058, 'epoch': 0.68}
{'loss': 1.7428, 'grad_norm': 0.35732755064964294, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7213345766067505, 'eval_runtime': 8.9217, 'eval_samples_per_second': 111.974, 'eval_steps_per_second': 7.061, 'epoch': 0.72}
{'loss': 1.7081, 'grad_norm': 0.32150182127952576, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7176547050476074, 'eval_runtime': 8.9512, 'eval_samples_per_second': 111.606, 'eval_steps_per_second': 7.038, 'epoch': 0.76}
{'loss': 1.7193, 'grad_norm': 0.39867356419563293, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.714989185333252, 'eval_runtime': 8.9577, 'eval_samples_per_second': 111.524, 'eval_steps_per_second': 7.033, 'epoch': 0.8}
{'loss': 1.7088, 'grad_norm': 0.34935489296913147, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7131952047348022, 'eval_runtime': 8.9583, 'eval_samples_per_second': 111.517, 'eval_steps_per_second': 7.033, 'epoch': 0.84}
{'loss': 1.7286, 'grad_norm': 0.33632999658584595, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7108408212661743, 'eval_runtime': 8.9568, 'eval_samples_per_second': 111.535, 'eval_steps_per_second': 7.034, 'epoch': 0.88}
{'loss': 1.722, 'grad_norm': 0.3285905718803406, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7100727558135986, 'eval_runtime': 8.9397, 'eval_samples_per_second': 111.748, 'eval_steps_per_second': 7.047, 'epoch': 0.92}
{'loss': 1.7891, 'grad_norm': 0.34378498792648315, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7090234756469727, 'eval_runtime': 8.9819, 'eval_samples_per_second': 111.224, 'eval_steps_per_second': 7.014, 'epoch': 0.96}
{'loss': 1.7146, 'grad_norm': 0.3878375291824341, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7084358930587769, 'eval_runtime': 8.9867, 'eval_samples_per_second': 111.165, 'eval_steps_per_second': 7.01, 'epoch': 1.0}
{'train_runtime': 359.419, 'train_samples_per_second': 27.82, 'train_steps_per_second': 1.739, 'train_loss': 1.9102832763671875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.314324140548706, 2.252399444580078, 1.9554376602172852, 1.876733660697937, 1.8371880054473877, 1.810331106185913, 1.794681429862976, 1.7809364795684814, 1.772722840309143, 1.7591902017593384, 1.7539020776748657, 1.748231053352356, 1.741839051246643, 1.7365248203277588, 1.731881022453308, 1.7264227867126465, 1.7243905067443848, 1.7213345766067505, 1.7176547050476074, 1.714989185333252, 1.7131952047348022, 1.7108408212661743, 1.7100727558135986, 1.7090234756469727, 1.7084358930587769], 'performance': [0.51, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:03,  3.23it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 43.90it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 67.07it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 79.28it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 88.33it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 95.54it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 103.91it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 114.07it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 116.31it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 118.59it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 122.83it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 131.17it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:01<00:01, 143.14it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 140.96it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 147.36it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 145.98it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 145.01it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 147.71it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 150.00it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 158.71it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:02<00:00, 158.53it/s]Running loglikelihood requests:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 363/400 [00:02<00:00, 170.29it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 191.47it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 128.57it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.51, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.0894432067871094
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2848 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.2667774226374484
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 1.0639753377043795
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.2446831601187514
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 1.1902814211415187
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.7271653994446494
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5265, dtype=torch.float64), 0, 0, tensor(0.4735, dtype=torch.float64), 0, 0, 0, 1, 1, 1, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(2.4122e-15, dtype=torch.float64), tensor(1.2737e-15, dtype=torch.float64), tensor(0.5265, dtype=torch.float64), tensor(2.9615e-15, dtype=torch.float64), tensor(2.2081e-15, dtype=torch.float64), tensor(0.4735, dtype=torch.float64), tensor(3.7725e-15, dtype=torch.float64), tensor(2.0881e-15, dtype=torch.float64), tensor(2.2780e-15, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.526
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.474
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.07it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.56it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.95it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 135.02it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.36it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.89it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 159.49it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 170.69it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 173.52it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 180.99it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 185.87it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 191.53it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 194.06it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 210.06it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 214.21it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 230.67it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 244.13it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 192.18it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.543, 'grad_norm': 0.0814131423830986, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 4.475401401519775, 'eval_runtime': 8.7561, 'eval_samples_per_second': 114.092, 'eval_steps_per_second': 7.195, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.15it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.28it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.26it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.92it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.12it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.98it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 157.08it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 168.21it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 170.47it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.67it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 184.23it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 186.85it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 192.56it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 200.93it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 214.08it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 223.39it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 240.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.06it/s]
Evaluation performance at step 50: 0.49
{'loss': 4.035, 'grad_norm': 0.20761339366436005, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 3.4656825065612793, 'eval_runtime': 8.8174, 'eval_samples_per_second': 113.299, 'eval_steps_per_second': 7.145, 'epoch': 0.08}
{'loss': 3.0691, 'grad_norm': 0.10167645663022995, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.844407558441162, 'eval_runtime': 8.7869, 'eval_samples_per_second': 113.691, 'eval_steps_per_second': 7.17, 'epoch': 0.12}
{'loss': 2.6371, 'grad_norm': 0.09496956318616867, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.5054047107696533, 'eval_runtime': 8.8311, 'eval_samples_per_second': 113.123, 'eval_steps_per_second': 7.134, 'epoch': 0.16}
{'loss': 2.3436, 'grad_norm': 0.08383230119943619, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.3186535835266113, 'eval_runtime': 8.8563, 'eval_samples_per_second': 112.801, 'eval_steps_per_second': 7.114, 'epoch': 0.2}
{'loss': 2.2405, 'grad_norm': 0.0659048780798912, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.2019529342651367, 'eval_runtime': 8.8481, 'eval_samples_per_second': 112.906, 'eval_steps_per_second': 7.12, 'epoch': 0.24}
{'loss': 2.1334, 'grad_norm': 0.07030153274536133, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.118879556655884, 'eval_runtime': 8.856, 'eval_samples_per_second': 112.805, 'eval_steps_per_second': 7.114, 'epoch': 0.28}
{'loss': 2.0091, 'grad_norm': 0.0598943717777729, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0571200847625732, 'eval_runtime': 8.8821, 'eval_samples_per_second': 112.474, 'eval_steps_per_second': 7.093, 'epoch': 0.32}
{'loss': 2.0332, 'grad_norm': 0.11957377940416336, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0140936374664307, 'eval_runtime': 8.9789, 'eval_samples_per_second': 111.261, 'eval_steps_per_second': 7.016, 'epoch': 0.36}
{'loss': 1.9926, 'grad_norm': 0.06499991565942764, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9842915534973145, 'eval_runtime': 8.9794, 'eval_samples_per_second': 111.254, 'eval_steps_per_second': 7.016, 'epoch': 0.4}
{'loss': 1.9729, 'grad_norm': 0.057164136320352554, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9622822999954224, 'eval_runtime': 8.9648, 'eval_samples_per_second': 111.436, 'eval_steps_per_second': 7.028, 'epoch': 0.44}
{'loss': 1.9202, 'grad_norm': 0.07054156810045242, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9453647136688232, 'eval_runtime': 9.0043, 'eval_samples_per_second': 110.947, 'eval_steps_per_second': 6.997, 'epoch': 0.48}
{'loss': 1.9503, 'grad_norm': 0.06099735200405121, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9323896169662476, 'eval_runtime': 8.9604, 'eval_samples_per_second': 111.49, 'eval_steps_per_second': 7.031, 'epoch': 0.52}
{'loss': 1.9055, 'grad_norm': 0.10372089594602585, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9222196340560913, 'eval_runtime': 8.9915, 'eval_samples_per_second': 111.105, 'eval_steps_per_second': 7.007, 'epoch': 0.56}
{'loss': 1.8828, 'grad_norm': 0.06512953341007233, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9123449325561523, 'eval_runtime': 8.9395, 'eval_samples_per_second': 111.752, 'eval_steps_per_second': 7.047, 'epoch': 0.6}
{'loss': 1.9236, 'grad_norm': 0.07434587925672531, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9039554595947266, 'eval_runtime': 8.9237, 'eval_samples_per_second': 111.949, 'eval_steps_per_second': 7.06, 'epoch': 0.64}
{'loss': 1.8832, 'grad_norm': 0.08350595086812973, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8982062339782715, 'eval_runtime': 8.9157, 'eval_samples_per_second': 112.049, 'eval_steps_per_second': 7.066, 'epoch': 0.68}
{'loss': 1.917, 'grad_norm': 0.058356545865535736, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8925150632858276, 'eval_runtime': 8.8723, 'eval_samples_per_second': 112.598, 'eval_steps_per_second': 7.101, 'epoch': 0.72}
{'loss': 1.8884, 'grad_norm': 0.08603435754776001, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8891825675964355, 'eval_runtime': 8.8727, 'eval_samples_per_second': 112.592, 'eval_steps_per_second': 7.1, 'epoch': 0.76}
{'loss': 1.904, 'grad_norm': 0.06620676070451736, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8852800130844116, 'eval_runtime': 8.8845, 'eval_samples_per_second': 112.443, 'eval_steps_per_second': 7.091, 'epoch': 0.8}
{'loss': 1.896, 'grad_norm': 0.06903347373008728, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8821754455566406, 'eval_runtime': 8.8901, 'eval_samples_per_second': 112.373, 'eval_steps_per_second': 7.087, 'epoch': 0.84}
{'loss': 1.9199, 'grad_norm': 0.06718223541975021, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8795723915100098, 'eval_runtime': 8.9241, 'eval_samples_per_second': 111.945, 'eval_steps_per_second': 7.06, 'epoch': 0.88}
{'loss': 1.9001, 'grad_norm': 0.08099609613418579, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.878058671951294, 'eval_runtime': 8.9203, 'eval_samples_per_second': 111.992, 'eval_steps_per_second': 7.063, 'epoch': 0.92}
{'loss': 1.9408, 'grad_norm': 0.06574892997741699, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8771268129348755, 'eval_runtime': 8.9165, 'eval_samples_per_second': 112.04, 'eval_steps_per_second': 7.066, 'epoch': 0.96}
{'loss': 1.8543, 'grad_norm': 0.06781883537769318, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8770171403884888, 'eval_runtime': 8.9234, 'eval_samples_per_second': 111.953, 'eval_steps_per_second': 7.06, 'epoch': 1.0}
{'train_runtime': 351.5728, 'train_samples_per_second': 28.441, 'train_steps_per_second': 1.778, 'train_loss': 2.227812927246094, 'epoch': 1.0}
train_results:  {'eval_loss': [4.475401401519775, 3.4656825065612793, 2.844407558441162, 2.5054047107696533, 2.3186535835266113, 2.2019529342651367, 2.118879556655884, 2.0571200847625732, 2.0140936374664307, 1.9842915534973145, 1.9622822999954224, 1.9453647136688232, 1.9323896169662476, 1.9222196340560913, 1.9123449325561523, 1.9039554595947266, 1.8982062339782715, 1.8925150632858276, 1.8891825675964355, 1.8852800130844116, 1.8821754455566406, 1.8795723915100098, 1.878058671951294, 1.8771268129348755, 1.8770171403884888], 'performance': [0.49, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:56,  3.42it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 45.62it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 69.23it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 80.66it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 89.15it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 96.65it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 104.96it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 115.09it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 116.97it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 119.19it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 123.08it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 131.40it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:01<00:01, 143.72it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 141.77it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 147.69it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 146.16it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 145.09it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 147.91it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 150.40it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 159.38it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 344/400 [00:02<00:00, 161.91it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 366/400 [00:02<00:00, 178.46it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 189.52it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 129.77it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.2964553833007812
current iteration best possible performance (full train run):  0.5355000000000001
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9626 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 1.2570749039904145
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 1.1728113057130258
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.2847781384357222
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.2641177907660426
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.0860920886601182
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4843, dtype=torch.float64), 0, 0, tensor(0.5157, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 1, 0, 2, 0.1, 1.480000019073501, 0]
normalized proposed parameters for next round by BO: [tensor(3.0134e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4843, dtype=torch.float64), tensor(3.7516e-16, dtype=torch.float64), tensor(1.3159e-16, dtype=torch.float64), tensor(0.5157, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.3016e-16, dtype=torch.float64), tensor(2.3100e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.484
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.516
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.480000019073501,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  1.480000019073501
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,031,616 || all params: 8,032,292,864 || trainable%: 0.0253
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 59.07it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 96.09it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 108.22it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 116.27it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 121.21it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 133.97it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 135.08it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 145.18it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 145.19it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 146.72it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 148.41it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 155.38it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 158.90it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 160.41it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 164.97it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 171.64it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 182.91it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 190.55it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 204.36it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 209.86it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 162.51it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.282, 'grad_norm': 2.2904512882232666, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.5423061847686768, 'eval_runtime': 10.0424, 'eval_samples_per_second': 99.478, 'eval_steps_per_second': 6.273, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 59.18it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 95.77it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 107.53it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 115.69it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 120.90it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 133.64it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 135.00it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 145.00it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 144.84it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 146.45it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 148.15it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 155.14it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 158.72it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 160.28it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 164.80it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 171.58it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 182.77it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 190.18it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 203.71it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 209.68it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 162.25it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.7261, 'grad_norm': 1.5673073530197144, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.0569896697998047, 'eval_runtime': 10.0541, 'eval_samples_per_second': 99.362, 'eval_steps_per_second': 6.266, 'epoch': 0.08}
{'loss': 1.8275, 'grad_norm': 0.5233035087585449, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6833070516586304, 'eval_runtime': 10.061, 'eval_samples_per_second': 99.294, 'eval_steps_per_second': 6.262, 'epoch': 0.12}
{'loss': 1.6254, 'grad_norm': 0.5277200937271118, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5841796398162842, 'eval_runtime': 10.0984, 'eval_samples_per_second': 98.927, 'eval_steps_per_second': 6.239, 'epoch': 0.16}
{'loss': 1.5467, 'grad_norm': 0.41771331429481506, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.553055763244629, 'eval_runtime': 10.1215, 'eval_samples_per_second': 98.701, 'eval_steps_per_second': 6.224, 'epoch': 0.2}
{'loss': 1.559, 'grad_norm': 0.42333006858825684, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5375423431396484, 'eval_runtime': 10.1984, 'eval_samples_per_second': 97.956, 'eval_steps_per_second': 6.177, 'epoch': 0.24}
{'loss': 1.5257, 'grad_norm': 0.3997223973274231, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.525768756866455, 'eval_runtime': 10.1622, 'eval_samples_per_second': 98.305, 'eval_steps_per_second': 6.199, 'epoch': 0.28}
{'loss': 1.5292, 'grad_norm': 0.42839378118515015, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5171809196472168, 'eval_runtime': 10.1464, 'eval_samples_per_second': 98.458, 'eval_steps_per_second': 6.209, 'epoch': 0.32}
{'loss': 1.5187, 'grad_norm': 0.48386046290397644, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.512797474861145, 'eval_runtime': 10.1365, 'eval_samples_per_second': 98.555, 'eval_steps_per_second': 6.215, 'epoch': 0.36}
{'loss': 1.5174, 'grad_norm': 0.4608703851699829, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5036009550094604, 'eval_runtime': 10.1482, 'eval_samples_per_second': 98.441, 'eval_steps_per_second': 6.208, 'epoch': 0.4}
{'loss': 1.5188, 'grad_norm': 0.47989127039909363, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4977830648422241, 'eval_runtime': 10.134, 'eval_samples_per_second': 98.579, 'eval_steps_per_second': 6.217, 'epoch': 0.44}
{'loss': 1.5143, 'grad_norm': 0.563940703868866, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.490712285041809, 'eval_runtime': 10.1684, 'eval_samples_per_second': 98.245, 'eval_steps_per_second': 6.196, 'epoch': 0.48}
{'loss': 1.5097, 'grad_norm': 0.4955245554447174, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4890788793563843, 'eval_runtime': 10.117, 'eval_samples_per_second': 98.744, 'eval_steps_per_second': 6.227, 'epoch': 0.52}
{'loss': 1.4897, 'grad_norm': 0.4498917758464813, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4840896129608154, 'eval_runtime': 10.1172, 'eval_samples_per_second': 98.743, 'eval_steps_per_second': 6.227, 'epoch': 0.56}
{'loss': 1.4874, 'grad_norm': 0.42043536901474, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4812358617782593, 'eval_runtime': 10.1161, 'eval_samples_per_second': 98.753, 'eval_steps_per_second': 6.228, 'epoch': 0.6}
{'loss': 1.4855, 'grad_norm': 0.44403916597366333, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4771180152893066, 'eval_runtime': 10.1228, 'eval_samples_per_second': 98.688, 'eval_steps_per_second': 6.224, 'epoch': 0.64}
{'loss': 1.4628, 'grad_norm': 0.49924930930137634, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.474028468132019, 'eval_runtime': 10.1229, 'eval_samples_per_second': 98.687, 'eval_steps_per_second': 6.224, 'epoch': 0.68}
{'loss': 1.5091, 'grad_norm': 0.5030922889709473, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4717366695404053, 'eval_runtime': 10.1222, 'eval_samples_per_second': 98.694, 'eval_steps_per_second': 6.224, 'epoch': 0.72}
{'loss': 1.5009, 'grad_norm': 0.462632417678833, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.469307541847229, 'eval_runtime': 10.1222, 'eval_samples_per_second': 98.694, 'eval_steps_per_second': 6.224, 'epoch': 0.76}
{'loss': 1.4454, 'grad_norm': 0.5705138444900513, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4671692848205566, 'eval_runtime': 10.128, 'eval_samples_per_second': 98.638, 'eval_steps_per_second': 6.22, 'epoch': 0.8}
{'loss': 1.4698, 'grad_norm': 0.5201191902160645, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4658167362213135, 'eval_runtime': 10.1309, 'eval_samples_per_second': 98.609, 'eval_steps_per_second': 6.219, 'epoch': 0.84}
{'loss': 1.4483, 'grad_norm': 0.508662760257721, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4638601541519165, 'eval_runtime': 10.1388, 'eval_samples_per_second': 98.532, 'eval_steps_per_second': 6.214, 'epoch': 0.88}
{'loss': 1.4434, 'grad_norm': 0.5571943521499634, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4624558687210083, 'eval_runtime': 10.1363, 'eval_samples_per_second': 98.557, 'eval_steps_per_second': 6.215, 'epoch': 0.92}
{'loss': 1.4907, 'grad_norm': 0.44359391927719116, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4613518714904785, 'eval_runtime': 10.1254, 'eval_samples_per_second': 98.663, 'eval_steps_per_second': 6.222, 'epoch': 0.96}
{'loss': 1.4807, 'grad_norm': 0.525869607925415, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.460992455482483, 'eval_runtime': 10.1711, 'eval_samples_per_second': 98.219, 'eval_steps_per_second': 6.194, 'epoch': 1.0}
{'train_runtime': 524.2055, 'train_samples_per_second': 19.075, 'train_steps_per_second': 1.192, 'train_loss': 1.676558856201172, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5423061847686768, 2.0569896697998047, 1.6833070516586304, 1.5841796398162842, 1.553055763244629, 1.5375423431396484, 1.525768756866455, 1.5171809196472168, 1.512797474861145, 1.5036009550094604, 1.4977830648422241, 1.490712285041809, 1.4890788793563843, 1.4840896129608154, 1.4812358617782593, 1.4771180152893066, 1.474028468132019, 1.4717366695404053, 1.469307541847229, 1.4671692848205566, 1.4658167362213135, 1.4638601541519165, 1.4624558687210083, 1.4613518714904785, 1.460992455482483], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<04:31,  1.47it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:17, 22.21it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:09, 40.42it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 53.66it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 64.58it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 73.90it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 83.29it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 94.12it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 97.65it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 100.64it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 105.15it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 113.15it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 124.01it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 122.49it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 128.01it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 126.71it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:00, 126.35it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 128.76it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 131.15it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 138.98it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 139.02it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 139.48it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 150.09it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 101.06it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  0.6156018972396851
current iteration best possible performance (full train run):  0.462
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0665 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 1.0547542895118587
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9474950434639953
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.1481886340286105
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7862563812498853
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.8867858520005564
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4934, dtype=torch.float64), 0, 0, tensor(0.3511, dtype=torch.float64), tensor(0.1555, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(2.5379e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4934, dtype=torch.float64), tensor(8.4911e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3511, dtype=torch.float64), tensor(0.1555, dtype=torch.float64), tensor(6.8752e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.493
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.351
  wikitext: 0.155
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 62.27it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 100.68it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 113.18it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 121.90it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 127.38it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 140.96it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 142.43it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 152.99it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 153.20it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 154.86it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 157.55it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 167.67it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 169.38it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 173.92it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 180.61it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 191.80it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 199.72it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 214.21it/s]Running loglikelihood requests:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 395/400 [00:02<00:00, 223.37it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 171.53it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.139, 'grad_norm': 0.29909780621528625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.543865919113159, 'eval_runtime': 9.6584, 'eval_samples_per_second': 103.433, 'eval_steps_per_second': 6.523, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 62.09it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 99.97it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 112.88it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 121.51it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 126.90it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 140.49it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 141.75it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 152.20it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 152.11it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 154.14it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 156.96it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 167.02it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 168.68it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 173.19it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 180.07it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 191.23it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 198.96it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 213.21it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 219.79it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 170.80it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.8322, 'grad_norm': 0.2546427249908447, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.24364972114563, 'eval_runtime': 9.6658, 'eval_samples_per_second': 103.354, 'eval_steps_per_second': 6.518, 'epoch': 0.08}
{'loss': 2.0473, 'grad_norm': 0.0703682228922844, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.862496018409729, 'eval_runtime': 9.7239, 'eval_samples_per_second': 102.736, 'eval_steps_per_second': 6.479, 'epoch': 0.12}
{'loss': 1.8092, 'grad_norm': 0.09128618240356445, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7605232000350952, 'eval_runtime': 9.8912, 'eval_samples_per_second': 100.999, 'eval_steps_per_second': 6.369, 'epoch': 0.16}
{'loss': 1.7472, 'grad_norm': 0.06529662013053894, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.701705813407898, 'eval_runtime': 9.8458, 'eval_samples_per_second': 101.464, 'eval_steps_per_second': 6.399, 'epoch': 0.2}
{'loss': 1.7011, 'grad_norm': 0.063104547560215, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6769835948944092, 'eval_runtime': 9.8471, 'eval_samples_per_second': 101.451, 'eval_steps_per_second': 6.398, 'epoch': 0.24}
{'loss': 1.7046, 'grad_norm': 0.07375456392765045, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6593295335769653, 'eval_runtime': 9.8426, 'eval_samples_per_second': 101.498, 'eval_steps_per_second': 6.401, 'epoch': 0.28}
{'loss': 1.6432, 'grad_norm': 0.06232217326760292, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6474767923355103, 'eval_runtime': 9.8385, 'eval_samples_per_second': 101.54, 'eval_steps_per_second': 6.403, 'epoch': 0.32}
{'loss': 1.6591, 'grad_norm': 0.06338822096586227, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6398886442184448, 'eval_runtime': 9.8346, 'eval_samples_per_second': 101.581, 'eval_steps_per_second': 6.406, 'epoch': 0.36}
{'loss': 1.6531, 'grad_norm': 0.08389695733785629, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6302188634872437, 'eval_runtime': 9.8371, 'eval_samples_per_second': 101.554, 'eval_steps_per_second': 6.404, 'epoch': 0.4}
{'loss': 1.6691, 'grad_norm': 0.06476873904466629, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6239044666290283, 'eval_runtime': 9.8723, 'eval_samples_per_second': 101.193, 'eval_steps_per_second': 6.382, 'epoch': 0.44}
{'loss': 1.6933, 'grad_norm': 0.07949884980916977, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6190582513809204, 'eval_runtime': 9.8619, 'eval_samples_per_second': 101.299, 'eval_steps_per_second': 6.388, 'epoch': 0.48}
{'loss': 1.6416, 'grad_norm': 0.07780808955430984, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6143276691436768, 'eval_runtime': 9.8493, 'eval_samples_per_second': 101.429, 'eval_steps_per_second': 6.396, 'epoch': 0.52}
{'loss': 1.6518, 'grad_norm': 0.060213588178157806, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6098549365997314, 'eval_runtime': 9.8213, 'eval_samples_per_second': 101.718, 'eval_steps_per_second': 6.415, 'epoch': 0.56}
{'loss': 1.6231, 'grad_norm': 0.06929786503314972, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.605712652206421, 'eval_runtime': 9.8008, 'eval_samples_per_second': 101.93, 'eval_steps_per_second': 6.428, 'epoch': 0.6}
{'loss': 1.626, 'grad_norm': 0.06509487330913544, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6014291048049927, 'eval_runtime': 9.7602, 'eval_samples_per_second': 102.354, 'eval_steps_per_second': 6.455, 'epoch': 0.64}
{'loss': 1.5667, 'grad_norm': 0.08463205397129059, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5986838340759277, 'eval_runtime': 9.769, 'eval_samples_per_second': 102.262, 'eval_steps_per_second': 6.449, 'epoch': 0.68}
{'loss': 1.6231, 'grad_norm': 0.07061213999986649, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5965678691864014, 'eval_runtime': 9.7792, 'eval_samples_per_second': 102.155, 'eval_steps_per_second': 6.442, 'epoch': 0.72}
{'loss': 1.6406, 'grad_norm': 0.0720231682062149, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5942901372909546, 'eval_runtime': 9.7763, 'eval_samples_per_second': 102.185, 'eval_steps_per_second': 6.444, 'epoch': 0.76}
{'loss': 1.585, 'grad_norm': 0.06761401891708374, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5928112268447876, 'eval_runtime': 9.8303, 'eval_samples_per_second': 101.625, 'eval_steps_per_second': 6.409, 'epoch': 0.8}
{'loss': 1.6192, 'grad_norm': 0.07842367142438889, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.590747356414795, 'eval_runtime': 9.8286, 'eval_samples_per_second': 101.642, 'eval_steps_per_second': 6.41, 'epoch': 0.84}
{'loss': 1.6518, 'grad_norm': 0.0684635117650032, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5894771814346313, 'eval_runtime': 9.8316, 'eval_samples_per_second': 101.611, 'eval_steps_per_second': 6.408, 'epoch': 0.88}
{'loss': 1.6182, 'grad_norm': 0.06621657311916351, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.588344693183899, 'eval_runtime': 9.8279, 'eval_samples_per_second': 101.649, 'eval_steps_per_second': 6.41, 'epoch': 0.92}
{'loss': 1.6058, 'grad_norm': 0.06871410459280014, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5877156257629395, 'eval_runtime': 9.8166, 'eval_samples_per_second': 101.766, 'eval_steps_per_second': 6.418, 'epoch': 0.96}
{'loss': 1.6445, 'grad_norm': 0.0741209164261818, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.587340235710144, 'eval_runtime': 9.8176, 'eval_samples_per_second': 101.756, 'eval_steps_per_second': 6.417, 'epoch': 1.0}
{'train_runtime': 503.2888, 'train_samples_per_second': 19.867, 'train_steps_per_second': 1.242, 'train_loss': 1.8158344665527344, 'epoch': 1.0}
train_results:  {'eval_loss': [3.543865919113159, 2.24364972114563, 1.862496018409729, 1.7605232000350952, 1.701705813407898, 1.6769835948944092, 1.6593295335769653, 1.6474767923355103, 1.6398886442184448, 1.6302188634872437, 1.6239044666290283, 1.6190582513809204, 1.6143276691436768, 1.6098549365997314, 1.605712652206421, 1.6014291048049927, 1.5986838340759277, 1.5965678691864014, 1.5942901372909546, 1.5928112268447876, 1.590747356414795, 1.5894771814346313, 1.588344693183899, 1.5877156257629395, 1.587340235710144], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:24,  1.95it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:15, 23.95it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:08, 43.22it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 56.78it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 68.00it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 77.57it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.21it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 98.25it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 101.84it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 105.11it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 109.81it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 118.17it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 129.43it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 127.64it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 133.37it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 132.14it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:00, 131.55it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 134.26it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 136.53it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 144.57it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 144.12it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 145.09it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 155.61it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 106.99it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4077579975128174
current iteration best possible performance (full train run):  0.399
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2878 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.8062397329665981
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 1.0294225634044132
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 1.0518997377759567
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 1.2441042099658517
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 1.1053329284765807
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4927, dtype=torch.float64), 0, 0, tensor(0.3069, dtype=torch.float64), 0, tensor(0.2004, dtype=torch.float64), 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.480000019073487, 0]
normalized proposed parameters for next round by BO: [tensor(5.6659e-18, dtype=torch.float64), tensor(4.7785e-18, dtype=torch.float64), tensor(0.4927, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.3127e-18, dtype=torch.float64), tensor(0.3069, dtype=torch.float64), tensor(3.4631e-17, dtype=torch.float64), tensor(0.2004, dtype=torch.float64), tensor(3.4418e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.493
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.307
  wikitext: 0
  mmlu: 0.2
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.11it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.97it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.22it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.94it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.76it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.15it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.49it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 138.08it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.93it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.30it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.82it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.55it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.89it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.87it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.47it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.10it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.48it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.58it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.54it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.63it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 201.19it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.33it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.0501, 'grad_norm': 0.19047102332115173, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.2021570205688477, 'eval_runtime': 10.7374, 'eval_samples_per_second': 93.039, 'eval_steps_per_second': 5.867, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.51it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.20it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.32it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.93it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.73it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.53it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.68it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.33it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.44it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 137.70it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.27it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 145.93it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 149.15it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.16it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 154.77it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 154.70it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 280/400 [00:02<00:00, 167.67it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 169.63it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 179.61it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 184.53it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 195.92it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.50it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.4842, 'grad_norm': 0.17513951659202576, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 2.0007450580596924, 'eval_runtime': 10.7862, 'eval_samples_per_second': 92.618, 'eval_steps_per_second': 5.841, 'epoch': 0.08}
{'loss': 1.8306, 'grad_norm': 0.06484145671129227, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7262028455734253, 'eval_runtime': 10.7729, 'eval_samples_per_second': 92.733, 'eval_steps_per_second': 5.848, 'epoch': 0.12}
{'loss': 1.6437, 'grad_norm': 0.043034136295318604, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6175408363342285, 'eval_runtime': 10.775, 'eval_samples_per_second': 92.714, 'eval_steps_per_second': 5.847, 'epoch': 0.16}
{'loss': 1.5943, 'grad_norm': 0.04466172680258751, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5914843082427979, 'eval_runtime': 10.7256, 'eval_samples_per_second': 93.142, 'eval_steps_per_second': 5.874, 'epoch': 0.2}
{'loss': 1.5533, 'grad_norm': 0.05313563346862793, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5735021829605103, 'eval_runtime': 10.7225, 'eval_samples_per_second': 93.169, 'eval_steps_per_second': 5.876, 'epoch': 0.24}
{'loss': 1.5638, 'grad_norm': 0.04676491394639015, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5567536354064941, 'eval_runtime': 10.7234, 'eval_samples_per_second': 93.16, 'eval_steps_per_second': 5.875, 'epoch': 0.28}
{'loss': 1.5914, 'grad_norm': 0.042650166898965836, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5449600219726562, 'eval_runtime': 10.7247, 'eval_samples_per_second': 93.149, 'eval_steps_per_second': 5.874, 'epoch': 0.32}
{'loss': 1.5585, 'grad_norm': 0.03500789403915405, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.53610098361969, 'eval_runtime': 10.7276, 'eval_samples_per_second': 93.124, 'eval_steps_per_second': 5.873, 'epoch': 0.36}
{'loss': 1.5689, 'grad_norm': 0.038822241127491, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5287771224975586, 'eval_runtime': 10.7208, 'eval_samples_per_second': 93.184, 'eval_steps_per_second': 5.876, 'epoch': 0.4}
{'loss': 1.5231, 'grad_norm': 0.04482844099402428, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5238882303237915, 'eval_runtime': 10.7316, 'eval_samples_per_second': 93.089, 'eval_steps_per_second': 5.87, 'epoch': 0.44}
{'loss': 1.5211, 'grad_norm': 0.04334712028503418, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5177234411239624, 'eval_runtime': 10.7337, 'eval_samples_per_second': 93.072, 'eval_steps_per_second': 5.869, 'epoch': 0.48}
{'loss': 1.5142, 'grad_norm': 0.042961638420820236, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.511847734451294, 'eval_runtime': 10.7374, 'eval_samples_per_second': 93.039, 'eval_steps_per_second': 5.867, 'epoch': 0.52}
{'loss': 1.4984, 'grad_norm': 0.0414537638425827, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5075241327285767, 'eval_runtime': 10.7329, 'eval_samples_per_second': 93.079, 'eval_steps_per_second': 5.87, 'epoch': 0.56}
{'loss': 1.547, 'grad_norm': 0.04167313128709793, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5015568733215332, 'eval_runtime': 10.7366, 'eval_samples_per_second': 93.046, 'eval_steps_per_second': 5.868, 'epoch': 0.6}
{'loss': 1.555, 'grad_norm': 0.04927552863955498, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.497934103012085, 'eval_runtime': 10.7339, 'eval_samples_per_second': 93.069, 'eval_steps_per_second': 5.869, 'epoch': 0.64}
{'loss': 1.531, 'grad_norm': 0.04615641012787819, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.493118405342102, 'eval_runtime': 10.7416, 'eval_samples_per_second': 93.003, 'eval_steps_per_second': 5.865, 'epoch': 0.68}
{'loss': 1.5029, 'grad_norm': 0.043152179569005966, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.489341378211975, 'eval_runtime': 10.7652, 'eval_samples_per_second': 92.799, 'eval_steps_per_second': 5.852, 'epoch': 0.72}
{'loss': 1.556, 'grad_norm': 0.042141105979681015, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.486257791519165, 'eval_runtime': 10.784, 'eval_samples_per_second': 92.637, 'eval_steps_per_second': 5.842, 'epoch': 0.76}
{'loss': 1.5049, 'grad_norm': 0.04089294373989105, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4824540615081787, 'eval_runtime': 10.785, 'eval_samples_per_second': 92.628, 'eval_steps_per_second': 5.841, 'epoch': 0.8}
{'loss': 1.5187, 'grad_norm': 0.05091174319386482, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4801058769226074, 'eval_runtime': 10.7858, 'eval_samples_per_second': 92.622, 'eval_steps_per_second': 5.841, 'epoch': 0.84}
{'loss': 1.4998, 'grad_norm': 0.04851889982819557, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4773896932601929, 'eval_runtime': 10.7921, 'eval_samples_per_second': 92.568, 'eval_steps_per_second': 5.838, 'epoch': 0.88}
{'loss': 1.491, 'grad_norm': 0.04723531752824783, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4759451150894165, 'eval_runtime': 10.7936, 'eval_samples_per_second': 92.555, 'eval_steps_per_second': 5.837, 'epoch': 0.92}
{'loss': 1.4355, 'grad_norm': 0.046134527772665024, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4745734930038452, 'eval_runtime': 10.8095, 'eval_samples_per_second': 92.419, 'eval_steps_per_second': 5.828, 'epoch': 0.96}
{'loss': 1.5105, 'grad_norm': 0.0728674903512001, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.473933458328247, 'eval_runtime': 10.8343, 'eval_samples_per_second': 92.208, 'eval_steps_per_second': 5.815, 'epoch': 1.0}
{'train_runtime': 547.8464, 'train_samples_per_second': 18.25, 'train_steps_per_second': 1.141, 'train_loss': 1.6859190063476563, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2021570205688477, 2.0007450580596924, 1.7262028455734253, 1.6175408363342285, 1.5914843082427979, 1.5735021829605103, 1.5567536354064941, 1.5449600219726562, 1.53610098361969, 1.5287771224975586, 1.5238882303237915, 1.5177234411239624, 1.511847734451294, 1.5075241327285767, 1.5015568733215332, 1.497934103012085, 1.493118405342102, 1.489341378211975, 1.486257791519165, 1.4824540615081787, 1.4801058769226074, 1.4773896932601929, 1.4759451150894165, 1.4745734930038452, 1.473933458328247], 'performance': [0.5, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:24,  4.74it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 48.54it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 66.52it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 73.67it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 79.24it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 84.06it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 89.46it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 97.42it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 98.52it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 99.65it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 102.82it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 109.43it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 119.05it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 116.95it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 121.45it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 120.11it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 119.02it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 121.20it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 123.86it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 131.22it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 131.19it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 131.64it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 141.61it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 111.38it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.423042893409729
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2113 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.9031850271910882
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 1.0139020701940127
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.9698043672524275
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.9672975421095898
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.9687127997027043
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5049, dtype=torch.float64), 0, tensor(0.4951, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734866, 0]
normalized proposed parameters for next round by BO: [tensor(0.5049, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4951, dtype=torch.float64), tensor(3.8549e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.2277e-17, dtype=torch.float64), tensor(3.2968e-17, dtype=torch.float64), tensor(6.9626e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.505
  gsm8k: 0
  rowan_hellaswag: 0.495
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 62.01it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 100.29it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 112.77it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 121.34it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 126.83it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 140.53it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 141.88it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 152.67it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 152.68it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 154.32it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 182/400 [00:01<00:01, 173.36it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 162.16it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 164.76it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 170.41it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 175.48it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 187.43it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 197.38it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 203.53it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 216.28it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 170.80it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.2262, 'grad_norm': 0.3497428297996521, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.4931747913360596, 'eval_runtime': 9.7589, 'eval_samples_per_second': 102.368, 'eval_steps_per_second': 6.456, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 61.85it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 99.98it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 112.36it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 120.96it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 126.25it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 139.89it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 141.39it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 152.01it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 152.10it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 153.96it/s]Running loglikelihood requests:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 181/400 [00:01<00:01, 170.19it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 162.52it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 164.67it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 169.95it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 174.87it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 186.87it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 197.34it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 203.37it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 215.98it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 170.27it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.7642, 'grad_norm': 0.24615217745304108, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 2.103910446166992, 'eval_runtime': 9.7602, 'eval_samples_per_second': 102.354, 'eval_steps_per_second': 6.455, 'epoch': 0.08}
{'loss': 1.8362, 'grad_norm': 0.11095139384269714, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6787807941436768, 'eval_runtime': 9.7682, 'eval_samples_per_second': 102.271, 'eval_steps_per_second': 6.45, 'epoch': 0.12}
{'loss': 1.6251, 'grad_norm': 0.07591265439987183, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.578390121459961, 'eval_runtime': 9.8996, 'eval_samples_per_second': 100.913, 'eval_steps_per_second': 6.364, 'epoch': 0.16}
{'loss': 1.5771, 'grad_norm': 0.06392298638820648, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.526774525642395, 'eval_runtime': 9.9492, 'eval_samples_per_second': 100.41, 'eval_steps_per_second': 6.332, 'epoch': 0.2}
{'loss': 1.5007, 'grad_norm': 0.05151372402906418, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.509299874305725, 'eval_runtime': 9.9766, 'eval_samples_per_second': 100.134, 'eval_steps_per_second': 6.315, 'epoch': 0.24}
{'loss': 1.5337, 'grad_norm': 0.06455384194850922, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4978855848312378, 'eval_runtime': 9.9727, 'eval_samples_per_second': 100.174, 'eval_steps_per_second': 6.317, 'epoch': 0.28}
{'loss': 1.5309, 'grad_norm': 0.05771723389625549, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.489347219467163, 'eval_runtime': 9.9143, 'eval_samples_per_second': 100.764, 'eval_steps_per_second': 6.354, 'epoch': 0.32}
{'loss': 1.4876, 'grad_norm': 0.054632507264614105, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4864263534545898, 'eval_runtime': 9.897, 'eval_samples_per_second': 100.939, 'eval_steps_per_second': 6.366, 'epoch': 0.36}
{'loss': 1.5046, 'grad_norm': 0.05290544405579567, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.477318286895752, 'eval_runtime': 9.9214, 'eval_samples_per_second': 100.691, 'eval_steps_per_second': 6.35, 'epoch': 0.4}
{'loss': 1.4881, 'grad_norm': 0.06742532551288605, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4722963571548462, 'eval_runtime': 9.9283, 'eval_samples_per_second': 100.621, 'eval_steps_per_second': 6.345, 'epoch': 0.44}
{'loss': 1.4611, 'grad_norm': 0.05466966703534126, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4684662818908691, 'eval_runtime': 9.952, 'eval_samples_per_second': 100.382, 'eval_steps_per_second': 6.33, 'epoch': 0.48}
{'loss': 1.4527, 'grad_norm': 0.06562555581331253, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.466625690460205, 'eval_runtime': 9.8986, 'eval_samples_per_second': 100.923, 'eval_steps_per_second': 6.365, 'epoch': 0.52}
{'loss': 1.47, 'grad_norm': 0.06548690050840378, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4628846645355225, 'eval_runtime': 9.86, 'eval_samples_per_second': 101.318, 'eval_steps_per_second': 6.389, 'epoch': 0.56}
{'loss': 1.4667, 'grad_norm': 0.06602702289819717, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4599374532699585, 'eval_runtime': 9.8315, 'eval_samples_per_second': 101.612, 'eval_steps_per_second': 6.408, 'epoch': 0.6}
{'loss': 1.4202, 'grad_norm': 0.0527002327144146, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4572776556015015, 'eval_runtime': 9.8329, 'eval_samples_per_second': 101.598, 'eval_steps_per_second': 6.407, 'epoch': 0.64}
{'loss': 1.4759, 'grad_norm': 0.07754868268966675, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4551740884780884, 'eval_runtime': 9.8296, 'eval_samples_per_second': 101.632, 'eval_steps_per_second': 6.409, 'epoch': 0.68}
{'loss': 1.4605, 'grad_norm': 0.061220683157444, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.453779935836792, 'eval_runtime': 9.8383, 'eval_samples_per_second': 101.542, 'eval_steps_per_second': 6.404, 'epoch': 0.72}
{'loss': 1.4151, 'grad_norm': 0.061336878687143326, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4516922235488892, 'eval_runtime': 9.8387, 'eval_samples_per_second': 101.538, 'eval_steps_per_second': 6.403, 'epoch': 0.76}
{'loss': 1.4826, 'grad_norm': 0.07347414642572403, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.450230598449707, 'eval_runtime': 9.9383, 'eval_samples_per_second': 100.52, 'eval_steps_per_second': 6.339, 'epoch': 0.8}
{'loss': 1.47, 'grad_norm': 0.06295230239629745, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4498478174209595, 'eval_runtime': 9.9334, 'eval_samples_per_second': 100.569, 'eval_steps_per_second': 6.342, 'epoch': 0.84}
{'loss': 1.4456, 'grad_norm': 0.059585604816675186, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4480290412902832, 'eval_runtime': 10.0025, 'eval_samples_per_second': 99.875, 'eval_steps_per_second': 6.298, 'epoch': 0.88}
{'loss': 1.4721, 'grad_norm': 0.06285560876131058, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4473391771316528, 'eval_runtime': 9.9891, 'eval_samples_per_second': 100.009, 'eval_steps_per_second': 6.307, 'epoch': 0.92}
{'loss': 1.4435, 'grad_norm': 0.06188485026359558, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4470319747924805, 'eval_runtime': 9.9672, 'eval_samples_per_second': 100.229, 'eval_steps_per_second': 6.321, 'epoch': 0.96}
{'loss': 1.4463, 'grad_norm': 0.06200498715043068, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.446682333946228, 'eval_runtime': 9.9334, 'eval_samples_per_second': 100.57, 'eval_steps_per_second': 6.342, 'epoch': 1.0}
{'train_runtime': 507.8137, 'train_samples_per_second': 19.69, 'train_steps_per_second': 1.231, 'train_loss': 1.658275634765625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4931747913360596, 2.103910446166992, 1.6787807941436768, 1.578390121459961, 1.526774525642395, 1.509299874305725, 1.4978855848312378, 1.489347219467163, 1.4864263534545898, 1.477318286895752, 1.4722963571548462, 1.4684662818908691, 1.466625690460205, 1.4628846645355225, 1.4599374532699585, 1.4572776556015015, 1.4551740884780884, 1.453779935836792, 1.4516922235488892, 1.450230598449707, 1.4498478174209595, 1.4480290412902832, 1.4473391771316528, 1.4470319747924805, 1.446682333946228], 'performance': [0.48, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:17,  5.16it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 52.97it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 72.55it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 80.52it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 86.43it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 91.39it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 97.74it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 106.32it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 107.03it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 108.09it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 111.76it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 119.27it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 129.87it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 127.58it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 132.71it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 131.48it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 130.86it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 133.14it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 135.36it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 142.41it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 142.18it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 143.00it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 153.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 121.39it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4045146703720093
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1660 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.9604041387485411
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.6809635457353346
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.9745040600829485
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.9582600955596785
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.9494251223026916
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5002, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.4998, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 128, 0.1, 1.48000001907349, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5002, dtype=torch.float64), tensor(2.0156e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.7777e-17, dtype=torch.float64), tensor(2.6021e-17, dtype=torch.float64), tensor(0.4998, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.5
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.5

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.48000001907349,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.48000001907349
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.33it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.25it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.40it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 110.13it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 115.04it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.22it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.29it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.90it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.86it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.47it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 141.15it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.89it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 151.08it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 152.08it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.68it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.12it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 282/400 [00:02<00:00, 175.35it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 169.76it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 180.60it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 185.50it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 197.26it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.28it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.8877, 'grad_norm': 0.15589582920074463, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.0236308574676514, 'eval_runtime': 10.6812, 'eval_samples_per_second': 93.529, 'eval_steps_per_second': 5.898, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.36it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.26it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.54it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 110.15it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.94it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.21it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.42it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 138.05it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 138.01it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.57it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 141.18it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.88it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 151.24it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 152.30it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.99it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.63it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 165.05it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 176.09it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.85it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.86it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 201.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.61it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.3139, 'grad_norm': 0.16156062483787537, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.8143160343170166, 'eval_runtime': 10.657, 'eval_samples_per_second': 93.741, 'eval_steps_per_second': 5.912, 'epoch': 0.08}
{'loss': 1.6434, 'grad_norm': 0.047285404056310654, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5803701877593994, 'eval_runtime': 10.6816, 'eval_samples_per_second': 93.525, 'eval_steps_per_second': 5.898, 'epoch': 0.12}
{'loss': 1.5268, 'grad_norm': 0.04295181483030319, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4876655340194702, 'eval_runtime': 10.7245, 'eval_samples_per_second': 93.152, 'eval_steps_per_second': 5.874, 'epoch': 0.16}
{'loss': 1.4439, 'grad_norm': 0.04190343990921974, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4622828960418701, 'eval_runtime': 10.7441, 'eval_samples_per_second': 92.981, 'eval_steps_per_second': 5.864, 'epoch': 0.2}
{'loss': 1.4384, 'grad_norm': 0.03597424179315567, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.448047161102295, 'eval_runtime': 10.7638, 'eval_samples_per_second': 92.811, 'eval_steps_per_second': 5.853, 'epoch': 0.24}
{'loss': 1.3958, 'grad_norm': 0.04093620926141739, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4341548681259155, 'eval_runtime': 10.769, 'eval_samples_per_second': 92.766, 'eval_steps_per_second': 5.85, 'epoch': 0.28}
{'loss': 1.4087, 'grad_norm': 0.04027427360415459, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4267034530639648, 'eval_runtime': 10.7755, 'eval_samples_per_second': 92.71, 'eval_steps_per_second': 5.847, 'epoch': 0.32}
{'loss': 1.4182, 'grad_norm': 0.04251735657453537, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4179242849349976, 'eval_runtime': 10.7744, 'eval_samples_per_second': 92.72, 'eval_steps_per_second': 5.847, 'epoch': 0.36}
{'loss': 1.3733, 'grad_norm': 0.042110491544008255, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4103862047195435, 'eval_runtime': 10.7507, 'eval_samples_per_second': 92.924, 'eval_steps_per_second': 5.86, 'epoch': 0.4}
{'loss': 1.3762, 'grad_norm': 0.03932013362646103, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4031623601913452, 'eval_runtime': 10.7979, 'eval_samples_per_second': 92.518, 'eval_steps_per_second': 5.834, 'epoch': 0.44}
{'loss': 1.3984, 'grad_norm': 0.0381520614027977, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.394791841506958, 'eval_runtime': 10.8352, 'eval_samples_per_second': 92.199, 'eval_steps_per_second': 5.814, 'epoch': 0.48}
{'loss': 1.4108, 'grad_norm': 0.040972933173179626, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3898472785949707, 'eval_runtime': 10.8662, 'eval_samples_per_second': 91.937, 'eval_steps_per_second': 5.798, 'epoch': 0.52}
{'loss': 1.3933, 'grad_norm': 0.03913283720612526, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3841712474822998, 'eval_runtime': 10.9043, 'eval_samples_per_second': 91.615, 'eval_steps_per_second': 5.778, 'epoch': 0.56}
{'loss': 1.3674, 'grad_norm': 0.047974806278944016, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3768938779830933, 'eval_runtime': 10.894, 'eval_samples_per_second': 91.702, 'eval_steps_per_second': 5.783, 'epoch': 0.6}
{'loss': 1.3914, 'grad_norm': 0.042950525879859924, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3690850734710693, 'eval_runtime': 10.9055, 'eval_samples_per_second': 91.605, 'eval_steps_per_second': 5.777, 'epoch': 0.64}
{'loss': 1.3304, 'grad_norm': 0.04453349485993385, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3644049167633057, 'eval_runtime': 10.9173, 'eval_samples_per_second': 91.506, 'eval_steps_per_second': 5.771, 'epoch': 0.68}
{'loss': 1.3805, 'grad_norm': 0.05518535524606705, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.357931137084961, 'eval_runtime': 10.8961, 'eval_samples_per_second': 91.684, 'eval_steps_per_second': 5.782, 'epoch': 0.72}
{'loss': 1.3717, 'grad_norm': 0.0517888218164444, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3523820638656616, 'eval_runtime': 10.909, 'eval_samples_per_second': 91.576, 'eval_steps_per_second': 5.775, 'epoch': 0.76}
{'loss': 1.35, 'grad_norm': 0.05188433825969696, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3478368520736694, 'eval_runtime': 10.8199, 'eval_samples_per_second': 92.33, 'eval_steps_per_second': 5.823, 'epoch': 0.8}
{'loss': 1.3804, 'grad_norm': 0.053408678621053696, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3429005146026611, 'eval_runtime': 10.8201, 'eval_samples_per_second': 92.328, 'eval_steps_per_second': 5.822, 'epoch': 0.84}
{'loss': 1.3494, 'grad_norm': 0.04713160917162895, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.339185118675232, 'eval_runtime': 10.81, 'eval_samples_per_second': 92.414, 'eval_steps_per_second': 5.828, 'epoch': 0.88}
{'loss': 1.3388, 'grad_norm': 0.051239751279354095, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3359184265136719, 'eval_runtime': 10.7786, 'eval_samples_per_second': 92.684, 'eval_steps_per_second': 5.845, 'epoch': 0.92}
{'loss': 1.3861, 'grad_norm': 0.04707658290863037, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3342663049697876, 'eval_runtime': 10.7544, 'eval_samples_per_second': 92.892, 'eval_steps_per_second': 5.858, 'epoch': 0.96}
{'loss': 1.3588, 'grad_norm': 0.05273868888616562, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3333925008773804, 'eval_runtime': 10.7446, 'eval_samples_per_second': 92.977, 'eval_steps_per_second': 5.863, 'epoch': 1.0}
{'train_runtime': 553.8859, 'train_samples_per_second': 18.052, 'train_steps_per_second': 1.128, 'train_loss': 1.5373483581542968, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0236308574676514, 1.8143160343170166, 1.5803701877593994, 1.4876655340194702, 1.4622828960418701, 1.448047161102295, 1.4341548681259155, 1.4267034530639648, 1.4179242849349976, 1.4103862047195435, 1.4031623601913452, 1.394791841506958, 1.3898472785949707, 1.3841712474822998, 1.3768938779830933, 1.3690850734710693, 1.3644049167633057, 1.357931137084961, 1.3523820638656616, 1.3478368520736694, 1.3429005146026611, 1.339185118675232, 1.3359184265136719, 1.3342663049697876, 1.3333925008773804], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:16,  2.04it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:15, 25.00it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:08, 43.61it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 55.91it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 65.68it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 73.67it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 82.10it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 91.84it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 94.54it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 96.96it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 101.30it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 108.76it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 118.79it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.06it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 121.94it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 120.55it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 119.92it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 122.50it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.06it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.63it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.32it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 132.75it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 142.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 100.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4201101064682007
current iteration best possible performance (full train run):  0.5355000000000001
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8336 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.833431493817256
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.9088254870625216
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.7807437467032805
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.7353763201098182
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8929932524507973
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4952, dtype=torch.float64), 0, tensor(0.5048, dtype=torch.float64), 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734885, 0]
normalized proposed parameters for next round by BO: [tensor(1.1931e-18, dtype=torch.float64), tensor(9.1241e-18, dtype=torch.float64), tensor(0.4952, dtype=torch.float64), tensor(9.9518e-17, dtype=torch.float64), tensor(0.5048, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.7484e-18, dtype=torch.float64), tensor(1.8870e-18, dtype=torch.float64), tensor(1.9392e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.495
  sciq: 0
  triviaqa: 0.505
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.57it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.40it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.88it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.70it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.68it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.21it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.37it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 138.06it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 138.07it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.57it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 141.30it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 148.19it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 151.54it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 152.59it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 157.20it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.77it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 165.07it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 176.01it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.70it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.83it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.78it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.42it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3392, 'grad_norm': 0.19253720343112946, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.30769419670105, 'eval_runtime': 10.5946, 'eval_samples_per_second': 94.293, 'eval_steps_per_second': 5.946, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.15it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.72it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.68it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.04it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.60it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.73it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.84it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.21it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 135.98it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 137.34it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 138.58it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 144.84it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 148.16it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 148.70it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 153.79it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 153.11it/s]Running loglikelihood requests:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 284/400 [00:02<00:00, 177.84it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 166.01it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 176.98it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 181.95it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 193.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 151.88it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.5797, 'grad_norm': 0.16900677978992462, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 2.0242233276367188, 'eval_runtime': 10.7012, 'eval_samples_per_second': 93.354, 'eval_steps_per_second': 5.887, 'epoch': 0.08}
{'loss': 1.855, 'grad_norm': 0.06491959095001221, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.763126254081726, 'eval_runtime': 10.7536, 'eval_samples_per_second': 92.899, 'eval_steps_per_second': 5.859, 'epoch': 0.12}
{'loss': 1.7208, 'grad_norm': 0.041551508009433746, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6568881273269653, 'eval_runtime': 10.7881, 'eval_samples_per_second': 92.602, 'eval_steps_per_second': 5.84, 'epoch': 0.16}
{'loss': 1.639, 'grad_norm': 0.03779665753245354, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6297633647918701, 'eval_runtime': 10.8256, 'eval_samples_per_second': 92.282, 'eval_steps_per_second': 5.82, 'epoch': 0.2}
{'loss': 1.6248, 'grad_norm': 0.037708401679992676, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6163607835769653, 'eval_runtime': 10.7868, 'eval_samples_per_second': 92.613, 'eval_steps_per_second': 5.84, 'epoch': 0.24}
{'loss': 1.5935, 'grad_norm': 0.05301567539572716, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6047823429107666, 'eval_runtime': 10.822, 'eval_samples_per_second': 92.312, 'eval_steps_per_second': 5.821, 'epoch': 0.28}
{'loss': 1.6074, 'grad_norm': 0.03593413904309273, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.598276138305664, 'eval_runtime': 10.8445, 'eval_samples_per_second': 92.121, 'eval_steps_per_second': 5.809, 'epoch': 0.32}
{'loss': 1.6153, 'grad_norm': 0.03936943784356117, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5900301933288574, 'eval_runtime': 10.8349, 'eval_samples_per_second': 92.202, 'eval_steps_per_second': 5.815, 'epoch': 0.36}
{'loss': 1.5923, 'grad_norm': 0.042949896305799484, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5839548110961914, 'eval_runtime': 10.8462, 'eval_samples_per_second': 92.106, 'eval_steps_per_second': 5.808, 'epoch': 0.4}
{'loss': 1.5882, 'grad_norm': 0.0407094806432724, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5788389444351196, 'eval_runtime': 10.8257, 'eval_samples_per_second': 92.28, 'eval_steps_per_second': 5.819, 'epoch': 0.44}
{'loss': 1.5728, 'grad_norm': 0.04210476577281952, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5730417966842651, 'eval_runtime': 10.8078, 'eval_samples_per_second': 92.433, 'eval_steps_per_second': 5.829, 'epoch': 0.48}
{'loss': 1.5997, 'grad_norm': 0.048151373863220215, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5712603330612183, 'eval_runtime': 10.7698, 'eval_samples_per_second': 92.759, 'eval_steps_per_second': 5.85, 'epoch': 0.52}
{'loss': 1.5875, 'grad_norm': 0.037102654576301575, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5671628713607788, 'eval_runtime': 10.7104, 'eval_samples_per_second': 93.273, 'eval_steps_per_second': 5.882, 'epoch': 0.56}
{'loss': 1.5662, 'grad_norm': 0.043771032243967056, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5640349388122559, 'eval_runtime': 10.6982, 'eval_samples_per_second': 93.38, 'eval_steps_per_second': 5.889, 'epoch': 0.6}
{'loss': 1.6022, 'grad_norm': 0.04612761363387108, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5599400997161865, 'eval_runtime': 10.697, 'eval_samples_per_second': 93.39, 'eval_steps_per_second': 5.889, 'epoch': 0.64}
{'loss': 1.5773, 'grad_norm': 0.04762014001607895, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5575634241104126, 'eval_runtime': 10.6987, 'eval_samples_per_second': 93.376, 'eval_steps_per_second': 5.889, 'epoch': 0.68}
{'loss': 1.6023, 'grad_norm': 0.046148404479026794, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.554805040359497, 'eval_runtime': 10.6954, 'eval_samples_per_second': 93.405, 'eval_steps_per_second': 5.89, 'epoch': 0.72}
{'loss': 1.5879, 'grad_norm': 0.04025039076805115, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.551318645477295, 'eval_runtime': 10.6943, 'eval_samples_per_second': 93.414, 'eval_steps_per_second': 5.891, 'epoch': 0.76}
{'loss': 1.5582, 'grad_norm': 0.05488855764269829, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.549673318862915, 'eval_runtime': 10.7219, 'eval_samples_per_second': 93.174, 'eval_steps_per_second': 5.876, 'epoch': 0.8}
{'loss': 1.5876, 'grad_norm': 0.050474654883146286, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5474810600280762, 'eval_runtime': 10.7198, 'eval_samples_per_second': 93.192, 'eval_steps_per_second': 5.877, 'epoch': 0.84}
{'loss': 1.5614, 'grad_norm': 0.053281236439943314, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5463706254959106, 'eval_runtime': 10.7418, 'eval_samples_per_second': 93.001, 'eval_steps_per_second': 5.865, 'epoch': 0.88}
{'loss': 1.5864, 'grad_norm': 0.04654180631041527, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5456491708755493, 'eval_runtime': 10.694, 'eval_samples_per_second': 93.417, 'eval_steps_per_second': 5.891, 'epoch': 0.92}
{'loss': 1.5948, 'grad_norm': 0.04093032702803612, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5446748733520508, 'eval_runtime': 10.7004, 'eval_samples_per_second': 93.361, 'eval_steps_per_second': 5.888, 'epoch': 0.96}
{'loss': 1.6, 'grad_norm': 0.04910274222493172, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5444142818450928, 'eval_runtime': 10.6942, 'eval_samples_per_second': 93.415, 'eval_steps_per_second': 5.891, 'epoch': 1.0}
{'train_runtime': 558.2821, 'train_samples_per_second': 17.91, 'train_steps_per_second': 1.12, 'train_loss': 1.757582147216797, 'epoch': 1.0}
train_results:  {'eval_loss': [3.30769419670105, 2.0242233276367188, 1.763126254081726, 1.6568881273269653, 1.6297633647918701, 1.6163607835769653, 1.6047823429107666, 1.598276138305664, 1.5900301933288574, 1.5839548110961914, 1.5788389444351196, 1.5730417966842651, 1.5712603330612183, 1.5671628713607788, 1.5640349388122559, 1.5599400997161865, 1.5575634241104126, 1.554805040359497, 1.551318645477295, 1.549673318862915, 1.5474810600280762, 1.5463706254959106, 1.5456491708755493, 1.5446748733520508, 1.5444142818450928], 'performance': [0.49, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:17,  2.02it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:12, 30.91it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 50.54it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 61.80it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 70.38it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 77.46it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 84.80it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 93.89it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.92it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 97.87it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 101.58it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 108.74it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 118.58it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 116.79it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 121.87it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 120.63it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 120.31it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 122.58it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 124.49it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 131.23it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 131.17it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 131.73it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 141.82it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 103.68it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.424170970916748
current iteration best possible performance (full train run):  0.441
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5640 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7666784070989856
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 1.0059098984595352
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.7985789834452905
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.6104266248299501
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8545887794133751
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5019, dtype=torch.float64), tensor(0.1599, dtype=torch.float64), 0, 0, 0, tensor(0.3383, dtype=torch.float64), 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734894, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.9545e-17, dtype=torch.float64), tensor(0.5019, dtype=torch.float64), tensor(0.1599, dtype=torch.float64), tensor(2.1102e-17, dtype=torch.float64), tensor(3.8720e-17, dtype=torch.float64), tensor(4.3254e-17, dtype=torch.float64), tensor(0.3383, dtype=torch.float64), tensor(3.7795e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.502
  sciq: 0.16
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.338
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734894,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734894
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.86it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 89.84it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.23it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.97it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.83it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.26it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.78it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.55it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.54it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.74it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.12it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.82it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 149.94it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.10it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.04it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.09it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 163.82it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 307/400 [00:02<00:00, 176.10it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 179.90it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 185.31it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 196.69it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.13it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.9267, 'grad_norm': 0.2023569494485855, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.109144449234009, 'eval_runtime': 10.7726, 'eval_samples_per_second': 92.736, 'eval_steps_per_second': 5.848, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.91it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.44it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.43it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.68it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 112.11it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 124.80it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 125.71it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 135.67it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.02it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 137.70it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.44it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.27it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 149.58it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.73it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.44it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.24it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 282/400 [00:02<00:00, 174.62it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 169.06it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 179.67it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 184.89it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 196.47it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.62it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.4952, 'grad_norm': 0.16548651456832886, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 2.005768060684204, 'eval_runtime': 10.7974, 'eval_samples_per_second': 92.522, 'eval_steps_per_second': 5.835, 'epoch': 0.08}
{'loss': 1.8478, 'grad_norm': 0.05240295082330704, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7547715902328491, 'eval_runtime': 10.8497, 'eval_samples_per_second': 92.076, 'eval_steps_per_second': 5.807, 'epoch': 0.12}
{'loss': 1.6681, 'grad_norm': 0.04250483214855194, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.654358148574829, 'eval_runtime': 10.8638, 'eval_samples_per_second': 91.957, 'eval_steps_per_second': 5.799, 'epoch': 0.16}
{'loss': 1.6293, 'grad_norm': 0.04782641679048538, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6271376609802246, 'eval_runtime': 10.8054, 'eval_samples_per_second': 92.454, 'eval_steps_per_second': 5.83, 'epoch': 0.2}
{'loss': 1.5907, 'grad_norm': 0.04235728085041046, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.609114408493042, 'eval_runtime': 10.8237, 'eval_samples_per_second': 92.297, 'eval_steps_per_second': 5.821, 'epoch': 0.24}
{'loss': 1.5751, 'grad_norm': 0.0484999418258667, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5974714756011963, 'eval_runtime': 10.8012, 'eval_samples_per_second': 92.49, 'eval_steps_per_second': 5.833, 'epoch': 0.28}
{'loss': 1.5784, 'grad_norm': 0.036067117005586624, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.587077021598816, 'eval_runtime': 10.7587, 'eval_samples_per_second': 92.855, 'eval_steps_per_second': 5.856, 'epoch': 0.32}
{'loss': 1.5664, 'grad_norm': 0.043847642838954926, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5804808139801025, 'eval_runtime': 10.7654, 'eval_samples_per_second': 92.798, 'eval_steps_per_second': 5.852, 'epoch': 0.36}
{'loss': 1.5668, 'grad_norm': 0.04196283966302872, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5753406286239624, 'eval_runtime': 10.7556, 'eval_samples_per_second': 92.882, 'eval_steps_per_second': 5.857, 'epoch': 0.4}
{'loss': 1.5354, 'grad_norm': 0.045793768018484116, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5708805322647095, 'eval_runtime': 10.7548, 'eval_samples_per_second': 92.889, 'eval_steps_per_second': 5.858, 'epoch': 0.44}
{'loss': 1.571, 'grad_norm': 0.038295429199934006, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5662046670913696, 'eval_runtime': 10.7969, 'eval_samples_per_second': 92.527, 'eval_steps_per_second': 5.835, 'epoch': 0.48}
{'loss': 1.546, 'grad_norm': 0.03586708754301071, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5611164569854736, 'eval_runtime': 10.795, 'eval_samples_per_second': 92.542, 'eval_steps_per_second': 5.836, 'epoch': 0.52}
{'loss': 1.5267, 'grad_norm': 0.03910546004772186, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5584467649459839, 'eval_runtime': 10.7647, 'eval_samples_per_second': 92.803, 'eval_steps_per_second': 5.852, 'epoch': 0.56}
{'loss': 1.5353, 'grad_norm': 0.04203220456838608, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5530787706375122, 'eval_runtime': 10.7547, 'eval_samples_per_second': 92.89, 'eval_steps_per_second': 5.858, 'epoch': 0.6}
{'loss': 1.5567, 'grad_norm': 0.039922624826431274, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5509405136108398, 'eval_runtime': 10.8953, 'eval_samples_per_second': 91.691, 'eval_steps_per_second': 5.782, 'epoch': 0.64}
{'loss': 1.5466, 'grad_norm': 0.03968564420938492, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5483713150024414, 'eval_runtime': 10.8838, 'eval_samples_per_second': 91.788, 'eval_steps_per_second': 5.788, 'epoch': 0.68}
{'loss': 1.5339, 'grad_norm': 0.04432813823223114, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5457357168197632, 'eval_runtime': 10.8687, 'eval_samples_per_second': 91.915, 'eval_steps_per_second': 5.796, 'epoch': 0.72}
{'loss': 1.5698, 'grad_norm': 0.039858169853687286, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5433006286621094, 'eval_runtime': 10.8481, 'eval_samples_per_second': 92.09, 'eval_steps_per_second': 5.807, 'epoch': 0.76}
{'loss': 1.5859, 'grad_norm': 0.038943350315093994, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5406986474990845, 'eval_runtime': 10.7834, 'eval_samples_per_second': 92.642, 'eval_steps_per_second': 5.842, 'epoch': 0.8}
{'loss': 1.5554, 'grad_norm': 0.042894147336483, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5392558574676514, 'eval_runtime': 10.8225, 'eval_samples_per_second': 92.308, 'eval_steps_per_second': 5.821, 'epoch': 0.84}
{'loss': 1.5135, 'grad_norm': 0.039695389568805695, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5377730131149292, 'eval_runtime': 10.8611, 'eval_samples_per_second': 91.979, 'eval_steps_per_second': 5.8, 'epoch': 0.88}
{'loss': 1.5062, 'grad_norm': 0.04684292525053024, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5366665124893188, 'eval_runtime': 10.8207, 'eval_samples_per_second': 92.323, 'eval_steps_per_second': 5.822, 'epoch': 0.92}
{'loss': 1.5616, 'grad_norm': 0.039349623024463654, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.535817265510559, 'eval_runtime': 10.8894, 'eval_samples_per_second': 91.74, 'eval_steps_per_second': 5.785, 'epoch': 0.96}
{'loss': 1.5504, 'grad_norm': 0.03850245848298073, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5355572700500488, 'eval_runtime': 10.9073, 'eval_samples_per_second': 91.59, 'eval_steps_per_second': 5.776, 'epoch': 1.0}
{'train_runtime': 552.4301, 'train_samples_per_second': 18.098, 'train_steps_per_second': 1.131, 'train_loss': 1.7055598571777344, 'epoch': 1.0}
train_results:  {'eval_loss': [3.109144449234009, 2.005768060684204, 1.7547715902328491, 1.654358148574829, 1.6271376609802246, 1.609114408493042, 1.5974714756011963, 1.587077021598816, 1.5804808139801025, 1.5753406286239624, 1.5708805322647095, 1.5662046670913696, 1.5611164569854736, 1.5584467649459839, 1.5530787706375122, 1.5509405136108398, 1.5483713150024414, 1.5457357168197632, 1.5433006286621094, 1.5406986474990845, 1.5392558574676514, 1.5377730131149292, 1.5366665124893188, 1.535817265510559, 1.5355572700500488], 'performance': [0.49, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<04:11,  1.59it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:14, 26.31it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:08, 45.21it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 57.18it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 66.64it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 74.47it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 82.36it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 91.86it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 94.49it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 96.72it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 100.43it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 107.43it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 117.30it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 115.22it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 120.11it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 119.31it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 118.99it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 121.33it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 122.88it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 130.73it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 131.06it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 131.72it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 141.76it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.81it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4226740598678589
current iteration best possible performance (full train run):  0.504
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1091 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.9518971512842181
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 1.1596712576617811
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.7372829672652685
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.7789967890284378
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.5434243135970447
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4905, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5095, dtype=torch.float64), 0, 32, 1, 1, 0, 0, 0, 128, 0.09999999999999999, 1.4800000190734872, 1]
normalized proposed parameters for next round by BO: [tensor(1.4131e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4905, dtype=torch.float64), tensor(1.3665e-16, dtype=torch.float64), tensor(1.1449e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3240e-18, dtype=torch.float64), tensor(0.5095, dtype=torch.float64), tensor(3.5204e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.49
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.51
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999999,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.09999999999999999
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 54,525,952 || all params: 8,084,787,200 || trainable%: 0.6744
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 64.23it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 103.82it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 117.08it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 125.96it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 131.70it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 145.45it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 146.92it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 158.06it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 158.02it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 160.88it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 168.04it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 172.25it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 177.21it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 179.11it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 193.44it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 196.31it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 210.99it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 219.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 176.76it/s]
Evaluation performance at step 25: 0.5
{'loss': 3.9255, 'grad_norm': 0.2019193321466446, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.6449222564697266, 'eval_runtime': 9.4444, 'eval_samples_per_second': 105.777, 'eval_steps_per_second': 6.671, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 64.33it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 104.24it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 117.40it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 126.11it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 131.38it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 145.50it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 146.27it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 156.88it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 157.12it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 159.63it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 166.95it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 171.01it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 175.34it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 176.82it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 191.71it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 195.11it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 210.09it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 220.08it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 176.02it/s]
Evaluation performance at step 50: 0.53
{'loss': 3.0122, 'grad_norm': 0.07149602472782135, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 2.5295050144195557, 'eval_runtime': 9.4682, 'eval_samples_per_second': 105.512, 'eval_steps_per_second': 6.654, 'epoch': 0.08}
{'loss': 2.2538, 'grad_norm': 0.09203960746526718, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.072300672531128, 'eval_runtime': 9.4494, 'eval_samples_per_second': 105.721, 'eval_steps_per_second': 6.667, 'epoch': 0.12}
{'loss': 1.9566, 'grad_norm': 0.05135717615485191, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.929229974746704, 'eval_runtime': 9.4736, 'eval_samples_per_second': 105.451, 'eval_steps_per_second': 6.65, 'epoch': 0.16}
{'loss': 1.8711, 'grad_norm': 0.0559331439435482, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.853091835975647, 'eval_runtime': 9.4488, 'eval_samples_per_second': 105.727, 'eval_steps_per_second': 6.667, 'epoch': 0.2}
{'loss': 1.8034, 'grad_norm': 0.0781802162528038, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8176357746124268, 'eval_runtime': 9.4504, 'eval_samples_per_second': 105.709, 'eval_steps_per_second': 6.666, 'epoch': 0.24}
{'loss': 1.7583, 'grad_norm': 0.06230347231030464, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7947349548339844, 'eval_runtime': 9.4453, 'eval_samples_per_second': 105.767, 'eval_steps_per_second': 6.67, 'epoch': 0.28}
{'loss': 1.7486, 'grad_norm': 0.05368223786354065, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7802067995071411, 'eval_runtime': 9.4402, 'eval_samples_per_second': 105.824, 'eval_steps_per_second': 6.674, 'epoch': 0.32}
{'loss': 1.7555, 'grad_norm': 0.05702442675828934, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.767488718032837, 'eval_runtime': 9.4264, 'eval_samples_per_second': 105.979, 'eval_steps_per_second': 6.683, 'epoch': 0.36}
{'loss': 1.7059, 'grad_norm': 0.06793132424354553, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.755948543548584, 'eval_runtime': 9.434, 'eval_samples_per_second': 105.893, 'eval_steps_per_second': 6.678, 'epoch': 0.4}
{'loss': 1.7212, 'grad_norm': 0.0687604546546936, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.746829628944397, 'eval_runtime': 9.4266, 'eval_samples_per_second': 105.977, 'eval_steps_per_second': 6.683, 'epoch': 0.44}
{'loss': 1.696, 'grad_norm': 0.0631612166762352, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.73905348777771, 'eval_runtime': 9.5031, 'eval_samples_per_second': 105.123, 'eval_steps_per_second': 6.629, 'epoch': 0.48}
{'loss': 1.7357, 'grad_norm': 0.06020248681306839, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7330454587936401, 'eval_runtime': 9.4601, 'eval_samples_per_second': 105.601, 'eval_steps_per_second': 6.66, 'epoch': 0.52}
{'loss': 1.6921, 'grad_norm': 0.055912084877491, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7269481420516968, 'eval_runtime': 9.4456, 'eval_samples_per_second': 105.764, 'eval_steps_per_second': 6.67, 'epoch': 0.56}
{'loss': 1.6948, 'grad_norm': 0.06520502269268036, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7217354774475098, 'eval_runtime': 9.4378, 'eval_samples_per_second': 105.85, 'eval_steps_per_second': 6.675, 'epoch': 0.6}
{'loss': 1.7129, 'grad_norm': 0.05592021346092224, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7174662351608276, 'eval_runtime': 9.4369, 'eval_samples_per_second': 105.861, 'eval_steps_per_second': 6.676, 'epoch': 0.64}
{'loss': 1.7079, 'grad_norm': 0.06891165673732758, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7128950357437134, 'eval_runtime': 9.4902, 'eval_samples_per_second': 105.267, 'eval_steps_per_second': 6.638, 'epoch': 0.68}
{'loss': 1.7107, 'grad_norm': 0.06506999582052231, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.708924412727356, 'eval_runtime': 9.4648, 'eval_samples_per_second': 105.549, 'eval_steps_per_second': 6.656, 'epoch': 0.72}
{'loss': 1.6845, 'grad_norm': 0.05755206570029259, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7049816846847534, 'eval_runtime': 9.4719, 'eval_samples_per_second': 105.47, 'eval_steps_per_second': 6.651, 'epoch': 0.76}
{'loss': 1.6331, 'grad_norm': 0.0632891058921814, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7015641927719116, 'eval_runtime': 9.5168, 'eval_samples_per_second': 104.972, 'eval_steps_per_second': 6.62, 'epoch': 0.8}
{'loss': 1.6784, 'grad_norm': 0.05736361816525459, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6985461711883545, 'eval_runtime': 9.4966, 'eval_samples_per_second': 105.196, 'eval_steps_per_second': 6.634, 'epoch': 0.84}
{'loss': 1.6949, 'grad_norm': 0.057445596903562546, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.695993423461914, 'eval_runtime': 9.4931, 'eval_samples_per_second': 105.235, 'eval_steps_per_second': 6.636, 'epoch': 0.88}
{'loss': 1.6652, 'grad_norm': 0.06650152802467346, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6936500072479248, 'eval_runtime': 9.4451, 'eval_samples_per_second': 105.769, 'eval_steps_per_second': 6.67, 'epoch': 0.92}
{'loss': 1.6646, 'grad_norm': 0.059856269508600235, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.691957950592041, 'eval_runtime': 9.4441, 'eval_samples_per_second': 105.781, 'eval_steps_per_second': 6.671, 'epoch': 0.96}
{'loss': 1.6487, 'grad_norm': 0.07189324498176575, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6914881467819214, 'eval_runtime': 9.4405, 'eval_samples_per_second': 105.821, 'eval_steps_per_second': 6.673, 'epoch': 1.0}
{'train_runtime': 480.9014, 'train_samples_per_second': 20.792, 'train_steps_per_second': 1.3, 'train_loss': 1.8852631530761719, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6449222564697266, 2.5295050144195557, 2.072300672531128, 1.929229974746704, 1.853091835975647, 1.8176357746124268, 1.7947349548339844, 1.7802067995071411, 1.767488718032837, 1.755948543548584, 1.746829628944397, 1.73905348777771, 1.7330454587936401, 1.7269481420516968, 1.7217354774475098, 1.7174662351608276, 1.7128950357437134, 1.708924412727356, 1.7049816846847534, 1.7015641927719116, 1.6985461711883545, 1.695993423461914, 1.6936500072479248, 1.691957950592041, 1.6914881467819214], 'performance': [0.5, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:10,  2.09it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:15, 24.87it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:08, 45.10it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 59.21it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 70.91it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 80.47it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 90.72it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 102.30it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 105.88it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 109.15it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 114.53it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 123.29it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 134.97it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 133.25it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 139.42it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 138.38it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 138.14it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 140.78it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 142.90it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 150.80it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 150.95it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 151.59it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 390/400 [00:03<00:00, 195.80it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 111.74it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.402753233909607
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3021 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.7233254886806738
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 1.0792034694155195
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.9153756997822331
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.7397826297517688
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.9512257780418614
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4929, dtype=torch.float64), tensor(0.1874, dtype=torch.float64), tensor(0.3197, dtype=torch.float64), 0, 0, 0, 0, 32, 0, 1, 0, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4929, dtype=torch.float64), tensor(0.1874, dtype=torch.float64), tensor(0.3197, dtype=torch.float64), tensor(2.7830e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7908e-15, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.493
  sciq: 0.187
  triviaqa: 0.32
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 67.95it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 109.80it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 123.46it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 132.67it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 138.62it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 153.37it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 155.11it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 166.65it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 168.20it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 171.00it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 181.31it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 183.18it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 188.58it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 196.51it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 208.74it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 217.85it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 234.02it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 187.29it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.4693, 'grad_norm': 0.24432872235774994, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 4.137564659118652, 'eval_runtime': 8.9271, 'eval_samples_per_second': 111.907, 'eval_steps_per_second': 7.057, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 67.62it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 108.85it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 122.28it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 131.71it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 137.59it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 152.10it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 153.63it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 135/400 [00:00<00:01, 185.91it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 154/400 [00:01<00:01, 165.82it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 164.08it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 175.82it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:00, 178.76it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 185.04it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 190.63it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 204.05it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:01<00:00, 215.84it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 222.87it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 240.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 186.22it/s]
Evaluation performance at step 50: 0.52
{'loss': 3.4925, 'grad_norm': 0.12856392562389374, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 2.82731032371521, 'eval_runtime': 8.9308, 'eval_samples_per_second': 111.86, 'eval_steps_per_second': 7.054, 'epoch': 0.08}
{'loss': 2.483, 'grad_norm': 0.08682584017515182, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1985318660736084, 'eval_runtime': 8.9585, 'eval_samples_per_second': 111.514, 'eval_steps_per_second': 7.032, 'epoch': 0.12}
{'loss': 2.0953, 'grad_norm': 0.09297391027212143, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0126726627349854, 'eval_runtime': 9.0007, 'eval_samples_per_second': 110.991, 'eval_steps_per_second': 6.999, 'epoch': 0.16}
{'loss': 1.9281, 'grad_norm': 0.075919009745121, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9061142206192017, 'eval_runtime': 9.0417, 'eval_samples_per_second': 110.488, 'eval_steps_per_second': 6.968, 'epoch': 0.2}
{'loss': 1.8876, 'grad_norm': 0.13382898271083832, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8622409105300903, 'eval_runtime': 9.1144, 'eval_samples_per_second': 109.607, 'eval_steps_per_second': 6.912, 'epoch': 0.24}
{'loss': 1.8489, 'grad_norm': 0.10904350131750107, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8327159881591797, 'eval_runtime': 9.1372, 'eval_samples_per_second': 109.333, 'eval_steps_per_second': 6.895, 'epoch': 0.28}
{'loss': 1.8272, 'grad_norm': 0.0892404243350029, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8171182870864868, 'eval_runtime': 9.1658, 'eval_samples_per_second': 108.992, 'eval_steps_per_second': 6.873, 'epoch': 0.32}
{'loss': 1.783, 'grad_norm': 0.07067066431045532, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8018908500671387, 'eval_runtime': 9.1624, 'eval_samples_per_second': 109.033, 'eval_steps_per_second': 6.876, 'epoch': 0.36}
{'loss': 1.8382, 'grad_norm': 0.08351735770702362, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7923905849456787, 'eval_runtime': 9.1849, 'eval_samples_per_second': 108.766, 'eval_steps_per_second': 6.859, 'epoch': 0.4}
{'loss': 1.7723, 'grad_norm': 0.0918407067656517, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7836236953735352, 'eval_runtime': 9.1776, 'eval_samples_per_second': 108.852, 'eval_steps_per_second': 6.865, 'epoch': 0.44}
{'loss': 1.7688, 'grad_norm': 0.11531567573547363, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7764060497283936, 'eval_runtime': 9.1692, 'eval_samples_per_second': 108.952, 'eval_steps_per_second': 6.871, 'epoch': 0.48}
{'loss': 1.7483, 'grad_norm': 0.08067972958087921, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7696008682250977, 'eval_runtime': 9.1486, 'eval_samples_per_second': 109.197, 'eval_steps_per_second': 6.886, 'epoch': 0.52}
{'loss': 1.7642, 'grad_norm': 0.08825880289077759, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7634522914886475, 'eval_runtime': 9.1908, 'eval_samples_per_second': 108.696, 'eval_steps_per_second': 6.855, 'epoch': 0.56}
{'loss': 1.7945, 'grad_norm': 0.11440275609493256, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7589762210845947, 'eval_runtime': 9.173, 'eval_samples_per_second': 108.907, 'eval_steps_per_second': 6.868, 'epoch': 0.6}
{'loss': 1.7671, 'grad_norm': 0.0837918147444725, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7531273365020752, 'eval_runtime': 9.182, 'eval_samples_per_second': 108.799, 'eval_steps_per_second': 6.861, 'epoch': 0.64}
{'loss': 1.7566, 'grad_norm': 0.07299361377954483, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7484198808670044, 'eval_runtime': 9.1774, 'eval_samples_per_second': 108.855, 'eval_steps_per_second': 6.865, 'epoch': 0.68}
{'loss': 1.7258, 'grad_norm': 0.07605239003896713, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7444915771484375, 'eval_runtime': 9.1659, 'eval_samples_per_second': 108.992, 'eval_steps_per_second': 6.873, 'epoch': 0.72}
{'loss': 1.7237, 'grad_norm': 0.07113239914178848, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7415025234222412, 'eval_runtime': 9.1834, 'eval_samples_per_second': 108.784, 'eval_steps_per_second': 6.86, 'epoch': 0.76}
{'loss': 1.7314, 'grad_norm': 0.14388976991176605, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7382886409759521, 'eval_runtime': 9.2342, 'eval_samples_per_second': 108.184, 'eval_steps_per_second': 6.822, 'epoch': 0.8}
{'loss': 1.7291, 'grad_norm': 0.08842068165540695, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7362760305404663, 'eval_runtime': 9.2022, 'eval_samples_per_second': 108.561, 'eval_steps_per_second': 6.846, 'epoch': 0.84}
{'loss': 1.71, 'grad_norm': 0.09531693905591965, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7328722476959229, 'eval_runtime': 9.1781, 'eval_samples_per_second': 108.846, 'eval_steps_per_second': 6.864, 'epoch': 0.88}
{'loss': 1.7079, 'grad_norm': 0.10239624977111816, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7315706014633179, 'eval_runtime': 9.1694, 'eval_samples_per_second': 108.949, 'eval_steps_per_second': 6.871, 'epoch': 0.92}
{'loss': 1.7219, 'grad_norm': 0.07413020730018616, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7302988767623901, 'eval_runtime': 9.1726, 'eval_samples_per_second': 108.911, 'eval_steps_per_second': 6.868, 'epoch': 0.96}
{'loss': 1.7432, 'grad_norm': 0.08558092266321182, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.730047345161438, 'eval_runtime': 9.1886, 'eval_samples_per_second': 108.721, 'eval_steps_per_second': 6.856, 'epoch': 1.0}
{'train_runtime': 463.895, 'train_samples_per_second': 21.552, 'train_steps_per_second': 1.347, 'train_loss': 1.9927187377929687, 'epoch': 1.0}
train_results:  {'eval_loss': [4.137564659118652, 2.82731032371521, 2.1985318660736084, 2.0126726627349854, 1.9061142206192017, 1.8622409105300903, 1.8327159881591797, 1.8171182870864868, 1.8018908500671387, 1.7923905849456787, 1.7836236953735352, 1.7764060497283936, 1.7696008682250977, 1.7634522914886475, 1.7589762210845947, 1.7531273365020752, 1.7484198808670044, 1.7444915771484375, 1.7415025234222412, 1.7382886409759521, 1.7362760305404663, 1.7328722476959229, 1.7315706014633179, 1.7302988767623901, 1.730047345161438], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:24,  4.72it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:11, 34.00it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 56.79it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 70.35it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 80.52it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 89.50it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 98.54it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 109.38it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 112.08it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 114.75it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 119.31it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 127.99it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 139.42it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 137.58it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 143.61it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 142.44it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 141.26it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 143.94it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 146.71it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 155.06it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 155.02it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 156.01it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 191.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 123.27it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4030667543411255
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2898 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.7326075842512962
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 1.2332513098110522
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 1.043631402917552
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.7625470041410485
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.7482551928577517
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5271, dtype=torch.float64), tensor(0.0572, dtype=torch.float64), tensor(0.3386, dtype=torch.float64), 0, 0, 0, tensor(0.0771, dtype=torch.float64), 1, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734883, 0]
normalized proposed parameters for next round by BO: [tensor(4.6799e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5271, dtype=torch.float64), tensor(0.0572, dtype=torch.float64), tensor(0.3386, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.2977e-18, dtype=torch.float64), tensor(5.0914e-17, dtype=torch.float64), tensor(0.0771, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.527
  sciq: 0.057
  triviaqa: 0.339
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.077

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734883,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734883
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 7,077,888 || all params: 8,037,339,136 || trainable%: 0.0881
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.17it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.21it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.10it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.81it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.22it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.25it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 116/400 [00:00<00:01, 179.27it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 163.85it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 167.96it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 172.44it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.69it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 187.12it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 193.37it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 201.88it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 215.03it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 224.40it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:01<00:00, 241.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.96it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3789, 'grad_norm': 0.06470492482185364, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 4.352750301361084, 'eval_runtime': 8.8012, 'eval_samples_per_second': 113.508, 'eval_steps_per_second': 7.158, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.84it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.20it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.09it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.88it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.77it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.09it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 157.20it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 169.16it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 171.27it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 174.07it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 184.66it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 187.69it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 193.42it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 201.72it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 214.75it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 224.34it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 241.09it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.50it/s]
Evaluation performance at step 50: 0.48
{'loss': 3.9981, 'grad_norm': 0.18983779847621918, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 3.3477041721343994, 'eval_runtime': 8.8381, 'eval_samples_per_second': 113.034, 'eval_steps_per_second': 7.128, 'epoch': 0.08}
{'loss': 3.0452, 'grad_norm': 0.14822277426719666, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.730268955230713, 'eval_runtime': 8.8454, 'eval_samples_per_second': 112.94, 'eval_steps_per_second': 7.122, 'epoch': 0.12}
{'loss': 2.4868, 'grad_norm': 0.07862089574337006, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.3700785636901855, 'eval_runtime': 8.8789, 'eval_samples_per_second': 112.514, 'eval_steps_per_second': 7.095, 'epoch': 0.16}
{'loss': 2.241, 'grad_norm': 0.08579276502132416, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.1956276893615723, 'eval_runtime': 8.9148, 'eval_samples_per_second': 112.061, 'eval_steps_per_second': 7.067, 'epoch': 0.2}
{'loss': 2.1261, 'grad_norm': 0.07872889935970306, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1145520210266113, 'eval_runtime': 8.9304, 'eval_samples_per_second': 111.865, 'eval_steps_per_second': 7.055, 'epoch': 0.24}
{'loss': 2.0704, 'grad_norm': 0.07185837626457214, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0680716037750244, 'eval_runtime': 8.9542, 'eval_samples_per_second': 111.567, 'eval_steps_per_second': 7.036, 'epoch': 0.28}
{'loss': 2.0857, 'grad_norm': 0.07911746203899384, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.0397188663482666, 'eval_runtime': 8.9657, 'eval_samples_per_second': 111.424, 'eval_steps_per_second': 7.027, 'epoch': 0.32}
{'loss': 2.0223, 'grad_norm': 0.06604794412851334, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0185444355010986, 'eval_runtime': 8.9521, 'eval_samples_per_second': 111.594, 'eval_steps_per_second': 7.037, 'epoch': 0.36}
{'loss': 1.9686, 'grad_norm': 0.06275666505098343, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0027694702148438, 'eval_runtime': 8.9604, 'eval_samples_per_second': 111.49, 'eval_steps_per_second': 7.031, 'epoch': 0.4}
{'loss': 1.9719, 'grad_norm': 0.07402437180280685, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9906476736068726, 'eval_runtime': 8.972, 'eval_samples_per_second': 111.346, 'eval_steps_per_second': 7.022, 'epoch': 0.44}
{'loss': 1.9917, 'grad_norm': 0.07346309721469879, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9812158346176147, 'eval_runtime': 8.9721, 'eval_samples_per_second': 111.346, 'eval_steps_per_second': 7.022, 'epoch': 0.48}
{'loss': 1.9641, 'grad_norm': 0.09320738166570663, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.973106861114502, 'eval_runtime': 8.9761, 'eval_samples_per_second': 111.295, 'eval_steps_per_second': 7.019, 'epoch': 0.52}
{'loss': 1.963, 'grad_norm': 0.09229674935340881, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9667670726776123, 'eval_runtime': 8.9658, 'eval_samples_per_second': 111.423, 'eval_steps_per_second': 7.027, 'epoch': 0.56}
{'loss': 1.9717, 'grad_norm': 0.08483313769102097, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9609342813491821, 'eval_runtime': 8.9792, 'eval_samples_per_second': 111.257, 'eval_steps_per_second': 7.016, 'epoch': 0.6}
{'loss': 1.9461, 'grad_norm': 0.11186765134334564, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.955613136291504, 'eval_runtime': 9.0158, 'eval_samples_per_second': 110.806, 'eval_steps_per_second': 6.988, 'epoch': 0.64}
{'loss': 1.9467, 'grad_norm': 0.07038318365812302, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.951328992843628, 'eval_runtime': 8.9832, 'eval_samples_per_second': 111.208, 'eval_steps_per_second': 7.013, 'epoch': 0.68}
{'loss': 1.9394, 'grad_norm': 0.08674897998571396, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9471888542175293, 'eval_runtime': 8.9912, 'eval_samples_per_second': 111.109, 'eval_steps_per_second': 7.007, 'epoch': 0.72}
{'loss': 1.9429, 'grad_norm': 0.08086127042770386, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9446864128112793, 'eval_runtime': 8.9551, 'eval_samples_per_second': 111.557, 'eval_steps_per_second': 7.035, 'epoch': 0.76}
{'loss': 1.9207, 'grad_norm': 0.07473276555538177, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.941376805305481, 'eval_runtime': 8.9475, 'eval_samples_per_second': 111.651, 'eval_steps_per_second': 7.041, 'epoch': 0.8}
{'loss': 1.919, 'grad_norm': 0.09101960808038712, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9395051002502441, 'eval_runtime': 8.9435, 'eval_samples_per_second': 111.701, 'eval_steps_per_second': 7.044, 'epoch': 0.84}
{'loss': 1.9063, 'grad_norm': 0.07151590287685394, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9376742839813232, 'eval_runtime': 8.9462, 'eval_samples_per_second': 111.668, 'eval_steps_per_second': 7.042, 'epoch': 0.88}
{'loss': 1.9114, 'grad_norm': 0.09389259666204453, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9363282918930054, 'eval_runtime': 8.9525, 'eval_samples_per_second': 111.589, 'eval_steps_per_second': 7.037, 'epoch': 0.92}
{'loss': 1.9002, 'grad_norm': 0.0677124559879303, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9357293844223022, 'eval_runtime': 8.9538, 'eval_samples_per_second': 111.572, 'eval_steps_per_second': 7.036, 'epoch': 0.96}
{'loss': 1.9241, 'grad_norm': 0.07842039316892624, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.935599446296692, 'eval_runtime': 8.9563, 'eval_samples_per_second': 111.542, 'eval_steps_per_second': 7.034, 'epoch': 1.0}
{'train_runtime': 356.2848, 'train_samples_per_second': 28.062, 'train_steps_per_second': 1.754, 'train_loss': 2.2217001342773437, 'epoch': 1.0}
train_results:  {'eval_loss': [4.352750301361084, 3.3477041721343994, 2.730268955230713, 2.3700785636901855, 2.1956276893615723, 2.1145520210266113, 2.0680716037750244, 2.0397188663482666, 2.0185444355010986, 2.0027694702148438, 1.9906476736068726, 1.9812158346176147, 1.973106861114502, 1.9667670726776123, 1.9609342813491821, 1.955613136291504, 1.951328992843628, 1.9471888542175293, 1.9446864128112793, 1.941376805305481, 1.9395051002502441, 1.9376742839813232, 1.9363282918930054, 1.9357293844223022, 1.935599446296692], 'performance': [0.49, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:46,  1.76it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:14, 26.35it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 47.76it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 62.66it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 75.11it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 85.23it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 95.35it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 107.36it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 111.33it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 114.93it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:01, 120.02it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 129.49it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 142.41it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 140.82it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 146.96it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 145.82it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 145.49it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 148.42it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 150.77it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 159.94it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 344/400 [00:03<00:00, 162.56it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 377/400 [00:03<00:00, 210.38it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 399/400 [00:03<00:00, 194.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 116.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.3090580701828003
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5063 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.791082396176449
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.6373187999449623
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.7703146086914039
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.9792188018164283
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.7081054781076239
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4709, dtype=torch.float64), 0, tensor(0.0235, dtype=torch.float64), 0, 0, 0, tensor(0.5056, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(2.1866e-17, dtype=torch.float64), tensor(3.0320e-17, dtype=torch.float64), tensor(0.4709, dtype=torch.float64), tensor(2.0812e-17, dtype=torch.float64), tensor(0.0235, dtype=torch.float64), tensor(1.7384e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2913e-17, dtype=torch.float64), tensor(0.5056, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.471
  sciq: 0
  triviaqa: 0.023
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.506

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.30it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.23it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.47it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 110.04it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.90it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.23it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.41it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 138.06it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 138.16it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.55it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 141.21it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.91it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 151.23it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 152.16it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.77it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.34it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 160.74it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 173.33it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 182.76it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 187.41it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 198.54it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.30it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.9153, 'grad_norm': 0.1818820834159851, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.013733386993408, 'eval_runtime': 10.6604, 'eval_samples_per_second': 93.711, 'eval_steps_per_second': 5.91, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.09it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.94it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.99it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.38it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.13it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.54it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.69it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.40it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.40it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.42it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.18it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.25it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.78it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.84it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.47it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.17it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.56it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.49it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.23it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.26it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.79it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.94it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.3111, 'grad_norm': 0.20098203420639038, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.7993940114974976, 'eval_runtime': 10.6625, 'eval_samples_per_second': 93.693, 'eval_steps_per_second': 5.909, 'epoch': 0.08}
{'loss': 1.6217, 'grad_norm': 0.06318548321723938, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5706120729446411, 'eval_runtime': 10.7264, 'eval_samples_per_second': 93.135, 'eval_steps_per_second': 5.873, 'epoch': 0.12}
{'loss': 1.4965, 'grad_norm': 0.043190229684114456, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4744651317596436, 'eval_runtime': 10.7998, 'eval_samples_per_second': 92.501, 'eval_steps_per_second': 5.833, 'epoch': 0.16}
{'loss': 1.4244, 'grad_norm': 0.040148116648197174, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4499223232269287, 'eval_runtime': 10.8598, 'eval_samples_per_second': 91.991, 'eval_steps_per_second': 5.801, 'epoch': 0.2}
{'loss': 1.4228, 'grad_norm': 0.03933889418840408, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4364055395126343, 'eval_runtime': 10.8787, 'eval_samples_per_second': 91.831, 'eval_steps_per_second': 5.791, 'epoch': 0.24}
{'loss': 1.4234, 'grad_norm': 0.043715860694646835, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4236807823181152, 'eval_runtime': 10.8885, 'eval_samples_per_second': 91.748, 'eval_steps_per_second': 5.786, 'epoch': 0.28}
{'loss': 1.4009, 'grad_norm': 0.03750380128622055, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.413057804107666, 'eval_runtime': 10.878, 'eval_samples_per_second': 91.837, 'eval_steps_per_second': 5.792, 'epoch': 0.32}
{'loss': 1.425, 'grad_norm': 0.04307050630450249, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4060533046722412, 'eval_runtime': 10.901, 'eval_samples_per_second': 91.643, 'eval_steps_per_second': 5.779, 'epoch': 0.36}
{'loss': 1.3566, 'grad_norm': 0.03626313805580139, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3969824314117432, 'eval_runtime': 10.8886, 'eval_samples_per_second': 91.747, 'eval_steps_per_second': 5.786, 'epoch': 0.4}
{'loss': 1.4165, 'grad_norm': 0.04169759526848793, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3883079290390015, 'eval_runtime': 10.8099, 'eval_samples_per_second': 92.415, 'eval_steps_per_second': 5.828, 'epoch': 0.44}
{'loss': 1.3504, 'grad_norm': 0.044062718749046326, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.380599021911621, 'eval_runtime': 10.8121, 'eval_samples_per_second': 92.397, 'eval_steps_per_second': 5.827, 'epoch': 0.48}
{'loss': 1.3795, 'grad_norm': 0.04075055941939354, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3768161535263062, 'eval_runtime': 10.8149, 'eval_samples_per_second': 92.373, 'eval_steps_per_second': 5.825, 'epoch': 0.52}
{'loss': 1.3396, 'grad_norm': 0.041716691106557846, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3708237409591675, 'eval_runtime': 10.7598, 'eval_samples_per_second': 92.846, 'eval_steps_per_second': 5.855, 'epoch': 0.56}
{'loss': 1.3321, 'grad_norm': 0.04398118332028389, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3631802797317505, 'eval_runtime': 10.7556, 'eval_samples_per_second': 92.881, 'eval_steps_per_second': 5.857, 'epoch': 0.6}
{'loss': 1.3955, 'grad_norm': 0.0452934131026268, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3562712669372559, 'eval_runtime': 10.7499, 'eval_samples_per_second': 92.931, 'eval_steps_per_second': 5.861, 'epoch': 0.64}
{'loss': 1.3304, 'grad_norm': 0.05439727008342743, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3507827520370483, 'eval_runtime': 10.7556, 'eval_samples_per_second': 92.882, 'eval_steps_per_second': 5.857, 'epoch': 0.68}
{'loss': 1.3274, 'grad_norm': 0.05155560001730919, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3432682752609253, 'eval_runtime': 10.744, 'eval_samples_per_second': 92.982, 'eval_steps_per_second': 5.864, 'epoch': 0.72}
{'loss': 1.3441, 'grad_norm': 0.05321398377418518, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.336995244026184, 'eval_runtime': 10.7452, 'eval_samples_per_second': 92.972, 'eval_steps_per_second': 5.863, 'epoch': 0.76}
{'loss': 1.3337, 'grad_norm': 0.05910652130842209, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3327480554580688, 'eval_runtime': 10.7514, 'eval_samples_per_second': 92.918, 'eval_steps_per_second': 5.86, 'epoch': 0.8}
{'loss': 1.3924, 'grad_norm': 0.05431210994720459, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3270593881607056, 'eval_runtime': 10.7936, 'eval_samples_per_second': 92.555, 'eval_steps_per_second': 5.837, 'epoch': 0.84}
{'loss': 1.3355, 'grad_norm': 0.05340536683797836, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3227986097335815, 'eval_runtime': 10.7732, 'eval_samples_per_second': 92.73, 'eval_steps_per_second': 5.848, 'epoch': 0.88}
{'loss': 1.2891, 'grad_norm': 0.05718519911170006, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3195397853851318, 'eval_runtime': 10.7572, 'eval_samples_per_second': 92.868, 'eval_steps_per_second': 5.857, 'epoch': 0.92}
{'loss': 1.3434, 'grad_norm': 0.05675289407372475, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.317899227142334, 'eval_runtime': 10.7574, 'eval_samples_per_second': 92.866, 'eval_steps_per_second': 5.856, 'epoch': 0.96}
{'loss': 1.3307, 'grad_norm': 0.056760285049676895, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3173153400421143, 'eval_runtime': 10.8272, 'eval_samples_per_second': 92.268, 'eval_steps_per_second': 5.819, 'epoch': 1.0}
{'train_runtime': 547.8213, 'train_samples_per_second': 18.252, 'train_steps_per_second': 1.141, 'train_loss': 1.5215219177246093, 'epoch': 1.0}
train_results:  {'eval_loss': [3.013733386993408, 1.7993940114974976, 1.5706120729446411, 1.4744651317596436, 1.4499223232269287, 1.4364055395126343, 1.4236807823181152, 1.413057804107666, 1.4060533046722412, 1.3969824314117432, 1.3883079290390015, 1.380599021911621, 1.3768161535263062, 1.3708237409591675, 1.3631802797317505, 1.3562712669372559, 1.3507827520370483, 1.3432682752609253, 1.336995244026184, 1.3327480554580688, 1.3270593881607056, 1.3227986097335815, 1.3195397853851318, 1.317899227142334, 1.3173153400421143], 'performance': [0.48, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:40,  2.49it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 34.85it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 54.46it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 65.05it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 72.83it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 79.26it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 86.18it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 94.77it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.65it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 98.39it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 101.97it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 108.80it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 118.76it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 116.87it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 121.98it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 120.76it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 120.04it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 122.36it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 124.70it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 131.96it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.01it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 132.51it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 142.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 106.11it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.420567274093628
current iteration best possible performance (full train run):  0.504
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2298 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.9183226597834409
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.815581753749326
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.674028493665708
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.6344905696586437
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 1.1265302503237544
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1156, dtype=torch.float64), tensor(0.4797, dtype=torch.float64), 0, 0, tensor(0.0920, dtype=torch.float64), 0, 0, tensor(0.3127, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 128, 0.09999999999999999, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1.2254e-17, dtype=torch.float64), tensor(0.1156, dtype=torch.float64), tensor(0.4797, dtype=torch.float64), tensor(4.0585e-17, dtype=torch.float64), tensor(4.1152e-05, dtype=torch.float64), tensor(0.0920, dtype=torch.float64), tensor(6.6189e-20, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3127, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.116
  rowan_hellaswag: 0.48
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.092
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.313

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999999,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.09999999999999999
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.93it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.75it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.65it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.10it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.88it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.11it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.32it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.67it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.75it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.34it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.03it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.74it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.09it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.10it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.57it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.33it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 280/400 [00:02<00:00, 168.95it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 170.37it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 180.70it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 185.13it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 195.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.06it/s]
Evaluation performance at step 25: 0.51
{'loss': 3.8276, 'grad_norm': 0.17464382946491241, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.51}
{'eval_loss': 2.97929048538208, 'eval_runtime': 10.7769, 'eval_samples_per_second': 92.698, 'eval_steps_per_second': 5.846, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.85it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.52it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.48it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.12it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.96it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.36it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.56it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.11it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.63it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.11it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.87it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.75it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.04it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.93it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.38it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.16it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 159.62it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 172.56it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 182.17it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 186.83it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 197.96it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.26it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.3425, 'grad_norm': 0.15482757985591888, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.8695502281188965, 'eval_runtime': 10.823, 'eval_samples_per_second': 92.303, 'eval_steps_per_second': 5.821, 'epoch': 0.08}
{'loss': 1.6437, 'grad_norm': 0.05173071101307869, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.603698968887329, 'eval_runtime': 10.8613, 'eval_samples_per_second': 91.978, 'eval_steps_per_second': 5.8, 'epoch': 0.12}
{'loss': 1.5313, 'grad_norm': 0.045209549367427826, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.497564673423767, 'eval_runtime': 10.8976, 'eval_samples_per_second': 91.671, 'eval_steps_per_second': 5.781, 'epoch': 0.16}
{'loss': 1.4166, 'grad_norm': 0.04882490634918213, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.468064308166504, 'eval_runtime': 10.9096, 'eval_samples_per_second': 91.571, 'eval_steps_per_second': 5.775, 'epoch': 0.2}
{'loss': 1.4562, 'grad_norm': 0.0412481427192688, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4528213739395142, 'eval_runtime': 10.9078, 'eval_samples_per_second': 91.585, 'eval_steps_per_second': 5.776, 'epoch': 0.24}
{'loss': 1.4924, 'grad_norm': 0.04060683026909828, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.441241979598999, 'eval_runtime': 10.907, 'eval_samples_per_second': 91.592, 'eval_steps_per_second': 5.776, 'epoch': 0.28}
{'loss': 1.4216, 'grad_norm': 0.03836855664849281, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.431483507156372, 'eval_runtime': 10.8916, 'eval_samples_per_second': 91.722, 'eval_steps_per_second': 5.784, 'epoch': 0.32}
{'loss': 1.3863, 'grad_norm': 0.03793834149837494, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4255555868148804, 'eval_runtime': 10.8839, 'eval_samples_per_second': 91.787, 'eval_steps_per_second': 5.788, 'epoch': 0.36}
{'loss': 1.3899, 'grad_norm': 0.039534132927656174, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4189871549606323, 'eval_runtime': 10.9017, 'eval_samples_per_second': 91.637, 'eval_steps_per_second': 5.779, 'epoch': 0.4}
{'loss': 1.3708, 'grad_norm': 0.038664303719997406, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4130783081054688, 'eval_runtime': 10.8765, 'eval_samples_per_second': 91.85, 'eval_steps_per_second': 5.792, 'epoch': 0.44}
{'loss': 1.3962, 'grad_norm': 0.039957787841558456, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.407546043395996, 'eval_runtime': 10.8255, 'eval_samples_per_second': 92.282, 'eval_steps_per_second': 5.82, 'epoch': 0.48}
{'loss': 1.4063, 'grad_norm': 0.03919651731848717, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4026705026626587, 'eval_runtime': 10.7735, 'eval_samples_per_second': 92.728, 'eval_steps_per_second': 5.848, 'epoch': 0.52}
{'loss': 1.363, 'grad_norm': 0.03825371712446213, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3992969989776611, 'eval_runtime': 10.7744, 'eval_samples_per_second': 92.72, 'eval_steps_per_second': 5.847, 'epoch': 0.56}
{'loss': 1.4138, 'grad_norm': 0.0400724858045578, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3956241607666016, 'eval_runtime': 10.7786, 'eval_samples_per_second': 92.683, 'eval_steps_per_second': 5.845, 'epoch': 0.6}
{'loss': 1.3283, 'grad_norm': 0.03618227690458298, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3914707899093628, 'eval_runtime': 10.8274, 'eval_samples_per_second': 92.266, 'eval_steps_per_second': 5.819, 'epoch': 0.64}
{'loss': 1.3883, 'grad_norm': 0.0429975725710392, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3874835968017578, 'eval_runtime': 10.8164, 'eval_samples_per_second': 92.36, 'eval_steps_per_second': 5.825, 'epoch': 0.68}
{'loss': 1.3512, 'grad_norm': 0.03898191079497337, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3849198818206787, 'eval_runtime': 10.7793, 'eval_samples_per_second': 92.678, 'eval_steps_per_second': 5.845, 'epoch': 0.72}
{'loss': 1.403, 'grad_norm': 0.03609873354434967, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3827862739562988, 'eval_runtime': 10.7883, 'eval_samples_per_second': 92.601, 'eval_steps_per_second': 5.84, 'epoch': 0.76}
{'loss': 1.3475, 'grad_norm': 0.05470970273017883, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3812291622161865, 'eval_runtime': 10.7861, 'eval_samples_per_second': 92.619, 'eval_steps_per_second': 5.841, 'epoch': 0.8}
{'loss': 1.376, 'grad_norm': 0.045147854834795, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3774328231811523, 'eval_runtime': 10.7565, 'eval_samples_per_second': 92.874, 'eval_steps_per_second': 5.857, 'epoch': 0.84}
{'loss': 1.3038, 'grad_norm': 0.044914815574884415, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.375651478767395, 'eval_runtime': 10.7588, 'eval_samples_per_second': 92.854, 'eval_steps_per_second': 5.856, 'epoch': 0.88}
{'loss': 1.3688, 'grad_norm': 0.040244538336992264, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3742223978042603, 'eval_runtime': 10.7534, 'eval_samples_per_second': 92.901, 'eval_steps_per_second': 5.859, 'epoch': 0.92}
{'loss': 1.352, 'grad_norm': 0.0412224680185318, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3733938932418823, 'eval_runtime': 10.7488, 'eval_samples_per_second': 92.94, 'eval_steps_per_second': 5.861, 'epoch': 0.96}
{'loss': 1.4033, 'grad_norm': 0.047472160309553146, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.372764229774475, 'eval_runtime': 10.7476, 'eval_samples_per_second': 92.951, 'eval_steps_per_second': 5.862, 'epoch': 1.0}
{'train_runtime': 562.7976, 'train_samples_per_second': 17.763, 'train_steps_per_second': 1.111, 'train_loss': 1.5392145446777343, 'epoch': 1.0}
train_results:  {'eval_loss': [2.97929048538208, 1.8695502281188965, 1.603698968887329, 1.497564673423767, 1.468064308166504, 1.4528213739395142, 1.441241979598999, 1.431483507156372, 1.4255555868148804, 1.4189871549606323, 1.4130783081054688, 1.407546043395996, 1.4026705026626587, 1.3992969989776611, 1.3956241607666016, 1.3914707899093628, 1.3874835968017578, 1.3849198818206787, 1.3827862739562988, 1.3812291622161865, 1.3774328231811523, 1.375651478767395, 1.3742223978042603, 1.3733938932418823, 1.372764229774475], 'performance': [0.51, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:30,  2.64it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 36.35it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 56.11it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 66.55it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 74.29it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 80.55it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.38it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 96.07it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 97.77it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 99.38it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 102.95it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 110.06it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 119.38it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.76it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.95it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 121.97it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 121.23it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 123.54it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.96it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 133.43it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 133.02it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 133.37it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 143.11it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 107.64it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.51, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.420900821685791
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2729 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.6098649751974811
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.5750973945551947
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.6745864266798155
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.850403761076557
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.6855743104377987
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4689, dtype=torch.float64), tensor(0.5311, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.7947e-17, dtype=torch.float64), tensor(0.4689, dtype=torch.float64), tensor(0.5311, dtype=torch.float64), tensor(1.3267e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1579e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.4694e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.469
  sciq: 0.531
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.46it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.42it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.65it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 110.26it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 115.13it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.66it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.49it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.98it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.95it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.34it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.69it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.67it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 151.15it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 152.24it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.75it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.44it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.66it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.52it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.44it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.30it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.43it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3009, 'grad_norm': 0.19397953152656555, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3413021564483643, 'eval_runtime': 10.6324, 'eval_samples_per_second': 93.958, 'eval_steps_per_second': 5.925, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.67it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.14it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.25it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.00it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.84it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.45it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.42it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 135.35it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 135.22it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 136.48it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 137.99it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 144.53it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 148.14it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 149.52it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 154.28it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.25it/s]Running loglikelihood requests:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 284/400 [00:02<00:00, 179.62it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 167.08it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 178.35it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 184.20it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 195.64it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.31it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.5302, 'grad_norm': 0.19784878194332123, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 1.9542454481124878, 'eval_runtime': 10.7029, 'eval_samples_per_second': 93.339, 'eval_steps_per_second': 5.886, 'epoch': 0.08}
{'loss': 1.7428, 'grad_norm': 0.07118053734302521, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.682349443435669, 'eval_runtime': 10.7426, 'eval_samples_per_second': 92.994, 'eval_steps_per_second': 5.865, 'epoch': 0.12}
{'loss': 1.6112, 'grad_norm': 0.04609755799174309, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5707786083221436, 'eval_runtime': 10.8068, 'eval_samples_per_second': 92.442, 'eval_steps_per_second': 5.83, 'epoch': 0.16}
{'loss': 1.527, 'grad_norm': 0.037439655512571335, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5490682125091553, 'eval_runtime': 10.8432, 'eval_samples_per_second': 92.131, 'eval_steps_per_second': 5.81, 'epoch': 0.2}
{'loss': 1.5277, 'grad_norm': 0.04589735344052315, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5342251062393188, 'eval_runtime': 10.8326, 'eval_samples_per_second': 92.222, 'eval_steps_per_second': 5.816, 'epoch': 0.24}
{'loss': 1.5157, 'grad_norm': 0.044944051653146744, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.524841547012329, 'eval_runtime': 10.8477, 'eval_samples_per_second': 92.093, 'eval_steps_per_second': 5.808, 'epoch': 0.28}
{'loss': 1.5193, 'grad_norm': 0.03786732256412506, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.515210747718811, 'eval_runtime': 10.8158, 'eval_samples_per_second': 92.365, 'eval_steps_per_second': 5.825, 'epoch': 0.32}
{'loss': 1.4956, 'grad_norm': 0.048283278942108154, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5125761032104492, 'eval_runtime': 10.794, 'eval_samples_per_second': 92.552, 'eval_steps_per_second': 5.837, 'epoch': 0.36}
{'loss': 1.5041, 'grad_norm': 0.04431966692209244, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5023961067199707, 'eval_runtime': 10.7393, 'eval_samples_per_second': 93.023, 'eval_steps_per_second': 5.866, 'epoch': 0.4}
{'loss': 1.527, 'grad_norm': 0.041043609380722046, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4958209991455078, 'eval_runtime': 10.712, 'eval_samples_per_second': 93.26, 'eval_steps_per_second': 5.881, 'epoch': 0.44}
{'loss': 1.5168, 'grad_norm': 0.047004882246255875, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4912232160568237, 'eval_runtime': 10.7602, 'eval_samples_per_second': 92.842, 'eval_steps_per_second': 5.855, 'epoch': 0.48}
{'loss': 1.5342, 'grad_norm': 0.04279444366693497, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4868923425674438, 'eval_runtime': 10.7485, 'eval_samples_per_second': 92.943, 'eval_steps_per_second': 5.861, 'epoch': 0.52}
{'loss': 1.5057, 'grad_norm': 0.04573408514261246, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4836699962615967, 'eval_runtime': 10.7549, 'eval_samples_per_second': 92.888, 'eval_steps_per_second': 5.858, 'epoch': 0.56}
{'loss': 1.4794, 'grad_norm': 0.04895073547959328, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.479610800743103, 'eval_runtime': 10.7668, 'eval_samples_per_second': 92.785, 'eval_steps_per_second': 5.851, 'epoch': 0.6}
{'loss': 1.5095, 'grad_norm': 0.043251488357782364, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4768257141113281, 'eval_runtime': 10.6774, 'eval_samples_per_second': 93.562, 'eval_steps_per_second': 5.9, 'epoch': 0.64}
{'loss': 1.4705, 'grad_norm': 0.05190988630056381, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4744889736175537, 'eval_runtime': 10.6768, 'eval_samples_per_second': 93.567, 'eval_steps_per_second': 5.901, 'epoch': 0.68}
{'loss': 1.4877, 'grad_norm': 0.04404616355895996, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.470792531967163, 'eval_runtime': 10.6738, 'eval_samples_per_second': 93.593, 'eval_steps_per_second': 5.902, 'epoch': 0.72}
{'loss': 1.5228, 'grad_norm': 0.0462348610162735, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4683955907821655, 'eval_runtime': 10.6793, 'eval_samples_per_second': 93.545, 'eval_steps_per_second': 5.899, 'epoch': 0.76}
{'loss': 1.475, 'grad_norm': 0.04743538796901703, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4655824899673462, 'eval_runtime': 10.6799, 'eval_samples_per_second': 93.54, 'eval_steps_per_second': 5.899, 'epoch': 0.8}
{'loss': 1.483, 'grad_norm': 0.04930957034230232, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4644688367843628, 'eval_runtime': 10.6792, 'eval_samples_per_second': 93.547, 'eval_steps_per_second': 5.899, 'epoch': 0.84}
{'loss': 1.499, 'grad_norm': 0.04096827283501625, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4626097679138184, 'eval_runtime': 10.6787, 'eval_samples_per_second': 93.55, 'eval_steps_per_second': 5.9, 'epoch': 0.88}
{'loss': 1.4707, 'grad_norm': 0.04491645470261574, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4615033864974976, 'eval_runtime': 10.6818, 'eval_samples_per_second': 93.523, 'eval_steps_per_second': 5.898, 'epoch': 0.92}
{'loss': 1.4983, 'grad_norm': 0.04035874083638191, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4606293439865112, 'eval_runtime': 10.6758, 'eval_samples_per_second': 93.576, 'eval_steps_per_second': 5.901, 'epoch': 0.96}
{'loss': 1.5188, 'grad_norm': 0.04531670734286308, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4604411125183105, 'eval_runtime': 10.6851, 'eval_samples_per_second': 93.495, 'eval_steps_per_second': 5.896, 'epoch': 1.0}
{'train_runtime': 545.8283, 'train_samples_per_second': 18.319, 'train_steps_per_second': 1.145, 'train_loss': 1.6709178344726563, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3413021564483643, 1.9542454481124878, 1.682349443435669, 1.5707786083221436, 1.5490682125091553, 1.5342251062393188, 1.524841547012329, 1.515210747718811, 1.5125761032104492, 1.5023961067199707, 1.4958209991455078, 1.4912232160568237, 1.4868923425674438, 1.4836699962615967, 1.479610800743103, 1.4768257141113281, 1.4744889736175537, 1.470792531967163, 1.4683955907821655, 1.4655824899673462, 1.4644688367843628, 1.4626097679138184, 1.4615033864974976, 1.4606293439865112, 1.4604411125183105], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<04:25,  1.50it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:15, 24.89it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:08, 43.51it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 55.90it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 65.68it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 73.89it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 82.15it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 92.03it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 94.96it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 97.20it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 101.11it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 108.62it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 118.06it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 116.44it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 121.81it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 120.85it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 120.41it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 122.63it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.13it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.68it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.81it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 133.49it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 143.48it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.53it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4245996475219727
current iteration best possible performance (full train run):  0.525
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5419 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 1.0129566666378134
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 1.0129107875414158
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 1.1668594044021976
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 1.0161177688025507
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.9583758257573499
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4655, dtype=torch.float64), 0, 0, tensor(0.5345, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.480000019073489, 0]
normalized proposed parameters for next round by BO: [tensor(2.1266e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4655, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.1374e-17, dtype=torch.float64), tensor(0.5345, dtype=torch.float64), tensor(4.7501e-17, dtype=torch.float64), tensor(1.5422e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.466
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.534
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073489,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.480000019073489
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.99it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.03it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.07it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.61it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.47it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.69it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.08it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.62it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.57it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.73it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.30it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.85it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.00it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.88it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.33it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.86it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 159.91it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 172.32it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 181.82it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 186.45it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 197.17it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.36it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.2308, 'grad_norm': 0.18779100477695465, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.312539577484131, 'eval_runtime': 10.713, 'eval_samples_per_second': 93.251, 'eval_steps_per_second': 5.881, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.93it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.50it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.38it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.88it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.61it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.55it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.81it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.01it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.18it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 137.79it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.38it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.13it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 149.48it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.44it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 154.86it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.60it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 279/400 [00:02<00:00, 165.47it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 170.79it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 180.71it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 185.51it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 196.66it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.75it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.5231, 'grad_norm': 0.1726652830839157, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.9366590976715088, 'eval_runtime': 10.7141, 'eval_samples_per_second': 93.242, 'eval_steps_per_second': 5.88, 'epoch': 0.08}
{'loss': 1.7639, 'grad_norm': 0.07561143487691879, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6658399105072021, 'eval_runtime': 10.7421, 'eval_samples_per_second': 92.998, 'eval_steps_per_second': 5.865, 'epoch': 0.12}
{'loss': 1.6081, 'grad_norm': 0.04927152395248413, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5563960075378418, 'eval_runtime': 10.7736, 'eval_samples_per_second': 92.727, 'eval_steps_per_second': 5.848, 'epoch': 0.16}
{'loss': 1.5037, 'grad_norm': 0.04512079805135727, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5282291173934937, 'eval_runtime': 10.8168, 'eval_samples_per_second': 92.356, 'eval_steps_per_second': 5.824, 'epoch': 0.2}
{'loss': 1.508, 'grad_norm': 0.0421287827193737, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5136897563934326, 'eval_runtime': 10.8188, 'eval_samples_per_second': 92.34, 'eval_steps_per_second': 5.823, 'epoch': 0.24}
{'loss': 1.4612, 'grad_norm': 0.04900427907705307, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4994759559631348, 'eval_runtime': 10.8049, 'eval_samples_per_second': 92.458, 'eval_steps_per_second': 5.831, 'epoch': 0.28}
{'loss': 1.4768, 'grad_norm': 0.04191204160451889, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4868834018707275, 'eval_runtime': 10.768, 'eval_samples_per_second': 92.774, 'eval_steps_per_second': 5.851, 'epoch': 0.32}
{'loss': 1.4633, 'grad_norm': 0.04833703860640526, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4790228605270386, 'eval_runtime': 10.6977, 'eval_samples_per_second': 93.384, 'eval_steps_per_second': 5.889, 'epoch': 0.36}
{'loss': 1.4643, 'grad_norm': 0.043239131569862366, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4674361944198608, 'eval_runtime': 10.7078, 'eval_samples_per_second': 93.296, 'eval_steps_per_second': 5.884, 'epoch': 0.4}
{'loss': 1.4626, 'grad_norm': 0.05222775787115097, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4569060802459717, 'eval_runtime': 10.7056, 'eval_samples_per_second': 93.316, 'eval_steps_per_second': 5.885, 'epoch': 0.44}
{'loss': 1.4535, 'grad_norm': 0.0435970164835453, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4473869800567627, 'eval_runtime': 10.7176, 'eval_samples_per_second': 93.211, 'eval_steps_per_second': 5.878, 'epoch': 0.48}
{'loss': 1.4723, 'grad_norm': 0.0631525069475174, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4378819465637207, 'eval_runtime': 10.7189, 'eval_samples_per_second': 93.2, 'eval_steps_per_second': 5.877, 'epoch': 0.52}
{'loss': 1.4241, 'grad_norm': 0.05983801558613777, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4306625127792358, 'eval_runtime': 10.7143, 'eval_samples_per_second': 93.24, 'eval_steps_per_second': 5.88, 'epoch': 0.56}
{'loss': 1.4369, 'grad_norm': 0.05226637050509453, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4192967414855957, 'eval_runtime': 10.7148, 'eval_samples_per_second': 93.236, 'eval_steps_per_second': 5.88, 'epoch': 0.6}
{'loss': 1.4413, 'grad_norm': 0.061818938702344894, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4095377922058105, 'eval_runtime': 10.7138, 'eval_samples_per_second': 93.244, 'eval_steps_per_second': 5.88, 'epoch': 0.64}
{'loss': 1.3975, 'grad_norm': 0.07441888004541397, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.400217056274414, 'eval_runtime': 10.7096, 'eval_samples_per_second': 93.281, 'eval_steps_per_second': 5.883, 'epoch': 0.68}
{'loss': 1.3889, 'grad_norm': 0.05549657344818115, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3943736553192139, 'eval_runtime': 10.6989, 'eval_samples_per_second': 93.374, 'eval_steps_per_second': 5.888, 'epoch': 0.72}
{'loss': 1.39, 'grad_norm': 0.08354208618402481, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3826764822006226, 'eval_runtime': 10.7778, 'eval_samples_per_second': 92.69, 'eval_steps_per_second': 5.845, 'epoch': 0.76}
{'loss': 1.3788, 'grad_norm': 0.08679144829511642, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.374630093574524, 'eval_runtime': 10.8257, 'eval_samples_per_second': 92.28, 'eval_steps_per_second': 5.819, 'epoch': 0.8}
{'loss': 1.4036, 'grad_norm': 0.07329811155796051, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3675323724746704, 'eval_runtime': 10.79, 'eval_samples_per_second': 92.586, 'eval_steps_per_second': 5.839, 'epoch': 0.84}
{'loss': 1.4274, 'grad_norm': 0.07290007919073105, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3618983030319214, 'eval_runtime': 10.7743, 'eval_samples_per_second': 92.72, 'eval_steps_per_second': 5.847, 'epoch': 0.88}
{'loss': 1.3661, 'grad_norm': 0.057779863476753235, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3578088283538818, 'eval_runtime': 10.7165, 'eval_samples_per_second': 93.221, 'eval_steps_per_second': 5.879, 'epoch': 0.92}
{'loss': 1.4147, 'grad_norm': 0.06284908205270767, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.355224609375, 'eval_runtime': 10.7312, 'eval_samples_per_second': 93.093, 'eval_steps_per_second': 5.871, 'epoch': 0.96}
{'loss': 1.4033, 'grad_norm': 0.09201253950595856, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3541604280471802, 'eval_runtime': 10.7676, 'eval_samples_per_second': 92.778, 'eval_steps_per_second': 5.851, 'epoch': 1.0}
{'train_runtime': 546.4768, 'train_samples_per_second': 18.297, 'train_steps_per_second': 1.144, 'train_loss': 1.6105648193359374, 'epoch': 1.0}
train_results:  {'eval_loss': [3.312539577484131, 1.9366590976715088, 1.6658399105072021, 1.5563960075378418, 1.5282291173934937, 1.5136897563934326, 1.4994759559631348, 1.4868834018707275, 1.4790228605270386, 1.4674361944198608, 1.4569060802459717, 1.4473869800567627, 1.4378819465637207, 1.4306625127792358, 1.4192967414855957, 1.4095377922058105, 1.400217056274414, 1.3943736553192139, 1.3826764822006226, 1.374630093574524, 1.3675323724746704, 1.3618983030319214, 1.3578088283538818, 1.355224609375, 1.3541604280471802], 'performance': [0.5, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:22,  1.97it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 27.50it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 46.82it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 58.81it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 68.02it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 75.80it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 83.60it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 93.02it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.53it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 97.78it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 101.54it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 108.86it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 118.80it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.26it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.57it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 121.49it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 120.92it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 123.00it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.18it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.54it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.51it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 132.88it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 142.81it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 102.35it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4239578247070312
current iteration best possible performance (full train run):  0.3885
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6119 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 1.0110332629332275
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 1.1842338707380937
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 1.1450569203818068
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.7187889530846343
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.9919255038513966
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4681, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.5319, dtype=torch.float64), 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(3.8147e-20, dtype=torch.float64), tensor(5.1931e-17, dtype=torch.float64), tensor(0.4681, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5493e-17, dtype=torch.float64), tensor(9.8705e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5319, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.468
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.532

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.30it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.21it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.40it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.87it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.41it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.25it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.31it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 135.70it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 135.81it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 137.29it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.02it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 145.74it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 149.04it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.05it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 154.59it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.21it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 158.79it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 171.65it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 181.40it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 185.81it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 196.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.33it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.9425, 'grad_norm': 0.18531696498394012, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.0078022480010986, 'eval_runtime': 10.7296, 'eval_samples_per_second': 93.107, 'eval_steps_per_second': 5.872, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.86it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.61it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.76it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.34it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.25it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.55it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.77it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.35it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.87it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.39it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.07it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.50it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.54it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.25it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.92it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.16it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.21it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.12it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.05it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.12it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.75it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.3257, 'grad_norm': 0.19810783863067627, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.7875548601150513, 'eval_runtime': 10.6816, 'eval_samples_per_second': 93.525, 'eval_steps_per_second': 5.898, 'epoch': 0.08}
{'loss': 1.6119, 'grad_norm': 0.05071641504764557, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5532329082489014, 'eval_runtime': 10.6854, 'eval_samples_per_second': 93.492, 'eval_steps_per_second': 5.896, 'epoch': 0.12}
{'loss': 1.4778, 'grad_norm': 0.05065459758043289, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4576704502105713, 'eval_runtime': 10.7347, 'eval_samples_per_second': 93.063, 'eval_steps_per_second': 5.869, 'epoch': 0.16}
{'loss': 1.4021, 'grad_norm': 0.049594324082136154, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4331700801849365, 'eval_runtime': 10.75, 'eval_samples_per_second': 92.931, 'eval_steps_per_second': 5.86, 'epoch': 0.2}
{'loss': 1.3866, 'grad_norm': 0.04595428332686424, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4178634881973267, 'eval_runtime': 10.745, 'eval_samples_per_second': 92.973, 'eval_steps_per_second': 5.863, 'epoch': 0.24}
{'loss': 1.3778, 'grad_norm': 0.03863052278757095, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4094123840332031, 'eval_runtime': 10.7678, 'eval_samples_per_second': 92.776, 'eval_steps_per_second': 5.851, 'epoch': 0.28}
{'loss': 1.3546, 'grad_norm': 0.03570001572370529, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.397035002708435, 'eval_runtime': 10.7597, 'eval_samples_per_second': 92.847, 'eval_steps_per_second': 5.855, 'epoch': 0.32}
{'loss': 1.391, 'grad_norm': 0.03728241100907326, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3893046379089355, 'eval_runtime': 10.7668, 'eval_samples_per_second': 92.785, 'eval_steps_per_second': 5.851, 'epoch': 0.36}
{'loss': 1.3248, 'grad_norm': 0.03414708748459816, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3802835941314697, 'eval_runtime': 10.7529, 'eval_samples_per_second': 92.905, 'eval_steps_per_second': 5.859, 'epoch': 0.4}
{'loss': 1.3852, 'grad_norm': 0.03828569874167442, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.370441198348999, 'eval_runtime': 10.769, 'eval_samples_per_second': 92.766, 'eval_steps_per_second': 5.85, 'epoch': 0.44}
{'loss': 1.355, 'grad_norm': 0.04030831158161163, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3643797636032104, 'eval_runtime': 10.7596, 'eval_samples_per_second': 92.847, 'eval_steps_per_second': 5.855, 'epoch': 0.48}
{'loss': 1.3606, 'grad_norm': 0.042279452085494995, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3583146333694458, 'eval_runtime': 10.7915, 'eval_samples_per_second': 92.573, 'eval_steps_per_second': 5.838, 'epoch': 0.52}
{'loss': 1.3659, 'grad_norm': 0.04378724843263626, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.350409746170044, 'eval_runtime': 10.8279, 'eval_samples_per_second': 92.262, 'eval_steps_per_second': 5.818, 'epoch': 0.56}
{'loss': 1.334, 'grad_norm': 0.043965693563222885, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.34262216091156, 'eval_runtime': 10.8913, 'eval_samples_per_second': 91.725, 'eval_steps_per_second': 5.784, 'epoch': 0.6}
{'loss': 1.3809, 'grad_norm': 0.0450160950422287, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.335484504699707, 'eval_runtime': 10.9166, 'eval_samples_per_second': 91.512, 'eval_steps_per_second': 5.771, 'epoch': 0.64}
{'loss': 1.2972, 'grad_norm': 0.05931944027543068, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3298391103744507, 'eval_runtime': 10.9055, 'eval_samples_per_second': 91.605, 'eval_steps_per_second': 5.777, 'epoch': 0.68}
{'loss': 1.3466, 'grad_norm': 0.051787570118904114, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3217815160751343, 'eval_runtime': 10.9262, 'eval_samples_per_second': 91.431, 'eval_steps_per_second': 5.766, 'epoch': 0.72}
{'loss': 1.358, 'grad_norm': 0.0531885027885437, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3155473470687866, 'eval_runtime': 10.9128, 'eval_samples_per_second': 91.544, 'eval_steps_per_second': 5.773, 'epoch': 0.76}
{'loss': 1.3289, 'grad_norm': 0.056287653744220734, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3081748485565186, 'eval_runtime': 10.8444, 'eval_samples_per_second': 92.121, 'eval_steps_per_second': 5.809, 'epoch': 0.8}
{'loss': 1.3227, 'grad_norm': 0.05474049970507622, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3037160634994507, 'eval_runtime': 10.83, 'eval_samples_per_second': 92.244, 'eval_steps_per_second': 5.817, 'epoch': 0.84}
{'loss': 1.3448, 'grad_norm': 0.050652116537094116, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.297959327697754, 'eval_runtime': 10.7696, 'eval_samples_per_second': 92.761, 'eval_steps_per_second': 5.85, 'epoch': 0.88}
{'loss': 1.2901, 'grad_norm': 0.050498150289058685, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2954741716384888, 'eval_runtime': 10.7972, 'eval_samples_per_second': 92.524, 'eval_steps_per_second': 5.835, 'epoch': 0.92}
{'loss': 1.3319, 'grad_norm': 0.052149370312690735, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2926322221755981, 'eval_runtime': 10.8001, 'eval_samples_per_second': 92.499, 'eval_steps_per_second': 5.833, 'epoch': 0.96}
{'loss': 1.3441, 'grad_norm': 0.06433351337909698, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2915478944778442, 'eval_runtime': 10.7688, 'eval_samples_per_second': 92.768, 'eval_steps_per_second': 5.85, 'epoch': 1.0}
{'train_runtime': 558.6792, 'train_samples_per_second': 17.898, 'train_steps_per_second': 1.119, 'train_loss': 1.5096381408691406, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0078022480010986, 1.7875548601150513, 1.5532329082489014, 1.4576704502105713, 1.4331700801849365, 1.4178634881973267, 1.4094123840332031, 1.397035002708435, 1.3893046379089355, 1.3802835941314697, 1.370441198348999, 1.3643797636032104, 1.3583146333694458, 1.350409746170044, 1.34262216091156, 1.335484504699707, 1.3298391103744507, 1.3217815160751343, 1.3155473470687866, 1.3081748485565186, 1.3037160634994507, 1.297959327697754, 1.2954741716384888, 1.2926322221755981, 1.2915478944778442], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:17,  2.91it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 36.15it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 55.85it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 66.33it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 74.07it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 80.33it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.29it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.86it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 97.55it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 99.02it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 102.66it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 109.79it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 119.47it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.75it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.87it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 121.60it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 120.77it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 123.03it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.45it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.85it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.62it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 133.01it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 143.07it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 107.69it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4198942184448242
current iteration best possible performance (full train run):  0.525
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9991 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.9554622339168866
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 1.3163638402125453
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.6367358827551863
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.8243066656938792
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.6182887026263084
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4719, dtype=torch.float64), 0, 0, 0, tensor(0.2066, dtype=torch.float64), 0, tensor(0.3215, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4719, dtype=torch.float64), tensor(5.0175e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2066, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3215, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.472
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.207
  mmlu: 0
  arc_challenge: 0.321

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.27it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.13it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.99it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.34it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.84it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.85it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.11it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.31it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.26it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.71it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.30it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.76it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.93it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.72it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 151.07it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.54it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.59it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.99it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 169.25it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 182.26it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 190.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.85it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.8382, 'grad_norm': 0.1327577531337738, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.0722665786743164, 'eval_runtime': 10.8848, 'eval_samples_per_second': 91.779, 'eval_steps_per_second': 5.788, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.47it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.21it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 99.13it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.49it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 111.13it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 123.01it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.08it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.29it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.18it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.56it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.07it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.36it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.44it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.19it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.57it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.23it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.31it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.71it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 169.13it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 182.17it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.99it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.74it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.4187, 'grad_norm': 0.18342044949531555, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.9868624210357666, 'eval_runtime': 10.9209, 'eval_samples_per_second': 91.476, 'eval_steps_per_second': 5.769, 'epoch': 0.08}
{'loss': 1.8489, 'grad_norm': 0.06353521347045898, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7435708045959473, 'eval_runtime': 10.9199, 'eval_samples_per_second': 91.485, 'eval_steps_per_second': 5.769, 'epoch': 0.12}
{'loss': 1.6388, 'grad_norm': 0.05822061374783516, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6650649309158325, 'eval_runtime': 10.9538, 'eval_samples_per_second': 91.201, 'eval_steps_per_second': 5.751, 'epoch': 0.16}
{'loss': 1.6284, 'grad_norm': 0.0341835618019104, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6225085258483887, 'eval_runtime': 10.9574, 'eval_samples_per_second': 91.171, 'eval_steps_per_second': 5.75, 'epoch': 0.2}
{'loss': 1.6435, 'grad_norm': 0.037470750510692596, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6007330417633057, 'eval_runtime': 10.974, 'eval_samples_per_second': 91.033, 'eval_steps_per_second': 5.741, 'epoch': 0.24}
{'loss': 1.6008, 'grad_norm': 0.045785270631313324, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.583571434020996, 'eval_runtime': 11.0995, 'eval_samples_per_second': 90.004, 'eval_steps_per_second': 5.676, 'epoch': 0.28}
{'loss': 1.5798, 'grad_norm': 0.039743125438690186, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5736126899719238, 'eval_runtime': 11.1306, 'eval_samples_per_second': 89.752, 'eval_steps_per_second': 5.66, 'epoch': 0.32}
{'loss': 1.5763, 'grad_norm': 0.05310773104429245, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5617585182189941, 'eval_runtime': 11.1214, 'eval_samples_per_second': 89.827, 'eval_steps_per_second': 5.665, 'epoch': 0.36}
{'loss': 1.5492, 'grad_norm': 0.03856779262423515, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5538700819015503, 'eval_runtime': 11.0917, 'eval_samples_per_second': 90.067, 'eval_steps_per_second': 5.68, 'epoch': 0.4}
{'loss': 1.5673, 'grad_norm': 0.04261845722794533, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5471394062042236, 'eval_runtime': 11.0982, 'eval_samples_per_second': 90.014, 'eval_steps_per_second': 5.677, 'epoch': 0.44}
{'loss': 1.5633, 'grad_norm': 0.04573335871100426, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5405659675598145, 'eval_runtime': 11.0967, 'eval_samples_per_second': 90.027, 'eval_steps_per_second': 5.677, 'epoch': 0.48}
{'loss': 1.5553, 'grad_norm': 0.04379456490278244, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5353504419326782, 'eval_runtime': 11.0791, 'eval_samples_per_second': 90.17, 'eval_steps_per_second': 5.686, 'epoch': 0.52}
{'loss': 1.5269, 'grad_norm': 0.048485781997442245, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.530400037765503, 'eval_runtime': 11.0342, 'eval_samples_per_second': 90.537, 'eval_steps_per_second': 5.71, 'epoch': 0.56}
{'loss': 1.5231, 'grad_norm': 0.051817405968904495, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5270766019821167, 'eval_runtime': 11.0032, 'eval_samples_per_second': 90.792, 'eval_steps_per_second': 5.726, 'epoch': 0.6}
{'loss': 1.5076, 'grad_norm': 0.053133320063352585, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.522334337234497, 'eval_runtime': 10.9739, 'eval_samples_per_second': 91.034, 'eval_steps_per_second': 5.741, 'epoch': 0.64}
{'loss': 1.5088, 'grad_norm': 0.04736842215061188, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.517716646194458, 'eval_runtime': 10.9599, 'eval_samples_per_second': 91.15, 'eval_steps_per_second': 5.748, 'epoch': 0.68}
{'loss': 1.4942, 'grad_norm': 0.047041043639183044, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5144438743591309, 'eval_runtime': 11.063, 'eval_samples_per_second': 90.301, 'eval_steps_per_second': 5.695, 'epoch': 0.72}
{'loss': 1.5423, 'grad_norm': 0.045715250074863434, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5110970735549927, 'eval_runtime': 11.0461, 'eval_samples_per_second': 90.439, 'eval_steps_per_second': 5.703, 'epoch': 0.76}
{'loss': 1.5421, 'grad_norm': 0.04206280782818794, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5072267055511475, 'eval_runtime': 10.9806, 'eval_samples_per_second': 90.979, 'eval_steps_per_second': 5.737, 'epoch': 0.8}
{'loss': 1.5375, 'grad_norm': 0.042379409074783325, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5048820972442627, 'eval_runtime': 10.9514, 'eval_samples_per_second': 91.221, 'eval_steps_per_second': 5.753, 'epoch': 0.84}
{'loss': 1.5318, 'grad_norm': 0.05184821039438248, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5021206140518188, 'eval_runtime': 10.9508, 'eval_samples_per_second': 91.226, 'eval_steps_per_second': 5.753, 'epoch': 0.88}
{'loss': 1.5236, 'grad_norm': 0.04437161237001419, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5002366304397583, 'eval_runtime': 10.9717, 'eval_samples_per_second': 91.052, 'eval_steps_per_second': 5.742, 'epoch': 0.92}
{'loss': 1.5121, 'grad_norm': 0.059846729040145874, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4996683597564697, 'eval_runtime': 10.9726, 'eval_samples_per_second': 91.045, 'eval_steps_per_second': 5.742, 'epoch': 0.96}
{'loss': 1.5743, 'grad_norm': 0.05096369609236717, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4991183280944824, 'eval_runtime': 10.9536, 'eval_samples_per_second': 91.203, 'eval_steps_per_second': 5.752, 'epoch': 1.0}
{'train_runtime': 564.0052, 'train_samples_per_second': 17.727, 'train_steps_per_second': 1.108, 'train_loss': 1.6933091552734374, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0722665786743164, 1.9868624210357666, 1.7435708045959473, 1.6650649309158325, 1.6225085258483887, 1.6007330417633057, 1.583571434020996, 1.5736126899719238, 1.5617585182189941, 1.5538700819015503, 1.5471394062042236, 1.5405659675598145, 1.5353504419326782, 1.530400037765503, 1.5270766019821167, 1.522334337234497, 1.517716646194458, 1.5144438743591309, 1.5110970735549927, 1.5072267055511475, 1.5048820972442627, 1.5021206140518188, 1.5002366304397583, 1.4996683597564697, 1.4991183280944824], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<08:48,  1.32s/it]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:28, 13.60it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:13, 27.19it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:02<00:09, 38.85it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:02<00:06, 49.62it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:02<00:05, 59.38it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:04, 69.37it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 80.48it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:03, 85.29it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:03<00:02, 89.41it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:03<00:02, 94.69it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:03<00:02, 102.76it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:03<00:01, 113.06it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 112.05it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 117.42it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 116.76it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:04<00:01, 116.42it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:04<00:00, 118.49it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:04<00:00, 121.09it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:04<00:00, 128.46it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 128.43it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 129.09it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 138.63it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 82.13it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4236700534820557
current iteration best possible performance (full train run):  0.525
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242, 1.4236700534820557]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0900 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.9590119358250934
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.8084014574695705
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.5890157651879688
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.9653887465388208
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.9810869172573782
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4713, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5287, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 128, 0.09999999999999998, 1.4800000190734866, 0]
normalized proposed parameters for next round by BO: [tensor(2.6304e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4713, dtype=torch.float64), tensor(3.3059e-18, dtype=torch.float64), tensor(6.2184e-17, dtype=torch.float64), tensor(3.3741e-17, dtype=torch.float64), tensor(9.4168e-18, dtype=torch.float64), tensor(0.5287, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.471
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.529
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999998,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.09999999999999998
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.13it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.94it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.85it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.08it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.62it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.56it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.75it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.00it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.01it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.08it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.19it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.64it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.85it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.78it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.19it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.80it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.88it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.33it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.77it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.90it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.70it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.30it/s]
Evaluation performance at step 25: 0.51
{'loss': 3.7217, 'grad_norm': 0.17538416385650635, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.51}
{'eval_loss': 2.976808786392212, 'eval_runtime': 11.0184, 'eval_samples_per_second': 90.666, 'eval_steps_per_second': 5.718, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.12it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.87it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.73it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.87it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.40it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.14it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.64it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.96it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.21it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.67it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.28it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.80it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.97it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.73it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.08it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.72it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.72it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.13it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.48it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.51it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.25it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.98it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.385, 'grad_norm': 0.1377813071012497, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.9381121397018433, 'eval_runtime': 11.0446, 'eval_samples_per_second': 90.451, 'eval_steps_per_second': 5.704, 'epoch': 0.08}
{'loss': 1.8355, 'grad_norm': 0.05585038289427757, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7149156332015991, 'eval_runtime': 11.0265, 'eval_samples_per_second': 90.6, 'eval_steps_per_second': 5.714, 'epoch': 0.12}
{'loss': 1.6587, 'grad_norm': 0.041348181664943695, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6242878437042236, 'eval_runtime': 11.0475, 'eval_samples_per_second': 90.428, 'eval_steps_per_second': 5.703, 'epoch': 0.16}
{'loss': 1.5648, 'grad_norm': 0.039822641760110855, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5926008224487305, 'eval_runtime': 11.0691, 'eval_samples_per_second': 90.251, 'eval_steps_per_second': 5.692, 'epoch': 0.2}
{'loss': 1.5726, 'grad_norm': 0.0332978256046772, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5791586637496948, 'eval_runtime': 11.0783, 'eval_samples_per_second': 90.176, 'eval_steps_per_second': 5.687, 'epoch': 0.24}
{'loss': 1.5472, 'grad_norm': 0.033635955303907394, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5694576501846313, 'eval_runtime': 11.0696, 'eval_samples_per_second': 90.247, 'eval_steps_per_second': 5.691, 'epoch': 0.28}
{'loss': 1.5586, 'grad_norm': 0.03668314963579178, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.562320590019226, 'eval_runtime': 11.0807, 'eval_samples_per_second': 90.156, 'eval_steps_per_second': 5.686, 'epoch': 0.32}
{'loss': 1.5589, 'grad_norm': 0.03321479633450508, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5533801317214966, 'eval_runtime': 11.0833, 'eval_samples_per_second': 90.135, 'eval_steps_per_second': 5.684, 'epoch': 0.36}
{'loss': 1.5228, 'grad_norm': 0.03426127880811691, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.548351764678955, 'eval_runtime': 11.1536, 'eval_samples_per_second': 89.567, 'eval_steps_per_second': 5.648, 'epoch': 0.4}
{'loss': 1.5392, 'grad_norm': 0.036013998091220856, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.541006088256836, 'eval_runtime': 11.2361, 'eval_samples_per_second': 88.91, 'eval_steps_per_second': 5.607, 'epoch': 0.44}
{'loss': 1.5517, 'grad_norm': 0.03854910284280777, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5353573560714722, 'eval_runtime': 11.1562, 'eval_samples_per_second': 89.547, 'eval_steps_per_second': 5.647, 'epoch': 0.48}
{'loss': 1.5473, 'grad_norm': 0.03865765780210495, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.531995415687561, 'eval_runtime': 11.1706, 'eval_samples_per_second': 89.431, 'eval_steps_per_second': 5.64, 'epoch': 0.52}
{'loss': 1.5609, 'grad_norm': 0.03556204214692116, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5278096199035645, 'eval_runtime': 11.2005, 'eval_samples_per_second': 89.192, 'eval_steps_per_second': 5.625, 'epoch': 0.56}
{'loss': 1.5364, 'grad_norm': 0.043432220816612244, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5243722200393677, 'eval_runtime': 11.179, 'eval_samples_per_second': 89.364, 'eval_steps_per_second': 5.636, 'epoch': 0.6}
{'loss': 1.5262, 'grad_norm': 0.043167855590581894, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5200004577636719, 'eval_runtime': 11.149, 'eval_samples_per_second': 89.604, 'eval_steps_per_second': 5.651, 'epoch': 0.64}
{'loss': 1.5761, 'grad_norm': 0.05868849158287048, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5186240673065186, 'eval_runtime': 11.2147, 'eval_samples_per_second': 89.08, 'eval_steps_per_second': 5.618, 'epoch': 0.68}
{'loss': 1.5442, 'grad_norm': 0.03685393184423447, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5146021842956543, 'eval_runtime': 11.2373, 'eval_samples_per_second': 88.9, 'eval_steps_per_second': 5.606, 'epoch': 0.72}
{'loss': 1.5381, 'grad_norm': 0.040254123508930206, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5127500295639038, 'eval_runtime': 11.2437, 'eval_samples_per_second': 88.85, 'eval_steps_per_second': 5.603, 'epoch': 0.76}
{'loss': 1.5223, 'grad_norm': 0.042068179696798325, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5102394819259644, 'eval_runtime': 11.2382, 'eval_samples_per_second': 88.893, 'eval_steps_per_second': 5.606, 'epoch': 0.8}
{'loss': 1.5536, 'grad_norm': 0.03800791874527931, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5085748434066772, 'eval_runtime': 11.2258, 'eval_samples_per_second': 88.991, 'eval_steps_per_second': 5.612, 'epoch': 0.84}
{'loss': 1.526, 'grad_norm': 0.0421162024140358, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5067968368530273, 'eval_runtime': 11.2158, 'eval_samples_per_second': 89.071, 'eval_steps_per_second': 5.617, 'epoch': 0.88}
{'loss': 1.5268, 'grad_norm': 0.040665071457624435, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5059019327163696, 'eval_runtime': 11.2293, 'eval_samples_per_second': 88.964, 'eval_steps_per_second': 5.61, 'epoch': 0.92}
{'loss': 1.5064, 'grad_norm': 0.03925905376672745, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.504825472831726, 'eval_runtime': 11.2152, 'eval_samples_per_second': 89.075, 'eval_steps_per_second': 5.617, 'epoch': 0.96}
{'loss': 1.4961, 'grad_norm': 0.04634838178753853, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.504428744316101, 'eval_runtime': 11.2274, 'eval_samples_per_second': 88.979, 'eval_steps_per_second': 5.611, 'epoch': 1.0}
{'train_runtime': 575.537, 'train_samples_per_second': 17.373, 'train_steps_per_second': 1.086, 'train_loss': 1.6790775390625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.976808786392212, 1.9381121397018433, 1.7149156332015991, 1.6242878437042236, 1.5926008224487305, 1.5791586637496948, 1.5694576501846313, 1.562320590019226, 1.5533801317214966, 1.548351764678955, 1.541006088256836, 1.5353573560714722, 1.531995415687561, 1.5278096199035645, 1.5243722200393677, 1.5200004577636719, 1.5186240673065186, 1.5146021842956543, 1.5127500295639038, 1.5102394819259644, 1.5085748434066772, 1.5067968368530273, 1.5059019327163696, 1.504825472831726, 1.504428744316101], 'performance': [0.51, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:42,  2.45it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:11, 32.55it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 51.61it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 62.19it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 70.12it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 76.52it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 83.16it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 91.80it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 93.52it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 95.40it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.95it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.83it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.31it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.51it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.50it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.16it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.45it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.30it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.41it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.76it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.77it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.28it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 137.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 102.49it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.51, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4240354299545288
current iteration best possible performance (full train run):  0.525
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242, 1.4236700534820557, 1.4240354299545288]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3340 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.9464971001768747
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 1.1298761202066405
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.9904872647653215
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 1.1197284589092003
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.9763858485489781
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4681, dtype=torch.float64), 0, tensor(0.5319, dtype=torch.float64), 0, 0, 0, 0, 32, 0, 1, 1, 0, 1, 128, 0.09999999999999999, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4681, dtype=torch.float64), tensor(4.9381e-17, dtype=torch.float64), tensor(0.5319, dtype=torch.float64), tensor(8.2841e-17, dtype=torch.float64), tensor(7.8095e-18, dtype=torch.float64), tensor(3.0826e-17, dtype=torch.float64), tensor(1.6537e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.468
  sciq: 0
  triviaqa: 0.532
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999999,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.09999999999999999
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.26it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 95.36it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 107.45it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 115.52it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 120.65it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 133.50it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 134.83it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 144.99it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 145.06it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 146.79it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 148.49it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 155.57it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 158.90it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 160.66it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 165.47it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 172.17it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 182.81it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 190.08it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 203.60it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 209.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 162.23it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.2968, 'grad_norm': 0.17709694802761078, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.457003116607666, 'eval_runtime': 10.0534, 'eval_samples_per_second': 99.369, 'eval_steps_per_second': 6.267, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.99it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 95.39it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 107.20it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 115.30it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 119.82it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 132.98it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 134.29it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 144.59it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 144.57it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 146.14it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 147.93it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 155.17it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 158.70it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 160.52it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 165.27it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 171.86it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 182.37it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 189.57it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 203.07it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 208.96it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 161.86it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.7363, 'grad_norm': 0.08054192364215851, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 2.1976308822631836, 'eval_runtime': 10.0823, 'eval_samples_per_second': 99.085, 'eval_steps_per_second': 6.249, 'epoch': 0.08}
{'loss': 2.0381, 'grad_norm': 0.060062289237976074, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9025263786315918, 'eval_runtime': 10.108, 'eval_samples_per_second': 98.832, 'eval_steps_per_second': 6.233, 'epoch': 0.12}
{'loss': 1.8452, 'grad_norm': 0.04539027437567711, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8199818134307861, 'eval_runtime': 10.1868, 'eval_samples_per_second': 98.068, 'eval_steps_per_second': 6.184, 'epoch': 0.16}
{'loss': 1.7907, 'grad_norm': 0.06542748212814331, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7812663316726685, 'eval_runtime': 10.2418, 'eval_samples_per_second': 97.541, 'eval_steps_per_second': 6.151, 'epoch': 0.2}
{'loss': 1.7697, 'grad_norm': 0.04623998701572418, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7554363012313843, 'eval_runtime': 10.2312, 'eval_samples_per_second': 97.643, 'eval_steps_per_second': 6.158, 'epoch': 0.24}
{'loss': 1.7509, 'grad_norm': 0.052797768265008926, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7400777339935303, 'eval_runtime': 10.2314, 'eval_samples_per_second': 97.641, 'eval_steps_per_second': 6.158, 'epoch': 0.28}
{'loss': 1.7408, 'grad_norm': 0.050659019500017166, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7173917293548584, 'eval_runtime': 10.2001, 'eval_samples_per_second': 97.941, 'eval_steps_per_second': 6.176, 'epoch': 0.32}
{'loss': 1.7235, 'grad_norm': 0.04640468955039978, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6861131191253662, 'eval_runtime': 10.2204, 'eval_samples_per_second': 97.745, 'eval_steps_per_second': 6.164, 'epoch': 0.36}
{'loss': 1.6665, 'grad_norm': 0.046533968299627304, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6317782402038574, 'eval_runtime': 10.1968, 'eval_samples_per_second': 97.972, 'eval_steps_per_second': 6.178, 'epoch': 0.4}
{'loss': 1.6182, 'grad_norm': 0.05708860233426094, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.613434910774231, 'eval_runtime': 10.1958, 'eval_samples_per_second': 97.981, 'eval_steps_per_second': 6.179, 'epoch': 0.44}
{'loss': 1.6584, 'grad_norm': 0.04501690715551376, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5884002447128296, 'eval_runtime': 10.244, 'eval_samples_per_second': 97.521, 'eval_steps_per_second': 6.15, 'epoch': 0.48}
{'loss': 1.6024, 'grad_norm': 0.0456339530646801, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5650454759597778, 'eval_runtime': 10.1939, 'eval_samples_per_second': 98.0, 'eval_steps_per_second': 6.18, 'epoch': 0.52}
{'loss': 1.5653, 'grad_norm': 0.06304124742746353, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5597386360168457, 'eval_runtime': 10.1872, 'eval_samples_per_second': 98.064, 'eval_steps_per_second': 6.184, 'epoch': 0.56}
{'loss': 1.5667, 'grad_norm': 0.05011952668428421, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5560966730117798, 'eval_runtime': 10.1962, 'eval_samples_per_second': 97.977, 'eval_steps_per_second': 6.179, 'epoch': 0.6}
{'loss': 1.5745, 'grad_norm': 0.0502852126955986, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5533360242843628, 'eval_runtime': 10.2011, 'eval_samples_per_second': 97.93, 'eval_steps_per_second': 6.176, 'epoch': 0.64}
{'loss': 1.5313, 'grad_norm': 0.051078297197818756, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5501501560211182, 'eval_runtime': 10.1929, 'eval_samples_per_second': 98.01, 'eval_steps_per_second': 6.181, 'epoch': 0.68}
{'loss': 1.5617, 'grad_norm': 0.04479563981294632, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5470002889633179, 'eval_runtime': 10.1991, 'eval_samples_per_second': 97.95, 'eval_steps_per_second': 6.177, 'epoch': 0.72}
{'loss': 1.5927, 'grad_norm': 0.0543716624379158, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5448226928710938, 'eval_runtime': 10.1946, 'eval_samples_per_second': 97.993, 'eval_steps_per_second': 6.18, 'epoch': 0.76}
{'loss': 1.5382, 'grad_norm': 0.06535565108060837, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5429413318634033, 'eval_runtime': 10.1976, 'eval_samples_per_second': 97.964, 'eval_steps_per_second': 6.178, 'epoch': 0.8}
{'loss': 1.5524, 'grad_norm': 0.04480033740401268, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5407181978225708, 'eval_runtime': 10.1937, 'eval_samples_per_second': 98.001, 'eval_steps_per_second': 6.18, 'epoch': 0.84}
{'loss': 1.5635, 'grad_norm': 0.054288625717163086, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5388766527175903, 'eval_runtime': 10.1933, 'eval_samples_per_second': 98.006, 'eval_steps_per_second': 6.181, 'epoch': 0.88}
{'loss': 1.5593, 'grad_norm': 0.047031912952661514, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5384794473648071, 'eval_runtime': 10.198, 'eval_samples_per_second': 97.96, 'eval_steps_per_second': 6.178, 'epoch': 0.92}
{'loss': 1.5726, 'grad_norm': 0.04579443857073784, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5373516082763672, 'eval_runtime': 10.1801, 'eval_samples_per_second': 98.133, 'eval_steps_per_second': 6.189, 'epoch': 0.96}
{'loss': 1.5846, 'grad_norm': 0.05101827159523964, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5372122526168823, 'eval_runtime': 10.1858, 'eval_samples_per_second': 98.077, 'eval_steps_per_second': 6.185, 'epoch': 1.0}
{'train_runtime': 523.3808, 'train_samples_per_second': 19.105, 'train_steps_per_second': 1.194, 'train_loss': 1.8000167785644532, 'epoch': 1.0}
train_results:  {'eval_loss': [3.457003116607666, 2.1976308822631836, 1.9025263786315918, 1.8199818134307861, 1.7812663316726685, 1.7554363012313843, 1.7400777339935303, 1.7173917293548584, 1.6861131191253662, 1.6317782402038574, 1.613434910774231, 1.5884002447128296, 1.5650454759597778, 1.5597386360168457, 1.5560966730117798, 1.5533360242843628, 1.5501501560211182, 1.5470002889633179, 1.5448226928710938, 1.5429413318634033, 1.5407181978225708, 1.5388766527175903, 1.5384794473648071, 1.5373516082763672, 1.5372122526168823], 'performance': [0.48, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:19,  5.04it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 51.72it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 70.70it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 78.43it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 84.17it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 89.07it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 94.88it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 103.09it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 104.29it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 105.16it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 108.20it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 115.52it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 125.21it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 122.99it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 128.34it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 127.40it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 126.37it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 128.99it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 131.56it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 139.25it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 138.39it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 138.79it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 149.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 117.90it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.413034439086914
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242, 1.4236700534820557, 1.4240354299545288, 1.413034439086914]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1133 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.9488580947430356
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.9979110857823366
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.737895462991595
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.9452796003137585
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.8872577971137702
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4779, dtype=torch.float64), 0, tensor(0.5221, dtype=torch.float64), 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.3093506273941245e-18, 1.480000019073488, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.0616e-17, dtype=torch.float64), tensor(0.4779, dtype=torch.float64), tensor(5.2736e-16, dtype=torch.float64), tensor(0.5221, dtype=torch.float64), tensor(4.7416e-17, dtype=torch.float64), tensor(1.6191e-17, dtype=torch.float64), tensor(1.2721e-17, dtype=torch.float64), tensor(5.7824e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.3094e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.478
  sciq: 0
  triviaqa: 0.522
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.3093506273941245e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.3093506273941245e-18
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.12it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.00it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.92it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.28it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.70it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.42it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.53it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.80it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.85it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.16it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.67it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.16it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.34it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.22it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.51it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 150.94it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.98it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.41it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.76it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.85it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.53it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.37it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.2552, 'grad_norm': 0.21748581528663635, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3031320571899414, 'eval_runtime': 10.9762, 'eval_samples_per_second': 91.015, 'eval_steps_per_second': 5.74, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.25it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.86it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.23it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.58it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.26it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.11it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.23it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.43it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.40it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.71it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.16it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.51it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.70it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.70it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.06it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.69it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.69it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.07it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.51it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.61it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.97it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.5979, 'grad_norm': 0.18619580566883087, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.0168280601501465, 'eval_runtime': 10.9851, 'eval_samples_per_second': 90.942, 'eval_steps_per_second': 5.735, 'epoch': 0.08}
{'loss': 1.8385, 'grad_norm': 0.07361136376857758, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7736811637878418, 'eval_runtime': 11.0271, 'eval_samples_per_second': 90.595, 'eval_steps_per_second': 5.713, 'epoch': 0.12}
{'loss': 1.7052, 'grad_norm': 0.07055210322141647, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6706687211990356, 'eval_runtime': 11.0325, 'eval_samples_per_second': 90.55, 'eval_steps_per_second': 5.71, 'epoch': 0.16}
{'loss': 1.5949, 'grad_norm': 0.04831523820757866, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6396479606628418, 'eval_runtime': 11.0, 'eval_samples_per_second': 90.819, 'eval_steps_per_second': 5.727, 'epoch': 0.2}
{'loss': 1.5957, 'grad_norm': 0.037595007568597794, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6269807815551758, 'eval_runtime': 11.0197, 'eval_samples_per_second': 90.656, 'eval_steps_per_second': 5.717, 'epoch': 0.24}
{'loss': 1.6137, 'grad_norm': 0.05001022294163704, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6141602993011475, 'eval_runtime': 11.0716, 'eval_samples_per_second': 90.231, 'eval_steps_per_second': 5.69, 'epoch': 0.28}
{'loss': 1.6346, 'grad_norm': 0.047214146703481674, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6077715158462524, 'eval_runtime': 11.0806, 'eval_samples_per_second': 90.158, 'eval_steps_per_second': 5.686, 'epoch': 0.32}
{'loss': 1.625, 'grad_norm': 0.03876804560422897, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.602134108543396, 'eval_runtime': 11.085, 'eval_samples_per_second': 90.121, 'eval_steps_per_second': 5.683, 'epoch': 0.36}
{'loss': 1.5679, 'grad_norm': 0.040753982961177826, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.595866322517395, 'eval_runtime': 11.0956, 'eval_samples_per_second': 90.036, 'eval_steps_per_second': 5.678, 'epoch': 0.4}
{'loss': 1.5658, 'grad_norm': 0.040074028074741364, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5881235599517822, 'eval_runtime': 11.0991, 'eval_samples_per_second': 90.008, 'eval_steps_per_second': 5.676, 'epoch': 0.44}
{'loss': 1.6051, 'grad_norm': 0.0440945103764534, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5834622383117676, 'eval_runtime': 11.0974, 'eval_samples_per_second': 90.021, 'eval_steps_per_second': 5.677, 'epoch': 0.48}
{'loss': 1.5763, 'grad_norm': 0.04543347284197807, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.578855276107788, 'eval_runtime': 11.1559, 'eval_samples_per_second': 89.549, 'eval_steps_per_second': 5.647, 'epoch': 0.52}
{'loss': 1.5946, 'grad_norm': 0.0425347238779068, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.575089454650879, 'eval_runtime': 11.1135, 'eval_samples_per_second': 89.891, 'eval_steps_per_second': 5.669, 'epoch': 0.56}
{'loss': 1.5908, 'grad_norm': 0.038677822798490524, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5727003812789917, 'eval_runtime': 11.1213, 'eval_samples_per_second': 89.828, 'eval_steps_per_second': 5.665, 'epoch': 0.6}
{'loss': 1.5724, 'grad_norm': 0.04403206706047058, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5687592029571533, 'eval_runtime': 11.1319, 'eval_samples_per_second': 89.742, 'eval_steps_per_second': 5.659, 'epoch': 0.64}
{'loss': 1.547, 'grad_norm': 0.04612210392951965, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5668015480041504, 'eval_runtime': 11.0906, 'eval_samples_per_second': 90.076, 'eval_steps_per_second': 5.68, 'epoch': 0.68}
{'loss': 1.5684, 'grad_norm': 0.03865169733762741, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5632538795471191, 'eval_runtime': 11.0913, 'eval_samples_per_second': 90.071, 'eval_steps_per_second': 5.68, 'epoch': 0.72}
{'loss': 1.5715, 'grad_norm': 0.039721809327602386, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5603916645050049, 'eval_runtime': 11.0531, 'eval_samples_per_second': 90.382, 'eval_steps_per_second': 5.7, 'epoch': 0.76}
{'loss': 1.5823, 'grad_norm': 0.050903987139463425, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5581475496292114, 'eval_runtime': 11.1119, 'eval_samples_per_second': 89.904, 'eval_steps_per_second': 5.67, 'epoch': 0.8}
{'loss': 1.5732, 'grad_norm': 0.047430988401174545, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5566420555114746, 'eval_runtime': 11.03, 'eval_samples_per_second': 90.571, 'eval_steps_per_second': 5.712, 'epoch': 0.84}
{'loss': 1.5615, 'grad_norm': 0.04763112962245941, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5547195672988892, 'eval_runtime': 11.0237, 'eval_samples_per_second': 90.623, 'eval_steps_per_second': 5.715, 'epoch': 0.88}
{'loss': 1.543, 'grad_norm': 0.04250464588403702, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.554019808769226, 'eval_runtime': 11.0205, 'eval_samples_per_second': 90.65, 'eval_steps_per_second': 5.717, 'epoch': 0.92}
{'loss': 1.555, 'grad_norm': 0.04501727968454361, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5535274744033813, 'eval_runtime': 11.1671, 'eval_samples_per_second': 89.459, 'eval_steps_per_second': 5.642, 'epoch': 0.96}
{'loss': 1.5708, 'grad_norm': 0.05211436003446579, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5527725219726562, 'eval_runtime': 11.2008, 'eval_samples_per_second': 89.19, 'eval_steps_per_second': 5.625, 'epoch': 1.0}
{'train_runtime': 567.09, 'train_samples_per_second': 17.632, 'train_steps_per_second': 1.102, 'train_loss': 1.7442517395019532, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3031320571899414, 2.0168280601501465, 1.7736811637878418, 1.6706687211990356, 1.6396479606628418, 1.6269807815551758, 1.6141602993011475, 1.6077715158462524, 1.602134108543396, 1.595866322517395, 1.5881235599517822, 1.5834622383117676, 1.578855276107788, 1.575089454650879, 1.5727003812789917, 1.5687592029571533, 1.5668015480041504, 1.5632538795471191, 1.5603916645050049, 1.5581475496292114, 1.5566420555114746, 1.5547195672988892, 1.554019808769226, 1.5535274744033813, 1.5527725219726562], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.59it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.11it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.74it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.77it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 76.89it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 81.56it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.24it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 94.49it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.61it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 96.67it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 99.69it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.21it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.72it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.44it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.31it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.23it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.37it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.50it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.71it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.82it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.55it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.31it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 137.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.427130937576294
current iteration best possible performance (full train run):  0.42000000000000004
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242, 1.4236700534820557, 1.4240354299545288, 1.413034439086914, 1.427130937576294]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0676 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.6224068363603172
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.9534490849440476
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.9091752958954564
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.9802309226281852
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 1.1345188077642248
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4756, dtype=torch.float64), 0, tensor(0.4214, dtype=torch.float64), 0, 0, 0, tensor(0.1030, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1.1837e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4756, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4214, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5175e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1030, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.476
  sciq: 0
  triviaqa: 0.421
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.103

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 57.76it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 93.83it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 105.46it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 113.30it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 118.14it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 130.23it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 131.60it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 141.41it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 141.46it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 142.99it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 144.73it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 151.59it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 154.85it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 154.99it/s]Running loglikelihood requests:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 249/400 [00:01<00:00, 173.60it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 157.26it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 173.15it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 176.22it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 190.11it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 198.68it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 158.06it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.1701, 'grad_norm': 0.24518685042858124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.3451502323150635, 'eval_runtime': 10.3669, 'eval_samples_per_second': 96.364, 'eval_steps_per_second': 6.077, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 57.76it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 93.61it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 105.00it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 112.64it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 117.18it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 129.79it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 131.25it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 141.01it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 141.14it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 142.74it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 144.43it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 151.39it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 154.80it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 155.63it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 160.38it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 164.36it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 174.79it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 184.46it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 189.82it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 201.25it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 157.87it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.6256, 'grad_norm': 0.17709851264953613, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.0768914222717285, 'eval_runtime': 10.3483, 'eval_samples_per_second': 96.538, 'eval_steps_per_second': 6.088, 'epoch': 0.08}
{'loss': 1.8684, 'grad_norm': 0.06116770952939987, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.756353735923767, 'eval_runtime': 10.3822, 'eval_samples_per_second': 96.223, 'eval_steps_per_second': 6.068, 'epoch': 0.12}
{'loss': 1.6317, 'grad_norm': 0.05023874342441559, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6410062313079834, 'eval_runtime': 10.4134, 'eval_samples_per_second': 95.934, 'eval_steps_per_second': 6.05, 'epoch': 0.16}
{'loss': 1.6262, 'grad_norm': 0.03819981962442398, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6108684539794922, 'eval_runtime': 10.4347, 'eval_samples_per_second': 95.739, 'eval_steps_per_second': 6.038, 'epoch': 0.2}
{'loss': 1.5823, 'grad_norm': 0.04411192983388901, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5961523056030273, 'eval_runtime': 10.4456, 'eval_samples_per_second': 95.638, 'eval_steps_per_second': 6.031, 'epoch': 0.24}
{'loss': 1.5799, 'grad_norm': 0.057641997933387756, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5814911127090454, 'eval_runtime': 10.4364, 'eval_samples_per_second': 95.723, 'eval_steps_per_second': 6.037, 'epoch': 0.28}
{'loss': 1.5791, 'grad_norm': 0.03681756556034088, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5736483335494995, 'eval_runtime': 10.4549, 'eval_samples_per_second': 95.554, 'eval_steps_per_second': 6.026, 'epoch': 0.32}
{'loss': 1.5437, 'grad_norm': 0.04430413618683815, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5664145946502686, 'eval_runtime': 10.4365, 'eval_samples_per_second': 95.722, 'eval_steps_per_second': 6.037, 'epoch': 0.36}
{'loss': 1.5754, 'grad_norm': 0.046945080161094666, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.562634825706482, 'eval_runtime': 10.5265, 'eval_samples_per_second': 94.903, 'eval_steps_per_second': 5.985, 'epoch': 0.4}
{'loss': 1.5428, 'grad_norm': 0.04335693269968033, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5571701526641846, 'eval_runtime': 10.5512, 'eval_samples_per_second': 94.681, 'eval_steps_per_second': 5.971, 'epoch': 0.44}
{'loss': 1.5838, 'grad_norm': 0.04716997593641281, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5535078048706055, 'eval_runtime': 10.5243, 'eval_samples_per_second': 94.923, 'eval_steps_per_second': 5.986, 'epoch': 0.48}
{'loss': 1.555, 'grad_norm': 0.04297415167093277, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5480502843856812, 'eval_runtime': 10.5353, 'eval_samples_per_second': 94.824, 'eval_steps_per_second': 5.98, 'epoch': 0.52}
{'loss': 1.5423, 'grad_norm': 0.041777510195970535, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.545639157295227, 'eval_runtime': 10.5157, 'eval_samples_per_second': 95.001, 'eval_steps_per_second': 5.991, 'epoch': 0.56}
{'loss': 1.5379, 'grad_norm': 0.048229336738586426, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5418856143951416, 'eval_runtime': 10.5226, 'eval_samples_per_second': 94.938, 'eval_steps_per_second': 5.987, 'epoch': 0.6}
{'loss': 1.5359, 'grad_norm': 0.04846873879432678, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.540054202079773, 'eval_runtime': 10.533, 'eval_samples_per_second': 94.845, 'eval_steps_per_second': 5.981, 'epoch': 0.64}
{'loss': 1.4996, 'grad_norm': 0.05045342072844505, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5366599559783936, 'eval_runtime': 10.5603, 'eval_samples_per_second': 94.6, 'eval_steps_per_second': 5.966, 'epoch': 0.68}
{'loss': 1.531, 'grad_norm': 0.045127466320991516, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5340269804000854, 'eval_runtime': 10.5015, 'eval_samples_per_second': 95.13, 'eval_steps_per_second': 5.999, 'epoch': 0.72}
{'loss': 1.562, 'grad_norm': 0.043752532452344894, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.532713770866394, 'eval_runtime': 10.4732, 'eval_samples_per_second': 95.386, 'eval_steps_per_second': 6.015, 'epoch': 0.76}
{'loss': 1.5139, 'grad_norm': 0.04520280659198761, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5304510593414307, 'eval_runtime': 10.4697, 'eval_samples_per_second': 95.418, 'eval_steps_per_second': 6.017, 'epoch': 0.8}
{'loss': 1.5145, 'grad_norm': 0.03788144141435623, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5288931131362915, 'eval_runtime': 10.5924, 'eval_samples_per_second': 94.313, 'eval_steps_per_second': 5.948, 'epoch': 0.84}
{'loss': 1.5284, 'grad_norm': 0.04410355165600777, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5273010730743408, 'eval_runtime': 10.5961, 'eval_samples_per_second': 94.28, 'eval_steps_per_second': 5.946, 'epoch': 0.88}
{'loss': 1.5362, 'grad_norm': 0.04917963221669197, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5263253450393677, 'eval_runtime': 10.5769, 'eval_samples_per_second': 94.451, 'eval_steps_per_second': 5.956, 'epoch': 0.92}
{'loss': 1.504, 'grad_norm': 0.044846080243587494, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5256482362747192, 'eval_runtime': 10.5334, 'eval_samples_per_second': 94.841, 'eval_steps_per_second': 5.981, 'epoch': 0.96}
{'loss': 1.5293, 'grad_norm': 0.06520180404186249, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5254355669021606, 'eval_runtime': 10.5269, 'eval_samples_per_second': 94.9, 'eval_steps_per_second': 5.985, 'epoch': 1.0}
{'train_runtime': 536.938, 'train_samples_per_second': 18.62, 'train_steps_per_second': 1.164, 'train_loss': 1.7119548461914063, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3451502323150635, 2.0768914222717285, 1.756353735923767, 1.6410062313079834, 1.6108684539794922, 1.5961523056030273, 1.5814911127090454, 1.5736483335494995, 1.5664145946502686, 1.562634825706482, 1.5571701526641846, 1.5535078048706055, 1.5480502843856812, 1.545639157295227, 1.5418856143951416, 1.540054202079773, 1.5366599559783936, 1.5340269804000854, 1.532713770866394, 1.5304510593414307, 1.5288931131362915, 1.5273010730743408, 1.5263253450393677, 1.5256482362747192, 1.5254355669021606], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:21,  4.87it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 49.94it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 68.48it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 76.03it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 81.72it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 86.52it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 92.47it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 100.62it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 101.60it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 102.68it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 105.92it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 112.93it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 122.88it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 120.73it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 125.83it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 124.52it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 123.73it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 126.10it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 128.30it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 135.86it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 135.64it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 135.86it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 145.95it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 115.01it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4212342500686646
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242, 1.4236700534820557, 1.4240354299545288, 1.413034439086914, 1.427130937576294, 1.4212342500686646]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9868 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.9839618796284193
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.6126990241329398
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.7054911102425234
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.7451799439962352
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.6643511238358865
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1959, dtype=torch.float64), 0, tensor(0.4728, dtype=torch.float64), tensor(0.0867, dtype=torch.float64), tensor(0.0345, dtype=torch.float64), 0, tensor(0.1041, dtype=torch.float64), tensor(0.1060, dtype=torch.float64), 0, 32, 0, 0, 1, 1, 1, 128, 0.002910017978018559, 1.4800000190734872, 0]
normalized proposed parameters for next round by BO: [tensor(0.1959, dtype=torch.float64), tensor(1.4296e-17, dtype=torch.float64), tensor(0.4728, dtype=torch.float64), tensor(0.0867, dtype=torch.float64), tensor(0.0345, dtype=torch.float64), tensor(2.5001e-07, dtype=torch.float64), tensor(0.1041, dtype=torch.float64), tensor(0.1060, dtype=torch.float64), tensor(8.1507e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.196
  gsm8k: 0
  rowan_hellaswag: 0.473
  sciq: 0.087
  triviaqa: 0.035
  truthfulqa_gen: 0
  wikitext: 0.104
  mmlu: 0.106
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.002910017978018559,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.002910017978018559
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.94it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.81it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.06it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.60it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.40it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.93it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.43it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.02it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.14it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.67it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.46it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.36it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.63it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.56it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.18it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.84it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.22it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.22it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.10it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.07it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.52it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.79it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.0927, 'grad_norm': 0.24481205642223358, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.1962931156158447, 'eval_runtime': 10.725, 'eval_samples_per_second': 93.147, 'eval_steps_per_second': 5.874, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.99it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.73it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.88it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.57it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.40it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.75it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.04it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.59it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.61it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.01it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.68it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.44it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.72it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.77it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.37it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.03it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.51it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.59it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.37it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 194.97it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.01it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.5594, 'grad_norm': 0.18356317281723022, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.031402587890625, 'eval_runtime': 10.6443, 'eval_samples_per_second': 93.853, 'eval_steps_per_second': 5.919, 'epoch': 0.08}
{'loss': 1.8986, 'grad_norm': 0.047423556447029114, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7639096975326538, 'eval_runtime': 10.6317, 'eval_samples_per_second': 93.964, 'eval_steps_per_second': 5.926, 'epoch': 0.12}
{'loss': 1.7347, 'grad_norm': 0.0666067972779274, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6723986864089966, 'eval_runtime': 10.6594, 'eval_samples_per_second': 93.72, 'eval_steps_per_second': 5.91, 'epoch': 0.16}
{'loss': 1.6275, 'grad_norm': 0.05590525642037392, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6343729496002197, 'eval_runtime': 10.7493, 'eval_samples_per_second': 92.936, 'eval_steps_per_second': 5.861, 'epoch': 0.2}
{'loss': 1.6424, 'grad_norm': 0.04833195358514786, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6180624961853027, 'eval_runtime': 10.7966, 'eval_samples_per_second': 92.529, 'eval_steps_per_second': 5.835, 'epoch': 0.24}
{'loss': 1.6263, 'grad_norm': 0.04132212698459625, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6030534505844116, 'eval_runtime': 10.7605, 'eval_samples_per_second': 92.84, 'eval_steps_per_second': 5.855, 'epoch': 0.28}
{'loss': 1.5353, 'grad_norm': 0.05345172435045242, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5938705205917358, 'eval_runtime': 10.7674, 'eval_samples_per_second': 92.78, 'eval_steps_per_second': 5.851, 'epoch': 0.32}
{'loss': 1.6199, 'grad_norm': 0.04465922340750694, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5835975408554077, 'eval_runtime': 10.7714, 'eval_samples_per_second': 92.745, 'eval_steps_per_second': 5.849, 'epoch': 0.36}
{'loss': 1.5922, 'grad_norm': 0.03873491287231445, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.57657790184021, 'eval_runtime': 10.7664, 'eval_samples_per_second': 92.789, 'eval_steps_per_second': 5.852, 'epoch': 0.4}
{'loss': 1.6327, 'grad_norm': 0.04143065586686134, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5677975416183472, 'eval_runtime': 10.7773, 'eval_samples_per_second': 92.695, 'eval_steps_per_second': 5.846, 'epoch': 0.44}
{'loss': 1.5989, 'grad_norm': 0.03732632100582123, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.560683012008667, 'eval_runtime': 10.7886, 'eval_samples_per_second': 92.598, 'eval_steps_per_second': 5.84, 'epoch': 0.48}
{'loss': 1.5705, 'grad_norm': 0.04741127789020538, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5563026666641235, 'eval_runtime': 10.8063, 'eval_samples_per_second': 92.446, 'eval_steps_per_second': 5.83, 'epoch': 0.52}
{'loss': 1.5985, 'grad_norm': 0.036409590393304825, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5515379905700684, 'eval_runtime': 10.8004, 'eval_samples_per_second': 92.496, 'eval_steps_per_second': 5.833, 'epoch': 0.56}
{'loss': 1.5896, 'grad_norm': 0.04372957721352577, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5470155477523804, 'eval_runtime': 10.8177, 'eval_samples_per_second': 92.349, 'eval_steps_per_second': 5.824, 'epoch': 0.6}
{'loss': 1.5215, 'grad_norm': 0.048068270087242126, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5429316759109497, 'eval_runtime': 10.8371, 'eval_samples_per_second': 92.184, 'eval_steps_per_second': 5.813, 'epoch': 0.64}
{'loss': 1.5852, 'grad_norm': 0.042644910514354706, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.540346622467041, 'eval_runtime': 10.8608, 'eval_samples_per_second': 91.983, 'eval_steps_per_second': 5.801, 'epoch': 0.68}
{'loss': 1.5686, 'grad_norm': 0.04846429452300072, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.537131905555725, 'eval_runtime': 10.8542, 'eval_samples_per_second': 92.038, 'eval_steps_per_second': 5.804, 'epoch': 0.72}
{'loss': 1.5514, 'grad_norm': 0.05870084464550018, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5344748497009277, 'eval_runtime': 10.8763, 'eval_samples_per_second': 91.851, 'eval_steps_per_second': 5.792, 'epoch': 0.76}
{'loss': 1.5496, 'grad_norm': 0.04328885301947594, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5328384637832642, 'eval_runtime': 10.8335, 'eval_samples_per_second': 92.214, 'eval_steps_per_second': 5.815, 'epoch': 0.8}
{'loss': 1.574, 'grad_norm': 0.042706411331892014, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5312353372573853, 'eval_runtime': 10.7709, 'eval_samples_per_second': 92.75, 'eval_steps_per_second': 5.849, 'epoch': 0.84}
{'loss': 1.5745, 'grad_norm': 0.04564954340457916, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5286988019943237, 'eval_runtime': 10.775, 'eval_samples_per_second': 92.715, 'eval_steps_per_second': 5.847, 'epoch': 0.88}
{'loss': 1.5886, 'grad_norm': 0.047465980052948, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5271925926208496, 'eval_runtime': 10.7743, 'eval_samples_per_second': 92.721, 'eval_steps_per_second': 5.847, 'epoch': 0.92}
{'loss': 1.5683, 'grad_norm': 0.04516452178359032, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5263574123382568, 'eval_runtime': 10.7785, 'eval_samples_per_second': 92.685, 'eval_steps_per_second': 5.845, 'epoch': 0.96}
{'loss': 1.5358, 'grad_norm': 0.06201733276247978, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5260732173919678, 'eval_runtime': 10.7757, 'eval_samples_per_second': 92.709, 'eval_steps_per_second': 5.846, 'epoch': 1.0}
{'train_runtime': 553.3553, 'train_samples_per_second': 18.064, 'train_steps_per_second': 1.129, 'train_loss': 1.7414631896972657, 'epoch': 1.0}
train_results:  {'eval_loss': [3.1962931156158447, 2.031402587890625, 1.7639096975326538, 1.6723986864089966, 1.6343729496002197, 1.6180624961853027, 1.6030534505844116, 1.5938705205917358, 1.5835975408554077, 1.57657790184021, 1.5677975416183472, 1.560683012008667, 1.5563026666641235, 1.5515379905700684, 1.5470155477523804, 1.5429316759109497, 1.540346622467041, 1.537131905555725, 1.5344748497009277, 1.5328384637832642, 1.5312353372573853, 1.5286988019943237, 1.5271925926208496, 1.5263574123382568, 1.5260732173919678], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<11:18,  1.70s/it]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:02<00:35, 10.94it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:02<00:16, 22.78it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:02<00:10, 33.83it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:02<00:07, 44.70it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:02<00:05, 55.07it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:04, 65.91it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:03<00:03, 78.04it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:03<00:03, 83.88it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:03<00:02, 88.71it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:03<00:02, 94.81it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:03<00:02, 103.52it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:03<00:01, 114.65it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 114.10it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:04<00:01, 119.99it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:04<00:01, 119.54it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:04<00:01, 119.39it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:04<00:00, 121.90it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:04<00:00, 124.47it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:04<00:00, 131.43it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 131.59it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:05<00:00, 132.26it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:05<00:00, 142.39it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:05<00:00, 77.00it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4223871231079102
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242, 1.4236700534820557, 1.4240354299545288, 1.413034439086914, 1.427130937576294, 1.4212342500686646, 1.4223871231079102]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.7371 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.9724783832086688
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 1.0118097973791773
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.6142205152766986
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.798571587488991
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 1.0880414571291557
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0523, dtype=torch.float64), 0, tensor(0.4771, dtype=torch.float64), tensor(0.1686, dtype=torch.float64), tensor(0.0106, dtype=torch.float64), tensor(0.1755, dtype=torch.float64), 0, 0, tensor(0.1061, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.07093225092969502, 1.4800000190734872, 0]
normalized proposed parameters for next round by BO: [tensor(0.0523, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4771, dtype=torch.float64), tensor(0.1686, dtype=torch.float64), tensor(0.0106, dtype=torch.float64), tensor(0.1755, dtype=torch.float64), tensor(0.0096, dtype=torch.float64), tensor(8.9052e-18, dtype=torch.float64), tensor(0.1061, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7093, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.052
  gsm8k: 0
  rowan_hellaswag: 0.477
  sciq: 0.169
  triviaqa: 0.011
  truthfulqa_gen: 0.176
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.106

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07093225092969502,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.07093225092969502
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9902
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  990
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.56it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 94.77it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 106.31it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 114.25it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 119.08it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 131.36it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 132.41it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 142.22it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 142.57it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 144.11it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 145.95it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 153.16it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 156.68it/s]Running loglikelihood requests:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 231/400 [00:01<00:00, 169.05it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 159.68it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 164.94it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 176.90it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 186.95it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 192.27it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 203.01it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 159.91it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.2532, 'grad_norm': 0.12012007087469101, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.474346399307251, 'eval_runtime': 10.1498, 'eval_samples_per_second': 97.539, 'eval_steps_per_second': 6.109, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.48it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 94.57it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 106.32it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 114.30it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 119.36it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 132.22it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 133.41it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 143.35it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 143.36it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 144.89it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 146.54it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 153.62it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 157.02it/s]Running loglikelihood requests:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 232/400 [00:01<00:00, 172.18it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 159.01it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 164.25it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 176.45it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 186.52it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 191.97it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 203.56it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 160.30it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.7516, 'grad_norm': 0.06254033744335175, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.201420545578003, 'eval_runtime': 10.1464, 'eval_samples_per_second': 97.572, 'eval_steps_per_second': 6.111, 'epoch': 0.08}
{'loss': 1.9636, 'grad_norm': 0.07269465178251266, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.8573399782180786, 'eval_runtime': 10.1456, 'eval_samples_per_second': 97.579, 'eval_steps_per_second': 6.111, 'epoch': 0.12}
{'loss': 1.7613, 'grad_norm': 0.04233570024371147, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.7447112798690796, 'eval_runtime': 10.1631, 'eval_samples_per_second': 97.411, 'eval_steps_per_second': 6.1, 'epoch': 0.16}
{'loss': 1.7149, 'grad_norm': 0.04116694629192352, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 1.7037718296051025, 'eval_runtime': 10.1977, 'eval_samples_per_second': 97.081, 'eval_steps_per_second': 6.08, 'epoch': 0.2}
{'loss': 1.6499, 'grad_norm': 0.04192091152071953, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 1.6792001724243164, 'eval_runtime': 10.2193, 'eval_samples_per_second': 96.875, 'eval_steps_per_second': 6.067, 'epoch': 0.24}
{'loss': 1.6611, 'grad_norm': 0.041781630367040634, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 1.6596988439559937, 'eval_runtime': 10.2702, 'eval_samples_per_second': 96.395, 'eval_steps_per_second': 6.037, 'epoch': 0.28}
{'loss': 1.6563, 'grad_norm': 0.04192523658275604, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 1.6417968273162842, 'eval_runtime': 10.3394, 'eval_samples_per_second': 95.751, 'eval_steps_per_second': 5.997, 'epoch': 0.32}
{'loss': 1.6258, 'grad_norm': 0.04205114394426346, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 1.623970866203308, 'eval_runtime': 10.2973, 'eval_samples_per_second': 96.142, 'eval_steps_per_second': 6.021, 'epoch': 0.36}
{'loss': 1.6041, 'grad_norm': 0.05217338725924492, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 1.60007643699646, 'eval_runtime': 10.2925, 'eval_samples_per_second': 96.187, 'eval_steps_per_second': 6.024, 'epoch': 0.4}
{'loss': 1.5814, 'grad_norm': 0.04989761859178543, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 1.5534616708755493, 'eval_runtime': 10.3008, 'eval_samples_per_second': 96.109, 'eval_steps_per_second': 6.019, 'epoch': 0.44}
{'loss': 1.5041, 'grad_norm': 0.052069175988435745, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 1.5300641059875488, 'eval_runtime': 10.2887, 'eval_samples_per_second': 96.222, 'eval_steps_per_second': 6.026, 'epoch': 0.48}
{'loss': 1.5074, 'grad_norm': 0.045706067234277725, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 1.5118694305419922, 'eval_runtime': 10.2771, 'eval_samples_per_second': 96.33, 'eval_steps_per_second': 6.033, 'epoch': 0.53}
{'loss': 1.5214, 'grad_norm': 0.04017283394932747, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 1.4907490015029907, 'eval_runtime': 10.2555, 'eval_samples_per_second': 96.534, 'eval_steps_per_second': 6.046, 'epoch': 0.57}
{'loss': 1.5081, 'grad_norm': 0.03988204523921013, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 1.4780958890914917, 'eval_runtime': 10.234, 'eval_samples_per_second': 96.737, 'eval_steps_per_second': 6.058, 'epoch': 0.61}
{'loss': 1.5008, 'grad_norm': 0.040741100907325745, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 1.473734974861145, 'eval_runtime': 10.2449, 'eval_samples_per_second': 96.634, 'eval_steps_per_second': 6.052, 'epoch': 0.65}
{'loss': 1.5016, 'grad_norm': 0.04038052633404732, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 1.4717915058135986, 'eval_runtime': 10.2839, 'eval_samples_per_second': 96.267, 'eval_steps_per_second': 6.029, 'epoch': 0.69}
{'loss': 1.4442, 'grad_norm': 0.04354244843125343, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 1.4680627584457397, 'eval_runtime': 10.3389, 'eval_samples_per_second': 95.755, 'eval_steps_per_second': 5.997, 'epoch': 0.73}
{'loss': 1.4849, 'grad_norm': 0.038550809025764465, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 1.4656299352645874, 'eval_runtime': 10.3602, 'eval_samples_per_second': 95.558, 'eval_steps_per_second': 5.984, 'epoch': 0.77}
{'loss': 1.4713, 'grad_norm': 0.05077095702290535, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 1.4637079238891602, 'eval_runtime': 10.341, 'eval_samples_per_second': 95.736, 'eval_steps_per_second': 5.996, 'epoch': 0.81}
{'loss': 1.4739, 'grad_norm': 0.04289424046874046, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 1.4616228342056274, 'eval_runtime': 10.359, 'eval_samples_per_second': 95.569, 'eval_steps_per_second': 5.985, 'epoch': 0.85}
{'loss': 1.4989, 'grad_norm': 0.04311802238225937, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 1.459548830986023, 'eval_runtime': 10.3498, 'eval_samples_per_second': 95.654, 'eval_steps_per_second': 5.99, 'epoch': 0.89}
{'loss': 1.4829, 'grad_norm': 0.045755352824926376, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 1.4586982727050781, 'eval_runtime': 10.338, 'eval_samples_per_second': 95.764, 'eval_steps_per_second': 5.997, 'epoch': 0.93}
{'loss': 1.4827, 'grad_norm': 0.041128236800432205, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 1.4578028917312622, 'eval_runtime': 10.3617, 'eval_samples_per_second': 95.544, 'eval_steps_per_second': 5.984, 'epoch': 0.97}
{'train_runtime': 517.671, 'train_samples_per_second': 19.128, 'train_steps_per_second': 1.196, 'train_loss': 1.7258298994073575, 'epoch': 1.0}
train_results:  {'eval_loss': [3.474346399307251, 2.201420545578003, 1.8573399782180786, 1.7447112798690796, 1.7037718296051025, 1.6792001724243164, 1.6596988439559937, 1.6417968273162842, 1.623970866203308, 1.60007643699646, 1.5534616708755493, 1.5300641059875488, 1.5118694305419922, 1.4907490015029907, 1.4780958890914917, 1.473734974861145, 1.4717915058135986, 1.4680627584457397, 1.4656299352645874, 1.4637079238891602, 1.4616228342056274, 1.459548830986023, 1.4586982727050781, 1.4578028917312622], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:21,  4.91it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 50.22it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 68.94it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 76.41it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 82.09it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 87.18it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 93.31it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 101.53it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 102.71it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 103.60it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 106.82it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 113.76it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 124.00it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 121.96it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 126.97it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 125.31it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 124.58it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 126.79it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 129.26it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 136.88it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 136.65it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 137.30it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 147.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 115.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.421783447265625
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.5565000000000001
BO observations:  [0.7225359678268433, 0.9564425945281982, 0.6223999857902527, 0.8614088296890259, 1.4104418754577637, 1.2963961362838745, 1.0894432067871094, 1.2964553833007812, 0.6156018972396851, 1.4077579975128174, 1.423042893409729, 1.4045146703720093, 1.4201101064682007, 1.424170970916748, 1.4226740598678589, 1.402753233909607, 1.4030667543411255, 1.3090580701828003, 1.420567274093628, 1.420900821685791, 1.4245996475219727, 1.4239578247070312, 1.4198942184448242, 1.4236700534820557, 1.4240354299545288, 1.413034439086914, 1.427130937576294, 1.4212342500686646, 1.4223871231079102, 1.421783447265625]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4650 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.842617899151096
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.642079788673841
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.961601841947568
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.5921640308997044
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.5029428629205034
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0413, dtype=torch.float64), tensor(0.4815, dtype=torch.float64), tensor(0.4772, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 1, 1, 1, 1, 128, 0.014338769598993387, 1.4800000190734883, 0]
normalized proposed parameters for next round by BO: [tensor(3.1024e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0.4815, dtype=torch.float64), tensor(0.4772, dtype=torch.float64), tensor(5.0782e-17, dtype=torch.float64), tensor(4.6638e-17, dtype=torch.float64), tensor(4.2784e-18, dtype=torch.float64), tensor(1.6536e-17, dtype=torch.float64), tensor(2.5337e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1434, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/arc_challenge/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 17, 1, 0, 1, 1, 0, 59, 0.00445603671922955, 41, 0]
Checking history sample input_X_between_0_1:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 0.53125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4609375, 0.044560367192295496, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.47
Checking history sample input_X:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 22, 1, 1, 1, 0, 0, 61, 0.08406775506720655, 5, 1]
Checking history sample input_X_between_0_1:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 0.6875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4765625, 0.8406775506720655, 0.10416666666666667, 1.0]
Checking history sample performance at 625 steps:  0.47
Checking history sample input_X:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 15, 1, 1, 1, 1, 0, 24, 0.05173325143341287, 28, 0]
Checking history sample input_X_between_0_1:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 0.46875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 0.5173325143341286, 0.5833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 1, 0, 1, 1, 0, 0, 114, 0.021033555465481826, 23, 1]
Checking history sample input_X_between_0_1:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 0.03125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.890625, 0.21033555465481824, 0.4791666666666667, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 27, 1, 1, 1, 1, 1, 70, 0.005963494977880357, 46, 1]
Checking history sample input_X_between_0_1:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.05963494977880357, 0.9583333333333334, 1.0]
Checking history sample performance at 625 steps:  0.46
Checking history sample input_X:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 12, 1, 0, 1, 0, 0, 3, 0.017954470577765235, 12, 1]
Checking history sample input_X_between_0_1:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0234375, 0.17954470577765233, 0.25, 1.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 18, 1, 0, 0, 0, 1, 5, 0.028541773579282805, 35, 0]
Checking history sample input_X_between_0_1:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 0.5625, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0390625, 0.28541773579282803, 0.7291666666666666, 0.0]
Checking history sample performance at 625 steps:  0.4
Checking history sample input_X:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 24, 0, 1, 0, 1, 0, 55, 0.08420649661865921, 22, 0]
Checking history sample input_X_between_0_1:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 0.75, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4296875, 0.8420649661865921, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 30, 1, 0, 1, 1, 0, 5, 0.09059974254780842, 29, 1]
Checking history sample input_X_between_0_1:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 0.9375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0390625, 0.9059974254780842, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.44
Checking history sample input_X:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 20, 1, 1, 0, 1, 0, 27, 0.07188580123073206, 22, 0]
Checking history sample input_X_between_0_1:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2109375, 0.7188580123073205, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 8, 0, 0, 0, 1, 1, 31, 0.06288146497812035, 13, 1]
Checking history sample input_X_between_0_1:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2421875, 0.6288146497812034, 0.2708333333333333, 1.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 32, 1, 1, 0, 1, 1, 114, 0.08252011949389138, 13, 0]
Checking history sample input_X_between_0_1:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.890625, 0.8252011949389138, 0.2708333333333333, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 11, 0, 1, 0, 0, 1, 100, 0.09203832421630076, 16, 1]
Checking history sample input_X_between_0_1:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 0.34375, 0.0, 1.0, 0.0, 0.0, 1.0, 0.78125, 0.9203832421630076, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 31, 1, 0, 1, 1, 1, 60, 0.008025447056787238, 26, 0]
Checking history sample input_X_between_0_1:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 0.96875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.46875, 0.08025447056787237, 0.5416666666666666, 0.0]
Checking history sample performance at 625 steps:  0.57
Checking history sample input_X:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 7, 0, 0, 1, 0, 0, 10, 0.06890300584266877, 3, 1]
Checking history sample input_X_between_0_1:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.078125, 0.6890300584266876, 0.0625, 1.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 1, 1, 1, 0, 1, 0, 28, 0.0879909943648135, 16, 0]
Checking history sample input_X_between_0_1:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 0.03125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21875, 0.879909943648135, 0.3333333333333333, 0.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 1, 1, 0, 0, 1, 0, 47, 0.04979780998318395, 37, 0]
Checking history sample input_X_between_0_1:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 0.03125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3671875, 0.4979780998318395, 0.7708333333333334, 0.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 30, 1, 0, 1, 0, 0, 86, 0.08085387005284786, 6, 1]
Checking history sample input_X_between_0_1:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 0.9375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.671875, 0.8085387005284785, 0.125, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 31, 0, 1, 1, 1, 1, 84, 0.015776955723827136, 41, 1]
Checking history sample input_X_between_0_1:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 0.96875, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.15776955723827135, 0.8541666666666666, 1.0]
Checking history sample performance at 625 steps:  0.43
Checking history sample input_X:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 24, 0, 1, 1, 0, 1, 1, 0.07052967196513893, 22, 0]
Checking history sample input_X_between_0_1:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.7052967196513893, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.46
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8700 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.603321762549632
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.603366961800696
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.6018943351770696
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.6033736650693575
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.6033736595345878
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, 0, tensor(0.3410, dtype=torch.float64), tensor(0.2671, dtype=torch.float64), tensor(0.1793, dtype=torch.float64), tensor(0.1508, dtype=torch.float64), tensor(0.0589, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 68, 0.0017964660814157977, 2.045682658071251, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3410, dtype=torch.float64), tensor(0.2671, dtype=torch.float64), tensor(0.1793, dtype=torch.float64), tensor(0.1508, dtype=torch.float64), tensor(0.0589, dtype=torch.float64), tensor(0.0030, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5289, dtype=torch.float64), tensor(0.0180, dtype=torch.float64), tensor(0.0426, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.341
  sciq: 0.267
  triviaqa: 0.179
  truthfulqa_gen: 0.151
  wikitext: 0.059
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (68,)
  lora_dropout: (0.0017964660814157977,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (2.045682658071251,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  68
lora dropout:  0.0017964660814157977
lora alpha:  2.045682658071251
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 138,149,888 || all params: 8,168,411,136 || trainable%: 1.6913
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9967
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  996
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 52.20it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 84.63it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 95.22it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 102.28it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:03, 106.61it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 118.01it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:01<00:02, 119.31it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 127.54it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:02, 127.61it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 128.91it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 130.36it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 136.48it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 139.74it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 140.58it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 144.61it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 145.17it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 148.95it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 161.41it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 163.43it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 340/400 [00:02<00:00, 191.08it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 360/400 [00:02<00:00, 173.28it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 182.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 143.01it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3393, 'grad_norm': 0.41325879096984863, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.2392072677612305, 'eval_runtime': 11.1075, 'eval_samples_per_second': 89.669, 'eval_steps_per_second': 5.672, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 52.05it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 84.30it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 94.78it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 101.87it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:03, 106.25it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 117.65it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:01<00:02, 119.00it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 127.80it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:02, 127.68it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 128.80it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 130.22it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 136.28it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 139.49it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 140.35it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 144.38it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 144.96it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 148.60it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 161.09it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 163.12it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 170.36it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 183.60it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 188.23it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 142.74it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.4841, 'grad_norm': 0.2129046618938446, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 1.9243531227111816, 'eval_runtime': 11.1126, 'eval_samples_per_second': 89.628, 'eval_steps_per_second': 5.669, 'epoch': 0.08}
{'loss': 1.7858, 'grad_norm': 0.08823559433221817, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.6873576641082764, 'eval_runtime': 11.1204, 'eval_samples_per_second': 89.565, 'eval_steps_per_second': 5.665, 'epoch': 0.12}
{'loss': 1.6248, 'grad_norm': 0.08978695422410965, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.5866572856903076, 'eval_runtime': 11.1582, 'eval_samples_per_second': 89.261, 'eval_steps_per_second': 5.646, 'epoch': 0.16}
{'loss': 1.549, 'grad_norm': 0.08740662038326263, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.5566996335983276, 'eval_runtime': 11.1892, 'eval_samples_per_second': 89.014, 'eval_steps_per_second': 5.63, 'epoch': 0.2}
{'loss': 1.566, 'grad_norm': 0.07995675504207611, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.5364906787872314, 'eval_runtime': 11.2346, 'eval_samples_per_second': 88.655, 'eval_steps_per_second': 5.608, 'epoch': 0.24}
{'loss': 1.5668, 'grad_norm': 0.06938891112804413, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.5241975784301758, 'eval_runtime': 11.2373, 'eval_samples_per_second': 88.634, 'eval_steps_per_second': 5.606, 'epoch': 0.28}
{'loss': 1.4802, 'grad_norm': 0.0840112715959549, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.5135746002197266, 'eval_runtime': 11.219, 'eval_samples_per_second': 88.778, 'eval_steps_per_second': 5.615, 'epoch': 0.32}
{'loss': 1.5015, 'grad_norm': 0.08878786861896515, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.5062663555145264, 'eval_runtime': 11.2222, 'eval_samples_per_second': 88.752, 'eval_steps_per_second': 5.614, 'epoch': 0.36}
{'loss': 1.5254, 'grad_norm': 0.07209770381450653, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.4985437393188477, 'eval_runtime': 11.262, 'eval_samples_per_second': 88.439, 'eval_steps_per_second': 5.594, 'epoch': 0.4}
{'loss': 1.5298, 'grad_norm': 0.08284803479909897, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.4885519742965698, 'eval_runtime': 11.2974, 'eval_samples_per_second': 88.162, 'eval_steps_per_second': 5.576, 'epoch': 0.44}
{'loss': 1.4742, 'grad_norm': 0.08190052956342697, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.4842416048049927, 'eval_runtime': 11.3202, 'eval_samples_per_second': 87.984, 'eval_steps_per_second': 5.565, 'epoch': 0.48}
{'loss': 1.4908, 'grad_norm': 0.06449837982654572, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.4769577980041504, 'eval_runtime': 11.3401, 'eval_samples_per_second': 87.83, 'eval_steps_per_second': 5.556, 'epoch': 0.52}
{'loss': 1.4526, 'grad_norm': 0.07419252395629883, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.4722239971160889, 'eval_runtime': 11.3485, 'eval_samples_per_second': 87.765, 'eval_steps_per_second': 5.551, 'epoch': 0.56}
{'loss': 1.4212, 'grad_norm': 0.08554458618164062, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.4686449766159058, 'eval_runtime': 11.3402, 'eval_samples_per_second': 87.829, 'eval_steps_per_second': 5.555, 'epoch': 0.6}
{'loss': 1.4577, 'grad_norm': 0.08307183533906937, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.4648158550262451, 'eval_runtime': 11.3435, 'eval_samples_per_second': 87.804, 'eval_steps_per_second': 5.554, 'epoch': 0.64}
{'loss': 1.4902, 'grad_norm': 0.07259326428174973, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.4600194692611694, 'eval_runtime': 11.3331, 'eval_samples_per_second': 87.885, 'eval_steps_per_second': 5.559, 'epoch': 0.68}
{'loss': 1.5028, 'grad_norm': 0.06967789679765701, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.4552806615829468, 'eval_runtime': 11.347, 'eval_samples_per_second': 87.776, 'eval_steps_per_second': 5.552, 'epoch': 0.72}
{'loss': 1.4372, 'grad_norm': 0.06198390573263168, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.4542913436889648, 'eval_runtime': 11.3446, 'eval_samples_per_second': 87.795, 'eval_steps_per_second': 5.553, 'epoch': 0.76}
{'loss': 1.4524, 'grad_norm': 0.08739933371543884, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.4502085447311401, 'eval_runtime': 11.3221, 'eval_samples_per_second': 87.97, 'eval_steps_per_second': 5.564, 'epoch': 0.8}
{'loss': 1.497, 'grad_norm': 0.07606752216815948, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.4487838745117188, 'eval_runtime': 11.2679, 'eval_samples_per_second': 88.393, 'eval_steps_per_second': 5.591, 'epoch': 0.84}
{'loss': 1.4594, 'grad_norm': 0.0840824618935585, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.4470505714416504, 'eval_runtime': 11.2599, 'eval_samples_per_second': 88.455, 'eval_steps_per_second': 5.595, 'epoch': 0.88}
{'loss': 1.4582, 'grad_norm': 0.07062762975692749, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.445544719696045, 'eval_runtime': 11.2367, 'eval_samples_per_second': 88.638, 'eval_steps_per_second': 5.607, 'epoch': 0.92}
{'loss': 1.4637, 'grad_norm': 0.07824216783046722, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.4441784620285034, 'eval_runtime': 11.2409, 'eval_samples_per_second': 88.605, 'eval_steps_per_second': 5.605, 'epoch': 0.96}
{'train_runtime': 569.6459, 'train_samples_per_second': 17.497, 'train_steps_per_second': 1.094, 'train_loss': 1.6585015224989497, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2392072677612305, 1.9243531227111816, 1.6873576641082764, 1.5866572856903076, 1.5566996335983276, 1.5364906787872314, 1.5241975784301758, 1.5135746002197266, 1.5062663555145264, 1.4985437393188477, 1.4885519742965698, 1.4842416048049927, 1.4769577980041504, 1.4722239971160889, 1.4686449766159058, 1.4648158550262451, 1.4600194692611694, 1.4552806615829468, 1.4542913436889648, 1.4502085447311401, 1.4487838745117188, 1.4470505714416504, 1.445544719696045, 1.4441784620285034], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:25,  2.74it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 36.11it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 54.71it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 63.99it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 70.81it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 76.43it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 82.73it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 90.71it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 92.07it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 93.54it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 96.67it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 103.09it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 112.30it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 110.41it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 115.18it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 114.01it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 113.08it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 115.09it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 117.38it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 124.46it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 124.51it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 125.14it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 134.48it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 101.88it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.0514142513275146
current iteration best possible performance (full train run):  0.504
max performance so far:  0.504
BO observations:  [1.0514142513275146]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9109 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.0221518732042514
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.9200245635353679
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.0221811363598454
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.0221558161900783
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.0224122785940284
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4375, dtype=torch.float64), tensor(0.3053, dtype=torch.float64), tensor(0.0146, dtype=torch.float64), tensor(0.2427, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 1, 0, 77, 0.0, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(2.6604e-17, dtype=torch.float64), tensor(7.6282e-18, dtype=torch.float64), tensor(0.4375, dtype=torch.float64), tensor(0.3053, dtype=torch.float64), tensor(0.0146, dtype=torch.float64), tensor(0.2427, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5984, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.438
  sciq: 0.305
  triviaqa: 0.015
  truthfulqa_gen: 0.243
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  77
lora dropout:  0.0
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 78,217,216 || all params: 8,108,478,464 || trainable%: 0.9646
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.64it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.27it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.18it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.89it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.76it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.08it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.48it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 138.13it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 138.02it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.09it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.78it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.49it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.92it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.75it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.15it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.61it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 278/400 [00:02<00:00, 163.36it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 172.80it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 182.65it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 187.37it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 198.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.10it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.3547, 'grad_norm': 0.41023728251457214, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.6110899448394775, 'eval_runtime': 10.521, 'eval_samples_per_second': 94.953, 'eval_steps_per_second': 5.988, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.23it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.94it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.26it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.88it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.62it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.73it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.87it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.11it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.70it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.20it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.89it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.56it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.17it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.07it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.50it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.61it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 159.72it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 172.94it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 182.62it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 187.25it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 198.13it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.53it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.8083, 'grad_norm': 0.29937461018562317, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.162346601486206, 'eval_runtime': 10.5315, 'eval_samples_per_second': 94.858, 'eval_steps_per_second': 5.982, 'epoch': 0.08}
{'loss': 1.916, 'grad_norm': 0.0745575800538063, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7400752305984497, 'eval_runtime': 10.6063, 'eval_samples_per_second': 94.189, 'eval_steps_per_second': 5.94, 'epoch': 0.12}
{'loss': 1.6113, 'grad_norm': 0.10685000568628311, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6187520027160645, 'eval_runtime': 10.6291, 'eval_samples_per_second': 93.987, 'eval_steps_per_second': 5.927, 'epoch': 0.16}
{'loss': 1.5714, 'grad_norm': 0.08703789114952087, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5701181888580322, 'eval_runtime': 10.6569, 'eval_samples_per_second': 93.742, 'eval_steps_per_second': 5.912, 'epoch': 0.2}
{'loss': 1.5758, 'grad_norm': 0.07657470554113388, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5533909797668457, 'eval_runtime': 10.6007, 'eval_samples_per_second': 94.239, 'eval_steps_per_second': 5.943, 'epoch': 0.24}
{'loss': 1.4873, 'grad_norm': 0.08212830126285553, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5387639999389648, 'eval_runtime': 10.5969, 'eval_samples_per_second': 94.273, 'eval_steps_per_second': 5.945, 'epoch': 0.28}
{'loss': 1.5389, 'grad_norm': 0.10155662149190903, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.529556155204773, 'eval_runtime': 10.5975, 'eval_samples_per_second': 94.268, 'eval_steps_per_second': 5.945, 'epoch': 0.32}
{'loss': 1.5223, 'grad_norm': 0.0881558433175087, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5221376419067383, 'eval_runtime': 10.6107, 'eval_samples_per_second': 94.15, 'eval_steps_per_second': 5.937, 'epoch': 0.36}
{'loss': 1.5336, 'grad_norm': 0.09232483059167862, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5175238847732544, 'eval_runtime': 10.612, 'eval_samples_per_second': 94.139, 'eval_steps_per_second': 5.937, 'epoch': 0.4}
{'loss': 1.4972, 'grad_norm': 0.09540075808763504, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.512445330619812, 'eval_runtime': 10.5699, 'eval_samples_per_second': 94.514, 'eval_steps_per_second': 5.96, 'epoch': 0.44}
{'loss': 1.546, 'grad_norm': 0.09285935014486313, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5093709230422974, 'eval_runtime': 10.5722, 'eval_samples_per_second': 94.493, 'eval_steps_per_second': 5.959, 'epoch': 0.48}
{'loss': 1.5108, 'grad_norm': 0.09484931826591492, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5053679943084717, 'eval_runtime': 10.5221, 'eval_samples_per_second': 94.943, 'eval_steps_per_second': 5.987, 'epoch': 0.52}
{'loss': 1.4685, 'grad_norm': 0.07867124676704407, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5028009414672852, 'eval_runtime': 10.4965, 'eval_samples_per_second': 95.174, 'eval_steps_per_second': 6.002, 'epoch': 0.56}
{'loss': 1.5233, 'grad_norm': 0.09484740346670151, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4990761280059814, 'eval_runtime': 10.5225, 'eval_samples_per_second': 94.939, 'eval_steps_per_second': 5.987, 'epoch': 0.6}
{'loss': 1.5264, 'grad_norm': 0.09730425477027893, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4966641664505005, 'eval_runtime': 10.6183, 'eval_samples_per_second': 94.083, 'eval_steps_per_second': 5.933, 'epoch': 0.64}
{'loss': 1.5012, 'grad_norm': 0.0828232541680336, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.495688796043396, 'eval_runtime': 10.6546, 'eval_samples_per_second': 93.763, 'eval_steps_per_second': 5.913, 'epoch': 0.68}
{'loss': 1.4965, 'grad_norm': 0.07381569594144821, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.493290662765503, 'eval_runtime': 10.6399, 'eval_samples_per_second': 93.892, 'eval_steps_per_second': 5.921, 'epoch': 0.72}
{'loss': 1.5009, 'grad_norm': 0.108092300593853, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4902461767196655, 'eval_runtime': 10.605, 'eval_samples_per_second': 94.201, 'eval_steps_per_second': 5.941, 'epoch': 0.76}
{'loss': 1.5368, 'grad_norm': 0.0754060223698616, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4878934621810913, 'eval_runtime': 10.6164, 'eval_samples_per_second': 94.1, 'eval_steps_per_second': 5.934, 'epoch': 0.8}
{'loss': 1.4591, 'grad_norm': 0.08385425060987473, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.487255573272705, 'eval_runtime': 10.6447, 'eval_samples_per_second': 93.849, 'eval_steps_per_second': 5.918, 'epoch': 0.84}
{'loss': 1.4419, 'grad_norm': 0.11362747848033905, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4858230352401733, 'eval_runtime': 10.669, 'eval_samples_per_second': 93.636, 'eval_steps_per_second': 5.905, 'epoch': 0.88}
{'loss': 1.4758, 'grad_norm': 0.08216657489538193, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4852242469787598, 'eval_runtime': 10.6073, 'eval_samples_per_second': 94.18, 'eval_steps_per_second': 5.939, 'epoch': 0.92}
{'loss': 1.4849, 'grad_norm': 0.08553297817707062, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4841889142990112, 'eval_runtime': 10.6188, 'eval_samples_per_second': 94.078, 'eval_steps_per_second': 5.933, 'epoch': 0.96}
{'loss': 1.5442, 'grad_norm': 0.09446433931589127, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.483905553817749, 'eval_runtime': 10.6091, 'eval_samples_per_second': 94.165, 'eval_steps_per_second': 5.938, 'epoch': 1.0}
{'train_runtime': 534.4999, 'train_samples_per_second': 18.705, 'train_steps_per_second': 1.169, 'train_loss': 1.697332421875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6110899448394775, 2.162346601486206, 1.7400752305984497, 1.6187520027160645, 1.5701181888580322, 1.5533909797668457, 1.5387639999389648, 1.529556155204773, 1.5221376419067383, 1.5175238847732544, 1.512445330619812, 1.5093709230422974, 1.5053679943084717, 1.5028009414672852, 1.4990761280059814, 1.4966641664505005, 1.495688796043396, 1.493290662765503, 1.4902461767196655, 1.4878934621810913, 1.487255573272705, 1.4858230352401733, 1.4852242469787598, 1.4841889142990112, 1.483905553817749], 'performance': [0.48, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:23,  4.77it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 49.07it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 67.08it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 74.45it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 79.79it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 84.47it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 90.49it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 98.25it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 98.95it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 100.32it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 103.64it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 110.31it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 120.01it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.77it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.42it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 121.00it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 120.26it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 122.59it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.01it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.51it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.24it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 132.74it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 142.70it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 112.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.0904109477996826
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.504
BO observations:  [1.0514142513275146, 1.0904109477996826]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2061 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.0200985175744015
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 1.016413478755238
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.067710468004874
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.9194908772603094
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.7723346219282996
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4133, dtype=torch.float64), tensor(0.1455, dtype=torch.float64), 0, 0, tensor(0.4412, dtype=torch.float64), 0, 0, 32, 1, 1, 0, 1, 0, 128, 3.9681799512969464e-19, 1.4800000190734897, 0]
normalized proposed parameters for next round by BO: [tensor(5.2827e-17, dtype=torch.float64), tensor(6.7390e-17, dtype=torch.float64), tensor(0.4133, dtype=torch.float64), tensor(0.1455, dtype=torch.float64), tensor(5.0634e-18, dtype=torch.float64), tensor(8.3268e-18, dtype=torch.float64), tensor(0.4412, dtype=torch.float64), tensor(1.6587e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(3.9682e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.413
  sciq: 0.145
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.441
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.9681799512969464e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734897,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  3.9681799512969464e-19
lora alpha:  1.4800000190734897
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 59.74it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 96.83it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 108.84it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 116.98it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 122.02it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 135.02it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 136.04it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 146.36it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 146.39it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 148.02it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 149.89it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 157.05it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 160.66it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 162.19it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 166.74it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 173.40it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 184.25it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 191.42it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 205.11it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 210.98it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 163.78it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.013, 'grad_norm': 0.2122119516134262, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.406620979309082, 'eval_runtime': 9.9785, 'eval_samples_per_second': 100.115, 'eval_steps_per_second': 6.314, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 59.84it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 96.78it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 108.91it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 117.24it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 122.49it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 135.59it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 136.90it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 147.10it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 147.02it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 148.61it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 150.36it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 157.53it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 161.12it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 162.59it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 167.09it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 173.83it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 184.67it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 191.95it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 205.67it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 211.47it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 164.26it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.9063, 'grad_norm': 0.2047814130783081, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 2.342183828353882, 'eval_runtime': 9.9534, 'eval_samples_per_second': 100.368, 'eval_steps_per_second': 6.33, 'epoch': 0.08}
{'loss': 2.1706, 'grad_norm': 0.06054966151714325, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.997645616531372, 'eval_runtime': 9.9811, 'eval_samples_per_second': 100.089, 'eval_steps_per_second': 6.312, 'epoch': 0.12}
{'loss': 2.0156, 'grad_norm': 0.08146382868289948, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9061568975448608, 'eval_runtime': 9.9982, 'eval_samples_per_second': 99.918, 'eval_steps_per_second': 6.301, 'epoch': 0.16}
{'loss': 1.9192, 'grad_norm': 0.106326624751091, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8347636461257935, 'eval_runtime': 10.0073, 'eval_samples_per_second': 99.827, 'eval_steps_per_second': 6.295, 'epoch': 0.2}
{'loss': 1.8816, 'grad_norm': 0.08164479583501816, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8128684759140015, 'eval_runtime': 10.0016, 'eval_samples_per_second': 99.884, 'eval_steps_per_second': 6.299, 'epoch': 0.24}
{'loss': 1.8726, 'grad_norm': 0.05256594717502594, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8035132884979248, 'eval_runtime': 10.0153, 'eval_samples_per_second': 99.748, 'eval_steps_per_second': 6.29, 'epoch': 0.28}
{'loss': 1.8804, 'grad_norm': 0.05951179191470146, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7892099618911743, 'eval_runtime': 10.1252, 'eval_samples_per_second': 98.665, 'eval_steps_per_second': 6.222, 'epoch': 0.32}
{'loss': 1.8194, 'grad_norm': 0.06487832963466644, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7814548015594482, 'eval_runtime': 10.0927, 'eval_samples_per_second': 98.982, 'eval_steps_per_second': 6.242, 'epoch': 0.36}
{'loss': 1.8457, 'grad_norm': 0.05336267128586769, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7746390104293823, 'eval_runtime': 10.0911, 'eval_samples_per_second': 98.998, 'eval_steps_per_second': 6.243, 'epoch': 0.4}
{'loss': 1.801, 'grad_norm': 0.060739170759916306, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7697957754135132, 'eval_runtime': 10.1321, 'eval_samples_per_second': 98.598, 'eval_steps_per_second': 6.218, 'epoch': 0.44}
{'loss': 1.828, 'grad_norm': 0.0630246102809906, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7645796537399292, 'eval_runtime': 10.0858, 'eval_samples_per_second': 99.05, 'eval_steps_per_second': 6.246, 'epoch': 0.48}
{'loss': 1.829, 'grad_norm': 0.07022533565759659, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7615267038345337, 'eval_runtime': 10.0725, 'eval_samples_per_second': 99.181, 'eval_steps_per_second': 6.255, 'epoch': 0.52}
{'loss': 1.8346, 'grad_norm': 0.09087680280208588, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.760109543800354, 'eval_runtime': 10.0639, 'eval_samples_per_second': 99.266, 'eval_steps_per_second': 6.26, 'epoch': 0.56}
{'loss': 1.7897, 'grad_norm': 0.06793860346078873, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7554322481155396, 'eval_runtime': 10.0953, 'eval_samples_per_second': 98.957, 'eval_steps_per_second': 6.241, 'epoch': 0.6}
{'loss': 1.8045, 'grad_norm': 0.07724913209676743, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7516603469848633, 'eval_runtime': 10.0947, 'eval_samples_per_second': 98.963, 'eval_steps_per_second': 6.241, 'epoch': 0.64}
{'loss': 1.7835, 'grad_norm': 0.05497836694121361, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7504161596298218, 'eval_runtime': 10.0788, 'eval_samples_per_second': 99.119, 'eval_steps_per_second': 6.251, 'epoch': 0.68}
{'loss': 1.8307, 'grad_norm': 0.07402238994836807, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7479183673858643, 'eval_runtime': 10.0891, 'eval_samples_per_second': 99.018, 'eval_steps_per_second': 6.244, 'epoch': 0.72}
{'loss': 1.7629, 'grad_norm': 0.08079981058835983, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7453163862228394, 'eval_runtime': 10.0776, 'eval_samples_per_second': 99.131, 'eval_steps_per_second': 6.251, 'epoch': 0.76}
{'loss': 1.7895, 'grad_norm': 0.06168084591627121, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.743154764175415, 'eval_runtime': 10.0723, 'eval_samples_per_second': 99.183, 'eval_steps_per_second': 6.255, 'epoch': 0.8}
{'loss': 1.7691, 'grad_norm': 0.07013129442930222, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7423676252365112, 'eval_runtime': 10.1012, 'eval_samples_per_second': 98.899, 'eval_steps_per_second': 6.237, 'epoch': 0.84}
{'loss': 1.7726, 'grad_norm': 0.07729833573102951, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7406628131866455, 'eval_runtime': 10.0868, 'eval_samples_per_second': 99.04, 'eval_steps_per_second': 6.246, 'epoch': 0.88}
{'loss': 1.7865, 'grad_norm': 0.08357885479927063, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7398680448532104, 'eval_runtime': 10.0276, 'eval_samples_per_second': 99.625, 'eval_steps_per_second': 6.283, 'epoch': 0.92}
{'loss': 1.7516, 'grad_norm': 0.0729513019323349, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7394195795059204, 'eval_runtime': 10.0037, 'eval_samples_per_second': 99.863, 'eval_steps_per_second': 6.298, 'epoch': 0.96}
{'loss': 1.7816, 'grad_norm': 0.07785526663064957, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7391663789749146, 'eval_runtime': 10.0016, 'eval_samples_per_second': 99.884, 'eval_steps_per_second': 6.299, 'epoch': 1.0}
{'train_runtime': 522.81, 'train_samples_per_second': 19.124, 'train_steps_per_second': 1.195, 'train_loss': 1.969559423828125, 'epoch': 1.0}
train_results:  {'eval_loss': [3.406620979309082, 2.342183828353882, 1.997645616531372, 1.9061568975448608, 1.8347636461257935, 1.8128684759140015, 1.8035132884979248, 1.7892099618911743, 1.7814548015594482, 1.7746390104293823, 1.7697957754135132, 1.7645796537399292, 1.7615267038345337, 1.760109543800354, 1.7554322481155396, 1.7516603469848633, 1.7504161596298218, 1.7479183673858643, 1.7453163862228394, 1.743154764175415, 1.7423676252365112, 1.7406628131866455, 1.7398680448532104, 1.7394195795059204, 1.7391663789749146], 'performance': [0.48, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<11:14,  1.69s/it]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:31, 12.21it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:02<00:14, 25.28it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:02<00:09, 37.22it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:02<00:06, 48.76it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:02<00:05, 59.79it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:04, 71.27it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 83.96it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:03<00:02, 89.98it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:03<00:02, 94.94it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:03<00:02, 101.28it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:03<00:01, 110.57it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:03<00:01, 122.35it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 121.72it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 127.93it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 127.27it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:04<00:00, 127.25it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:04<00:00, 130.04it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:04<00:00, 132.79it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:04<00:00, 140.56it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 140.36it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 141.00it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 151.54it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 82.62it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4104732275009155
current iteration best possible performance (full train run):  0.504
max performance so far:  0.504
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0128 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 1.2567135701483625
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.2206694990214646
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 1.2202957587768017
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.2336190255617538
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.0259490757144913
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4509, dtype=torch.float64), 0, 0, 0, tensor(0.5491, dtype=torch.float64), 0, 0, 1, 0, 1, 0, 1, 0, 2, 0.1, 1.4800000190734877, 0]
normalized proposed parameters for next round by BO: [tensor(5.8192e-17, dtype=torch.float64), tensor(6.1680e-17, dtype=torch.float64), tensor(0.4509, dtype=torch.float64), tensor(1.0090e-17, dtype=torch.float64), tensor(7.1506e-17, dtype=torch.float64), tensor(3.5788e-17, dtype=torch.float64), tensor(0.5491, dtype=torch.float64), tensor(7.3421e-18, dtype=torch.float64), tensor(9.4047e-19, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.451
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.549
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734877,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  1.4800000190734877
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 47,104 || all params: 8,030,308,352 || trainable%: 0.0006
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.07it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.50it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.89it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.75it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.60it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.86it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 158.59it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 169.75it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 172.61it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 179.84it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 185.02it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 190.51it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 192.99it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 208.91it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 213.27it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 229.92it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 243.75it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.40it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.9587, 'grad_norm': 0.29774633049964905, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.8949315547943115, 'eval_runtime': 8.7467, 'eval_samples_per_second': 114.215, 'eval_steps_per_second': 7.203, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 67.54it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 108.98it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 122.68it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 131.71it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 137.43it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 153.00it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 154.07it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 165.56it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 166.92it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 169.41it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 179.67it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 182.29it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 188.19it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 197.27it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 211.31it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 221.68it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 238.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 187.79it/s]
Evaluation performance at step 50: 0.5
{'loss': 3.7253, 'grad_norm': 0.8171309232711792, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 3.4795665740966797, 'eval_runtime': 8.8074, 'eval_samples_per_second': 113.427, 'eval_steps_per_second': 7.153, 'epoch': 0.08}
{'loss': 3.2918, 'grad_norm': 0.620212197303772, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.1100709438323975, 'eval_runtime': 8.8524, 'eval_samples_per_second': 112.851, 'eval_steps_per_second': 7.117, 'epoch': 0.12}
{'loss': 2.9684, 'grad_norm': 0.5459604263305664, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.8434972763061523, 'eval_runtime': 8.8665, 'eval_samples_per_second': 112.671, 'eval_steps_per_second': 7.105, 'epoch': 0.16}
{'loss': 2.7175, 'grad_norm': 0.4357091784477234, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.6886565685272217, 'eval_runtime': 8.9118, 'eval_samples_per_second': 112.099, 'eval_steps_per_second': 7.069, 'epoch': 0.2}
{'loss': 2.6608, 'grad_norm': 0.4968598783016205, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.605184316635132, 'eval_runtime': 8.937, 'eval_samples_per_second': 111.782, 'eval_steps_per_second': 7.049, 'epoch': 0.24}
{'loss': 2.5933, 'grad_norm': 0.5526940822601318, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.551032066345215, 'eval_runtime': 8.9529, 'eval_samples_per_second': 111.584, 'eval_steps_per_second': 7.037, 'epoch': 0.28}
{'loss': 2.5698, 'grad_norm': 0.37929239869117737, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.5114095211029053, 'eval_runtime': 8.9926, 'eval_samples_per_second': 111.091, 'eval_steps_per_second': 7.006, 'epoch': 0.32}
{'loss': 2.5108, 'grad_norm': 0.3530018627643585, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.4807498455047607, 'eval_runtime': 8.9831, 'eval_samples_per_second': 111.209, 'eval_steps_per_second': 7.013, 'epoch': 0.36}
{'loss': 2.465, 'grad_norm': 0.3057299256324768, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.457355499267578, 'eval_runtime': 8.9856, 'eval_samples_per_second': 111.178, 'eval_steps_per_second': 7.011, 'epoch': 0.4}
{'loss': 2.4495, 'grad_norm': 0.3576849400997162, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.440385580062866, 'eval_runtime': 8.9793, 'eval_samples_per_second': 111.255, 'eval_steps_per_second': 7.016, 'epoch': 0.44}
{'loss': 2.4423, 'grad_norm': 0.483501672744751, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.4282641410827637, 'eval_runtime': 8.9964, 'eval_samples_per_second': 111.044, 'eval_steps_per_second': 7.003, 'epoch': 0.48}
{'loss': 2.3795, 'grad_norm': 0.4206082224845886, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.418210983276367, 'eval_runtime': 8.9421, 'eval_samples_per_second': 111.718, 'eval_steps_per_second': 7.045, 'epoch': 0.52}
{'loss': 2.4191, 'grad_norm': 0.4461108148097992, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.409167528152466, 'eval_runtime': 8.9078, 'eval_samples_per_second': 112.149, 'eval_steps_per_second': 7.072, 'epoch': 0.56}
{'loss': 2.4267, 'grad_norm': 0.3499302268028259, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.401906728744507, 'eval_runtime': 8.8919, 'eval_samples_per_second': 112.349, 'eval_steps_per_second': 7.085, 'epoch': 0.6}
{'loss': 2.4299, 'grad_norm': 0.33393117785453796, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.3961424827575684, 'eval_runtime': 8.8965, 'eval_samples_per_second': 112.291, 'eval_steps_per_second': 7.081, 'epoch': 0.64}
{'loss': 2.4343, 'grad_norm': 0.3805607855319977, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.3908731937408447, 'eval_runtime': 8.9014, 'eval_samples_per_second': 112.229, 'eval_steps_per_second': 7.078, 'epoch': 0.68}
{'loss': 2.3692, 'grad_norm': 0.31883010268211365, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.3868610858917236, 'eval_runtime': 8.8842, 'eval_samples_per_second': 112.447, 'eval_steps_per_second': 7.091, 'epoch': 0.72}
{'loss': 2.3762, 'grad_norm': 0.38195091485977173, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3829023838043213, 'eval_runtime': 8.8988, 'eval_samples_per_second': 112.263, 'eval_steps_per_second': 7.08, 'epoch': 0.76}
{'loss': 2.4151, 'grad_norm': 0.3770185112953186, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.379997730255127, 'eval_runtime': 8.8951, 'eval_samples_per_second': 112.309, 'eval_steps_per_second': 7.083, 'epoch': 0.8}
{'loss': 2.3631, 'grad_norm': 0.32879382371902466, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.3777596950531006, 'eval_runtime': 8.8985, 'eval_samples_per_second': 112.266, 'eval_steps_per_second': 7.08, 'epoch': 0.84}
{'loss': 2.4157, 'grad_norm': 0.32108792662620544, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3756613731384277, 'eval_runtime': 8.9143, 'eval_samples_per_second': 112.067, 'eval_steps_per_second': 7.067, 'epoch': 0.88}
{'loss': 2.4053, 'grad_norm': 0.3635176122188568, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.374159812927246, 'eval_runtime': 8.904, 'eval_samples_per_second': 112.196, 'eval_steps_per_second': 7.075, 'epoch': 0.92}
{'loss': 2.3601, 'grad_norm': 0.4012833833694458, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.3732357025146484, 'eval_runtime': 8.9135, 'eval_samples_per_second': 112.077, 'eval_steps_per_second': 7.068, 'epoch': 0.96}
{'loss': 2.3808, 'grad_norm': 0.3249508738517761, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.372873067855835, 'eval_runtime': 8.9074, 'eval_samples_per_second': 112.154, 'eval_steps_per_second': 7.073, 'epoch': 1.0}
{'train_runtime': 355.0299, 'train_samples_per_second': 28.164, 'train_steps_per_second': 1.76, 'train_loss': 2.621141827392578, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8949315547943115, 3.4795665740966797, 3.1100709438323975, 2.8434972763061523, 2.6886565685272217, 2.605184316635132, 2.551032066345215, 2.5114095211029053, 2.4807498455047607, 2.457355499267578, 2.440385580062866, 2.4282641410827637, 2.418210983276367, 2.409167528152466, 2.401906728744507, 2.3961424827575684, 2.3908731937408447, 2.3868610858917236, 2.3829023838043213, 2.379997730255127, 2.3777596950531006, 2.3756613731384277, 2.374159812927246, 2.3732357025146484, 2.372873067855835], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:09,  5.75it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:06, 58.48it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:04, 80.21it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:03, 89.04it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 95.46it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:00<00:03, 101.37it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 108.52it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 117.94it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 118.95it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 120.58it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 124.11it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 132.45it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:01<00:01, 144.24it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:01<00:01, 141.77it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 147.68it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 146.03it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 145.21it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 147.88it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 150.65it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 159.54it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 344/400 [00:02<00:00, 161.80it/s]Running loglikelihood requests:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 363/400 [00:02<00:00, 169.81it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 191.34it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 134.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  0.4789614677429199
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.5565000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2171 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.056347357562581
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 1.133535493727597
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.1474442295714433
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 1.1348104184048915
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.8829880321036724
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5476, dtype=torch.float64), 0, 0, 0, tensor(0.4524, dtype=torch.float64), 0, 0, 1, 0, 1, 0, 1, 0, 128, 5.551115123125794e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1.0316e-17, dtype=torch.float64), tensor(2.8764e-19, dtype=torch.float64), tensor(0.5476, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4524, dtype=torch.float64), tensor(1.8036e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(5.5511e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.548
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.452
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.551115123125794e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  5.551115123125794e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,014,656 || all params: 8,033,275,904 || trainable%: 0.0375
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.99it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.72it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.42it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.77it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.80it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.96it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 158.54it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 169.61it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 172.60it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 180.02it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 185.23it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 191.26it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 194.22it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 210.31it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 214.31it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 230.59it/s]Running loglikelihood requests:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 380/400 [00:02<00:00, 246.72it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.56it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.0036, 'grad_norm': 0.04527014493942261, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.9727139472961426, 'eval_runtime': 8.7563, 'eval_samples_per_second': 114.089, 'eval_steps_per_second': 7.195, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.87it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 112.60it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 126.30it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 135.80it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.61it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.71it/s]Running loglikelihood requests:  28%|‚ñà‚ñà‚ñä       | 112/400 [00:00<00:01, 167.11it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 167.12it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 170.36it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.97it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 184.81it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 188.07it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 194.23it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 202.72it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 215.65it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 225.03it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:01<00:00, 241.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 192.49it/s]
Evaluation performance at step 50: 0.51
{'loss': 3.7486, 'grad_norm': 0.14683672785758972, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 3.4210195541381836, 'eval_runtime': 8.7667, 'eval_samples_per_second': 113.954, 'eval_steps_per_second': 7.186, 'epoch': 0.08}
{'loss': 3.1217, 'grad_norm': 0.08188606053590775, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.9359636306762695, 'eval_runtime': 8.8134, 'eval_samples_per_second': 113.35, 'eval_steps_per_second': 7.148, 'epoch': 0.12}
{'loss': 2.8592, 'grad_norm': 0.06140479817986488, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.7185893058776855, 'eval_runtime': 8.8652, 'eval_samples_per_second': 112.688, 'eval_steps_per_second': 7.106, 'epoch': 0.16}
{'loss': 2.6749, 'grad_norm': 0.07537127286195755, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.584355354309082, 'eval_runtime': 8.915, 'eval_samples_per_second': 112.058, 'eval_steps_per_second': 7.067, 'epoch': 0.2}
{'loss': 2.5319, 'grad_norm': 0.05980288237333298, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.4884908199310303, 'eval_runtime': 8.9124, 'eval_samples_per_second': 112.091, 'eval_steps_per_second': 7.069, 'epoch': 0.24}
{'loss': 2.4853, 'grad_norm': 0.06652792543172836, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.4207310676574707, 'eval_runtime': 8.9081, 'eval_samples_per_second': 112.145, 'eval_steps_per_second': 7.072, 'epoch': 0.28}
{'loss': 2.3656, 'grad_norm': 0.061730775982141495, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.3684322834014893, 'eval_runtime': 8.8854, 'eval_samples_per_second': 112.431, 'eval_steps_per_second': 7.09, 'epoch': 0.32}
{'loss': 2.3693, 'grad_norm': 0.10408952087163925, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.332749843597412, 'eval_runtime': 8.8704, 'eval_samples_per_second': 112.621, 'eval_steps_per_second': 7.102, 'epoch': 0.36}
{'loss': 2.3473, 'grad_norm': 0.06329606473445892, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.304997444152832, 'eval_runtime': 8.8717, 'eval_samples_per_second': 112.605, 'eval_steps_per_second': 7.101, 'epoch': 0.4}
{'loss': 2.3116, 'grad_norm': 0.07640305161476135, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.283416271209717, 'eval_runtime': 8.8741, 'eval_samples_per_second': 112.574, 'eval_steps_per_second': 7.099, 'epoch': 0.44}
{'loss': 2.3065, 'grad_norm': 0.07524674385786057, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.268345594406128, 'eval_runtime': 8.8832, 'eval_samples_per_second': 112.459, 'eval_steps_per_second': 7.092, 'epoch': 0.48}
{'loss': 2.2726, 'grad_norm': 0.06337063014507294, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.256657123565674, 'eval_runtime': 8.9088, 'eval_samples_per_second': 112.136, 'eval_steps_per_second': 7.072, 'epoch': 0.52}
{'loss': 2.2484, 'grad_norm': 0.06980736553668976, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.2468957901000977, 'eval_runtime': 8.882, 'eval_samples_per_second': 112.475, 'eval_steps_per_second': 7.093, 'epoch': 0.56}
{'loss': 2.2781, 'grad_norm': 0.06731588393449783, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2393620014190674, 'eval_runtime': 8.8906, 'eval_samples_per_second': 112.366, 'eval_steps_per_second': 7.086, 'epoch': 0.6}
{'loss': 2.27, 'grad_norm': 0.06889311224222183, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.231924533843994, 'eval_runtime': 8.8841, 'eval_samples_per_second': 112.448, 'eval_steps_per_second': 7.091, 'epoch': 0.64}
{'loss': 2.2389, 'grad_norm': 0.07073819637298584, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.2264695167541504, 'eval_runtime': 8.88, 'eval_samples_per_second': 112.5, 'eval_steps_per_second': 7.095, 'epoch': 0.68}
{'loss': 2.2549, 'grad_norm': 0.0709771141409874, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.222130298614502, 'eval_runtime': 8.8893, 'eval_samples_per_second': 112.382, 'eval_steps_per_second': 7.087, 'epoch': 0.72}
{'loss': 2.2406, 'grad_norm': 0.07485996186733246, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.2179126739501953, 'eval_runtime': 8.8844, 'eval_samples_per_second': 112.444, 'eval_steps_per_second': 7.091, 'epoch': 0.76}
{'loss': 2.2143, 'grad_norm': 0.07732381671667099, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.2147939205169678, 'eval_runtime': 8.8763, 'eval_samples_per_second': 112.547, 'eval_steps_per_second': 7.098, 'epoch': 0.8}
{'loss': 2.2476, 'grad_norm': 0.07759332656860352, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.212238073348999, 'eval_runtime': 8.8877, 'eval_samples_per_second': 112.403, 'eval_steps_per_second': 7.088, 'epoch': 0.84}
{'loss': 2.234, 'grad_norm': 0.07493340969085693, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.210460901260376, 'eval_runtime': 8.8871, 'eval_samples_per_second': 112.41, 'eval_steps_per_second': 7.089, 'epoch': 0.88}
{'loss': 2.2625, 'grad_norm': 0.08311745524406433, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.209152936935425, 'eval_runtime': 8.8785, 'eval_samples_per_second': 112.519, 'eval_steps_per_second': 7.096, 'epoch': 0.92}
{'loss': 2.2005, 'grad_norm': 0.07553008943796158, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2080600261688232, 'eval_runtime': 8.8908, 'eval_samples_per_second': 112.363, 'eval_steps_per_second': 7.086, 'epoch': 0.96}
{'loss': 2.2324, 'grad_norm': 0.0815008357167244, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.207920789718628, 'eval_runtime': 8.8878, 'eval_samples_per_second': 112.402, 'eval_steps_per_second': 7.088, 'epoch': 1.0}
{'train_runtime': 350.0964, 'train_samples_per_second': 28.561, 'train_steps_per_second': 1.785, 'train_loss': 2.4928088439941405, 'epoch': 1.0}
train_results:  {'eval_loss': [3.9727139472961426, 3.4210195541381836, 2.9359636306762695, 2.7185893058776855, 2.584355354309082, 2.4884908199310303, 2.4207310676574707, 2.3684322834014893, 2.332749843597412, 2.304997444152832, 2.283416271209717, 2.268345594406128, 2.256657123565674, 2.2468957901000977, 2.2393620014190674, 2.231924533843994, 2.2264695167541504, 2.222130298614502, 2.2179126739501953, 2.2147939205169678, 2.212238073348999, 2.210460901260376, 2.209152936935425, 2.2080600261688232, 2.207920789718628], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:43,  1.78it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:14, 26.98it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 48.52it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 63.54it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 75.48it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 85.83it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 96.36it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 108.89it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 112.97it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 116.44it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:01, 121.50it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 130.28it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 143.18it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 141.64it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 147.73it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 146.58it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 145.84it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 148.93it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 151.06it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 160.09it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 345/400 [00:03<00:00, 165.67it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 377/400 [00:03<00:00, 209.79it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 399/400 [00:03<00:00, 194.26it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 117.77it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.2945060729980469
current iteration best possible performance (full train run):  0.5355000000000001
max performance so far:  0.5565000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4884 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.1310054910628726
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 1.176304695913903
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.8214157751318353
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 1.0753657868822004
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.1388798834519118
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4353, dtype=torch.float64), 0, 0, 0, tensor(0.5647, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 2, 6.716536632422937e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1.4080e-16, dtype=torch.float64), tensor(1.3730e-16, dtype=torch.float64), tensor(0.4353, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.0548e-17, dtype=torch.float64), tensor(3.2688e-17, dtype=torch.float64), tensor(0.5647, dtype=torch.float64), tensor(6.7661e-17, dtype=torch.float64), tensor(3.5632e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(6.7165e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.435
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.565
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (6.716536632422937e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  6.716536632422937e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,507,328 || all params: 8,031,768,576 || trainable%: 0.0188
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 61.59it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 99.71it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 112.15it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 120.41it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 125.78it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 138.95it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 140.01it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 150.40it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 150.51it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 152.48it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 178/400 [00:01<00:01, 160.40it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 163.82it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 165.03it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 170.15it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 174.79it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 186.98it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 197.51it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 203.54it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 216.02it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 169.52it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.8055, 'grad_norm': 1.4731855392456055, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.3709545135498047, 'eval_runtime': 9.7051, 'eval_samples_per_second': 102.936, 'eval_steps_per_second': 6.491, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 61.53it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 99.62it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 111.78it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 120.32it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 125.48it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 138.54it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 139.96it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 150.53it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 150.82it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 152.68it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 154.69it/s]Running loglikelihood requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 196/400 [00:01<00:01, 167.88it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 164.83it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 169.72it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 171.44it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 185.96it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 189.17it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 203.72it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 213.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 169.32it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.8283, 'grad_norm': 1.3771827220916748, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 2.388947010040283, 'eval_runtime': 9.7171, 'eval_samples_per_second': 102.808, 'eval_steps_per_second': 6.483, 'epoch': 0.08}
{'loss': 2.2165, 'grad_norm': 0.4872879683971405, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1090731620788574, 'eval_runtime': 9.719, 'eval_samples_per_second': 102.788, 'eval_steps_per_second': 6.482, 'epoch': 0.12}
{'loss': 2.1093, 'grad_norm': 0.5165981650352478, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0080738067626953, 'eval_runtime': 9.74, 'eval_samples_per_second': 102.567, 'eval_steps_per_second': 6.468, 'epoch': 0.16}
{'loss': 1.9817, 'grad_norm': 0.5545522570610046, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9496039152145386, 'eval_runtime': 9.7482, 'eval_samples_per_second': 102.48, 'eval_steps_per_second': 6.463, 'epoch': 0.2}
{'loss': 1.9245, 'grad_norm': 0.5157064199447632, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9262263774871826, 'eval_runtime': 9.7531, 'eval_samples_per_second': 102.428, 'eval_steps_per_second': 6.459, 'epoch': 0.24}
{'loss': 1.9673, 'grad_norm': 0.4611903131008148, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.911102533340454, 'eval_runtime': 9.7527, 'eval_samples_per_second': 102.433, 'eval_steps_per_second': 6.46, 'epoch': 0.28}
{'loss': 1.938, 'grad_norm': 0.4669734239578247, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.9001997709274292, 'eval_runtime': 9.7566, 'eval_samples_per_second': 102.392, 'eval_steps_per_second': 6.457, 'epoch': 0.32}
{'loss': 1.9331, 'grad_norm': 0.5852861404418945, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8956923484802246, 'eval_runtime': 9.6954, 'eval_samples_per_second': 103.038, 'eval_steps_per_second': 6.498, 'epoch': 0.36}
{'loss': 1.8557, 'grad_norm': 0.5066039562225342, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8869673013687134, 'eval_runtime': 9.6921, 'eval_samples_per_second': 103.074, 'eval_steps_per_second': 6.5, 'epoch': 0.4}
{'loss': 1.9291, 'grad_norm': 0.5238909721374512, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8806790113449097, 'eval_runtime': 9.6931, 'eval_samples_per_second': 103.063, 'eval_steps_per_second': 6.499, 'epoch': 0.44}
{'loss': 1.9158, 'grad_norm': 0.5303548574447632, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8756603002548218, 'eval_runtime': 9.6919, 'eval_samples_per_second': 103.076, 'eval_steps_per_second': 6.5, 'epoch': 0.48}
{'loss': 1.91, 'grad_norm': 0.49448007345199585, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8721541166305542, 'eval_runtime': 9.7234, 'eval_samples_per_second': 102.742, 'eval_steps_per_second': 6.479, 'epoch': 0.52}
{'loss': 1.8591, 'grad_norm': 0.4538331925868988, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8696810007095337, 'eval_runtime': 9.8322, 'eval_samples_per_second': 101.605, 'eval_steps_per_second': 6.408, 'epoch': 0.56}
{'loss': 1.8778, 'grad_norm': 0.4949796795845032, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8659149408340454, 'eval_runtime': 9.7886, 'eval_samples_per_second': 102.058, 'eval_steps_per_second': 6.436, 'epoch': 0.6}
{'loss': 1.8765, 'grad_norm': 0.48146528005599976, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.862706184387207, 'eval_runtime': 9.7923, 'eval_samples_per_second': 102.019, 'eval_steps_per_second': 6.434, 'epoch': 0.64}
{'loss': 1.8993, 'grad_norm': 0.7494974732398987, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8597145080566406, 'eval_runtime': 9.7836, 'eval_samples_per_second': 102.11, 'eval_steps_per_second': 6.439, 'epoch': 0.68}
{'loss': 1.8773, 'grad_norm': 0.635224461555481, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8584070205688477, 'eval_runtime': 9.8226, 'eval_samples_per_second': 101.704, 'eval_steps_per_second': 6.414, 'epoch': 0.72}
{'loss': 1.8841, 'grad_norm': 0.4812454283237457, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8558402061462402, 'eval_runtime': 9.8508, 'eval_samples_per_second': 101.413, 'eval_steps_per_second': 6.395, 'epoch': 0.76}
{'loss': 1.8757, 'grad_norm': 0.5340518355369568, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8543248176574707, 'eval_runtime': 9.8323, 'eval_samples_per_second': 101.604, 'eval_steps_per_second': 6.407, 'epoch': 0.8}
{'loss': 1.8989, 'grad_norm': 0.5699262022972107, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8526643514633179, 'eval_runtime': 9.7735, 'eval_samples_per_second': 102.215, 'eval_steps_per_second': 6.446, 'epoch': 0.84}
{'loss': 1.8868, 'grad_norm': 0.44189009070396423, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8520604372024536, 'eval_runtime': 9.7646, 'eval_samples_per_second': 102.309, 'eval_steps_per_second': 6.452, 'epoch': 0.88}
{'loss': 1.9093, 'grad_norm': 0.49851280450820923, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.850843071937561, 'eval_runtime': 9.7792, 'eval_samples_per_second': 102.156, 'eval_steps_per_second': 6.442, 'epoch': 0.92}
{'loss': 1.8297, 'grad_norm': 0.518440306186676, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8502686023712158, 'eval_runtime': 9.7531, 'eval_samples_per_second': 102.429, 'eval_steps_per_second': 6.459, 'epoch': 0.96}
{'loss': 1.8813, 'grad_norm': 0.4855113923549652, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.850006341934204, 'eval_runtime': 9.7619, 'eval_samples_per_second': 102.337, 'eval_steps_per_second': 6.454, 'epoch': 1.0}
{'train_runtime': 498.015, 'train_samples_per_second': 20.078, 'train_steps_per_second': 1.255, 'train_loss': 2.034821990966797, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3709545135498047, 2.388947010040283, 2.1090731620788574, 2.0080738067626953, 1.9496039152145386, 1.9262263774871826, 1.911102533340454, 1.9001997709274292, 1.8956923484802246, 1.8869673013687134, 1.8806790113449097, 1.8756603002548218, 1.8721541166305542, 1.8696810007095337, 1.8659149408340454, 1.862706184387207, 1.8597145080566406, 1.8584070205688477, 1.8558402061462402, 1.8543248176574707, 1.8526643514633179, 1.8520604372024536, 1.850843071937561, 1.8502686023712158, 1.850006341934204], 'performance': [0.48, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:32,  4.32it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 48.60it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 68.87it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 77.83it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 84.59it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 90.35it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 97.02it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 105.61it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 106.92it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 108.36it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 111.71it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 119.04it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 129.22it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 127.15it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 132.71it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 130.98it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 130.62it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 133.13it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 135.68it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 143.49it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 143.41it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 143.64it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 154.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 120.10it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  0.6097627878189087
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5565000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3869 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.0509982629292267
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 1.0754410623235944
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.0422514222615606
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.9764832718562197
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.090846211054841
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4314, dtype=torch.float64), 0, tensor(0.5237, dtype=torch.float64), 0, tensor(0.0449, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 1, 0, 128, 8.673617379883919e-19, 1.4800000190734879, 0]
normalized proposed parameters for next round by BO: [tensor(1.7630e-16, dtype=torch.float64), tensor(3.1107e-16, dtype=torch.float64), tensor(0.4314, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5237, dtype=torch.float64), tensor(3.2428e-17, dtype=torch.float64), tensor(0.0449, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(8.6736e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.431
  sciq: 0
  triviaqa: 0.524
  truthfulqa_gen: 0
  wikitext: 0.045
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (8.673617379883919e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (1.4800000190734879,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  8.673617379883919e-19
lora alpha:  1.4800000190734879
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 64.14it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 103.38it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 116.32it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 125.26it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 131.07it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 144.87it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 146.01it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 156.78it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 157.22it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 160.21it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 167.79it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 172.28it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 177.20it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 179.31it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 193.92it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 197.49it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 212.60it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 223.02it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 177.11it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.3652, 'grad_norm': 0.303500771522522, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.755565643310547, 'eval_runtime': 9.4324, 'eval_samples_per_second': 105.911, 'eval_steps_per_second': 6.679, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 64.21it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 102.91it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 114.81it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 122.82it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 128.55it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 142.73it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 144.47it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 155.53it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 156.23it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 159.65it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 167.19it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 171.84it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 176.74it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 178.77it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 193.45it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 197.00it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 211.77it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 222.38it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 176.07it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.9609, 'grad_norm': 0.2539435923099518, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.343991756439209, 'eval_runtime': 9.4663, 'eval_samples_per_second': 105.532, 'eval_steps_per_second': 6.655, 'epoch': 0.08}
{'loss': 2.0006, 'grad_norm': 0.0911608338356018, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8688361644744873, 'eval_runtime': 9.4914, 'eval_samples_per_second': 105.254, 'eval_steps_per_second': 6.638, 'epoch': 0.12}
{'loss': 1.8225, 'grad_norm': 0.07217424362897873, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7426856756210327, 'eval_runtime': 9.5244, 'eval_samples_per_second': 104.888, 'eval_steps_per_second': 6.615, 'epoch': 0.16}
{'loss': 1.7381, 'grad_norm': 0.06840343028306961, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6922106742858887, 'eval_runtime': 9.5509, 'eval_samples_per_second': 104.597, 'eval_steps_per_second': 6.596, 'epoch': 0.2}
{'loss': 1.7217, 'grad_norm': 0.054115861654281616, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6691936254501343, 'eval_runtime': 9.5766, 'eval_samples_per_second': 104.317, 'eval_steps_per_second': 6.579, 'epoch': 0.24}
{'loss': 1.6418, 'grad_norm': 0.07930701225996017, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6530784368515015, 'eval_runtime': 9.6059, 'eval_samples_per_second': 103.998, 'eval_steps_per_second': 6.558, 'epoch': 0.28}
{'loss': 1.6564, 'grad_norm': 0.07006636261940002, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6439735889434814, 'eval_runtime': 9.6492, 'eval_samples_per_second': 103.531, 'eval_steps_per_second': 6.529, 'epoch': 0.32}
{'loss': 1.6564, 'grad_norm': 0.0554373674094677, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6352847814559937, 'eval_runtime': 9.6853, 'eval_samples_per_second': 103.146, 'eval_steps_per_second': 6.505, 'epoch': 0.36}
{'loss': 1.5924, 'grad_norm': 0.06345032900571823, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6279546022415161, 'eval_runtime': 9.647, 'eval_samples_per_second': 103.555, 'eval_steps_per_second': 6.531, 'epoch': 0.4}
{'loss': 1.6586, 'grad_norm': 0.05811060592532158, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6222294569015503, 'eval_runtime': 9.6387, 'eval_samples_per_second': 103.645, 'eval_steps_per_second': 6.536, 'epoch': 0.44}
{'loss': 1.5815, 'grad_norm': 0.05255923047661781, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.617126703262329, 'eval_runtime': 9.632, 'eval_samples_per_second': 103.717, 'eval_steps_per_second': 6.541, 'epoch': 0.48}
{'loss': 1.5945, 'grad_norm': 0.05501207709312439, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6139510869979858, 'eval_runtime': 9.6183, 'eval_samples_per_second': 103.865, 'eval_steps_per_second': 6.55, 'epoch': 0.52}
{'loss': 1.6042, 'grad_norm': 0.059818606823682785, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.609163522720337, 'eval_runtime': 9.6211, 'eval_samples_per_second': 103.834, 'eval_steps_per_second': 6.548, 'epoch': 0.56}
{'loss': 1.6106, 'grad_norm': 0.07009225338697433, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6060771942138672, 'eval_runtime': 9.6465, 'eval_samples_per_second': 103.561, 'eval_steps_per_second': 6.531, 'epoch': 0.6}
{'loss': 1.5965, 'grad_norm': 0.0747886523604393, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6031599044799805, 'eval_runtime': 9.6357, 'eval_samples_per_second': 103.677, 'eval_steps_per_second': 6.538, 'epoch': 0.64}
{'loss': 1.5848, 'grad_norm': 0.05942830443382263, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6019679307937622, 'eval_runtime': 9.6174, 'eval_samples_per_second': 103.874, 'eval_steps_per_second': 6.551, 'epoch': 0.68}
{'loss': 1.6462, 'grad_norm': 0.06463690102100372, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5991966724395752, 'eval_runtime': 9.6162, 'eval_samples_per_second': 103.888, 'eval_steps_per_second': 6.551, 'epoch': 0.72}
{'loss': 1.6156, 'grad_norm': 0.062226079404354095, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5971828699111938, 'eval_runtime': 9.6291, 'eval_samples_per_second': 103.748, 'eval_steps_per_second': 6.543, 'epoch': 0.76}
{'loss': 1.5924, 'grad_norm': 0.06385543197393417, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5957287549972534, 'eval_runtime': 9.6229, 'eval_samples_per_second': 103.815, 'eval_steps_per_second': 6.547, 'epoch': 0.8}
{'loss': 1.6284, 'grad_norm': 0.0635337382555008, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5945910215377808, 'eval_runtime': 9.5912, 'eval_samples_per_second': 104.158, 'eval_steps_per_second': 6.568, 'epoch': 0.84}
{'loss': 1.6046, 'grad_norm': 0.06333069503307343, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5932857990264893, 'eval_runtime': 9.5967, 'eval_samples_per_second': 104.099, 'eval_steps_per_second': 6.565, 'epoch': 0.88}
{'loss': 1.5865, 'grad_norm': 0.062014393508434296, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.592951774597168, 'eval_runtime': 9.5736, 'eval_samples_per_second': 104.35, 'eval_steps_per_second': 6.581, 'epoch': 0.92}
{'loss': 1.638, 'grad_norm': 0.06288504600524902, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5921121835708618, 'eval_runtime': 9.5682, 'eval_samples_per_second': 104.409, 'eval_steps_per_second': 6.584, 'epoch': 0.96}
{'loss': 1.6388, 'grad_norm': 0.06553645431995392, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.591533899307251, 'eval_runtime': 9.5718, 'eval_samples_per_second': 104.369, 'eval_steps_per_second': 6.582, 'epoch': 1.0}
{'train_runtime': 481.2984, 'train_samples_per_second': 20.775, 'train_steps_per_second': 1.299, 'train_loss': 1.8134881958007814, 'epoch': 1.0}
train_results:  {'eval_loss': [3.755565643310547, 2.343991756439209, 1.8688361644744873, 1.7426856756210327, 1.6922106742858887, 1.6691936254501343, 1.6530784368515015, 1.6439735889434814, 1.6352847814559937, 1.6279546022415161, 1.6222294569015503, 1.617126703262329, 1.6139510869979858, 1.609163522720337, 1.6060771942138672, 1.6031599044799805, 1.6019679307937622, 1.5991966724395752, 1.5971828699111938, 1.5957287549972534, 1.5945910215377808, 1.5932857990264893, 1.592951774597168, 1.5921121835708618, 1.591533899307251], 'performance': [0.5, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:16,  2.03it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:11, 32.16it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 53.91it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 67.04it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 76.90it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 85.09it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 94.03it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 104.21it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 107.08it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 109.63it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 113.89it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 121.64it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 133.30it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 131.30it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 137.22it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 136.07it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 135.65it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 138.25it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 140.55it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 148.65it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 148.12it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 149.05it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 160.26it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 114.90it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.414524793624878
current iteration best possible performance (full train run):  0.525
max performance so far:  0.5565000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3713 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.0512627372780705
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.8657696755481649
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.0143159633899872
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.8453599314201004
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.6720129082152496
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5784, dtype=torch.float64), 0, tensor(0.4216, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 128, 2.460439600698641e-18, 1.4800000190734908, 0]
normalized proposed parameters for next round by BO: [tensor(0.5784, dtype=torch.float64), tensor(2.5490e-16, dtype=torch.float64), tensor(0.4216, dtype=torch.float64), tensor(2.4423e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.0915e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6216e-16, dtype=torch.float64), tensor(1.3251e-16, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(2.4604e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.578
  gsm8k: 0
  rowan_hellaswag: 0.422
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.460439600698641e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734908,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  2.460439600698641e-18
lora alpha:  1.4800000190734908
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,014,656 || all params: 8,033,275,904 || trainable%: 0.0375
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.24it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.50it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.65it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.80it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.07it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.51it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 159.10it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 170.43it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 173.33it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 180.68it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 185.44it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 191.10it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 194.00it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 209.91it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 214.35it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 231.16it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 244.77it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 192.18it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.4348, 'grad_norm': 0.06531841307878494, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 4.424671649932861, 'eval_runtime': 8.821, 'eval_samples_per_second': 113.252, 'eval_steps_per_second': 7.142, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.46it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.85it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.64it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 135.24it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.44it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.51it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 116/400 [00:00<00:01, 179.49it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 164.05it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 167.99it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 172.19it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.51it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 186.94it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 193.34it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 201.90it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 214.96it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 224.41it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:01<00:00, 241.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 192.12it/s]
Evaluation performance at step 50: 0.51
{'loss': 4.1136, 'grad_norm': 0.20268961787223816, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 3.4574060440063477, 'eval_runtime': 8.8152, 'eval_samples_per_second': 113.327, 'eval_steps_per_second': 7.147, 'epoch': 0.08}
{'loss': 3.1119, 'grad_norm': 0.11434943228960037, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.8023953437805176, 'eval_runtime': 8.8048, 'eval_samples_per_second': 113.461, 'eval_steps_per_second': 7.155, 'epoch': 0.12}
{'loss': 2.6741, 'grad_norm': 0.08666177093982697, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.4936270713806152, 'eval_runtime': 8.8573, 'eval_samples_per_second': 112.789, 'eval_steps_per_second': 7.113, 'epoch': 0.16}
{'loss': 2.4183, 'grad_norm': 0.07444403320550919, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.3230512142181396, 'eval_runtime': 8.8819, 'eval_samples_per_second': 112.476, 'eval_steps_per_second': 7.093, 'epoch': 0.2}
{'loss': 2.2779, 'grad_norm': 0.07712677121162415, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.2009167671203613, 'eval_runtime': 8.9033, 'eval_samples_per_second': 112.206, 'eval_steps_per_second': 7.076, 'epoch': 0.24}
{'loss': 2.1972, 'grad_norm': 0.0603451170027256, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.115614175796509, 'eval_runtime': 8.9721, 'eval_samples_per_second': 111.345, 'eval_steps_per_second': 7.022, 'epoch': 0.28}
{'loss': 2.1307, 'grad_norm': 0.08023970574140549, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.048731565475464, 'eval_runtime': 8.9719, 'eval_samples_per_second': 111.347, 'eval_steps_per_second': 7.022, 'epoch': 0.32}
{'loss': 2.0177, 'grad_norm': 0.09623430669307709, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.9949922561645508, 'eval_runtime': 8.9852, 'eval_samples_per_second': 111.183, 'eval_steps_per_second': 7.012, 'epoch': 0.36}
{'loss': 1.9891, 'grad_norm': 0.0585610494017601, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.9522712230682373, 'eval_runtime': 8.985, 'eval_samples_per_second': 111.185, 'eval_steps_per_second': 7.012, 'epoch': 0.4}
{'loss': 1.9489, 'grad_norm': 0.06454849243164062, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.9192918539047241, 'eval_runtime': 8.9772, 'eval_samples_per_second': 111.282, 'eval_steps_per_second': 7.018, 'epoch': 0.44}
{'loss': 1.9255, 'grad_norm': 0.05886106565594673, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8918383121490479, 'eval_runtime': 8.9808, 'eval_samples_per_second': 111.237, 'eval_steps_per_second': 7.015, 'epoch': 0.48}
{'loss': 1.8786, 'grad_norm': 0.07049525529146194, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8698322772979736, 'eval_runtime': 8.9652, 'eval_samples_per_second': 111.431, 'eval_steps_per_second': 7.027, 'epoch': 0.52}
{'loss': 1.8657, 'grad_norm': 0.061237633228302, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.852450966835022, 'eval_runtime': 8.9396, 'eval_samples_per_second': 111.75, 'eval_steps_per_second': 7.047, 'epoch': 0.56}
{'loss': 1.8869, 'grad_norm': 0.08946789801120758, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8376679420471191, 'eval_runtime': 8.9458, 'eval_samples_per_second': 111.673, 'eval_steps_per_second': 7.042, 'epoch': 0.6}
{'loss': 1.8264, 'grad_norm': 0.065719373524189, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8259766101837158, 'eval_runtime': 8.9397, 'eval_samples_per_second': 111.748, 'eval_steps_per_second': 7.047, 'epoch': 0.64}
{'loss': 1.8462, 'grad_norm': 0.05671596899628639, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8163292407989502, 'eval_runtime': 8.9409, 'eval_samples_per_second': 111.734, 'eval_steps_per_second': 7.046, 'epoch': 0.68}
{'loss': 1.7616, 'grad_norm': 0.06681383401155472, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.808062195777893, 'eval_runtime': 8.9343, 'eval_samples_per_second': 111.816, 'eval_steps_per_second': 7.051, 'epoch': 0.72}
{'loss': 1.8123, 'grad_norm': 0.06126008555293083, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.801567792892456, 'eval_runtime': 8.9309, 'eval_samples_per_second': 111.859, 'eval_steps_per_second': 7.054, 'epoch': 0.76}
{'loss': 1.8161, 'grad_norm': 0.07274919003248215, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.796410322189331, 'eval_runtime': 8.9386, 'eval_samples_per_second': 111.763, 'eval_steps_per_second': 7.048, 'epoch': 0.8}
{'loss': 1.7615, 'grad_norm': 0.06462615728378296, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7920836210250854, 'eval_runtime': 8.9507, 'eval_samples_per_second': 111.612, 'eval_steps_per_second': 7.039, 'epoch': 0.84}
{'loss': 1.7848, 'grad_norm': 0.0671568438410759, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.789023756980896, 'eval_runtime': 9.047, 'eval_samples_per_second': 110.423, 'eval_steps_per_second': 6.964, 'epoch': 0.88}
{'loss': 1.8112, 'grad_norm': 0.0642484501004219, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7865618467330933, 'eval_runtime': 9.0854, 'eval_samples_per_second': 109.956, 'eval_steps_per_second': 6.934, 'epoch': 0.92}
{'loss': 1.7253, 'grad_norm': 0.06443711370229721, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7850441932678223, 'eval_runtime': 9.0506, 'eval_samples_per_second': 110.38, 'eval_steps_per_second': 6.961, 'epoch': 0.96}
{'loss': 1.7777, 'grad_norm': 0.07001646608114243, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7848647832870483, 'eval_runtime': 9.0555, 'eval_samples_per_second': 110.32, 'eval_steps_per_second': 6.957, 'epoch': 1.0}
{'train_runtime': 361.5387, 'train_samples_per_second': 27.657, 'train_steps_per_second': 1.729, 'train_loss': 2.1917546325683595, 'epoch': 1.0}
train_results:  {'eval_loss': [4.424671649932861, 3.4574060440063477, 2.8023953437805176, 2.4936270713806152, 2.3230512142181396, 2.2009167671203613, 2.115614175796509, 2.048731565475464, 1.9949922561645508, 1.9522712230682373, 1.9192918539047241, 1.8918383121490479, 1.8698322772979736, 1.852450966835022, 1.8376679420471191, 1.8259766101837158, 1.8163292407989502, 1.808062195777893, 1.801567792892456, 1.796410322189331, 1.7920836210250854, 1.789023756980896, 1.7865618467330933, 1.7850441932678223, 1.7848647832870483], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:39,  4.02it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 49.91it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 72.99it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 83.74it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 91.62it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 98.03it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 105.87it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 116.19it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 118.22it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 119.99it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 123.47it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 131.88it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:01<00:01, 143.89it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 141.61it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 147.44it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 145.73it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 144.73it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 147.53it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 149.42it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 157.13it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:02<00:00, 156.90it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 157.64it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 192.97it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 131.20it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.2918140888214111
current iteration best possible performance (full train run):  0.5775000000000001
max performance so far:  0.5775000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5243 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 1.1133803731925735
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 1.0448753879398966
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.042456455826665
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.0035909255383206
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.0003883676878416
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5539, dtype=torch.float64), tensor(0.4400, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.4940117962016473e-18, 1.4800000190734943, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.5539, dtype=torch.float64), tensor(0.4400, dtype=torch.float64), tensor(1.5138e-17, dtype=torch.float64), tensor(1.2197e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0061, dtype=torch.float64), tensor(6.6879e-18, dtype=torch.float64), tensor(2.3978e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.4940e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.554
  rowan_hellaswag: 0.44
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.4940117962016473e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734943,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.4940117962016473e-18
lora alpha:  1.4800000190734943
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9938
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  993
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.97it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.84it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.61it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.05it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.73it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.60it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.72it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.00it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.04it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.38it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.84it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.25it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.31it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.16it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.57it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.13it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.18it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.50it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.87it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 182.02it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.75it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.46it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.1555, 'grad_norm': 0.13623012602329254, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 2.5153520107269287, 'eval_runtime': 11.0369, 'eval_samples_per_second': 89.971, 'eval_steps_per_second': 5.708, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.08it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.85it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.49it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.74it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 109.87it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.80it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.03it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.25it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.39it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.74it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.11it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.51it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.65it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.66it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.02it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.69it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.69it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.06it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.46it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.54it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.27it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.92it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.0508, 'grad_norm': 0.13858821988105774, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.661813497543335, 'eval_runtime': 11.0547, 'eval_samples_per_second': 89.826, 'eval_steps_per_second': 5.699, 'epoch': 0.08}
{'loss': 1.5427, 'grad_norm': 0.04810734838247299, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.4147510528564453, 'eval_runtime': 11.1056, 'eval_samples_per_second': 89.414, 'eval_steps_per_second': 5.673, 'epoch': 0.12}
{'loss': 1.3923, 'grad_norm': 0.04787498340010643, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.3351892232894897, 'eval_runtime': 11.1124, 'eval_samples_per_second': 89.36, 'eval_steps_per_second': 5.669, 'epoch': 0.16}
{'loss': 1.3542, 'grad_norm': 0.02849896252155304, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.314261794090271, 'eval_runtime': 11.1144, 'eval_samples_per_second': 89.344, 'eval_steps_per_second': 5.668, 'epoch': 0.2}
{'loss': 1.3417, 'grad_norm': 0.0359739251434803, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.301572561264038, 'eval_runtime': 11.0665, 'eval_samples_per_second': 89.73, 'eval_steps_per_second': 5.693, 'epoch': 0.24}
{'loss': 1.3131, 'grad_norm': 0.04206456243991852, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.2919235229492188, 'eval_runtime': 11.0733, 'eval_samples_per_second': 89.675, 'eval_steps_per_second': 5.689, 'epoch': 0.28}
{'loss': 1.3229, 'grad_norm': 0.034975565969944, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.284810185432434, 'eval_runtime': 11.0872, 'eval_samples_per_second': 89.562, 'eval_steps_per_second': 5.682, 'epoch': 0.32}
{'loss': 1.2988, 'grad_norm': 0.030672786757349968, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.2784423828125, 'eval_runtime': 11.1717, 'eval_samples_per_second': 88.885, 'eval_steps_per_second': 5.639, 'epoch': 0.36}
{'loss': 1.2724, 'grad_norm': 0.03386829048395157, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.272649884223938, 'eval_runtime': 11.161, 'eval_samples_per_second': 88.971, 'eval_steps_per_second': 5.645, 'epoch': 0.4}
{'loss': 1.243, 'grad_norm': 0.033558037132024765, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.266004204750061, 'eval_runtime': 11.1513, 'eval_samples_per_second': 89.048, 'eval_steps_per_second': 5.65, 'epoch': 0.44}
{'loss': 1.2777, 'grad_norm': 0.03586621582508087, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.26328706741333, 'eval_runtime': 11.1467, 'eval_samples_per_second': 89.085, 'eval_steps_per_second': 5.652, 'epoch': 0.48}
{'loss': 1.2483, 'grad_norm': 0.034706033766269684, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 1.2588770389556885, 'eval_runtime': 11.15, 'eval_samples_per_second': 89.058, 'eval_steps_per_second': 5.65, 'epoch': 0.52}
{'loss': 1.3216, 'grad_norm': 0.03656308352947235, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 1.2563201189041138, 'eval_runtime': 11.1648, 'eval_samples_per_second': 88.94, 'eval_steps_per_second': 5.643, 'epoch': 0.56}
{'loss': 1.2548, 'grad_norm': 0.03268132731318474, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 1.2523573637008667, 'eval_runtime': 11.167, 'eval_samples_per_second': 88.923, 'eval_steps_per_second': 5.642, 'epoch': 0.6}
{'loss': 1.2568, 'grad_norm': 0.03570239245891571, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 1.248576283454895, 'eval_runtime': 11.1616, 'eval_samples_per_second': 88.966, 'eval_steps_per_second': 5.644, 'epoch': 0.64}
{'loss': 1.249, 'grad_norm': 0.03720109164714813, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 1.2463128566741943, 'eval_runtime': 11.1894, 'eval_samples_per_second': 88.745, 'eval_steps_per_second': 5.63, 'epoch': 0.68}
{'loss': 1.2602, 'grad_norm': 0.03587818518280983, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 1.2430671453475952, 'eval_runtime': 11.1667, 'eval_samples_per_second': 88.925, 'eval_steps_per_second': 5.642, 'epoch': 0.72}
{'loss': 1.2698, 'grad_norm': 0.03744952008128166, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 1.2410091161727905, 'eval_runtime': 11.1602, 'eval_samples_per_second': 88.977, 'eval_steps_per_second': 5.645, 'epoch': 0.76}
{'loss': 1.2329, 'grad_norm': 0.03718156740069389, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 1.239532232284546, 'eval_runtime': 11.1592, 'eval_samples_per_second': 88.985, 'eval_steps_per_second': 5.646, 'epoch': 0.8}
{'loss': 1.2411, 'grad_norm': 0.039953310042619705, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 1.2375330924987793, 'eval_runtime': 11.1451, 'eval_samples_per_second': 89.098, 'eval_steps_per_second': 5.653, 'epoch': 0.84}
{'loss': 1.2445, 'grad_norm': 0.038799483329057693, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 1.2361340522766113, 'eval_runtime': 11.194, 'eval_samples_per_second': 88.708, 'eval_steps_per_second': 5.628, 'epoch': 0.88}
{'loss': 1.2514, 'grad_norm': 0.031212029978632927, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 1.2352124452590942, 'eval_runtime': 11.1609, 'eval_samples_per_second': 88.972, 'eval_steps_per_second': 5.645, 'epoch': 0.92}
{'loss': 1.2318, 'grad_norm': 0.038216251879930496, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 1.2341053485870361, 'eval_runtime': 11.1345, 'eval_samples_per_second': 89.182, 'eval_steps_per_second': 5.658, 'epoch': 0.96}
{'train_runtime': 557.4683, 'train_samples_per_second': 17.827, 'train_steps_per_second': 1.116, 'train_loss': 1.3965512579277013, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5153520107269287, 1.661813497543335, 1.4147510528564453, 1.3351892232894897, 1.314261794090271, 1.301572561264038, 1.2919235229492188, 1.284810185432434, 1.2784423828125, 1.272649884223938, 1.266004204750061, 1.26328706741333, 1.2588770389556885, 1.2563201189041138, 1.2523573637008667, 1.248576283454895, 1.2463128566741943, 1.2430671453475952, 1.2410091161727905, 1.239532232284546, 1.2375330924987793, 1.2361340522766113, 1.2352124452590942, 1.2341053485870361], 'performance': [0.48, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:09,  2.10it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 27.40it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 46.12it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 57.59it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 66.50it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 73.83it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 81.38it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 90.53it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 92.68it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 94.72it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.52it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.46it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.15it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.62it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.68it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 117.26it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.60it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.85it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.25it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.43it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.23it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.78it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.63it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.88it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.424363136291504
current iteration best possible performance (full train run):  0.462
max performance so far:  0.5775000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0972 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.9898713773637974
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9475028727586398
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.229905227612414
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.867510364046771
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.8026220142332767
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4358, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5642, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 128, 7.342764528662472e-19, 1.480000019073497, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(9.4331e-17, dtype=torch.float64), tensor(0.4358, dtype=torch.float64), tensor(2.1742e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.0101e-17, dtype=torch.float64), tensor(7.4110e-19, dtype=torch.float64), tensor(0.5642, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(7.3428e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.436
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.564
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (7.342764528662472e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073497,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  7.342764528662472e-19
lora alpha:  1.480000019073497
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.16it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.93it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.77it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.01it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.60it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.47it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.54it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.71it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.75it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.12it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.71it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.18it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.23it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.16it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.57it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.09it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.03it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.57it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.23it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.50it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.29it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.25it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.745, 'grad_norm': 0.20300668478012085, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 2.9314682483673096, 'eval_runtime': 11.022, 'eval_samples_per_second': 90.637, 'eval_steps_per_second': 5.716, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.12it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.76it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.62it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.05it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.64it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.38it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.55it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.79it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.94it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.41it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.88it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.29it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.38it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.18it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.51it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.10it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.19it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.54it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.83it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.82it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.99it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.30it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.4002, 'grad_norm': 0.13136279582977295, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.9387956857681274, 'eval_runtime': 11.0327, 'eval_samples_per_second': 90.549, 'eval_steps_per_second': 5.71, 'epoch': 0.08}
{'loss': 1.7728, 'grad_norm': 0.053433410823345184, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7210242748260498, 'eval_runtime': 11.1347, 'eval_samples_per_second': 89.719, 'eval_steps_per_second': 5.658, 'epoch': 0.12}
{'loss': 1.6615, 'grad_norm': 0.044050492346286774, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.62900710105896, 'eval_runtime': 11.1328, 'eval_samples_per_second': 89.735, 'eval_steps_per_second': 5.659, 'epoch': 0.16}
{'loss': 1.5388, 'grad_norm': 0.03991607949137688, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5976799726486206, 'eval_runtime': 11.1367, 'eval_samples_per_second': 89.704, 'eval_steps_per_second': 5.657, 'epoch': 0.2}
{'loss': 1.5976, 'grad_norm': 0.032479237765073776, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5827069282531738, 'eval_runtime': 11.09, 'eval_samples_per_second': 90.081, 'eval_steps_per_second': 5.681, 'epoch': 0.24}
{'loss': 1.5167, 'grad_norm': 0.03686065599322319, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5742263793945312, 'eval_runtime': 11.0625, 'eval_samples_per_second': 90.305, 'eval_steps_per_second': 5.695, 'epoch': 0.28}
{'loss': 1.5631, 'grad_norm': 0.03923358768224716, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5659677982330322, 'eval_runtime': 11.0826, 'eval_samples_per_second': 90.141, 'eval_steps_per_second': 5.685, 'epoch': 0.32}
{'loss': 1.5569, 'grad_norm': 0.033495329320430756, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5571402311325073, 'eval_runtime': 11.0721, 'eval_samples_per_second': 90.227, 'eval_steps_per_second': 5.69, 'epoch': 0.36}
{'loss': 1.5393, 'grad_norm': 0.03394198417663574, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.550427794456482, 'eval_runtime': 11.0806, 'eval_samples_per_second': 90.158, 'eval_steps_per_second': 5.686, 'epoch': 0.4}
{'loss': 1.5759, 'grad_norm': 0.03708259016275406, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5443353652954102, 'eval_runtime': 11.1042, 'eval_samples_per_second': 89.966, 'eval_steps_per_second': 5.674, 'epoch': 0.44}
{'loss': 1.5346, 'grad_norm': 0.04666224494576454, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5396205186843872, 'eval_runtime': 11.1594, 'eval_samples_per_second': 89.521, 'eval_steps_per_second': 5.645, 'epoch': 0.48}
{'loss': 1.5391, 'grad_norm': 0.03773592785000801, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5356930494308472, 'eval_runtime': 11.2479, 'eval_samples_per_second': 88.816, 'eval_steps_per_second': 5.601, 'epoch': 0.52}
{'loss': 1.524, 'grad_norm': 0.040906962007284164, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5317543745040894, 'eval_runtime': 11.2318, 'eval_samples_per_second': 88.944, 'eval_steps_per_second': 5.609, 'epoch': 0.56}
{'loss': 1.4995, 'grad_norm': 0.035964176058769226, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5279744863510132, 'eval_runtime': 11.1634, 'eval_samples_per_second': 89.489, 'eval_steps_per_second': 5.643, 'epoch': 0.6}
{'loss': 1.5265, 'grad_norm': 0.03908354789018631, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5246672630310059, 'eval_runtime': 11.2117, 'eval_samples_per_second': 89.104, 'eval_steps_per_second': 5.619, 'epoch': 0.64}
{'loss': 1.4981, 'grad_norm': 0.03681733086705208, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5212076902389526, 'eval_runtime': 11.1577, 'eval_samples_per_second': 89.535, 'eval_steps_per_second': 5.646, 'epoch': 0.68}
{'loss': 1.5326, 'grad_norm': 0.03826764225959778, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5184623003005981, 'eval_runtime': 11.1763, 'eval_samples_per_second': 89.385, 'eval_steps_per_second': 5.637, 'epoch': 0.72}
{'loss': 1.5182, 'grad_norm': 0.040491972118616104, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5159173011779785, 'eval_runtime': 11.1224, 'eval_samples_per_second': 89.819, 'eval_steps_per_second': 5.664, 'epoch': 0.76}
{'loss': 1.5027, 'grad_norm': 0.039530593901872635, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5137367248535156, 'eval_runtime': 11.071, 'eval_samples_per_second': 90.236, 'eval_steps_per_second': 5.691, 'epoch': 0.8}
{'loss': 1.5118, 'grad_norm': 0.045109737664461136, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5115464925765991, 'eval_runtime': 11.0796, 'eval_samples_per_second': 90.166, 'eval_steps_per_second': 5.686, 'epoch': 0.84}
{'loss': 1.5279, 'grad_norm': 0.04255935549736023, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5100443363189697, 'eval_runtime': 11.0801, 'eval_samples_per_second': 90.162, 'eval_steps_per_second': 5.686, 'epoch': 0.88}
{'loss': 1.526, 'grad_norm': 0.03699919581413269, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5088385343551636, 'eval_runtime': 11.0901, 'eval_samples_per_second': 90.081, 'eval_steps_per_second': 5.681, 'epoch': 0.92}
{'loss': 1.5513, 'grad_norm': 0.0408657044172287, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5081708431243896, 'eval_runtime': 11.1075, 'eval_samples_per_second': 89.94, 'eval_steps_per_second': 5.672, 'epoch': 0.96}
{'loss': 1.5439, 'grad_norm': 0.04947102814912796, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5079398155212402, 'eval_runtime': 11.1389, 'eval_samples_per_second': 89.686, 'eval_steps_per_second': 5.656, 'epoch': 1.0}
{'train_runtime': 571.0578, 'train_samples_per_second': 17.51, 'train_steps_per_second': 1.094, 'train_loss': 1.672160235595703, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9314682483673096, 1.9387956857681274, 1.7210242748260498, 1.62900710105896, 1.5976799726486206, 1.5827069282531738, 1.5742263793945312, 1.5659677982330322, 1.5571402311325073, 1.550427794456482, 1.5443353652954102, 1.5396205186843872, 1.5356930494308472, 1.5317543745040894, 1.5279744863510132, 1.5246672630310059, 1.5212076902389526, 1.5184623003005981, 1.5159173011779785, 1.5137367248535156, 1.5115464925765991, 1.5100443363189697, 1.5088385343551636, 1.5081708431243896, 1.5079398155212402], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.62it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.38it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.93it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.83it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.31it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 81.97it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.59it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 94.73it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.96it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.25it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.45it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 107.05it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 116.31it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.13it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.79it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.78it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 117.10it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 119.30it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.34it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.41it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.32it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.87it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.33it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.84it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4244529008865356
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5775000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2891 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.7482777280238591
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.920889765910949
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 1.0115131279151663
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 1.16087151998656
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 1.0506846635798404
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4441, dtype=torch.float64), tensor(0.5559, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734919, 0]
normalized proposed parameters for next round by BO: [tensor(2.6391e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4441, dtype=torch.float64), tensor(0.5559, dtype=torch.float64), tensor(1.0895e-16, dtype=torch.float64), tensor(7.1521e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.0318e-16, dtype=torch.float64), tensor(3.7270e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.444
  sciq: 0.556
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734919,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734919
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.98it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.78it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.70it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.07it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.71it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.60it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.79it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.76it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.74it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.08it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.57it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.00it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.39it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.41it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.91it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.54it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.62it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.12it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.44it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.46it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.14it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3777, 'grad_norm': 0.20802846550941467, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3605380058288574, 'eval_runtime': 11.0182, 'eval_samples_per_second': 90.669, 'eval_steps_per_second': 5.718, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.02it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.51it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.35it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.64it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.31it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.18it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.37it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.43it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.40it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.68it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.20it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.77it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.95it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.44it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.91it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.51it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.36it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.65it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.90it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 180.76it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.81it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.5535, 'grad_norm': 0.20595955848693848, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 1.9583077430725098, 'eval_runtime': 10.9646, 'eval_samples_per_second': 91.111, 'eval_steps_per_second': 5.746, 'epoch': 0.08}
{'loss': 1.7531, 'grad_norm': 0.06045466288924217, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.677959680557251, 'eval_runtime': 10.9346, 'eval_samples_per_second': 91.362, 'eval_steps_per_second': 5.762, 'epoch': 0.12}
{'loss': 1.596, 'grad_norm': 0.047555290162563324, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5641043186187744, 'eval_runtime': 10.9664, 'eval_samples_per_second': 91.097, 'eval_steps_per_second': 5.745, 'epoch': 0.16}
{'loss': 1.5168, 'grad_norm': 0.054268982261419296, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5400456190109253, 'eval_runtime': 10.9884, 'eval_samples_per_second': 90.914, 'eval_steps_per_second': 5.733, 'epoch': 0.2}
{'loss': 1.531, 'grad_norm': 0.04174283891916275, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5257161855697632, 'eval_runtime': 11.0055, 'eval_samples_per_second': 90.773, 'eval_steps_per_second': 5.724, 'epoch': 0.24}
{'loss': 1.4691, 'grad_norm': 0.044232673943042755, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5128365755081177, 'eval_runtime': 11.1162, 'eval_samples_per_second': 89.869, 'eval_steps_per_second': 5.667, 'epoch': 0.28}
{'loss': 1.4544, 'grad_norm': 0.0429040864109993, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5088832378387451, 'eval_runtime': 11.1178, 'eval_samples_per_second': 89.856, 'eval_steps_per_second': 5.667, 'epoch': 0.32}
{'loss': 1.5163, 'grad_norm': 0.043831001967191696, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5027072429656982, 'eval_runtime': 11.11, 'eval_samples_per_second': 89.919, 'eval_steps_per_second': 5.671, 'epoch': 0.36}
{'loss': 1.4841, 'grad_norm': 0.04216771572828293, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4932950735092163, 'eval_runtime': 11.1224, 'eval_samples_per_second': 89.819, 'eval_steps_per_second': 5.664, 'epoch': 0.4}
{'loss': 1.4984, 'grad_norm': 0.039718400686979294, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4871515035629272, 'eval_runtime': 11.1342, 'eval_samples_per_second': 89.724, 'eval_steps_per_second': 5.658, 'epoch': 0.44}
{'loss': 1.471, 'grad_norm': 0.0481114387512207, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4827775955200195, 'eval_runtime': 11.1946, 'eval_samples_per_second': 89.239, 'eval_steps_per_second': 5.628, 'epoch': 0.48}
{'loss': 1.4918, 'grad_norm': 0.042032625526189804, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4798914194107056, 'eval_runtime': 11.1374, 'eval_samples_per_second': 89.698, 'eval_steps_per_second': 5.657, 'epoch': 0.52}
{'loss': 1.4938, 'grad_norm': 0.04609740525484085, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4745979309082031, 'eval_runtime': 11.0968, 'eval_samples_per_second': 90.026, 'eval_steps_per_second': 5.677, 'epoch': 0.56}
{'loss': 1.4695, 'grad_norm': 0.044269025325775146, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4704508781433105, 'eval_runtime': 11.0463, 'eval_samples_per_second': 90.438, 'eval_steps_per_second': 5.703, 'epoch': 0.6}
{'loss': 1.4891, 'grad_norm': 0.04393437132239342, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4669928550720215, 'eval_runtime': 10.9888, 'eval_samples_per_second': 90.911, 'eval_steps_per_second': 5.733, 'epoch': 0.64}
{'loss': 1.4595, 'grad_norm': 0.050445035099983215, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4646738767623901, 'eval_runtime': 11.0472, 'eval_samples_per_second': 90.43, 'eval_steps_per_second': 5.703, 'epoch': 0.68}
{'loss': 1.5019, 'grad_norm': 0.04278946295380592, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4615358114242554, 'eval_runtime': 11.1198, 'eval_samples_per_second': 89.84, 'eval_steps_per_second': 5.666, 'epoch': 0.72}
{'loss': 1.4899, 'grad_norm': 0.047728415578603745, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4588714838027954, 'eval_runtime': 11.1212, 'eval_samples_per_second': 89.828, 'eval_steps_per_second': 5.665, 'epoch': 0.76}
{'loss': 1.4718, 'grad_norm': 0.04360748827457428, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.456304907798767, 'eval_runtime': 11.0914, 'eval_samples_per_second': 90.07, 'eval_steps_per_second': 5.68, 'epoch': 0.8}
{'loss': 1.4669, 'grad_norm': 0.044922877103090286, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.455631136894226, 'eval_runtime': 11.0584, 'eval_samples_per_second': 90.339, 'eval_steps_per_second': 5.697, 'epoch': 0.84}
{'loss': 1.4637, 'grad_norm': 0.04258204996585846, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.453745722770691, 'eval_runtime': 10.9966, 'eval_samples_per_second': 90.846, 'eval_steps_per_second': 5.729, 'epoch': 0.88}
{'loss': 1.4556, 'grad_norm': 0.04776136577129364, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4520705938339233, 'eval_runtime': 10.9902, 'eval_samples_per_second': 90.899, 'eval_steps_per_second': 5.732, 'epoch': 0.92}
{'loss': 1.4989, 'grad_norm': 0.040977589786052704, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.451279878616333, 'eval_runtime': 11.002, 'eval_samples_per_second': 90.802, 'eval_steps_per_second': 5.726, 'epoch': 0.96}
{'loss': 1.4644, 'grad_norm': 0.047460030764341354, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4508681297302246, 'eval_runtime': 11.0014, 'eval_samples_per_second': 90.807, 'eval_steps_per_second': 5.727, 'epoch': 1.0}
{'train_runtime': 556.7992, 'train_samples_per_second': 17.958, 'train_steps_per_second': 1.122, 'train_loss': 1.6575312072753907, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3605380058288574, 1.9583077430725098, 1.677959680557251, 1.5641043186187744, 1.5400456190109253, 1.5257161855697632, 1.5128365755081177, 1.5088832378387451, 1.5027072429656982, 1.4932950735092163, 1.4871515035629272, 1.4827775955200195, 1.4798914194107056, 1.4745979309082031, 1.4704508781433105, 1.4669928550720215, 1.4646738767623901, 1.4615358114242554, 1.4588714838027954, 1.456304907798767, 1.455631136894226, 1.453745722770691, 1.4520705938339233, 1.451279878616333, 1.4508681297302246], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:48,  2.37it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:12, 29.53it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 48.49it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 59.67it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 68.19it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 75.09it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 82.44it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 91.11it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 93.25it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 95.34it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 99.11it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.28it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.90it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.97it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.85it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.79it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 117.25it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.85it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.00it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.45it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.50it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 129.11it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 101.53it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4275639057159424
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5775000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0052 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.8151324585065028
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 1.0264538377436423
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.950961767319346
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 1.0162215044743008
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.9506475981024087
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1100, dtype=torch.float64), tensor(0.4543, dtype=torch.float64), tensor(0.1890, dtype=torch.float64), 0, 0, 0, tensor(0.0428, dtype=torch.float64), tensor(0.2039, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 4.163336342344338e-19, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1100, dtype=torch.float64), tensor(0.4543, dtype=torch.float64), tensor(0.1890, dtype=torch.float64), tensor(2.3305e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4411e-16, dtype=torch.float64), tensor(0.0428, dtype=torch.float64), tensor(0.2039, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(4.1633e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.11
  rowan_hellaswag: 0.454
  sciq: 0.189
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.043
  arc_challenge: 0.204

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.163336342344338e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  4.163336342344338e-19
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.27it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.17it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 99.10it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.50it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 111.18it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 123.08it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.26it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.46it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.45it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.82it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.23it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.70it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.80it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.65it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.92it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.45it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.39it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.85it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 169.10it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 182.23it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.85it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.7589, 'grad_norm': 0.16050709784030914, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 2.9984242916107178, 'eval_runtime': 10.9657, 'eval_samples_per_second': 91.103, 'eval_steps_per_second': 5.745, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.45it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.05it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.97it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.42it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 111.07it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.97it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.08it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.29it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.24it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.58it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.07it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.60it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.74it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.58it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.93it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.44it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.46it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.75it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 169.05it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 182.13it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.81it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.76it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.42, 'grad_norm': 0.20413872599601746, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.890439510345459, 'eval_runtime': 10.9771, 'eval_samples_per_second': 91.008, 'eval_steps_per_second': 5.739, 'epoch': 0.08}
{'loss': 1.7074, 'grad_norm': 0.0893801897764206, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6212598085403442, 'eval_runtime': 11.0073, 'eval_samples_per_second': 90.758, 'eval_steps_per_second': 5.723, 'epoch': 0.12}
{'loss': 1.526, 'grad_norm': 0.051819831132888794, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5144280195236206, 'eval_runtime': 11.1119, 'eval_samples_per_second': 89.904, 'eval_steps_per_second': 5.67, 'epoch': 0.16}
{'loss': 1.4545, 'grad_norm': 0.036789268255233765, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4878015518188477, 'eval_runtime': 11.1492, 'eval_samples_per_second': 89.603, 'eval_steps_per_second': 5.651, 'epoch': 0.2}
{'loss': 1.5049, 'grad_norm': 0.03745010122656822, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4726985692977905, 'eval_runtime': 11.1889, 'eval_samples_per_second': 89.285, 'eval_steps_per_second': 5.631, 'epoch': 0.24}
{'loss': 1.4256, 'grad_norm': 0.04388965666294098, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4605355262756348, 'eval_runtime': 11.2593, 'eval_samples_per_second': 88.727, 'eval_steps_per_second': 5.595, 'epoch': 0.28}
{'loss': 1.4062, 'grad_norm': 0.040018267929553986, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4508094787597656, 'eval_runtime': 11.2297, 'eval_samples_per_second': 88.961, 'eval_steps_per_second': 5.61, 'epoch': 0.32}
{'loss': 1.4603, 'grad_norm': 0.04167558252811432, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4439092874526978, 'eval_runtime': 11.1408, 'eval_samples_per_second': 89.671, 'eval_steps_per_second': 5.655, 'epoch': 0.36}
{'loss': 1.4385, 'grad_norm': 0.04350046440958977, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4363051652908325, 'eval_runtime': 11.1283, 'eval_samples_per_second': 89.771, 'eval_steps_per_second': 5.661, 'epoch': 0.4}
{'loss': 1.3911, 'grad_norm': 0.03953777998685837, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.430825114250183, 'eval_runtime': 11.1193, 'eval_samples_per_second': 89.844, 'eval_steps_per_second': 5.666, 'epoch': 0.44}
{'loss': 1.395, 'grad_norm': 0.036161985248327255, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4264100790023804, 'eval_runtime': 11.1473, 'eval_samples_per_second': 89.618, 'eval_steps_per_second': 5.652, 'epoch': 0.48}
{'loss': 1.3865, 'grad_norm': 0.04447496309876442, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.421464204788208, 'eval_runtime': 11.1242, 'eval_samples_per_second': 89.804, 'eval_steps_per_second': 5.663, 'epoch': 0.52}
{'loss': 1.4584, 'grad_norm': 0.039387281984090805, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4178252220153809, 'eval_runtime': 11.1981, 'eval_samples_per_second': 89.212, 'eval_steps_per_second': 5.626, 'epoch': 0.56}
{'loss': 1.3935, 'grad_norm': 0.040705230087041855, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4143073558807373, 'eval_runtime': 11.1538, 'eval_samples_per_second': 89.566, 'eval_steps_per_second': 5.648, 'epoch': 0.6}
{'loss': 1.4143, 'grad_norm': 0.047322019934654236, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4095304012298584, 'eval_runtime': 11.1592, 'eval_samples_per_second': 89.523, 'eval_steps_per_second': 5.646, 'epoch': 0.64}
{'loss': 1.3947, 'grad_norm': 0.045240581035614014, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.408352255821228, 'eval_runtime': 11.1581, 'eval_samples_per_second': 89.531, 'eval_steps_per_second': 5.646, 'epoch': 0.68}
{'loss': 1.3793, 'grad_norm': 0.037287551909685135, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4029960632324219, 'eval_runtime': 11.1622, 'eval_samples_per_second': 89.499, 'eval_steps_per_second': 5.644, 'epoch': 0.72}
{'loss': 1.3238, 'grad_norm': 0.04594636335968971, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4023219347000122, 'eval_runtime': 11.111, 'eval_samples_per_second': 89.911, 'eval_steps_per_second': 5.67, 'epoch': 0.76}
{'loss': 1.4202, 'grad_norm': 0.039461929351091385, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3990463018417358, 'eval_runtime': 11.0512, 'eval_samples_per_second': 90.397, 'eval_steps_per_second': 5.701, 'epoch': 0.8}
{'loss': 1.4321, 'grad_norm': 0.04034906253218651, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3965953588485718, 'eval_runtime': 11.1463, 'eval_samples_per_second': 89.626, 'eval_steps_per_second': 5.652, 'epoch': 0.84}
{'loss': 1.3822, 'grad_norm': 0.04722904413938522, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3949437141418457, 'eval_runtime': 11.1515, 'eval_samples_per_second': 89.584, 'eval_steps_per_second': 5.649, 'epoch': 0.88}
{'loss': 1.3523, 'grad_norm': 0.03971312567591667, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3935370445251465, 'eval_runtime': 11.1553, 'eval_samples_per_second': 89.554, 'eval_steps_per_second': 5.648, 'epoch': 0.92}
{'loss': 1.3937, 'grad_norm': 0.0490112341940403, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.392693281173706, 'eval_runtime': 11.145, 'eval_samples_per_second': 89.636, 'eval_steps_per_second': 5.653, 'epoch': 0.96}
{'loss': 1.3838, 'grad_norm': 0.0598180778324604, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.392665982246399, 'eval_runtime': 11.133, 'eval_samples_per_second': 89.734, 'eval_steps_per_second': 5.659, 'epoch': 1.0}
{'train_runtime': 571.2149, 'train_samples_per_second': 17.5, 'train_steps_per_second': 1.094, 'train_loss': 1.560129766845703, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9984242916107178, 1.890439510345459, 1.6212598085403442, 1.5144280195236206, 1.4878015518188477, 1.4726985692977905, 1.4605355262756348, 1.4508094787597656, 1.4439092874526978, 1.4363051652908325, 1.430825114250183, 1.4264100790023804, 1.421464204788208, 1.4178252220153809, 1.4143073558807373, 1.4095304012298584, 1.408352255821228, 1.4029960632324219, 1.4023219347000122, 1.3990463018417358, 1.3965953588485718, 1.3949437141418457, 1.3935370445251465, 1.392693281173706, 1.392665982246399], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.61it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.41it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 65.09it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 72.01it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.55it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 82.23it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.74it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.35it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.38it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.61it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.74it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 107.40it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 116.54it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.42it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 119.30it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.69it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.95it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 119.12it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.37it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.58it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.38it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 129.05it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 109.09it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4248653650283813
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.5775000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0055 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.9898981217202141
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.7325643729999646
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 1.0452390433305334
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.8977415372598168
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.9942608723480211
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0236, dtype=torch.float64), tensor(0.4348, dtype=torch.float64), tensor(0.0703, dtype=torch.float64), 0, 0, 0, 0, tensor(0.4714, dtype=torch.float64), 1, 0, 0, 1, 1, 1, 128, 8.743006318923108e-19, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(1.2514e-17, dtype=torch.float64), tensor(0.0236, dtype=torch.float64), tensor(0.4348, dtype=torch.float64), tensor(0.0703, dtype=torch.float64), tensor(1.5105e-17, dtype=torch.float64), tensor(1.7050e-17, dtype=torch.float64), tensor(1.9654e-18, dtype=torch.float64), tensor(4.0289e-18, dtype=torch.float64), tensor(0.4714, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(8.7430e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.024
  rowan_hellaswag: 0.435
  sciq: 0.07
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.471

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (8.743006318923108e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  8.743006318923108e-19
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 7,077,888 || all params: 8,037,339,136 || trainable%: 0.0881
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.82it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.57it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.32it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.44it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.47it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.88it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 157.11it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 168.59it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 170.55it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.78it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 184.64it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 187.24it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 192.65it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 201.19it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 213.89it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 223.63it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 240.56it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.08it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.1177, 'grad_norm': 0.05997509881854057, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 4.094134330749512, 'eval_runtime': 8.8803, 'eval_samples_per_second': 112.496, 'eval_steps_per_second': 7.094, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.46it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 110.51it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.31it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.09it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.19it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 154.73it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 155.74it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 165.74it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 152/400 [00:01<00:01, 167.50it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 170.79it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 181.45it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:00, 183.61it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 189.91it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 195.63it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 209.13it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:01<00:00, 221.12it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:01<00:00, 228.46it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 245.96it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 189.27it/s]
Evaluation performance at step 50: 0.48
{'loss': 3.7243, 'grad_norm': 0.1593305766582489, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 3.104142665863037, 'eval_runtime': 8.9844, 'eval_samples_per_second': 111.193, 'eval_steps_per_second': 7.012, 'epoch': 0.08}
{'loss': 2.7933, 'grad_norm': 0.09197098016738892, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.461819648742676, 'eval_runtime': 9.0046, 'eval_samples_per_second': 110.944, 'eval_steps_per_second': 6.996, 'epoch': 0.12}
{'loss': 2.176, 'grad_norm': 0.08069928735494614, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1136715412139893, 'eval_runtime': 9.0426, 'eval_samples_per_second': 110.477, 'eval_steps_per_second': 6.967, 'epoch': 0.16}
{'loss': 2.0039, 'grad_norm': 0.07394982874393463, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9378501176834106, 'eval_runtime': 9.0568, 'eval_samples_per_second': 110.304, 'eval_steps_per_second': 6.956, 'epoch': 0.2}
{'loss': 1.8608, 'grad_norm': 0.0689283162355423, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8543239831924438, 'eval_runtime': 9.0936, 'eval_samples_per_second': 109.857, 'eval_steps_per_second': 6.928, 'epoch': 0.24}
{'loss': 1.786, 'grad_norm': 0.05887533724308014, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.807700276374817, 'eval_runtime': 9.1181, 'eval_samples_per_second': 109.562, 'eval_steps_per_second': 6.909, 'epoch': 0.28}
{'loss': 1.7879, 'grad_norm': 0.06459804624319077, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.779666781425476, 'eval_runtime': 9.0812, 'eval_samples_per_second': 110.007, 'eval_steps_per_second': 6.937, 'epoch': 0.32}
{'loss': 1.726, 'grad_norm': 0.06457015126943588, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7587751150131226, 'eval_runtime': 9.0789, 'eval_samples_per_second': 110.035, 'eval_steps_per_second': 6.939, 'epoch': 0.36}
{'loss': 1.7525, 'grad_norm': 0.06960991024971008, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7431714534759521, 'eval_runtime': 9.0794, 'eval_samples_per_second': 110.029, 'eval_steps_per_second': 6.939, 'epoch': 0.4}
{'loss': 1.6662, 'grad_norm': 0.07190565764904022, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.731412649154663, 'eval_runtime': 9.0764, 'eval_samples_per_second': 110.066, 'eval_steps_per_second': 6.941, 'epoch': 0.44}
{'loss': 1.756, 'grad_norm': 0.08747576922178268, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7220443487167358, 'eval_runtime': 9.0656, 'eval_samples_per_second': 110.196, 'eval_steps_per_second': 6.949, 'epoch': 0.48}
{'loss': 1.6807, 'grad_norm': 0.07514552026987076, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7144122123718262, 'eval_runtime': 9.0697, 'eval_samples_per_second': 110.147, 'eval_steps_per_second': 6.946, 'epoch': 0.52}
{'loss': 1.6634, 'grad_norm': 0.09521665424108505, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7081854343414307, 'eval_runtime': 9.0789, 'eval_samples_per_second': 110.035, 'eval_steps_per_second': 6.939, 'epoch': 0.56}
{'loss': 1.6914, 'grad_norm': 0.0677948147058487, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7016702890396118, 'eval_runtime': 9.075, 'eval_samples_per_second': 110.083, 'eval_steps_per_second': 6.942, 'epoch': 0.6}
{'loss': 1.7183, 'grad_norm': 0.0763348788022995, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.69696843624115, 'eval_runtime': 9.0652, 'eval_samples_per_second': 110.202, 'eval_steps_per_second': 6.95, 'epoch': 0.64}
{'loss': 1.6455, 'grad_norm': 0.08747716248035431, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.693700909614563, 'eval_runtime': 9.1073, 'eval_samples_per_second': 109.692, 'eval_steps_per_second': 6.918, 'epoch': 0.68}
{'loss': 1.6454, 'grad_norm': 0.07606907933950424, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6896281242370605, 'eval_runtime': 9.046, 'eval_samples_per_second': 110.435, 'eval_steps_per_second': 6.964, 'epoch': 0.72}
{'loss': 1.7294, 'grad_norm': 0.0708051472902298, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6861608028411865, 'eval_runtime': 9.0415, 'eval_samples_per_second': 110.49, 'eval_steps_per_second': 6.968, 'epoch': 0.76}
{'loss': 1.7013, 'grad_norm': 0.06743964552879333, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6838890314102173, 'eval_runtime': 9.0316, 'eval_samples_per_second': 110.611, 'eval_steps_per_second': 6.975, 'epoch': 0.8}
{'loss': 1.6442, 'grad_norm': 0.07428205013275146, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6821670532226562, 'eval_runtime': 9.0398, 'eval_samples_per_second': 110.511, 'eval_steps_per_second': 6.969, 'epoch': 0.84}
{'loss': 1.6457, 'grad_norm': 0.06738804280757904, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6803933382034302, 'eval_runtime': 9.0173, 'eval_samples_per_second': 110.787, 'eval_steps_per_second': 6.987, 'epoch': 0.88}
{'loss': 1.6582, 'grad_norm': 0.07751810550689697, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6794633865356445, 'eval_runtime': 9.0475, 'eval_samples_per_second': 110.417, 'eval_steps_per_second': 6.963, 'epoch': 0.92}
{'loss': 1.6184, 'grad_norm': 0.07374899834394455, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6789690256118774, 'eval_runtime': 9.0421, 'eval_samples_per_second': 110.483, 'eval_steps_per_second': 6.967, 'epoch': 0.96}
{'loss': 1.7193, 'grad_norm': 0.08917245268821716, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6787902116775513, 'eval_runtime': 8.9904, 'eval_samples_per_second': 111.118, 'eval_steps_per_second': 7.007, 'epoch': 1.0}
{'train_runtime': 352.8181, 'train_samples_per_second': 28.338, 'train_steps_per_second': 1.771, 'train_loss': 1.9564764282226563, 'epoch': 1.0}
train_results:  {'eval_loss': [4.094134330749512, 3.104142665863037, 2.461819648742676, 2.1136715412139893, 1.9378501176834106, 1.8543239831924438, 1.807700276374817, 1.779666781425476, 1.7587751150131226, 1.7431714534759521, 1.731412649154663, 1.7220443487167358, 1.7144122123718262, 1.7081854343414307, 1.7016702890396118, 1.69696843624115, 1.693700909614563, 1.6896281242370605, 1.6861608028411865, 1.6838890314102173, 1.6821670532226562, 1.6803933382034302, 1.6794633865356445, 1.6789690256118774, 1.6787902116775513], 'performance': [0.49, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:41,  1.80it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:15, 25.00it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 45.86it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 60.66it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 73.06it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 83.62it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 94.54it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 106.92it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 110.89it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 114.64it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:01, 119.93it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 128.97it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 141.50it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 139.85it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 146.19it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 145.19it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 144.52it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 147.48it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 150.26it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 159.36it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 344/400 [00:03<00:00, 161.61it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 361/400 [00:03<00:00, 163.90it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 192.44it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 115.43it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.3060916662216187
current iteration best possible performance (full train run):  0.5880000000000001
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5844 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.9854809136117342
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.7886727346845275
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.8233643270540929
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.7524809499209689
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8499811110835767
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1416, dtype=torch.float64), tensor(0.4594, dtype=torch.float64), tensor(0.1658, dtype=torch.float64), 0, 0, tensor(0.1319, dtype=torch.float64), tensor(0.1012, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 128, 0.07257568108600239, 1.480000019073492, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1416, dtype=torch.float64), tensor(0.4594, dtype=torch.float64), tensor(0.1658, dtype=torch.float64), tensor(9.2226e-17, dtype=torch.float64), tensor(1.1255e-17, dtype=torch.float64), tensor(0.1319, dtype=torch.float64), tensor(0.1012, dtype=torch.float64), tensor(1.4466e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.7258, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.142
  rowan_hellaswag: 0.459
  sciq: 0.166
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.132
  mmlu: 0.101
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07257568108600239,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073492,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.07257568108600239
lora alpha:  1.480000019073492
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.18it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.80it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.70it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.12it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.69it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.43it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.61it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.87it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.89it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.25it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.68it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.18it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.39it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.28it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.66it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.19it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.19it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.51it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.79it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.85it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.40it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.7562, 'grad_norm': 0.1733791083097458, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 2.975517511367798, 'eval_runtime': 10.9644, 'eval_samples_per_second': 91.113, 'eval_steps_per_second': 5.746, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.40it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.09it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.92it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.21it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.75it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.67it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.91it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.14it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.07it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.32it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.77it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.21it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.45it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.35it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.76it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.34it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.38it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.68it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.99it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 182.09it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.77it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.59it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.4602, 'grad_norm': 0.1524137705564499, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.993599772453308, 'eval_runtime': 10.9754, 'eval_samples_per_second': 91.022, 'eval_steps_per_second': 5.74, 'epoch': 0.08}
{'loss': 1.8023, 'grad_norm': 0.0543210543692112, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7263001203536987, 'eval_runtime': 10.9823, 'eval_samples_per_second': 90.965, 'eval_steps_per_second': 5.737, 'epoch': 0.12}
{'loss': 1.6796, 'grad_norm': 0.05371485650539398, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6298879384994507, 'eval_runtime': 11.0318, 'eval_samples_per_second': 90.556, 'eval_steps_per_second': 5.711, 'epoch': 0.16}
{'loss': 1.658, 'grad_norm': 0.05752451717853546, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5960975885391235, 'eval_runtime': 11.119, 'eval_samples_per_second': 89.846, 'eval_steps_per_second': 5.666, 'epoch': 0.2}
{'loss': 1.6173, 'grad_norm': 0.040664974600076675, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5780572891235352, 'eval_runtime': 11.0865, 'eval_samples_per_second': 90.11, 'eval_steps_per_second': 5.683, 'epoch': 0.24}
{'loss': 1.5536, 'grad_norm': 0.04070911556482315, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5623410940170288, 'eval_runtime': 11.0614, 'eval_samples_per_second': 90.314, 'eval_steps_per_second': 5.695, 'epoch': 0.28}
{'loss': 1.5562, 'grad_norm': 0.04138009622693062, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.552375316619873, 'eval_runtime': 11.0194, 'eval_samples_per_second': 90.658, 'eval_steps_per_second': 5.717, 'epoch': 0.32}
{'loss': 1.5274, 'grad_norm': 0.03857125714421272, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5428991317749023, 'eval_runtime': 11.0341, 'eval_samples_per_second': 90.538, 'eval_steps_per_second': 5.71, 'epoch': 0.36}
{'loss': 1.4947, 'grad_norm': 0.03933002054691315, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5342706441879272, 'eval_runtime': 11.0288, 'eval_samples_per_second': 90.581, 'eval_steps_per_second': 5.712, 'epoch': 0.4}
{'loss': 1.5414, 'grad_norm': 0.04067632183432579, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5273637771606445, 'eval_runtime': 11.0315, 'eval_samples_per_second': 90.559, 'eval_steps_per_second': 5.711, 'epoch': 0.44}
{'loss': 1.5312, 'grad_norm': 0.0403975248336792, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5227715969085693, 'eval_runtime': 11.0487, 'eval_samples_per_second': 90.418, 'eval_steps_per_second': 5.702, 'epoch': 0.48}
{'loss': 1.543, 'grad_norm': 0.054014209657907486, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.517379641532898, 'eval_runtime': 11.1101, 'eval_samples_per_second': 89.918, 'eval_steps_per_second': 5.671, 'epoch': 0.52}
{'loss': 1.5341, 'grad_norm': 0.03702978789806366, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5127395391464233, 'eval_runtime': 11.1369, 'eval_samples_per_second': 89.702, 'eval_steps_per_second': 5.657, 'epoch': 0.56}
{'loss': 1.5229, 'grad_norm': 0.047939885407686234, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5084604024887085, 'eval_runtime': 11.1946, 'eval_samples_per_second': 89.24, 'eval_steps_per_second': 5.628, 'epoch': 0.6}
{'loss': 1.5132, 'grad_norm': 0.041705675423145294, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5052999258041382, 'eval_runtime': 11.1074, 'eval_samples_per_second': 89.94, 'eval_steps_per_second': 5.672, 'epoch': 0.64}
{'loss': 1.4812, 'grad_norm': 0.04038689285516739, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.50288987159729, 'eval_runtime': 11.1, 'eval_samples_per_second': 90.0, 'eval_steps_per_second': 5.676, 'epoch': 0.68}
{'loss': 1.5243, 'grad_norm': 0.03838805854320526, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.500108242034912, 'eval_runtime': 11.1045, 'eval_samples_per_second': 89.963, 'eval_steps_per_second': 5.673, 'epoch': 0.72}
{'loss': 1.5115, 'grad_norm': 0.04317261278629303, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.498083233833313, 'eval_runtime': 11.1384, 'eval_samples_per_second': 89.69, 'eval_steps_per_second': 5.656, 'epoch': 0.76}
{'loss': 1.5002, 'grad_norm': 0.038786690682172775, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4954593181610107, 'eval_runtime': 11.1862, 'eval_samples_per_second': 89.307, 'eval_steps_per_second': 5.632, 'epoch': 0.8}
{'loss': 1.4775, 'grad_norm': 0.04106908291578293, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4935094118118286, 'eval_runtime': 11.1901, 'eval_samples_per_second': 89.275, 'eval_steps_per_second': 5.63, 'epoch': 0.84}
{'loss': 1.5237, 'grad_norm': 0.04549405723810196, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4929287433624268, 'eval_runtime': 11.1466, 'eval_samples_per_second': 89.624, 'eval_steps_per_second': 5.652, 'epoch': 0.88}
{'loss': 1.5314, 'grad_norm': 0.04323091357946396, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4912948608398438, 'eval_runtime': 11.0977, 'eval_samples_per_second': 90.019, 'eval_steps_per_second': 5.677, 'epoch': 0.92}
{'loss': 1.5593, 'grad_norm': 0.03903034329414368, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.490599513053894, 'eval_runtime': 11.0585, 'eval_samples_per_second': 90.338, 'eval_steps_per_second': 5.697, 'epoch': 0.96}
{'loss': 1.5112, 'grad_norm': 0.04364553466439247, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.490385890007019, 'eval_runtime': 11.0389, 'eval_samples_per_second': 90.498, 'eval_steps_per_second': 5.707, 'epoch': 1.0}
{'train_runtime': 568.8464, 'train_samples_per_second': 17.576, 'train_steps_per_second': 1.099, 'train_loss': 1.6764667541503906, 'epoch': 1.0}
train_results:  {'eval_loss': [2.975517511367798, 1.993599772453308, 1.7263001203536987, 1.6298879384994507, 1.5960975885391235, 1.5780572891235352, 1.5623410940170288, 1.552375316619873, 1.5428991317749023, 1.5342706441879272, 1.5273637771606445, 1.5227715969085693, 1.517379641532898, 1.5127395391464233, 1.5084604024887085, 1.5052999258041382, 1.50288987159729, 1.500108242034912, 1.498083233833313, 1.4954593181610107, 1.4935094118118286, 1.4929287433624268, 1.4912948608398438, 1.490599513053894, 1.490385890007019], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:17,  2.91it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 37.83it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 56.97it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 66.36it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 73.50it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 79.20it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 85.72it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 93.90it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.38it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 96.92it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 100.33it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.91it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 116.46it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.67it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 119.54it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 118.22it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 117.60it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 119.83it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 122.10it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 129.08it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 129.00it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 129.61it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 139.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 105.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4254796504974365
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6111 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.871689205286443
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 1.0072627563369427
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.9663802122287453
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.5861276002650856
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8317220364576323
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4672, dtype=torch.float64), 0, 0, 0, tensor(0.5328, dtype=torch.float64), 0, 0, 32, 0, 0, 1, 1, 1, 128, 7.416883122540729e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4672, dtype=torch.float64), tensor(4.6866e-17, dtype=torch.float64), tensor(5.7476e-17, dtype=torch.float64), tensor(5.3220e-17, dtype=torch.float64), tensor(0.5328, dtype=torch.float64), tensor(3.4853e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(7.4169e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.467
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.533
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (7.416883122540729e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  7.416883122540729e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 55.93it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.95it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.31it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 110.00it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.71it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.92it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.06it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.06it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.01it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.66it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.49it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.32it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.63it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.58it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.22it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.97it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.40it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.58it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.42it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.48it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 201.00it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.05it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.7728, 'grad_norm': 0.13032424449920654, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.2285380363464355, 'eval_runtime': 10.575, 'eval_samples_per_second': 94.468, 'eval_steps_per_second': 5.957, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.36it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 91.32it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.53it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 110.19it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.97it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 127.20it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 128.41it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.96it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.99it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 139.47it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 141.07it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.83it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 151.13it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.99it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.54it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 157.20it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.50it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.62it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.57it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.58it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 201.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 154.45it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.6818, 'grad_norm': 0.16462281346321106, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.2879793643951416, 'eval_runtime': 10.6068, 'eval_samples_per_second': 94.185, 'eval_steps_per_second': 5.94, 'epoch': 0.08}
{'loss': 2.1453, 'grad_norm': 0.051025666296482086, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0471928119659424, 'eval_runtime': 10.6431, 'eval_samples_per_second': 93.864, 'eval_steps_per_second': 5.919, 'epoch': 0.12}
{'loss': 2.0203, 'grad_norm': 0.044660843908786774, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9592238664627075, 'eval_runtime': 10.6579, 'eval_samples_per_second': 93.733, 'eval_steps_per_second': 5.911, 'epoch': 0.16}
{'loss': 1.9152, 'grad_norm': 0.04713911563158035, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9084335565567017, 'eval_runtime': 10.7343, 'eval_samples_per_second': 93.066, 'eval_steps_per_second': 5.869, 'epoch': 0.2}
{'loss': 1.9043, 'grad_norm': 0.04716617986559868, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8908361196517944, 'eval_runtime': 10.7298, 'eval_samples_per_second': 93.105, 'eval_steps_per_second': 5.871, 'epoch': 0.24}
{'loss': 1.9004, 'grad_norm': 0.05741294100880623, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8787779808044434, 'eval_runtime': 10.7197, 'eval_samples_per_second': 93.193, 'eval_steps_per_second': 5.877, 'epoch': 0.28}
{'loss': 1.8938, 'grad_norm': 0.05612070485949516, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.871647834777832, 'eval_runtime': 10.7773, 'eval_samples_per_second': 92.695, 'eval_steps_per_second': 5.846, 'epoch': 0.32}
{'loss': 1.8642, 'grad_norm': 0.10288359969854355, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8616732358932495, 'eval_runtime': 10.7774, 'eval_samples_per_second': 92.694, 'eval_steps_per_second': 5.846, 'epoch': 0.36}
{'loss': 1.8999, 'grad_norm': 0.07684651762247086, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8513076305389404, 'eval_runtime': 10.7222, 'eval_samples_per_second': 93.171, 'eval_steps_per_second': 5.876, 'epoch': 0.4}
{'loss': 1.8603, 'grad_norm': 0.06179371103644371, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8457822799682617, 'eval_runtime': 10.7273, 'eval_samples_per_second': 93.127, 'eval_steps_per_second': 5.873, 'epoch': 0.44}
{'loss': 1.879, 'grad_norm': 0.06786299496889114, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8397399187088013, 'eval_runtime': 10.7251, 'eval_samples_per_second': 93.146, 'eval_steps_per_second': 5.874, 'epoch': 0.48}
{'loss': 1.8645, 'grad_norm': 0.05808739736676216, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8350296020507812, 'eval_runtime': 10.6747, 'eval_samples_per_second': 93.586, 'eval_steps_per_second': 5.902, 'epoch': 0.52}
{'loss': 1.8677, 'grad_norm': 0.062206871807575226, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8311196565628052, 'eval_runtime': 10.6797, 'eval_samples_per_second': 93.542, 'eval_steps_per_second': 5.899, 'epoch': 0.56}
{'loss': 1.8714, 'grad_norm': 0.05298803001642227, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8266372680664062, 'eval_runtime': 10.6706, 'eval_samples_per_second': 93.622, 'eval_steps_per_second': 5.904, 'epoch': 0.6}
{'loss': 1.8587, 'grad_norm': 0.04526553303003311, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8214572668075562, 'eval_runtime': 10.6806, 'eval_samples_per_second': 93.534, 'eval_steps_per_second': 5.899, 'epoch': 0.64}
{'loss': 1.8373, 'grad_norm': 0.06847276538610458, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8199007511138916, 'eval_runtime': 10.675, 'eval_samples_per_second': 93.583, 'eval_steps_per_second': 5.902, 'epoch': 0.68}
{'loss': 1.8389, 'grad_norm': 0.04739871993660927, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8160643577575684, 'eval_runtime': 10.6972, 'eval_samples_per_second': 93.389, 'eval_steps_per_second': 5.889, 'epoch': 0.72}
{'loss': 1.8238, 'grad_norm': 0.07499729841947556, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8134300708770752, 'eval_runtime': 10.7187, 'eval_samples_per_second': 93.202, 'eval_steps_per_second': 5.878, 'epoch': 0.76}
{'loss': 1.8179, 'grad_norm': 0.0662192553281784, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.810681700706482, 'eval_runtime': 10.6899, 'eval_samples_per_second': 93.453, 'eval_steps_per_second': 5.893, 'epoch': 0.8}
{'loss': 1.8439, 'grad_norm': 0.061986565589904785, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8079229593276978, 'eval_runtime': 10.6678, 'eval_samples_per_second': 93.647, 'eval_steps_per_second': 5.906, 'epoch': 0.84}
{'loss': 1.8245, 'grad_norm': 0.056973788887262344, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.806570291519165, 'eval_runtime': 10.6062, 'eval_samples_per_second': 94.19, 'eval_steps_per_second': 5.94, 'epoch': 0.88}
{'loss': 1.8481, 'grad_norm': 0.05630578473210335, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8052654266357422, 'eval_runtime': 10.6032, 'eval_samples_per_second': 94.217, 'eval_steps_per_second': 5.942, 'epoch': 0.92}
{'loss': 1.8096, 'grad_norm': 0.06274361163377762, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8043876886367798, 'eval_runtime': 10.6052, 'eval_samples_per_second': 94.199, 'eval_steps_per_second': 5.94, 'epoch': 0.96}
{'loss': 1.8676, 'grad_norm': 0.05991077423095703, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8042446374893188, 'eval_runtime': 10.6087, 'eval_samples_per_second': 94.168, 'eval_steps_per_second': 5.939, 'epoch': 1.0}
{'train_runtime': 549.4877, 'train_samples_per_second': 18.197, 'train_steps_per_second': 1.137, 'train_loss': 1.9884477722167968, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2285380363464355, 2.2879793643951416, 2.0471928119659424, 1.9592238664627075, 1.9084335565567017, 1.8908361196517944, 1.8787779808044434, 1.871647834777832, 1.8616732358932495, 1.8513076305389404, 1.8457822799682617, 1.8397399187088013, 1.8350296020507812, 1.8311196565628052, 1.8266372680664062, 1.8214572668075562, 1.8199007511138916, 1.8160643577575684, 1.8134300708770752, 1.810681700706482, 1.8079229593276978, 1.806570291519165, 1.8052654266357422, 1.8043876886367798, 1.8042446374893188], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:23,  4.77it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 49.08it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 67.25it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 74.67it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 80.34it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 85.05it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 90.89it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 98.73it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 99.52it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 100.63it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 103.71it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 110.67it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 120.45it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.32it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.12it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 121.33it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 120.83it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 123.08it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.52it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.83it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.77it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 133.23it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 143.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 112.67it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.423478126525879
current iteration best possible performance (full train run):  0.441
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9393 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.9932876948569789
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 1.2008572501528731
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.7351569540151377
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.7229322838136651
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.535038585045232
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4644, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5356, dtype=torch.float64), 0, 32, 0, 1, 0, 0, 1, 128, 2.2084803842197775e-18, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.0110e-16, dtype=torch.float64), tensor(0.4644, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.3092e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5356, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.2085e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.464
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.536
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.2084803842197775e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  128
lora dropout:  2.2084803842197775e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 63.33it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 102.20it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 115.03it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 123.78it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 128.60it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 142.66it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 144.16it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 155.05it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 155.07it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 158.04it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 164.97it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 169.11it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 173.87it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 175.71it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 189.86it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 193.04it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 207.27it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 216.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 173.66it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.7981, 'grad_norm': 0.15749062597751617, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.366804838180542, 'eval_runtime': 9.6041, 'eval_samples_per_second': 104.018, 'eval_steps_per_second': 6.56, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 63.22it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 102.19it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 114.82it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 123.31it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 128.82it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 142.45it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 143.64it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 154.47it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 154.72it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 176.54it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 185/400 [00:01<00:01, 155.85it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 164.82it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 168.15it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 173.84it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 181.02it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 192.06it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 200.32it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 215.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 173.19it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.7085, 'grad_norm': 0.0707254707813263, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 2.2298951148986816, 'eval_runtime': 9.6107, 'eval_samples_per_second': 103.946, 'eval_steps_per_second': 6.555, 'epoch': 0.08}
{'loss': 2.0046, 'grad_norm': 0.048528965562582016, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9213497638702393, 'eval_runtime': 9.6595, 'eval_samples_per_second': 103.421, 'eval_steps_per_second': 6.522, 'epoch': 0.12}
{'loss': 1.8132, 'grad_norm': 0.046703170984983444, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8065348863601685, 'eval_runtime': 9.6743, 'eval_samples_per_second': 103.263, 'eval_steps_per_second': 6.512, 'epoch': 0.16}
{'loss': 1.7611, 'grad_norm': 0.04093613475561142, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7594764232635498, 'eval_runtime': 9.6895, 'eval_samples_per_second': 103.102, 'eval_steps_per_second': 6.502, 'epoch': 0.2}
{'loss': 1.7417, 'grad_norm': 0.058904968202114105, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7275075912475586, 'eval_runtime': 9.7062, 'eval_samples_per_second': 102.924, 'eval_steps_per_second': 6.491, 'epoch': 0.24}
{'loss': 1.6629, 'grad_norm': 0.046994131058454514, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.706670880317688, 'eval_runtime': 9.7312, 'eval_samples_per_second': 102.66, 'eval_steps_per_second': 6.474, 'epoch': 0.28}
{'loss': 1.674, 'grad_norm': 0.06073882058262825, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.691112756729126, 'eval_runtime': 9.8435, 'eval_samples_per_second': 101.489, 'eval_steps_per_second': 6.4, 'epoch': 0.32}
{'loss': 1.67, 'grad_norm': 0.050394054502248764, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6780548095703125, 'eval_runtime': 9.8189, 'eval_samples_per_second': 101.742, 'eval_steps_per_second': 6.416, 'epoch': 0.36}
{'loss': 1.6726, 'grad_norm': 0.05111135542392731, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6639806032180786, 'eval_runtime': 9.8084, 'eval_samples_per_second': 101.851, 'eval_steps_per_second': 6.423, 'epoch': 0.4}
{'loss': 1.6773, 'grad_norm': 0.04690129682421684, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6466377973556519, 'eval_runtime': 9.8144, 'eval_samples_per_second': 101.789, 'eval_steps_per_second': 6.419, 'epoch': 0.44}
{'loss': 1.6383, 'grad_norm': 0.045196350663900375, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6194294691085815, 'eval_runtime': 9.8077, 'eval_samples_per_second': 101.859, 'eval_steps_per_second': 6.424, 'epoch': 0.48}
{'loss': 1.6179, 'grad_norm': 0.04871327430009842, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5942686796188354, 'eval_runtime': 9.8022, 'eval_samples_per_second': 101.916, 'eval_steps_per_second': 6.427, 'epoch': 0.52}
{'loss': 1.5858, 'grad_norm': 0.04063563793897629, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.58106529712677, 'eval_runtime': 9.7692, 'eval_samples_per_second': 102.26, 'eval_steps_per_second': 6.449, 'epoch': 0.56}
{'loss': 1.5996, 'grad_norm': 0.04836054891347885, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5680668354034424, 'eval_runtime': 9.7322, 'eval_samples_per_second': 102.649, 'eval_steps_per_second': 6.473, 'epoch': 0.6}
{'loss': 1.5805, 'grad_norm': 0.04970857873558998, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5547789335250854, 'eval_runtime': 9.7206, 'eval_samples_per_second': 102.772, 'eval_steps_per_second': 6.481, 'epoch': 0.64}
{'loss': 1.5812, 'grad_norm': 0.04008276388049126, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5487626791000366, 'eval_runtime': 9.7118, 'eval_samples_per_second': 102.865, 'eval_steps_per_second': 6.487, 'epoch': 0.68}
{'loss': 1.5578, 'grad_norm': 0.046329278498888016, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5459026098251343, 'eval_runtime': 9.7621, 'eval_samples_per_second': 102.335, 'eval_steps_per_second': 6.454, 'epoch': 0.72}
{'loss': 1.5372, 'grad_norm': 0.041381169110536575, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5434274673461914, 'eval_runtime': 9.7476, 'eval_samples_per_second': 102.486, 'eval_steps_per_second': 6.463, 'epoch': 0.76}
{'loss': 1.585, 'grad_norm': 0.05420298874378204, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.541367530822754, 'eval_runtime': 9.7508, 'eval_samples_per_second': 102.453, 'eval_steps_per_second': 6.461, 'epoch': 0.8}
{'loss': 1.5661, 'grad_norm': 0.04524451866745949, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5398017168045044, 'eval_runtime': 9.7645, 'eval_samples_per_second': 102.309, 'eval_steps_per_second': 6.452, 'epoch': 0.84}
{'loss': 1.5458, 'grad_norm': 0.056739211082458496, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5382401943206787, 'eval_runtime': 9.7553, 'eval_samples_per_second': 102.406, 'eval_steps_per_second': 6.458, 'epoch': 0.88}
{'loss': 1.5562, 'grad_norm': 0.04321115463972092, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5376338958740234, 'eval_runtime': 9.7584, 'eval_samples_per_second': 102.373, 'eval_steps_per_second': 6.456, 'epoch': 0.92}
{'loss': 1.5837, 'grad_norm': 0.04016575589776039, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.536806344985962, 'eval_runtime': 9.7656, 'eval_samples_per_second': 102.298, 'eval_steps_per_second': 6.451, 'epoch': 0.96}
{'loss': 1.56, 'grad_norm': 0.046090707182884216, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5364969968795776, 'eval_runtime': 9.758, 'eval_samples_per_second': 102.377, 'eval_steps_per_second': 6.456, 'epoch': 1.0}
{'train_runtime': 489.3082, 'train_samples_per_second': 20.435, 'train_steps_per_second': 1.277, 'train_loss': 1.7711647521972655, 'epoch': 1.0}
train_results:  {'eval_loss': [3.366804838180542, 2.2298951148986816, 1.9213497638702393, 1.8065348863601685, 1.7594764232635498, 1.7275075912475586, 1.706670880317688, 1.691112756729126, 1.6780548095703125, 1.6639806032180786, 1.6466377973556519, 1.6194294691085815, 1.5942686796188354, 1.58106529712677, 1.5680668354034424, 1.5547789335250854, 1.5487626791000366, 1.5459026098251343, 1.5434274673461914, 1.541367530822754, 1.5398017168045044, 1.5382401943206787, 1.5376338958740234, 1.536806344985962, 1.5364969968795776], 'performance': [0.48, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:50,  2.35it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 28.99it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 49.89it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 63.32it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 73.99it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 82.88it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 92.03it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 102.68it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 105.24it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 107.79it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 112.36it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 120.45it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 131.88it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 130.10it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 135.89it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 134.27it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 133.71it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 136.18it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 138.37it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 146.19it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 146.57it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 147.22it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 158.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 112.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4052627086639404
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.1016 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.7817863643807796
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 1.004327721980839
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.9758851423121606
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.5936269199616321
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 1.015520238195454
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4460, dtype=torch.float64), tensor(0.3771, dtype=torch.float64), 0, 0, tensor(0.1769, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 0, 0, 128, 0.1, 1.480000019073491, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4460, dtype=torch.float64), tensor(0.3771, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1769, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5349e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.446
  sciq: 0.377
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.177
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (1.480000019073491,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.480000019073491
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 66.61it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 107.03it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 121.26it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 130.79it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 136.88it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 151.55it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 152.93it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 164.08it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 152/400 [00:01<00:01, 165.41it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 167.88it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 177.17it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:00, 179.33it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 184.74it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 190.31it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 202.06it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:01<00:00, 213.82it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 220.45it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 237.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 184.39it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.3487, 'grad_norm': 0.2543598711490631, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 4.117644309997559, 'eval_runtime': 9.0874, 'eval_samples_per_second': 109.933, 'eval_steps_per_second': 6.933, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 66.96it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 107.71it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 121.10it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 130.37it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 136.44it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 150.80it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 152.24it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 162.65it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 152/400 [00:01<00:01, 164.03it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 167.32it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 177.30it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:00, 179.23it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 184.61it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 189.64it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 202.00it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:01<00:00, 213.27it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 220.10it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 393/400 [00:02<00:00, 251.96it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 183.92it/s]
Evaluation performance at step 50: 0.52
{'loss': 3.5058, 'grad_norm': 0.11465329676866531, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 2.8979427814483643, 'eval_runtime': 9.0826, 'eval_samples_per_second': 109.99, 'eval_steps_per_second': 6.936, 'epoch': 0.08}
{'loss': 2.512, 'grad_norm': 0.09098459780216217, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.264442205429077, 'eval_runtime': 9.1153, 'eval_samples_per_second': 109.596, 'eval_steps_per_second': 6.911, 'epoch': 0.12}
{'loss': 2.1595, 'grad_norm': 0.06757915765047073, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.090214729309082, 'eval_runtime': 9.1495, 'eval_samples_per_second': 109.187, 'eval_steps_per_second': 6.886, 'epoch': 0.16}
{'loss': 2.0168, 'grad_norm': 0.08026932924985886, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9826617240905762, 'eval_runtime': 9.1409, 'eval_samples_per_second': 109.289, 'eval_steps_per_second': 6.892, 'epoch': 0.2}
{'loss': 1.9323, 'grad_norm': 0.08725671470165253, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9282490015029907, 'eval_runtime': 9.1304, 'eval_samples_per_second': 109.415, 'eval_steps_per_second': 6.9, 'epoch': 0.24}
{'loss': 1.8984, 'grad_norm': 0.08249928802251816, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9039002656936646, 'eval_runtime': 9.1503, 'eval_samples_per_second': 109.176, 'eval_steps_per_second': 6.885, 'epoch': 0.28}
{'loss': 1.8776, 'grad_norm': 0.0679410919547081, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8827699422836304, 'eval_runtime': 9.1712, 'eval_samples_per_second': 108.928, 'eval_steps_per_second': 6.869, 'epoch': 0.32}
{'loss': 1.8317, 'grad_norm': 0.1153278797864914, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.869786024093628, 'eval_runtime': 9.1732, 'eval_samples_per_second': 108.904, 'eval_steps_per_second': 6.868, 'epoch': 0.36}
{'loss': 1.8587, 'grad_norm': 0.07120194286108017, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8602663278579712, 'eval_runtime': 9.1517, 'eval_samples_per_second': 109.16, 'eval_steps_per_second': 6.884, 'epoch': 0.4}
{'loss': 1.8343, 'grad_norm': 0.07530181109905243, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8519344329833984, 'eval_runtime': 9.1557, 'eval_samples_per_second': 109.112, 'eval_steps_per_second': 6.881, 'epoch': 0.44}
{'loss': 1.8178, 'grad_norm': 0.08454804122447968, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8431178331375122, 'eval_runtime': 9.1512, 'eval_samples_per_second': 109.166, 'eval_steps_per_second': 6.884, 'epoch': 0.48}
{'loss': 1.8432, 'grad_norm': 0.09223228693008423, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8359706401824951, 'eval_runtime': 9.183, 'eval_samples_per_second': 108.788, 'eval_steps_per_second': 6.86, 'epoch': 0.52}
{'loss': 1.7879, 'grad_norm': 0.08352640271186829, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8301926851272583, 'eval_runtime': 9.123, 'eval_samples_per_second': 109.504, 'eval_steps_per_second': 6.906, 'epoch': 0.56}
{'loss': 1.8072, 'grad_norm': 0.07387144863605499, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.825081706047058, 'eval_runtime': 9.0698, 'eval_samples_per_second': 110.145, 'eval_steps_per_second': 6.946, 'epoch': 0.6}
{'loss': 1.8349, 'grad_norm': 0.08194831758737564, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8195611238479614, 'eval_runtime': 9.0789, 'eval_samples_per_second': 110.035, 'eval_steps_per_second': 6.939, 'epoch': 0.64}
{'loss': 1.8324, 'grad_norm': 0.07594876736402512, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8156577348709106, 'eval_runtime': 9.0661, 'eval_samples_per_second': 110.191, 'eval_steps_per_second': 6.949, 'epoch': 0.68}
{'loss': 1.7687, 'grad_norm': 0.11205277591943741, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8129668235778809, 'eval_runtime': 9.03, 'eval_samples_per_second': 110.631, 'eval_steps_per_second': 6.977, 'epoch': 0.72}
{'loss': 1.7759, 'grad_norm': 0.08285441249608994, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8100606203079224, 'eval_runtime': 9.0214, 'eval_samples_per_second': 110.736, 'eval_steps_per_second': 6.983, 'epoch': 0.76}
{'loss': 1.7756, 'grad_norm': 0.09371974319219589, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8066405057907104, 'eval_runtime': 9.0725, 'eval_samples_per_second': 110.113, 'eval_steps_per_second': 6.944, 'epoch': 0.8}
{'loss': 1.7665, 'grad_norm': 0.1052331030368805, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8043465614318848, 'eval_runtime': 9.0597, 'eval_samples_per_second': 110.269, 'eval_steps_per_second': 6.954, 'epoch': 0.84}
{'loss': 1.7814, 'grad_norm': 0.10176254063844681, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.802043080329895, 'eval_runtime': 9.0581, 'eval_samples_per_second': 110.287, 'eval_steps_per_second': 6.955, 'epoch': 0.88}
{'loss': 1.7875, 'grad_norm': 0.07992690801620483, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8007268905639648, 'eval_runtime': 9.0527, 'eval_samples_per_second': 110.354, 'eval_steps_per_second': 6.959, 'epoch': 0.92}
{'loss': 1.7738, 'grad_norm': 0.09425580501556396, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7998342514038086, 'eval_runtime': 9.0502, 'eval_samples_per_second': 110.384, 'eval_steps_per_second': 6.961, 'epoch': 0.96}
{'loss': 1.767, 'grad_norm': 0.08680244535207748, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7994686365127563, 'eval_runtime': 9.0828, 'eval_samples_per_second': 109.988, 'eval_steps_per_second': 6.936, 'epoch': 1.0}
{'train_runtime': 471.4271, 'train_samples_per_second': 21.208, 'train_steps_per_second': 1.326, 'train_loss': 2.0358208374023437, 'epoch': 1.0}
train_results:  {'eval_loss': [4.117644309997559, 2.8979427814483643, 2.264442205429077, 2.090214729309082, 1.9826617240905762, 1.9282490015029907, 1.9039002656936646, 1.8827699422836304, 1.869786024093628, 1.8602663278579712, 1.8519344329833984, 1.8431178331375122, 1.8359706401824951, 1.8301926851272583, 1.825081706047058, 1.8195611238479614, 1.8156577348709106, 1.8129668235778809, 1.8100606203079224, 1.8066405057907104, 1.8043465614318848, 1.802043080329895, 1.8007268905639648, 1.7998342514038086, 1.7994686365127563], 'performance': [0.48, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:02,  3.27it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 44.15it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 67.34it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 79.00it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 87.68it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 94.61it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 102.54it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 112.51it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 114.26it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 116.24it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 120.60it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 128.57it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 140.43it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 138.27it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 144.48it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 143.10it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 142.32it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 144.40it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 147.30it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 155.88it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:02<00:00, 155.44it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 156.14it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 190.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 126.73it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4027016162872314
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5415 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.6710196015312979
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 1.1942639577293968
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 1.1896615135870539
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.5864989383406793
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.8211666210014414
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5458, dtype=torch.float64), tensor(0.4542, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 0, 128, 4.142954771576703e-19, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.5458, dtype=torch.float64), tensor(0.4542, dtype=torch.float64), tensor(2.8804e-17, dtype=torch.float64), tensor(1.7499e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9715e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3668e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(4.1430e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.546
  rowan_hellaswag: 0.454
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.142954771576703e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  4.142954771576703e-19
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 57.68it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 93.45it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 105.03it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 112.88it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 117.93it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 130.63it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 131.93it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 141.69it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 141.65it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 143.12it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 144.91it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 151.79it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 155.18it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 156.00it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 160.67it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 164.69it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 176.02it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 185.45it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 190.56it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 201.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 158.40it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.3052, 'grad_norm': 0.18119576573371887, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 2.612208843231201, 'eval_runtime': 10.4523, 'eval_samples_per_second': 95.577, 'eval_steps_per_second': 6.027, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 57.76it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 93.64it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 105.23it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 113.01it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 117.97it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 130.56it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 131.82it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 141.61it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 141.72it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 143.28it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 144.88it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 151.78it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 155.05it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 155.91it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 160.82it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 165.01it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 176.22it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 185.64it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 190.78it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 202.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 158.50it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.1363, 'grad_norm': 0.13813050091266632, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 1.720725178718567, 'eval_runtime': 10.4276, 'eval_samples_per_second': 95.803, 'eval_steps_per_second': 6.042, 'epoch': 0.08}
{'loss': 1.5462, 'grad_norm': 0.04535069316625595, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4401583671569824, 'eval_runtime': 10.4781, 'eval_samples_per_second': 95.342, 'eval_steps_per_second': 6.013, 'epoch': 0.12}
{'loss': 1.3811, 'grad_norm': 0.042674846947193146, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.352012038230896, 'eval_runtime': 10.5227, 'eval_samples_per_second': 94.938, 'eval_steps_per_second': 5.987, 'epoch': 0.16}
{'loss': 1.3766, 'grad_norm': 0.03722909465432167, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3322690725326538, 'eval_runtime': 10.5944, 'eval_samples_per_second': 94.295, 'eval_steps_per_second': 5.947, 'epoch': 0.2}
{'loss': 1.3394, 'grad_norm': 0.031121769919991493, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3192954063415527, 'eval_runtime': 10.6661, 'eval_samples_per_second': 93.661, 'eval_steps_per_second': 5.907, 'epoch': 0.24}
{'loss': 1.3465, 'grad_norm': 0.03231362998485565, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.309677243232727, 'eval_runtime': 10.7431, 'eval_samples_per_second': 92.99, 'eval_steps_per_second': 5.864, 'epoch': 0.28}
{'loss': 1.331, 'grad_norm': 0.032673850655555725, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3019235134124756, 'eval_runtime': 10.7333, 'eval_samples_per_second': 93.075, 'eval_steps_per_second': 5.87, 'epoch': 0.32}
{'loss': 1.2798, 'grad_norm': 0.031007351353764534, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2973248958587646, 'eval_runtime': 10.7298, 'eval_samples_per_second': 93.105, 'eval_steps_per_second': 5.872, 'epoch': 0.36}
{'loss': 1.3198, 'grad_norm': 0.03180907666683197, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.291463017463684, 'eval_runtime': 10.7188, 'eval_samples_per_second': 93.201, 'eval_steps_per_second': 5.878, 'epoch': 0.4}
{'loss': 1.3198, 'grad_norm': 0.03783640265464783, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2861053943634033, 'eval_runtime': 10.7131, 'eval_samples_per_second': 93.25, 'eval_steps_per_second': 5.881, 'epoch': 0.44}
{'loss': 1.2958, 'grad_norm': 0.03461558744311333, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2820782661437988, 'eval_runtime': 10.7086, 'eval_samples_per_second': 93.29, 'eval_steps_per_second': 5.883, 'epoch': 0.48}
{'loss': 1.2821, 'grad_norm': 0.042159613221883774, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2781034708023071, 'eval_runtime': 10.7482, 'eval_samples_per_second': 92.946, 'eval_steps_per_second': 5.861, 'epoch': 0.52}
{'loss': 1.3185, 'grad_norm': 0.03686542809009552, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2737959623336792, 'eval_runtime': 10.7536, 'eval_samples_per_second': 92.899, 'eval_steps_per_second': 5.859, 'epoch': 0.56}
{'loss': 1.3058, 'grad_norm': 0.03576858341693878, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2711061239242554, 'eval_runtime': 10.6344, 'eval_samples_per_second': 93.94, 'eval_steps_per_second': 5.924, 'epoch': 0.6}
{'loss': 1.2494, 'grad_norm': 0.03995642438530922, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.268193006515503, 'eval_runtime': 10.6964, 'eval_samples_per_second': 93.396, 'eval_steps_per_second': 5.89, 'epoch': 0.64}
{'loss': 1.2786, 'grad_norm': 0.03585547208786011, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2656480073928833, 'eval_runtime': 10.6275, 'eval_samples_per_second': 94.001, 'eval_steps_per_second': 5.928, 'epoch': 0.68}
{'loss': 1.266, 'grad_norm': 0.04130863770842552, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2631909847259521, 'eval_runtime': 10.5989, 'eval_samples_per_second': 94.255, 'eval_steps_per_second': 5.944, 'epoch': 0.72}
{'loss': 1.2675, 'grad_norm': 0.035717833787202835, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2616387605667114, 'eval_runtime': 10.5948, 'eval_samples_per_second': 94.292, 'eval_steps_per_second': 5.946, 'epoch': 0.76}
{'loss': 1.2625, 'grad_norm': 0.04212629050016403, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2596299648284912, 'eval_runtime': 10.6086, 'eval_samples_per_second': 94.169, 'eval_steps_per_second': 5.939, 'epoch': 0.8}
{'loss': 1.2629, 'grad_norm': 0.040929168462753296, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2582365274429321, 'eval_runtime': 10.5996, 'eval_samples_per_second': 94.249, 'eval_steps_per_second': 5.944, 'epoch': 0.84}
{'loss': 1.2643, 'grad_norm': 0.03741537034511566, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2570339441299438, 'eval_runtime': 10.6188, 'eval_samples_per_second': 94.079, 'eval_steps_per_second': 5.933, 'epoch': 0.88}
{'loss': 1.2706, 'grad_norm': 0.040126483887434006, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2558856010437012, 'eval_runtime': 10.6216, 'eval_samples_per_second': 94.053, 'eval_steps_per_second': 5.931, 'epoch': 0.92}
{'loss': 1.2372, 'grad_norm': 0.03831586241722107, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.255316138267517, 'eval_runtime': 10.6347, 'eval_samples_per_second': 93.937, 'eval_steps_per_second': 5.924, 'epoch': 0.96}
{'loss': 1.2471, 'grad_norm': 0.04299883544445038, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2551289796829224, 'eval_runtime': 10.6305, 'eval_samples_per_second': 93.975, 'eval_steps_per_second': 5.926, 'epoch': 1.0}
{'train_runtime': 547.8566, 'train_samples_per_second': 18.251, 'train_steps_per_second': 1.141, 'train_loss': 1.4196025512695312, 'epoch': 1.0}
train_results:  {'eval_loss': [2.612208843231201, 1.720725178718567, 1.4401583671569824, 1.352012038230896, 1.3322690725326538, 1.3192954063415527, 1.309677243232727, 1.3019235134124756, 1.2973248958587646, 1.291463017463684, 1.2861053943634033, 1.2820782661437988, 1.2781034708023071, 1.2737959623336792, 1.2711061239242554, 1.268193006515503, 1.2656480073928833, 1.2631909847259521, 1.2616387605667114, 1.2596299648284912, 1.2582365274429321, 1.2570339441299438, 1.2558856010437012, 1.255316138267517, 1.2551289796829224], 'performance': [0.48, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:32,  2.62it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 36.41it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 56.60it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 67.38it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 75.38it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 81.92it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 88.98it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 97.93it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 99.70it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 101.27it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 104.87it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 112.15it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 122.38it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 120.44it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 125.62it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 124.37it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 123.60it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 125.87it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 128.12it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 135.74it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 135.52it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 135.98it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 146.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 109.48it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.419787883758545
current iteration best possible performance (full train run):  0.441
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7135 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.9392192363132857
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.6606236452737656
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.7910133664189489
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 1.0740092019411822
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.7887998935945593
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4545, dtype=torch.float64), tensor(0.5455, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.0624521344288573e-18, 1.480000019073488, 0]
normalized proposed parameters for next round by BO: [tensor(7.5571e-17, dtype=torch.float64), tensor(2.1787e-17, dtype=torch.float64), tensor(0.4545, dtype=torch.float64), tensor(0.5455, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.2938e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9602e-17, dtype=torch.float64), tensor(1.3107e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.0625e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.454
  sciq: 0.546
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.0624521344288573e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.0624521344288573e-18
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.07it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.86it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.76it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.90it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.36it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.24it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.37it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.44it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.45it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.80it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.33it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.82it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.88it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.74it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.99it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.58it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.56it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.76it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.07it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.10it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.94it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3231, 'grad_norm': 0.21550077199935913, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3320765495300293, 'eval_runtime': 10.9906, 'eval_samples_per_second': 90.896, 'eval_steps_per_second': 5.732, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.20it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.95it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.60it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.86it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.47it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.31it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.27it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.67it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.87it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.52it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.16it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.71it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.80it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.65it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.06it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.65it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.73it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.98it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.35it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.50it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.35it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.97it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.5527, 'grad_norm': 0.17921148240566254, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.9448100328445435, 'eval_runtime': 10.978, 'eval_samples_per_second': 91.0, 'eval_steps_per_second': 5.739, 'epoch': 0.08}
{'loss': 1.769, 'grad_norm': 0.058896254748106, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6727018356323242, 'eval_runtime': 11.0223, 'eval_samples_per_second': 90.634, 'eval_steps_per_second': 5.716, 'epoch': 0.12}
{'loss': 1.5853, 'grad_norm': 0.05568302050232887, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5622283220291138, 'eval_runtime': 11.0177, 'eval_samples_per_second': 90.673, 'eval_steps_per_second': 5.718, 'epoch': 0.16}
{'loss': 1.493, 'grad_norm': 0.05072207376360893, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5395681858062744, 'eval_runtime': 11.0785, 'eval_samples_per_second': 90.175, 'eval_steps_per_second': 5.687, 'epoch': 0.2}
{'loss': 1.5524, 'grad_norm': 0.043033383786678314, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5268315076828003, 'eval_runtime': 11.1336, 'eval_samples_per_second': 89.728, 'eval_steps_per_second': 5.659, 'epoch': 0.24}
{'loss': 1.4933, 'grad_norm': 0.04220642149448395, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5162404775619507, 'eval_runtime': 11.0592, 'eval_samples_per_second': 90.332, 'eval_steps_per_second': 5.697, 'epoch': 0.28}
{'loss': 1.4966, 'grad_norm': 0.034135591238737106, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.506834626197815, 'eval_runtime': 11.0935, 'eval_samples_per_second': 90.053, 'eval_steps_per_second': 5.679, 'epoch': 0.32}
{'loss': 1.512, 'grad_norm': 0.04128767177462578, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5027384757995605, 'eval_runtime': 11.1281, 'eval_samples_per_second': 89.773, 'eval_steps_per_second': 5.661, 'epoch': 0.36}
{'loss': 1.4682, 'grad_norm': 0.0425783209502697, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4934344291687012, 'eval_runtime': 11.0975, 'eval_samples_per_second': 90.02, 'eval_steps_per_second': 5.677, 'epoch': 0.4}
{'loss': 1.5409, 'grad_norm': 0.037395115941762924, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4881764650344849, 'eval_runtime': 11.066, 'eval_samples_per_second': 90.277, 'eval_steps_per_second': 5.693, 'epoch': 0.44}
{'loss': 1.4985, 'grad_norm': 0.04552764073014259, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4825254678726196, 'eval_runtime': 11.0661, 'eval_samples_per_second': 90.276, 'eval_steps_per_second': 5.693, 'epoch': 0.48}
{'loss': 1.5117, 'grad_norm': 0.046409327536821365, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4797507524490356, 'eval_runtime': 11.0713, 'eval_samples_per_second': 90.234, 'eval_steps_per_second': 5.69, 'epoch': 0.52}
{'loss': 1.4505, 'grad_norm': 0.049112070351839066, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4757113456726074, 'eval_runtime': 11.0623, 'eval_samples_per_second': 90.306, 'eval_steps_per_second': 5.695, 'epoch': 0.56}
{'loss': 1.493, 'grad_norm': 0.04280433431267738, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4728714227676392, 'eval_runtime': 11.0739, 'eval_samples_per_second': 90.212, 'eval_steps_per_second': 5.689, 'epoch': 0.6}
{'loss': 1.508, 'grad_norm': 0.04239480942487717, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.468508005142212, 'eval_runtime': 11.0687, 'eval_samples_per_second': 90.254, 'eval_steps_per_second': 5.692, 'epoch': 0.64}
{'loss': 1.4464, 'grad_norm': 0.046700529754161835, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4653953313827515, 'eval_runtime': 11.0862, 'eval_samples_per_second': 90.112, 'eval_steps_per_second': 5.683, 'epoch': 0.68}
{'loss': 1.5245, 'grad_norm': 0.03713814914226532, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.461836576461792, 'eval_runtime': 11.132, 'eval_samples_per_second': 89.741, 'eval_steps_per_second': 5.659, 'epoch': 0.72}
{'loss': 1.4643, 'grad_norm': 0.04701213166117668, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4602373838424683, 'eval_runtime': 11.1155, 'eval_samples_per_second': 89.875, 'eval_steps_per_second': 5.668, 'epoch': 0.76}
{'loss': 1.4715, 'grad_norm': 0.04784521088004112, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4578781127929688, 'eval_runtime': 11.1443, 'eval_samples_per_second': 89.642, 'eval_steps_per_second': 5.653, 'epoch': 0.8}
{'loss': 1.4656, 'grad_norm': 0.04225488752126694, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4557267427444458, 'eval_runtime': 11.1034, 'eval_samples_per_second': 89.972, 'eval_steps_per_second': 5.674, 'epoch': 0.84}
{'loss': 1.4744, 'grad_norm': 0.04700677841901779, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.454502820968628, 'eval_runtime': 11.1308, 'eval_samples_per_second': 89.751, 'eval_steps_per_second': 5.66, 'epoch': 0.88}
{'loss': 1.4636, 'grad_norm': 0.044782381504774094, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.453378677368164, 'eval_runtime': 11.1079, 'eval_samples_per_second': 89.936, 'eval_steps_per_second': 5.672, 'epoch': 0.92}
{'loss': 1.4987, 'grad_norm': 0.04230885952711105, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4526357650756836, 'eval_runtime': 11.1274, 'eval_samples_per_second': 89.779, 'eval_steps_per_second': 5.662, 'epoch': 0.96}
{'loss': 1.4808, 'grad_norm': 0.04637466371059418, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4522955417633057, 'eval_runtime': 11.1133, 'eval_samples_per_second': 89.892, 'eval_steps_per_second': 5.669, 'epoch': 1.0}
{'train_runtime': 571.6709, 'train_samples_per_second': 17.491, 'train_steps_per_second': 1.093, 'train_loss': 1.6615109497070313, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3320765495300293, 1.9448100328445435, 1.6727018356323242, 1.5622283220291138, 1.5395681858062744, 1.5268315076828003, 1.5162404775619507, 1.506834626197815, 1.5027384757995605, 1.4934344291687012, 1.4881764650344849, 1.4825254678726196, 1.4797507524490356, 1.4757113456726074, 1.4728714227676392, 1.468508005142212, 1.4653953313827515, 1.461836576461792, 1.4602373838424683, 1.4578781127929688, 1.4557267427444458, 1.454502820968628, 1.453378677368164, 1.4526357650756836, 1.4522955417633057], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<09:44,  1.46s/it]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:28, 13.53it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:13, 27.08it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:02<00:09, 38.65it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:02<00:06, 49.39it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:02<00:05, 59.22it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:04, 69.10it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 80.18it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:03, 84.96it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:03<00:02, 88.99it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:03<00:02, 94.24it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:03<00:02, 102.34it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:03<00:01, 112.64it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 111.59it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 116.83it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 116.41it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:04<00:01, 116.31it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:04<00:00, 118.55it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:04<00:00, 120.77it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:04<00:00, 128.13it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 128.12it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 128.97it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 138.78it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 81.21it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4275535345077515
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5550 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.9608696787198838
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.6015594294123602
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.7616474213380373
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.7078218664204085
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 1.1361115289505102
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0493, dtype=torch.float64), tensor(0.4451, dtype=torch.float64), tensor(0.1691, dtype=torch.float64), 0, tensor(0.1599, dtype=torch.float64), tensor(0.1767, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0493, dtype=torch.float64), tensor(0.4451, dtype=torch.float64), tensor(0.1691, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1599, dtype=torch.float64), tensor(0.1767, dtype=torch.float64), tensor(5.1174e-17, dtype=torch.float64), tensor(4.2404e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.049
  rowan_hellaswag: 0.445
  sciq: 0.169
  triviaqa: 0
  truthfulqa_gen: 0.16
  wikitext: 0.177
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.18it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.02it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.87it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.25it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.82it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.69it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.90it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.19it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.10it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.39it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.51it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.98it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.09it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.95it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.31it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 150.87it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.91it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.26it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.67it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.82it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.49it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.40it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.9792, 'grad_norm': 0.14887356758117676, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.1945300102233887, 'eval_runtime': 10.9715, 'eval_samples_per_second': 91.054, 'eval_steps_per_second': 5.742, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.97it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.62it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.49it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.74it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.46it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.28it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.45it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.61it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.65it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.09it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.32it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.37it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.45it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.52it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.02it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.57it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.62it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.05it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.49it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.64it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.37it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.02it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.5277, 'grad_norm': 0.18619400262832642, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.0589072704315186, 'eval_runtime': 10.9812, 'eval_samples_per_second': 90.974, 'eval_steps_per_second': 5.737, 'epoch': 0.08}
{'loss': 1.8705, 'grad_norm': 0.07270561158657074, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.796977162361145, 'eval_runtime': 11.0223, 'eval_samples_per_second': 90.634, 'eval_steps_per_second': 5.716, 'epoch': 0.12}
{'loss': 1.7771, 'grad_norm': 0.05672755092382431, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7022764682769775, 'eval_runtime': 11.0509, 'eval_samples_per_second': 90.4, 'eval_steps_per_second': 5.701, 'epoch': 0.16}
{'loss': 1.6683, 'grad_norm': 0.06413993239402771, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6622334718704224, 'eval_runtime': 11.0762, 'eval_samples_per_second': 90.193, 'eval_steps_per_second': 5.688, 'epoch': 0.2}
{'loss': 1.6948, 'grad_norm': 0.04501373693346977, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6381219625473022, 'eval_runtime': 11.1855, 'eval_samples_per_second': 89.312, 'eval_steps_per_second': 5.632, 'epoch': 0.24}
{'loss': 1.6376, 'grad_norm': 0.03950386494398117, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.619144320487976, 'eval_runtime': 11.1832, 'eval_samples_per_second': 89.331, 'eval_steps_per_second': 5.633, 'epoch': 0.28}
{'loss': 1.5988, 'grad_norm': 0.04456610232591629, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6061804294586182, 'eval_runtime': 11.1866, 'eval_samples_per_second': 89.303, 'eval_steps_per_second': 5.632, 'epoch': 0.32}
{'loss': 1.6039, 'grad_norm': 0.1181788295507431, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5997800827026367, 'eval_runtime': 11.1735, 'eval_samples_per_second': 89.408, 'eval_steps_per_second': 5.638, 'epoch': 0.36}
{'loss': 1.6223, 'grad_norm': 0.049825407564640045, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5921974182128906, 'eval_runtime': 11.1671, 'eval_samples_per_second': 89.459, 'eval_steps_per_second': 5.642, 'epoch': 0.4}
{'loss': 1.5481, 'grad_norm': 0.041749559342861176, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5851261615753174, 'eval_runtime': 11.1806, 'eval_samples_per_second': 89.352, 'eval_steps_per_second': 5.635, 'epoch': 0.44}
{'loss': 1.5805, 'grad_norm': 0.047080062329769135, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5780558586120605, 'eval_runtime': 11.1595, 'eval_samples_per_second': 89.52, 'eval_steps_per_second': 5.645, 'epoch': 0.48}
{'loss': 1.601, 'grad_norm': 0.03610839322209358, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5738507509231567, 'eval_runtime': 11.18, 'eval_samples_per_second': 89.356, 'eval_steps_per_second': 5.635, 'epoch': 0.52}
{'loss': 1.5982, 'grad_norm': 0.04212905094027519, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5687617063522339, 'eval_runtime': 11.1823, 'eval_samples_per_second': 89.337, 'eval_steps_per_second': 5.634, 'epoch': 0.56}
{'loss': 1.6014, 'grad_norm': 0.052501387894153595, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5654538869857788, 'eval_runtime': 11.1652, 'eval_samples_per_second': 89.474, 'eval_steps_per_second': 5.643, 'epoch': 0.6}
{'loss': 1.5688, 'grad_norm': 0.042883142828941345, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.56065034866333, 'eval_runtime': 11.1113, 'eval_samples_per_second': 89.908, 'eval_steps_per_second': 5.67, 'epoch': 0.64}
{'loss': 1.6053, 'grad_norm': 0.044154513627290726, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5574238300323486, 'eval_runtime': 11.0985, 'eval_samples_per_second': 90.012, 'eval_steps_per_second': 5.676, 'epoch': 0.68}
{'loss': 1.5746, 'grad_norm': 0.05426163226366043, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5537784099578857, 'eval_runtime': 11.0533, 'eval_samples_per_second': 90.38, 'eval_steps_per_second': 5.7, 'epoch': 0.72}
{'loss': 1.6277, 'grad_norm': 0.04254120588302612, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5511904954910278, 'eval_runtime': 11.0203, 'eval_samples_per_second': 90.65, 'eval_steps_per_second': 5.717, 'epoch': 0.76}
{'loss': 1.5001, 'grad_norm': 0.05578482523560524, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.549357533454895, 'eval_runtime': 11.0178, 'eval_samples_per_second': 90.671, 'eval_steps_per_second': 5.718, 'epoch': 0.8}
{'loss': 1.5156, 'grad_norm': 0.05076098069548607, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5469064712524414, 'eval_runtime': 11.0217, 'eval_samples_per_second': 90.639, 'eval_steps_per_second': 5.716, 'epoch': 0.84}
{'loss': 1.5589, 'grad_norm': 0.056438419967889786, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5445001125335693, 'eval_runtime': 11.1414, 'eval_samples_per_second': 89.665, 'eval_steps_per_second': 5.655, 'epoch': 0.88}
{'loss': 1.5928, 'grad_norm': 0.04462277889251709, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.54372239112854, 'eval_runtime': 11.1471, 'eval_samples_per_second': 89.62, 'eval_steps_per_second': 5.652, 'epoch': 0.92}
{'loss': 1.5624, 'grad_norm': 0.04189932346343994, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5429489612579346, 'eval_runtime': 11.1038, 'eval_samples_per_second': 89.969, 'eval_steps_per_second': 5.674, 'epoch': 0.96}
{'loss': 1.55, 'grad_norm': 0.04753807932138443, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5425002574920654, 'eval_runtime': 11.1285, 'eval_samples_per_second': 89.77, 'eval_steps_per_second': 5.661, 'epoch': 1.0}
{'train_runtime': 552.2411, 'train_samples_per_second': 18.103, 'train_steps_per_second': 1.132, 'train_loss': 1.7426405029296874, 'epoch': 1.0}
train_results:  {'eval_loss': [3.1945300102233887, 2.0589072704315186, 1.796977162361145, 1.7022764682769775, 1.6622334718704224, 1.6381219625473022, 1.619144320487976, 1.6061804294586182, 1.5997800827026367, 1.5921974182128906, 1.5851261615753174, 1.5780558586120605, 1.5738507509231567, 1.5687617063522339, 1.5654538869857788, 1.56065034866333, 1.5574238300323486, 1.5537784099578857, 1.5511904954910278, 1.549357533454895, 1.5469064712524414, 1.5445001125335693, 1.54372239112854, 1.5429489612579346, 1.5425002574920654], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<05:08,  1.30it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:17, 21.35it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:09, 38.50it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 50.59it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 60.38it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 68.70it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 77.19it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 87.18it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 90.22it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 92.67it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 96.81it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 104.00it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 114.02it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 112.87it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 117.87it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 116.83it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.17it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.48it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.93it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.06it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.90it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 128.50it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 138.30it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 93.49it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4265819787979126
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8690 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.7292394016159837
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.5870673143698153
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.6371799984635973
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.7847561870395502
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.5400157161871246
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4454, dtype=torch.float64), 0, 0, tensor(0.5543, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734894, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4454, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.5612e-17, dtype=torch.float64), tensor(0.5543, dtype=torch.float64), tensor(0.0003, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5176e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.445
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.554
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734894,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734894
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.17it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.84it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.77it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.16it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.81it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.71it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.50it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.68it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.73it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.18it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.65it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.09it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.16it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.97it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.48it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.27it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.44it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.93it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.50it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.70it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.46it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.21it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.292, 'grad_norm': 0.19596484303474426, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3306527137756348, 'eval_runtime': 10.9374, 'eval_samples_per_second': 91.338, 'eval_steps_per_second': 5.76, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.17it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.94it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.70it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.97it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.18it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 120.76it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.31it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.72it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.96it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.56it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.19it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.67it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.87it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.72it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.07it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.74it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.81it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.20it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.63it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.49it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.02it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.82it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.5377, 'grad_norm': 0.2043493688106537, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.9693632125854492, 'eval_runtime': 10.9492, 'eval_samples_per_second': 91.239, 'eval_steps_per_second': 5.754, 'epoch': 0.08}
{'loss': 1.7422, 'grad_norm': 0.07318905740976334, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6846896409988403, 'eval_runtime': 11.0613, 'eval_samples_per_second': 90.315, 'eval_steps_per_second': 5.696, 'epoch': 0.12}
{'loss': 1.5472, 'grad_norm': 0.05364600941538811, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5761555433273315, 'eval_runtime': 11.1022, 'eval_samples_per_second': 89.982, 'eval_steps_per_second': 5.675, 'epoch': 0.16}
{'loss': 1.5276, 'grad_norm': 0.04399445280432701, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5427939891815186, 'eval_runtime': 11.141, 'eval_samples_per_second': 89.669, 'eval_steps_per_second': 5.655, 'epoch': 0.2}
{'loss': 1.4995, 'grad_norm': 0.045565083622932434, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.52805495262146, 'eval_runtime': 11.1351, 'eval_samples_per_second': 89.717, 'eval_steps_per_second': 5.658, 'epoch': 0.24}
{'loss': 1.4781, 'grad_norm': 0.05578242242336273, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5135626792907715, 'eval_runtime': 11.1432, 'eval_samples_per_second': 89.651, 'eval_steps_per_second': 5.654, 'epoch': 0.28}
{'loss': 1.536, 'grad_norm': 0.03898119181394577, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.499212622642517, 'eval_runtime': 11.158, 'eval_samples_per_second': 89.532, 'eval_steps_per_second': 5.646, 'epoch': 0.32}
{'loss': 1.469, 'grad_norm': 0.05708625167608261, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4901158809661865, 'eval_runtime': 11.1648, 'eval_samples_per_second': 89.477, 'eval_steps_per_second': 5.643, 'epoch': 0.36}
{'loss': 1.477, 'grad_norm': 0.04898726940155029, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4774305820465088, 'eval_runtime': 11.1078, 'eval_samples_per_second': 89.937, 'eval_steps_per_second': 5.672, 'epoch': 0.4}
{'loss': 1.4717, 'grad_norm': 0.06412773579359055, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.466719150543213, 'eval_runtime': 11.1176, 'eval_samples_per_second': 89.858, 'eval_steps_per_second': 5.667, 'epoch': 0.44}
{'loss': 1.4518, 'grad_norm': 0.04578068107366562, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4568657875061035, 'eval_runtime': 11.1403, 'eval_samples_per_second': 89.674, 'eval_steps_per_second': 5.655, 'epoch': 0.48}
{'loss': 1.4332, 'grad_norm': 0.048939697444438934, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4455982446670532, 'eval_runtime': 11.0944, 'eval_samples_per_second': 90.045, 'eval_steps_per_second': 5.679, 'epoch': 0.52}
{'loss': 1.422, 'grad_norm': 0.06426160037517548, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4357742071151733, 'eval_runtime': 11.066, 'eval_samples_per_second': 90.276, 'eval_steps_per_second': 5.693, 'epoch': 0.56}
{'loss': 1.4205, 'grad_norm': 0.062057897448539734, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4246207475662231, 'eval_runtime': 11.056, 'eval_samples_per_second': 90.359, 'eval_steps_per_second': 5.698, 'epoch': 0.6}
{'loss': 1.4098, 'grad_norm': 0.04914136230945587, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4157074689865112, 'eval_runtime': 11.0585, 'eval_samples_per_second': 90.338, 'eval_steps_per_second': 5.697, 'epoch': 0.64}
{'loss': 1.4108, 'grad_norm': 0.06949546933174133, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4032909870147705, 'eval_runtime': 11.057, 'eval_samples_per_second': 90.35, 'eval_steps_per_second': 5.698, 'epoch': 0.68}
{'loss': 1.3724, 'grad_norm': 0.0607563853263855, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3964169025421143, 'eval_runtime': 10.9897, 'eval_samples_per_second': 90.903, 'eval_steps_per_second': 5.733, 'epoch': 0.72}
{'loss': 1.3418, 'grad_norm': 0.08687396347522736, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.387255072593689, 'eval_runtime': 10.9785, 'eval_samples_per_second': 90.996, 'eval_steps_per_second': 5.738, 'epoch': 0.76}
{'loss': 1.3896, 'grad_norm': 0.08747880905866623, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3790596723556519, 'eval_runtime': 10.9845, 'eval_samples_per_second': 90.946, 'eval_steps_per_second': 5.735, 'epoch': 0.8}
{'loss': 1.3396, 'grad_norm': 0.0715249553322792, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3708734512329102, 'eval_runtime': 10.9913, 'eval_samples_per_second': 90.89, 'eval_steps_per_second': 5.732, 'epoch': 0.84}
{'loss': 1.3647, 'grad_norm': 0.06271512061357498, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3648773431777954, 'eval_runtime': 11.0064, 'eval_samples_per_second': 90.765, 'eval_steps_per_second': 5.724, 'epoch': 0.88}
{'loss': 1.2911, 'grad_norm': 0.09392993152141571, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3599426746368408, 'eval_runtime': 11.1319, 'eval_samples_per_second': 89.742, 'eval_steps_per_second': 5.659, 'epoch': 0.92}
{'loss': 1.2966, 'grad_norm': 0.09751755744218826, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.356464147567749, 'eval_runtime': 11.053, 'eval_samples_per_second': 90.383, 'eval_steps_per_second': 5.7, 'epoch': 0.96}
{'loss': 1.3433, 'grad_norm': 0.08960699290037155, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3554257154464722, 'eval_runtime': 10.9938, 'eval_samples_per_second': 90.87, 'eval_steps_per_second': 5.731, 'epoch': 1.0}
{'train_runtime': 558.3403, 'train_samples_per_second': 17.903, 'train_steps_per_second': 1.119, 'train_loss': 1.594606591796875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3306527137756348, 1.9693632125854492, 1.6846896409988403, 1.5761555433273315, 1.5427939891815186, 1.52805495262146, 1.5135626792907715, 1.499212622642517, 1.4901158809661865, 1.4774305820465088, 1.466719150543213, 1.4568657875061035, 1.4455982446670532, 1.4357742071151733, 1.4246207475662231, 1.4157074689865112, 1.4032909870147705, 1.3964169025421143, 1.387255072593689, 1.3790596723556519, 1.3708734512329102, 1.3648773431777954, 1.3599426746368408, 1.356464147567749, 1.3554257154464722], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.63it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.49it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 65.11it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 72.37it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.76it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 82.54it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 88.07it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.86it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.82it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.82it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.83it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 107.51it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 116.99it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.88it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 119.44it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 118.33it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 117.65it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 119.83it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.63it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.92it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.96it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 129.58it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 139.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 109.47it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.426941156387329
current iteration best possible performance (full train run):  0.4305
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1717 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 1.1330704447598632
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 1.0348908367421135
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 1.3120691096818633
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 1.1384736713338517
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 1.0291258764855555
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4490, dtype=torch.float64), 0, 0, tensor(0.5510, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 6.294810148539685e-18, 1.4800000190734932, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.2738e-16, dtype=torch.float64), tensor(0.4490, dtype=torch.float64), tensor(3.1988e-17, dtype=torch.float64), tensor(1.3570e-17, dtype=torch.float64), tensor(0.5510, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.2780e-18, dtype=torch.float64), tensor(5.9180e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(6.2948e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.449
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.551
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (6.294810148539685e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734932,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  6.294810148539685e-18
lora alpha:  1.4800000190734932
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.72it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.30it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.28it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.65it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 109.58it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.36it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.80it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.28it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.39it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.76it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.30it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.72it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.74it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.38it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.71it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.35it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.45it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.89it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.36it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.50it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.98it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.71it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.2701, 'grad_norm': 0.18744710087776184, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3117623329162598, 'eval_runtime': 10.8343, 'eval_samples_per_second': 92.207, 'eval_steps_per_second': 5.815, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.01it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.48it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.34it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.64it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.21it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.99it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.08it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.29it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.29it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.59it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.05it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.49it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.64it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.51it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.78it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.28it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.23it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.52it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.66it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 180.60it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.25it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.62it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.5215, 'grad_norm': 0.19229933619499207, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.9352577924728394, 'eval_runtime': 10.8796, 'eval_samples_per_second': 91.823, 'eval_steps_per_second': 5.791, 'epoch': 0.08}
{'loss': 1.7305, 'grad_norm': 0.06913906335830688, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6591366529464722, 'eval_runtime': 10.8898, 'eval_samples_per_second': 91.737, 'eval_steps_per_second': 5.785, 'epoch': 0.12}
{'loss': 1.5887, 'grad_norm': 0.07282332330942154, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5420784950256348, 'eval_runtime': 10.9231, 'eval_samples_per_second': 91.457, 'eval_steps_per_second': 5.768, 'epoch': 0.16}
{'loss': 1.4812, 'grad_norm': 0.044289540499448776, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5113729238510132, 'eval_runtime': 10.9396, 'eval_samples_per_second': 91.32, 'eval_steps_per_second': 5.759, 'epoch': 0.2}
{'loss': 1.5322, 'grad_norm': 0.042102087289094925, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4926880598068237, 'eval_runtime': 10.9485, 'eval_samples_per_second': 91.246, 'eval_steps_per_second': 5.754, 'epoch': 0.24}
{'loss': 1.4922, 'grad_norm': 0.04506305232644081, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.476528286933899, 'eval_runtime': 10.958, 'eval_samples_per_second': 91.166, 'eval_steps_per_second': 5.749, 'epoch': 0.28}
{'loss': 1.4567, 'grad_norm': 0.044533900916576385, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.464720368385315, 'eval_runtime': 10.9613, 'eval_samples_per_second': 91.139, 'eval_steps_per_second': 5.747, 'epoch': 0.32}
{'loss': 1.4672, 'grad_norm': 0.043034106492996216, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4536917209625244, 'eval_runtime': 10.9463, 'eval_samples_per_second': 91.263, 'eval_steps_per_second': 5.755, 'epoch': 0.36}
{'loss': 1.472, 'grad_norm': 0.04729009047150612, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4395188093185425, 'eval_runtime': 10.8865, 'eval_samples_per_second': 91.765, 'eval_steps_per_second': 5.787, 'epoch': 0.4}
{'loss': 1.4655, 'grad_norm': 0.04816269129514694, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4291244745254517, 'eval_runtime': 10.8906, 'eval_samples_per_second': 91.731, 'eval_steps_per_second': 5.785, 'epoch': 0.44}
{'loss': 1.467, 'grad_norm': 0.05605895817279816, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4188165664672852, 'eval_runtime': 10.8786, 'eval_samples_per_second': 91.831, 'eval_steps_per_second': 5.791, 'epoch': 0.48}
{'loss': 1.4357, 'grad_norm': 0.06098850071430206, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4070701599121094, 'eval_runtime': 10.8813, 'eval_samples_per_second': 91.809, 'eval_steps_per_second': 5.79, 'epoch': 0.52}
{'loss': 1.3937, 'grad_norm': 0.0547938346862793, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3963007926940918, 'eval_runtime': 10.8862, 'eval_samples_per_second': 91.768, 'eval_steps_per_second': 5.787, 'epoch': 0.56}
{'loss': 1.4345, 'grad_norm': 0.05501792952418327, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3856905698776245, 'eval_runtime': 10.9668, 'eval_samples_per_second': 91.093, 'eval_steps_per_second': 5.745, 'epoch': 0.6}
{'loss': 1.4345, 'grad_norm': 0.06632398068904877, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.374765157699585, 'eval_runtime': 10.9765, 'eval_samples_per_second': 91.013, 'eval_steps_per_second': 5.74, 'epoch': 0.64}
{'loss': 1.3788, 'grad_norm': 0.07045574486255646, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3641362190246582, 'eval_runtime': 10.9421, 'eval_samples_per_second': 91.299, 'eval_steps_per_second': 5.758, 'epoch': 0.68}
{'loss': 1.4314, 'grad_norm': 0.065294548869133, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3546561002731323, 'eval_runtime': 11.003, 'eval_samples_per_second': 90.793, 'eval_steps_per_second': 5.726, 'epoch': 0.72}
{'loss': 1.422, 'grad_norm': 0.06261066347360611, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3499773740768433, 'eval_runtime': 10.9882, 'eval_samples_per_second': 90.916, 'eval_steps_per_second': 5.733, 'epoch': 0.76}
{'loss': 1.3947, 'grad_norm': 0.07401598244905472, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3381626605987549, 'eval_runtime': 10.9728, 'eval_samples_per_second': 91.044, 'eval_steps_per_second': 5.741, 'epoch': 0.8}
{'loss': 1.3814, 'grad_norm': 0.07028933614492416, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3283987045288086, 'eval_runtime': 10.9682, 'eval_samples_per_second': 91.081, 'eval_steps_per_second': 5.744, 'epoch': 0.84}
{'loss': 1.3795, 'grad_norm': 0.09835443645715714, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3227450847625732, 'eval_runtime': 10.9612, 'eval_samples_per_second': 91.139, 'eval_steps_per_second': 5.748, 'epoch': 0.88}
{'loss': 1.3277, 'grad_norm': 0.0642247125506401, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3178545236587524, 'eval_runtime': 11.0393, 'eval_samples_per_second': 90.495, 'eval_steps_per_second': 5.707, 'epoch': 0.92}
{'loss': 1.4025, 'grad_norm': 0.07677128165960312, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3140575885772705, 'eval_runtime': 11.0075, 'eval_samples_per_second': 90.756, 'eval_steps_per_second': 5.723, 'epoch': 0.96}
{'loss': 1.363, 'grad_norm': 0.07667919993400574, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3129310607910156, 'eval_runtime': 11.0012, 'eval_samples_per_second': 90.809, 'eval_steps_per_second': 5.727, 'epoch': 1.0}
{'train_runtime': 569.3494, 'train_samples_per_second': 17.562, 'train_steps_per_second': 1.098, 'train_loss': 1.6049696838378906, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3117623329162598, 1.9352577924728394, 1.6591366529464722, 1.5420784950256348, 1.5113729238510132, 1.4926880598068237, 1.476528286933899, 1.464720368385315, 1.4536917209625244, 1.4395188093185425, 1.4291244745254517, 1.4188165664672852, 1.4070701599121094, 1.3963007926940918, 1.3856905698776245, 1.374765157699585, 1.3641362190246582, 1.3546561002731323, 1.3499773740768433, 1.3381626605987549, 1.3283987045288086, 1.3227450847625732, 1.3178545236587524, 1.3140575885772705, 1.3129310607910156], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:40,  2.48it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:12, 31.84it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 50.90it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 61.51it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 69.32it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 75.89it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 82.76it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 91.63it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 93.46it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 95.24it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.82it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.67it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.16it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.43it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.29it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.13it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.52it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.49it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.68it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.98it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.03it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.65it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.18it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 102.25it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4269388914108276
current iteration best possible performance (full train run):  0.42000000000000004
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1073 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.9425870024185916
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 1.2579666687125484
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 1.1771166199593426
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.6161421387178817
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 1.184775896363756
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4330, dtype=torch.float64), 0, tensor(0.4513, dtype=torch.float64), 0, 0, 0, tensor(0.0150, dtype=torch.float64), 0, tensor(0.1007, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.0, 1.480000019073491, 0]
normalized proposed parameters for next round by BO: [tensor(0.4330, dtype=torch.float64), tensor(8.3067e-17, dtype=torch.float64), tensor(0.4513, dtype=torch.float64), tensor(5.2223e-17, dtype=torch.float64), tensor(1.8070e-17, dtype=torch.float64), tensor(1.2151e-16, dtype=torch.float64), tensor(0.0150, dtype=torch.float64), tensor(1.1118e-17, dtype=torch.float64), tensor(0.1007, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.433
  gsm8k: 0
  rowan_hellaswag: 0.451
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.015
  mmlu: 0
  arc_challenge: 0.101

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073491,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.480000019073491
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.23it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.02it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.80it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.20it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.28it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.11it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.33it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.53it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.63it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.98it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.54it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.95it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.00it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.81it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.11it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.57it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.62it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.95it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.38it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.61it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.43it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.14it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.1217, 'grad_norm': 0.17672869563102722, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.1824123859405518, 'eval_runtime': 10.9642, 'eval_samples_per_second': 91.115, 'eval_steps_per_second': 5.746, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.29it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.35it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 97.84it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.36it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.14it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.05it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.14it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.40it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.51it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.93it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.41it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.99it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.18it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.97it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.25it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.72it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.70it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.06it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.39it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.58it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.34it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.99it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.4317, 'grad_norm': 0.19101059436798096, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.8552358150482178, 'eval_runtime': 10.9651, 'eval_samples_per_second': 91.107, 'eval_steps_per_second': 5.746, 'epoch': 0.08}
{'loss': 1.6637, 'grad_norm': 0.052420131862163544, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6070917844772339, 'eval_runtime': 10.9944, 'eval_samples_per_second': 90.865, 'eval_steps_per_second': 5.73, 'epoch': 0.12}
{'loss': 1.5781, 'grad_norm': 0.06453932076692581, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5103390216827393, 'eval_runtime': 11.1122, 'eval_samples_per_second': 89.901, 'eval_steps_per_second': 5.669, 'epoch': 0.16}
{'loss': 1.4993, 'grad_norm': 0.049471188336610794, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4801650047302246, 'eval_runtime': 11.0659, 'eval_samples_per_second': 90.277, 'eval_steps_per_second': 5.693, 'epoch': 0.2}
{'loss': 1.5094, 'grad_norm': 0.03568844497203827, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4633510112762451, 'eval_runtime': 11.0766, 'eval_samples_per_second': 90.19, 'eval_steps_per_second': 5.688, 'epoch': 0.24}
{'loss': 1.4689, 'grad_norm': 0.042351726442575455, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4536473751068115, 'eval_runtime': 11.1097, 'eval_samples_per_second': 89.921, 'eval_steps_per_second': 5.671, 'epoch': 0.28}
{'loss': 1.4393, 'grad_norm': 0.0343329943716526, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4448739290237427, 'eval_runtime': 11.2092, 'eval_samples_per_second': 89.123, 'eval_steps_per_second': 5.62, 'epoch': 0.32}
{'loss': 1.4465, 'grad_norm': 0.04157554358243942, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.438801646232605, 'eval_runtime': 11.1836, 'eval_samples_per_second': 89.327, 'eval_steps_per_second': 5.633, 'epoch': 0.36}
{'loss': 1.4016, 'grad_norm': 0.03671359643340111, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.431115984916687, 'eval_runtime': 11.1814, 'eval_samples_per_second': 89.345, 'eval_steps_per_second': 5.634, 'epoch': 0.4}
{'loss': 1.4203, 'grad_norm': 0.03913648799061775, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4257677793502808, 'eval_runtime': 11.1851, 'eval_samples_per_second': 89.316, 'eval_steps_per_second': 5.633, 'epoch': 0.44}
{'loss': 1.4155, 'grad_norm': 0.04039635881781578, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4207041263580322, 'eval_runtime': 11.1823, 'eval_samples_per_second': 89.337, 'eval_steps_per_second': 5.634, 'epoch': 0.48}
{'loss': 1.4437, 'grad_norm': 0.03936522454023361, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4174829721450806, 'eval_runtime': 11.1518, 'eval_samples_per_second': 89.582, 'eval_steps_per_second': 5.649, 'epoch': 0.52}
{'loss': 1.4127, 'grad_norm': 0.050533778965473175, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4126230478286743, 'eval_runtime': 11.1616, 'eval_samples_per_second': 89.503, 'eval_steps_per_second': 5.644, 'epoch': 0.56}
{'loss': 1.426, 'grad_norm': 0.03648649901151657, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4090009927749634, 'eval_runtime': 11.0659, 'eval_samples_per_second': 90.277, 'eval_steps_per_second': 5.693, 'epoch': 0.6}
{'loss': 1.3686, 'grad_norm': 0.04120553284883499, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4059218168258667, 'eval_runtime': 11.0747, 'eval_samples_per_second': 90.205, 'eval_steps_per_second': 5.689, 'epoch': 0.64}
{'loss': 1.4094, 'grad_norm': 0.04142361506819725, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4029605388641357, 'eval_runtime': 11.0848, 'eval_samples_per_second': 90.123, 'eval_steps_per_second': 5.683, 'epoch': 0.68}
{'loss': 1.4181, 'grad_norm': 0.042687371373176575, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4007513523101807, 'eval_runtime': 11.0775, 'eval_samples_per_second': 90.183, 'eval_steps_per_second': 5.687, 'epoch': 0.72}
{'loss': 1.4286, 'grad_norm': 0.04542002081871033, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3989181518554688, 'eval_runtime': 11.0739, 'eval_samples_per_second': 90.212, 'eval_steps_per_second': 5.689, 'epoch': 0.76}
{'loss': 1.3808, 'grad_norm': 0.04689086601138115, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3954195976257324, 'eval_runtime': 11.0155, 'eval_samples_per_second': 90.69, 'eval_steps_per_second': 5.719, 'epoch': 0.8}
{'loss': 1.4185, 'grad_norm': 0.04390500858426094, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3931732177734375, 'eval_runtime': 11.0142, 'eval_samples_per_second': 90.701, 'eval_steps_per_second': 5.72, 'epoch': 0.84}
{'loss': 1.4482, 'grad_norm': 0.0413726307451725, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.391663670539856, 'eval_runtime': 11.0204, 'eval_samples_per_second': 90.65, 'eval_steps_per_second': 5.717, 'epoch': 0.88}
{'loss': 1.4009, 'grad_norm': 0.04569938778877258, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3901758193969727, 'eval_runtime': 11.0563, 'eval_samples_per_second': 90.356, 'eval_steps_per_second': 5.698, 'epoch': 0.92}
{'loss': 1.4408, 'grad_norm': 0.03974893316626549, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3895719051361084, 'eval_runtime': 11.0222, 'eval_samples_per_second': 90.636, 'eval_steps_per_second': 5.716, 'epoch': 0.96}
{'loss': 1.4122, 'grad_norm': 0.04342847689986229, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3891911506652832, 'eval_runtime': 11.0356, 'eval_samples_per_second': 90.526, 'eval_steps_per_second': 5.709, 'epoch': 1.0}
{'train_runtime': 561.6108, 'train_samples_per_second': 17.802, 'train_steps_per_second': 1.113, 'train_loss': 1.592171435546875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.1824123859405518, 1.8552358150482178, 1.6070917844772339, 1.5103390216827393, 1.4801650047302246, 1.4633510112762451, 1.4536473751068115, 1.4448739290237427, 1.438801646232605, 1.431115984916687, 1.4257677793502808, 1.4207041263580322, 1.4174829721450806, 1.4126230478286743, 1.4090009927749634, 1.4059218168258667, 1.4029605388641357, 1.4007513523101807, 1.3989181518554688, 1.3954195976257324, 1.3931732177734375, 1.391663670539856, 1.3901758193969727, 1.3895719051361084, 1.3891911506652832], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.60it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.13it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.67it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.73it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.31it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 81.94it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.67it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.29it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.17it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.31it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.44it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 107.02it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 116.34it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.25it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.99it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.88it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 117.03it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 119.02it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.08it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.28it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.25it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.82it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.82it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4230360984802246
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5235 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 1.0017026491794017
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 1.3322303507414994
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.5644036691575943
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.809265616374052
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.5649141215790143
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4503, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.5497, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4503, dtype=torch.float64), tensor(8.2010e-17, dtype=torch.float64), tensor(6.5691e-17, dtype=torch.float64), tensor(3.7470e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.2787e-17, dtype=torch.float64), tensor(0.5497, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.45
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.55

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.56it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 95.15it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 106.84it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 114.63it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 119.60it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 132.56it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 133.85it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 143.81it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 143.68it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 145.21it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 146.96it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 154.00it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 157.28it/s]Running loglikelihood requests:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 232/400 [00:01<00:00, 172.39it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 159.33it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 164.83it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 176.47it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 186.69it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 192.18it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 203.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 160.56it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.9718, 'grad_norm': 0.12469911575317383, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.206021308898926, 'eval_runtime': 10.274, 'eval_samples_per_second': 97.236, 'eval_steps_per_second': 6.132, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.53it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 94.71it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 106.33it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 114.27it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 119.24it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 131.93it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 133.06it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 143.10it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 143.12it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 144.80it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 146.37it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 153.39it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 156.76it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 228/400 [00:01<00:01, 160.52it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 161.72it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 166.10it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 177.64it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 187.32it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 192.57it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 204.06it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 160.16it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.4656, 'grad_norm': 0.054175373166799545, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.919310212135315, 'eval_runtime': 10.2933, 'eval_samples_per_second': 97.054, 'eval_steps_per_second': 6.12, 'epoch': 0.08}
{'loss': 1.7351, 'grad_norm': 0.07214310020208359, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6787395477294922, 'eval_runtime': 10.2528, 'eval_samples_per_second': 97.437, 'eval_steps_per_second': 6.145, 'epoch': 0.12}
{'loss': 1.6172, 'grad_norm': 0.040549490600824356, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5792149305343628, 'eval_runtime': 10.2994, 'eval_samples_per_second': 96.996, 'eval_steps_per_second': 6.117, 'epoch': 0.16}
{'loss': 1.5415, 'grad_norm': 0.03286068141460419, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5423840284347534, 'eval_runtime': 10.3654, 'eval_samples_per_second': 96.379, 'eval_steps_per_second': 6.078, 'epoch': 0.2}
{'loss': 1.5353, 'grad_norm': 0.030914682894945145, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5199569463729858, 'eval_runtime': 10.3914, 'eval_samples_per_second': 96.137, 'eval_steps_per_second': 6.063, 'epoch': 0.24}
{'loss': 1.4879, 'grad_norm': 0.03287438675761223, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5001956224441528, 'eval_runtime': 10.3885, 'eval_samples_per_second': 96.164, 'eval_steps_per_second': 6.064, 'epoch': 0.28}
{'loss': 1.4737, 'grad_norm': 0.03482590243220329, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.485379934310913, 'eval_runtime': 10.4968, 'eval_samples_per_second': 95.172, 'eval_steps_per_second': 6.002, 'epoch': 0.32}
{'loss': 1.4789, 'grad_norm': 0.035989098250865936, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4672245979309082, 'eval_runtime': 10.5012, 'eval_samples_per_second': 95.132, 'eval_steps_per_second': 5.999, 'epoch': 0.36}
{'loss': 1.4256, 'grad_norm': 0.05010540783405304, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4397552013397217, 'eval_runtime': 10.5175, 'eval_samples_per_second': 94.985, 'eval_steps_per_second': 5.99, 'epoch': 0.4}
{'loss': 1.4199, 'grad_norm': 0.04531040042638779, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.393845796585083, 'eval_runtime': 10.4933, 'eval_samples_per_second': 95.204, 'eval_steps_per_second': 6.004, 'epoch': 0.44}
{'loss': 1.3733, 'grad_norm': 0.03998930752277374, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3762383460998535, 'eval_runtime': 10.5208, 'eval_samples_per_second': 94.955, 'eval_steps_per_second': 5.988, 'epoch': 0.48}
{'loss': 1.3738, 'grad_norm': 0.03924320638179779, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3577821254730225, 'eval_runtime': 10.5179, 'eval_samples_per_second': 94.981, 'eval_steps_per_second': 5.99, 'epoch': 0.52}
{'loss': 1.3388, 'grad_norm': 0.043383531272411346, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3376413583755493, 'eval_runtime': 10.4842, 'eval_samples_per_second': 95.286, 'eval_steps_per_second': 6.009, 'epoch': 0.56}
{'loss': 1.3705, 'grad_norm': 0.03976632282137871, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3231427669525146, 'eval_runtime': 10.5159, 'eval_samples_per_second': 94.999, 'eval_steps_per_second': 5.991, 'epoch': 0.6}
{'loss': 1.3589, 'grad_norm': 0.04370417818427086, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.316658616065979, 'eval_runtime': 10.4916, 'eval_samples_per_second': 95.219, 'eval_steps_per_second': 6.005, 'epoch': 0.64}
{'loss': 1.3177, 'grad_norm': 0.04646345227956772, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.310049057006836, 'eval_runtime': 10.4976, 'eval_samples_per_second': 95.164, 'eval_steps_per_second': 6.001, 'epoch': 0.68}
{'loss': 1.3318, 'grad_norm': 0.03990647569298744, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3041892051696777, 'eval_runtime': 10.4461, 'eval_samples_per_second': 95.634, 'eval_steps_per_second': 6.031, 'epoch': 0.72}
{'loss': 1.3399, 'grad_norm': 0.04729684442281723, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2983318567276, 'eval_runtime': 10.4192, 'eval_samples_per_second': 95.881, 'eval_steps_per_second': 6.047, 'epoch': 0.76}
{'loss': 1.3388, 'grad_norm': 0.047478992491960526, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2935426235198975, 'eval_runtime': 10.4093, 'eval_samples_per_second': 95.972, 'eval_steps_per_second': 6.052, 'epoch': 0.8}
{'loss': 1.3336, 'grad_norm': 0.046754781156778336, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2892165184020996, 'eval_runtime': 10.4044, 'eval_samples_per_second': 96.017, 'eval_steps_per_second': 6.055, 'epoch': 0.84}
{'loss': 1.2967, 'grad_norm': 0.05150587856769562, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2846657037734985, 'eval_runtime': 10.4103, 'eval_samples_per_second': 95.963, 'eval_steps_per_second': 6.052, 'epoch': 0.88}
{'loss': 1.3163, 'grad_norm': 0.048659030348062515, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2817294597625732, 'eval_runtime': 10.4014, 'eval_samples_per_second': 96.044, 'eval_steps_per_second': 6.057, 'epoch': 0.92}
{'loss': 1.3464, 'grad_norm': 0.05230220407247543, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2795348167419434, 'eval_runtime': 10.4139, 'eval_samples_per_second': 95.93, 'eval_steps_per_second': 6.05, 'epoch': 0.96}
{'loss': 1.3252, 'grad_norm': 0.05878181383013725, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2788302898406982, 'eval_runtime': 10.4224, 'eval_samples_per_second': 95.852, 'eval_steps_per_second': 6.045, 'epoch': 1.0}
{'train_runtime': 524.0285, 'train_samples_per_second': 19.081, 'train_steps_per_second': 1.193, 'train_loss': 1.556564501953125, 'epoch': 1.0}
train_results:  {'eval_loss': [3.206021308898926, 1.919310212135315, 1.6787395477294922, 1.5792149305343628, 1.5423840284347534, 1.5199569463729858, 1.5001956224441528, 1.485379934310913, 1.4672245979309082, 1.4397552013397217, 1.393845796585083, 1.3762383460998535, 1.3577821254730225, 1.3376413583755493, 1.3231427669525146, 1.316658616065979, 1.310049057006836, 1.3041892051696777, 1.2983318567276, 1.2935426235198975, 1.2892165184020996, 1.2846657037734985, 1.2817294597625732, 1.2795348167419434, 1.2788302898406982], 'performance': [0.49, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<06:35,  1.01it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:28, 13.36it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:13, 27.14it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:08, 39.15it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:02<00:06, 50.45it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:02<00:05, 60.98it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:04, 71.65it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 83.61it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 88.97it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 93.58it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:03<00:02, 99.40it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:03<00:01, 108.30it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:03<00:01, 119.66it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 118.93it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 124.92it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 124.45it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 123.98it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 126.63it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:04<00:00, 129.13it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:04<00:00, 136.67it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 136.53it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 137.10it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 147.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 86.91it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4188897609710693
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246, 1.4188897609710693]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8339 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.9991677110248884
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.9643079607949967
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.6046603604911412
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.98724290733704
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.969526975185997
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0212, dtype=torch.float64), tensor(0.4654, dtype=torch.float64), tensor(0.1290, dtype=torch.float64), 0, tensor(0.1844, dtype=torch.float64), tensor(0.2000, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 6.9875063456652065, 0]
normalized proposed parameters for next round by BO: [tensor(1.6183e-16, dtype=torch.float64), tensor(0.0212, dtype=torch.float64), tensor(0.4654, dtype=torch.float64), tensor(0.1290, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1844, dtype=torch.float64), tensor(0.2000, dtype=torch.float64), tensor(8.6740e-19, dtype=torch.float64), tensor(6.8321e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1456, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.021
  rowan_hellaswag: 0.465
  sciq: 0.129
  triviaqa: 0
  truthfulqa_gen: 0.184
  wikitext: 0.2
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (6.9875063456652065,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  6.9875063456652065
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.18it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.96it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 97.62it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.25it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.08it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.95it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.32it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.70it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.84it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.25it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.76it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.25it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.40it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.47it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.94it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.57it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.70it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.20it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.44it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.69it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.08it/s]
Evaluation performance at step 25: 0.51
{'loss': 3.6937, 'grad_norm': 0.30732178688049316, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.51}
{'eval_loss': 2.6241657733917236, 'eval_runtime': 10.9493, 'eval_samples_per_second': 91.238, 'eval_steps_per_second': 5.754, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.25it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.92it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.69it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.01it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.59it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.50it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.72it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.87it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.79it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.15it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.56it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.88it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.01it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.88it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.24it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.78it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.83it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.30it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.64it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.83it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.30it/s]
Evaluation performance at step 50: 0.43
{'loss': 2.1174, 'grad_norm': 0.15135496854782104, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.43}
{'eval_loss': 1.8867745399475098, 'eval_runtime': 10.9955, 'eval_samples_per_second': 90.855, 'eval_steps_per_second': 5.73, 'epoch': 0.08}
{'loss': 1.8314, 'grad_norm': 0.09768099337816238, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7438231706619263, 'eval_runtime': 10.9352, 'eval_samples_per_second': 91.357, 'eval_steps_per_second': 5.761, 'epoch': 0.12}
{'loss': 1.7356, 'grad_norm': 0.09661593288183212, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.683618187904358, 'eval_runtime': 10.9673, 'eval_samples_per_second': 91.089, 'eval_steps_per_second': 5.744, 'epoch': 0.16}
{'loss': 1.6852, 'grad_norm': 0.08532528579235077, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6597745418548584, 'eval_runtime': 11.0079, 'eval_samples_per_second': 90.753, 'eval_steps_per_second': 5.723, 'epoch': 0.2}
{'loss': 1.6445, 'grad_norm': 0.0787476971745491, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6441378593444824, 'eval_runtime': 11.0398, 'eval_samples_per_second': 90.491, 'eval_steps_per_second': 5.707, 'epoch': 0.24}
{'loss': 1.6753, 'grad_norm': 0.08462546765804291, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6320561170578003, 'eval_runtime': 11.0612, 'eval_samples_per_second': 90.316, 'eval_steps_per_second': 5.696, 'epoch': 0.28}
{'loss': 1.6614, 'grad_norm': 0.09823638945817947, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6218674182891846, 'eval_runtime': 11.0578, 'eval_samples_per_second': 90.344, 'eval_steps_per_second': 5.697, 'epoch': 0.32}
{'loss': 1.6366, 'grad_norm': 0.08868876844644547, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6133145093917847, 'eval_runtime': 11.0515, 'eval_samples_per_second': 90.395, 'eval_steps_per_second': 5.701, 'epoch': 0.36}
{'loss': 1.6236, 'grad_norm': 0.0663519948720932, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.603838324546814, 'eval_runtime': 11.036, 'eval_samples_per_second': 90.522, 'eval_steps_per_second': 5.709, 'epoch': 0.4}
{'loss': 1.5708, 'grad_norm': 0.07521215826272964, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.601015329360962, 'eval_runtime': 11.0506, 'eval_samples_per_second': 90.402, 'eval_steps_per_second': 5.701, 'epoch': 0.44}
{'loss': 1.5884, 'grad_norm': 0.08527576923370361, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.593833327293396, 'eval_runtime': 11.0483, 'eval_samples_per_second': 90.421, 'eval_steps_per_second': 5.702, 'epoch': 0.48}
{'loss': 1.6221, 'grad_norm': 0.07477094233036041, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5894341468811035, 'eval_runtime': 11.0903, 'eval_samples_per_second': 90.079, 'eval_steps_per_second': 5.681, 'epoch': 0.52}
{'loss': 1.5698, 'grad_norm': 0.08929522335529327, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5829734802246094, 'eval_runtime': 11.0916, 'eval_samples_per_second': 90.068, 'eval_steps_per_second': 5.68, 'epoch': 0.56}
{'loss': 1.6605, 'grad_norm': 0.10156012326478958, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5764403343200684, 'eval_runtime': 11.0329, 'eval_samples_per_second': 90.547, 'eval_steps_per_second': 5.71, 'epoch': 0.6}
{'loss': 1.6019, 'grad_norm': 0.10573559254407883, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.571245789527893, 'eval_runtime': 11.0006, 'eval_samples_per_second': 90.813, 'eval_steps_per_second': 5.727, 'epoch': 0.64}
{'loss': 1.573, 'grad_norm': 0.0826106145977974, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5668234825134277, 'eval_runtime': 10.9598, 'eval_samples_per_second': 91.151, 'eval_steps_per_second': 5.748, 'epoch': 0.68}
{'loss': 1.5449, 'grad_norm': 0.0883885994553566, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5614662170410156, 'eval_runtime': 10.9693, 'eval_samples_per_second': 91.072, 'eval_steps_per_second': 5.743, 'epoch': 0.72}
{'loss': 1.5704, 'grad_norm': 0.09217235445976257, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5573538541793823, 'eval_runtime': 11.024, 'eval_samples_per_second': 90.62, 'eval_steps_per_second': 5.715, 'epoch': 0.76}
{'loss': 1.5823, 'grad_norm': 0.09678033739328384, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5534113645553589, 'eval_runtime': 11.1343, 'eval_samples_per_second': 89.722, 'eval_steps_per_second': 5.658, 'epoch': 0.8}
{'loss': 1.5768, 'grad_norm': 0.07682594656944275, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.550442099571228, 'eval_runtime': 11.1324, 'eval_samples_per_second': 89.738, 'eval_steps_per_second': 5.659, 'epoch': 0.84}
{'loss': 1.5777, 'grad_norm': 0.10176731646060944, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.546816349029541, 'eval_runtime': 11.1328, 'eval_samples_per_second': 89.735, 'eval_steps_per_second': 5.659, 'epoch': 0.88}
{'loss': 1.5607, 'grad_norm': 0.08925444632768631, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.544596552848816, 'eval_runtime': 11.1366, 'eval_samples_per_second': 89.704, 'eval_steps_per_second': 5.657, 'epoch': 0.92}
{'loss': 1.5429, 'grad_norm': 0.07665193825960159, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5428533554077148, 'eval_runtime': 11.1498, 'eval_samples_per_second': 89.598, 'eval_steps_per_second': 5.65, 'epoch': 0.96}
{'loss': 1.5776, 'grad_norm': 0.10280051827430725, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.542079210281372, 'eval_runtime': 11.1471, 'eval_samples_per_second': 89.62, 'eval_steps_per_second': 5.652, 'epoch': 1.0}
{'train_runtime': 557.6809, 'train_samples_per_second': 17.928, 'train_steps_per_second': 1.121, 'train_loss': 1.7209738220214843, 'epoch': 1.0}
train_results:  {'eval_loss': [2.6241657733917236, 1.8867745399475098, 1.7438231706619263, 1.683618187904358, 1.6597745418548584, 1.6441378593444824, 1.6320561170578003, 1.6218674182891846, 1.6133145093917847, 1.603838324546814, 1.601015329360962, 1.593833327293396, 1.5894341468811035, 1.5829734802246094, 1.5764403343200684, 1.571245789527893, 1.5668234825134277, 1.5614662170410156, 1.5573538541793823, 1.5534113645553589, 1.550442099571228, 1.546816349029541, 1.544596552848816, 1.5428533554077148, 1.542079210281372], 'performance': [0.51, 0.43]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:34,  4.22it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 44.35it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 62.30it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 70.20it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 75.94it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 80.78it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 86.52it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 94.21it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.29it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 96.37it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 99.40it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.89it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.06it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 112.80it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 117.43it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 116.43it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.10it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.40it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.73it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.64it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.46it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.03it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 137.19it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 107.26it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.51, 0.43]
current iteration observed (possibly low-fid or predicted) performance:  1.3949439525604248
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246, 1.4188897609710693, 1.3949439525604248]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7131 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 1.0755826487002094
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 1.2202929397074442
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 1.0723979057272186
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 1.121419018735275
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 1.0365293211960818
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4501, dtype=torch.float64), 0, 0, 0, tensor(0.5499, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(8.5279e-19, dtype=torch.float64), tensor(1.6398e-16, dtype=torch.float64), tensor(0.4501, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.9810e-17, dtype=torch.float64), tensor(2.1872e-17, dtype=torch.float64), tensor(0.5499, dtype=torch.float64), tensor(3.8011e-17, dtype=torch.float64), tensor(1.5553e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.45
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.55
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.98it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.86it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.75it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.14it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.61it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.42it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.41it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.50it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.50it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.95it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.52it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.09it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.29it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.14it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.42it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.88it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.86it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.32it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.59it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.78it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.54it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.25it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.7753, 'grad_norm': 0.13763375580310822, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.1737167835235596, 'eval_runtime': 10.9282, 'eval_samples_per_second': 91.415, 'eval_steps_per_second': 5.765, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.94it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.46it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.10it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.20it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.00it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.81it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.92it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.12it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.14it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.54it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.04it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.65it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.79it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.73it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.11it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.65it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.77it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.17it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.51it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.67it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.02it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.80it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.6955, 'grad_norm': 0.17514947056770325, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.2689549922943115, 'eval_runtime': 10.9472, 'eval_samples_per_second': 91.257, 'eval_steps_per_second': 5.755, 'epoch': 0.08}
{'loss': 2.1558, 'grad_norm': 0.07856260240077972, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0325028896331787, 'eval_runtime': 10.9878, 'eval_samples_per_second': 90.919, 'eval_steps_per_second': 5.734, 'epoch': 0.12}
{'loss': 1.9716, 'grad_norm': 0.09467817842960358, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9466400146484375, 'eval_runtime': 11.0049, 'eval_samples_per_second': 90.778, 'eval_steps_per_second': 5.725, 'epoch': 0.16}
{'loss': 1.9629, 'grad_norm': 0.04285323619842529, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8992255926132202, 'eval_runtime': 11.0193, 'eval_samples_per_second': 90.659, 'eval_steps_per_second': 5.717, 'epoch': 0.2}
{'loss': 1.931, 'grad_norm': 0.06114573031663895, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8802645206451416, 'eval_runtime': 11.0049, 'eval_samples_per_second': 90.777, 'eval_steps_per_second': 5.725, 'epoch': 0.24}
{'loss': 1.8821, 'grad_norm': 0.07300418615341187, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8683441877365112, 'eval_runtime': 11.0693, 'eval_samples_per_second': 90.25, 'eval_steps_per_second': 5.691, 'epoch': 0.28}
{'loss': 1.918, 'grad_norm': 0.06625600904226303, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8595309257507324, 'eval_runtime': 11.0103, 'eval_samples_per_second': 90.733, 'eval_steps_per_second': 5.722, 'epoch': 0.32}
{'loss': 1.8799, 'grad_norm': 0.04152347892522812, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8514461517333984, 'eval_runtime': 10.9972, 'eval_samples_per_second': 90.841, 'eval_steps_per_second': 5.729, 'epoch': 0.36}
{'loss': 1.8815, 'grad_norm': 0.046689629554748535, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8416578769683838, 'eval_runtime': 10.9984, 'eval_samples_per_second': 90.832, 'eval_steps_per_second': 5.728, 'epoch': 0.4}
{'loss': 1.8618, 'grad_norm': 0.04547157138586044, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8339169025421143, 'eval_runtime': 11.0011, 'eval_samples_per_second': 90.809, 'eval_steps_per_second': 5.727, 'epoch': 0.44}
{'loss': 1.8681, 'grad_norm': 0.05582553893327713, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.827343225479126, 'eval_runtime': 10.9875, 'eval_samples_per_second': 90.921, 'eval_steps_per_second': 5.734, 'epoch': 0.48}
{'loss': 1.8973, 'grad_norm': 0.05688287690281868, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8239985704421997, 'eval_runtime': 10.9947, 'eval_samples_per_second': 90.862, 'eval_steps_per_second': 5.73, 'epoch': 0.52}
{'loss': 1.9162, 'grad_norm': 0.0711270198225975, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8188034296035767, 'eval_runtime': 11.024, 'eval_samples_per_second': 90.621, 'eval_steps_per_second': 5.715, 'epoch': 0.56}
{'loss': 1.8843, 'grad_norm': 0.04430743679404259, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8140311241149902, 'eval_runtime': 11.0557, 'eval_samples_per_second': 90.36, 'eval_steps_per_second': 5.698, 'epoch': 0.6}
{'loss': 1.8621, 'grad_norm': 0.06387560069561005, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8098711967468262, 'eval_runtime': 10.9978, 'eval_samples_per_second': 90.837, 'eval_steps_per_second': 5.728, 'epoch': 0.64}
{'loss': 1.8205, 'grad_norm': 0.08895952999591827, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8076008558273315, 'eval_runtime': 10.9739, 'eval_samples_per_second': 91.035, 'eval_steps_per_second': 5.741, 'epoch': 0.68}
{'loss': 1.8604, 'grad_norm': 0.05241968855261803, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8036153316497803, 'eval_runtime': 10.9366, 'eval_samples_per_second': 91.345, 'eval_steps_per_second': 5.76, 'epoch': 0.72}
{'loss': 1.8384, 'grad_norm': 0.0514601469039917, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8012473583221436, 'eval_runtime': 10.9688, 'eval_samples_per_second': 91.076, 'eval_steps_per_second': 5.744, 'epoch': 0.76}
{'loss': 1.8096, 'grad_norm': 0.0513085275888443, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7985146045684814, 'eval_runtime': 10.947, 'eval_samples_per_second': 91.258, 'eval_steps_per_second': 5.755, 'epoch': 0.8}
{'loss': 1.8562, 'grad_norm': 0.05066286399960518, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.795700192451477, 'eval_runtime': 10.9395, 'eval_samples_per_second': 91.321, 'eval_steps_per_second': 5.759, 'epoch': 0.84}
{'loss': 1.8645, 'grad_norm': 0.049229130148887634, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.79373037815094, 'eval_runtime': 10.9367, 'eval_samples_per_second': 91.343, 'eval_steps_per_second': 5.76, 'epoch': 0.88}
{'loss': 1.8265, 'grad_norm': 0.052112847566604614, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.7928962707519531, 'eval_runtime': 10.9324, 'eval_samples_per_second': 91.379, 'eval_steps_per_second': 5.763, 'epoch': 0.92}
{'loss': 1.8471, 'grad_norm': 0.06445696204900742, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7917652130126953, 'eval_runtime': 10.9274, 'eval_samples_per_second': 91.422, 'eval_steps_per_second': 5.765, 'epoch': 0.96}
{'loss': 1.825, 'grad_norm': 0.060547348111867905, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7913073301315308, 'eval_runtime': 10.9373, 'eval_samples_per_second': 91.339, 'eval_steps_per_second': 5.76, 'epoch': 1.0}
{'train_runtime': 553.1192, 'train_samples_per_second': 18.077, 'train_steps_per_second': 1.13, 'train_loss': 1.9956645874023438, 'epoch': 1.0}
train_results:  {'eval_loss': [3.1737167835235596, 2.2689549922943115, 2.0325028896331787, 1.9466400146484375, 1.8992255926132202, 1.8802645206451416, 1.8683441877365112, 1.8595309257507324, 1.8514461517333984, 1.8416578769683838, 1.8339169025421143, 1.827343225479126, 1.8239985704421997, 1.8188034296035767, 1.8140311241149902, 1.8098711967468262, 1.8076008558273315, 1.8036153316497803, 1.8012473583221436, 1.7985146045684814, 1.795700192451477, 1.79373037815094, 1.7928962707519531, 1.7917652130126953, 1.7913073301315308], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:51,  1.72it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:20, 18.63it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:10, 34.85it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:07, 46.97it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 57.43it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 66.46it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 75.51it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 85.83it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 89.24it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 92.34it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 96.76it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 104.15it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 114.50it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 112.77it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 118.13it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 117.23it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 117.01it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 119.44it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.81it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.81it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 128.89it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 129.66it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 139.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 92.79it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.425980567932129
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246, 1.4188897609710693, 1.3949439525604248, 1.425980567932129]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1851 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.9841444522507066
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 1.0261412423509997
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.7027849685678412
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.9794529484858192
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.9708667143824539
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4474, dtype=torch.float64), 0, 0, tensor(0.5526, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.561251128379127e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4474, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2467e-17, dtype=torch.float64), tensor(0.5526, dtype=torch.float64), tensor(7.0038e-16, dtype=torch.float64), tensor(3.5911e-17, dtype=torch.float64), tensor(8.1556e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.5613e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.447
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.553
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.561251128379127e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.561251128379127e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.38it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.02it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 97.91it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.26it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 109.81it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.54it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.72it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.95it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.76it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 132.44it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.12it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 140.65it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 143.42it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 144.28it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 148.50it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 149.05it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 153.08it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 165.05it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 166.75it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 180.16it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.03it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 146.86it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.3161, 'grad_norm': 0.2050642967224121, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.331235885620117, 'eval_runtime': 11.0672, 'eval_samples_per_second': 90.267, 'eval_steps_per_second': 5.693, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.01it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.61it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.34it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.52it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 109.99it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.84it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.99it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.62it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.78it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.08it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.72it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.17it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.33it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.14it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.48it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 149.88it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 153.71it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.15it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.34it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 180.31it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.10it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.5217, 'grad_norm': 0.2028968781232834, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.9529578685760498, 'eval_runtime': 11.0706, 'eval_samples_per_second': 90.239, 'eval_steps_per_second': 5.691, 'epoch': 0.08}
{'loss': 1.7353, 'grad_norm': 0.06407979130744934, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.673614740371704, 'eval_runtime': 11.121, 'eval_samples_per_second': 89.83, 'eval_steps_per_second': 5.665, 'epoch': 0.12}
{'loss': 1.6098, 'grad_norm': 0.05114481970667839, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5553513765335083, 'eval_runtime': 11.0978, 'eval_samples_per_second': 90.017, 'eval_steps_per_second': 5.677, 'epoch': 0.16}
{'loss': 1.5232, 'grad_norm': 0.048233821988105774, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5262680053710938, 'eval_runtime': 11.0932, 'eval_samples_per_second': 90.056, 'eval_steps_per_second': 5.679, 'epoch': 0.2}
{'loss': 1.5003, 'grad_norm': 0.03919230028986931, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.507442593574524, 'eval_runtime': 11.1061, 'eval_samples_per_second': 89.951, 'eval_steps_per_second': 5.673, 'epoch': 0.24}
{'loss': 1.4422, 'grad_norm': 0.04866161569952965, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4964977502822876, 'eval_runtime': 11.0677, 'eval_samples_per_second': 90.263, 'eval_steps_per_second': 5.692, 'epoch': 0.28}
{'loss': 1.4676, 'grad_norm': 0.03904620185494423, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4817932844161987, 'eval_runtime': 11.0791, 'eval_samples_per_second': 90.17, 'eval_steps_per_second': 5.686, 'epoch': 0.32}
{'loss': 1.4617, 'grad_norm': 0.04131050780415535, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4727586507797241, 'eval_runtime': 11.0845, 'eval_samples_per_second': 90.126, 'eval_steps_per_second': 5.684, 'epoch': 0.36}
{'loss': 1.43, 'grad_norm': 0.04744992405176163, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4559757709503174, 'eval_runtime': 11.0913, 'eval_samples_per_second': 90.07, 'eval_steps_per_second': 5.68, 'epoch': 0.4}
{'loss': 1.4733, 'grad_norm': 0.0456232875585556, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4441179037094116, 'eval_runtime': 11.0882, 'eval_samples_per_second': 90.096, 'eval_steps_per_second': 5.682, 'epoch': 0.44}
{'loss': 1.4123, 'grad_norm': 0.0555533803999424, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4332191944122314, 'eval_runtime': 11.0968, 'eval_samples_per_second': 90.026, 'eval_steps_per_second': 5.677, 'epoch': 0.48}
{'loss': 1.4706, 'grad_norm': 0.055847566574811935, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.423733115196228, 'eval_runtime': 11.06, 'eval_samples_per_second': 90.326, 'eval_steps_per_second': 5.696, 'epoch': 0.52}
{'loss': 1.3838, 'grad_norm': 0.0658504068851471, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4131208658218384, 'eval_runtime': 11.0074, 'eval_samples_per_second': 90.757, 'eval_steps_per_second': 5.723, 'epoch': 0.56}
{'loss': 1.3824, 'grad_norm': 0.06185469403862953, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4020370244979858, 'eval_runtime': 11.0013, 'eval_samples_per_second': 90.807, 'eval_steps_per_second': 5.727, 'epoch': 0.6}
{'loss': 1.4628, 'grad_norm': 0.05196249857544899, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.391098141670227, 'eval_runtime': 11.0022, 'eval_samples_per_second': 90.8, 'eval_steps_per_second': 5.726, 'epoch': 0.64}
{'loss': 1.3913, 'grad_norm': 0.07583930343389511, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.383419394493103, 'eval_runtime': 11.0089, 'eval_samples_per_second': 90.745, 'eval_steps_per_second': 5.723, 'epoch': 0.68}
{'loss': 1.4186, 'grad_norm': 0.058745212852954865, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3723567724227905, 'eval_runtime': 10.9968, 'eval_samples_per_second': 90.845, 'eval_steps_per_second': 5.729, 'epoch': 0.72}
{'loss': 1.4065, 'grad_norm': 0.07011207193136215, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.364851951599121, 'eval_runtime': 11.0617, 'eval_samples_per_second': 90.312, 'eval_steps_per_second': 5.695, 'epoch': 0.76}
{'loss': 1.3664, 'grad_norm': 0.0882955938577652, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3552955389022827, 'eval_runtime': 11.1086, 'eval_samples_per_second': 89.93, 'eval_steps_per_second': 5.671, 'epoch': 0.8}
{'loss': 1.4063, 'grad_norm': 0.08677221834659576, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.346694827079773, 'eval_runtime': 11.0872, 'eval_samples_per_second': 90.104, 'eval_steps_per_second': 5.682, 'epoch': 0.84}
{'loss': 1.354, 'grad_norm': 0.0840899795293808, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3398606777191162, 'eval_runtime': 11.1153, 'eval_samples_per_second': 89.876, 'eval_steps_per_second': 5.668, 'epoch': 0.88}
{'loss': 1.3751, 'grad_norm': 0.06645309925079346, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3357478380203247, 'eval_runtime': 11.1123, 'eval_samples_per_second': 89.9, 'eval_steps_per_second': 5.669, 'epoch': 0.92}
{'loss': 1.3919, 'grad_norm': 0.07120543718338013, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3324092626571655, 'eval_runtime': 11.0915, 'eval_samples_per_second': 90.069, 'eval_steps_per_second': 5.68, 'epoch': 0.96}
{'loss': 1.4069, 'grad_norm': 0.08784357458353043, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3312392234802246, 'eval_runtime': 11.0736, 'eval_samples_per_second': 90.214, 'eval_steps_per_second': 5.689, 'epoch': 1.0}
{'train_runtime': 574.3779, 'train_samples_per_second': 17.408, 'train_steps_per_second': 1.088, 'train_loss': 1.6044103393554687, 'epoch': 1.0}
train_results:  {'eval_loss': [3.331235885620117, 1.9529578685760498, 1.673614740371704, 1.5553513765335083, 1.5262680053710938, 1.507442593574524, 1.4964977502822876, 1.4817932844161987, 1.4727586507797241, 1.4559757709503174, 1.4441179037094116, 1.4332191944122314, 1.423733115196228, 1.4131208658218384, 1.4020370244979858, 1.391098141670227, 1.383419394493103, 1.3723567724227905, 1.364851951599121, 1.3552955389022827, 1.346694827079773, 1.3398606777191162, 1.3357478380203247, 1.3324092626571655, 1.3312392234802246], 'performance': [0.5, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.60it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.21it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.92it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 72.19it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.29it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 81.96it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.54it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.26it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.23it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.23it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.25it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.66it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 116.18it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.43it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 119.17it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.89it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 117.32it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 119.53it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.79it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.77it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.74it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.98it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.98it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4268748760223389
current iteration best possible performance (full train run):  0.42000000000000004
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246, 1.4188897609710693, 1.3949439525604248, 1.425980567932129, 1.4268748760223389]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.8652 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.7048548347803548
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 1.0153562990850464
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.9618486356499976
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 1.0846409080754171
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 1.2206725156180391
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2061, dtype=torch.float64), tensor(0.4458, dtype=torch.float64), 0, 0, tensor(0.0670, dtype=torch.float64), tensor(0.2811, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 0, 1, 128, 0.01795969549658864, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(6.3228e-17, dtype=torch.float64), tensor(0.2061, dtype=torch.float64), tensor(0.4458, dtype=torch.float64), tensor(1.7298e-18, dtype=torch.float64), tensor(8.0061e-18, dtype=torch.float64), tensor(0.0670, dtype=torch.float64), tensor(0.2811, dtype=torch.float64), tensor(1.7387e-18, dtype=torch.float64), tensor(1.1230e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.1796, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.206
  rowan_hellaswag: 0.446
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.067
  wikitext: 0.281
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.01795969549658864,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.01795969549658864
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.04it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 94.61it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 105.99it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 113.86it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 119.09it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 132.02it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 133.12it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 143.28it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 143.35it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 144.97it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 146.54it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 153.53it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 156.89it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 228/400 [00:01<00:01, 160.64it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 161.90it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 166.39it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 178.04it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 187.68it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 192.81it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 204.23it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 160.23it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.6465, 'grad_norm': 0.09364314377307892, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.0598058700561523, 'eval_runtime': 10.2329, 'eval_samples_per_second': 97.626, 'eval_steps_per_second': 6.157, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.37it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 94.51it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 106.12it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 114.03it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 118.99it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 131.77it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 132.98it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 142.11it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 142.41it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 144.31it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 146.07it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 153.19it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 156.60it/s]Running loglikelihood requests:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 232/400 [00:01<00:00, 171.83it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 158.87it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 164.35it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 176.48it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 186.59it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 192.03it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 203.75it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 160.01it/s]
Evaluation performance at step 50: 0.49
{'loss': 2.5777, 'grad_norm': 0.05558372288942337, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 2.1332614421844482, 'eval_runtime': 10.2549, 'eval_samples_per_second': 97.416, 'eval_steps_per_second': 6.143, 'epoch': 0.08}
{'loss': 2.0158, 'grad_norm': 0.05184336006641388, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8616465330123901, 'eval_runtime': 10.2688, 'eval_samples_per_second': 97.285, 'eval_steps_per_second': 6.135, 'epoch': 0.12}
{'loss': 1.8499, 'grad_norm': 0.038477133959531784, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7627683877944946, 'eval_runtime': 10.2928, 'eval_samples_per_second': 97.058, 'eval_steps_per_second': 6.121, 'epoch': 0.16}
{'loss': 1.7698, 'grad_norm': 0.03901230916380882, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7246756553649902, 'eval_runtime': 10.2681, 'eval_samples_per_second': 97.292, 'eval_steps_per_second': 6.136, 'epoch': 0.2}
{'loss': 1.7021, 'grad_norm': 0.03654683753848076, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.699098825454712, 'eval_runtime': 10.277, 'eval_samples_per_second': 97.207, 'eval_steps_per_second': 6.13, 'epoch': 0.24}
{'loss': 1.6564, 'grad_norm': 0.03681169077754021, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.682417392730713, 'eval_runtime': 10.2831, 'eval_samples_per_second': 97.15, 'eval_steps_per_second': 6.127, 'epoch': 0.28}
{'loss': 1.6866, 'grad_norm': 0.04336230456829071, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6671113967895508, 'eval_runtime': 10.3119, 'eval_samples_per_second': 96.879, 'eval_steps_per_second': 6.109, 'epoch': 0.32}
{'loss': 1.6549, 'grad_norm': 0.0408005565404892, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6571016311645508, 'eval_runtime': 10.3478, 'eval_samples_per_second': 96.542, 'eval_steps_per_second': 6.088, 'epoch': 0.36}
{'loss': 1.6974, 'grad_norm': 0.03532892465591431, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6438493728637695, 'eval_runtime': 10.3503, 'eval_samples_per_second': 96.519, 'eval_steps_per_second': 6.087, 'epoch': 0.4}
{'loss': 1.6461, 'grad_norm': 0.03679665923118591, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6315467357635498, 'eval_runtime': 10.3465, 'eval_samples_per_second': 96.555, 'eval_steps_per_second': 6.089, 'epoch': 0.44}
{'loss': 1.6133, 'grad_norm': 0.044722992926836014, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6166398525238037, 'eval_runtime': 10.3433, 'eval_samples_per_second': 96.584, 'eval_steps_per_second': 6.091, 'epoch': 0.48}
{'loss': 1.582, 'grad_norm': 0.05859670415520668, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.590681791305542, 'eval_runtime': 10.3439, 'eval_samples_per_second': 96.579, 'eval_steps_per_second': 6.091, 'epoch': 0.52}
{'loss': 1.6017, 'grad_norm': 0.03808124363422394, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5825499296188354, 'eval_runtime': 10.3461, 'eval_samples_per_second': 96.558, 'eval_steps_per_second': 6.089, 'epoch': 0.56}
{'loss': 1.5645, 'grad_norm': 0.04575136676430702, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5748708248138428, 'eval_runtime': 10.3335, 'eval_samples_per_second': 96.676, 'eval_steps_per_second': 6.097, 'epoch': 0.6}
{'loss': 1.5886, 'grad_norm': 0.035542793571949005, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5666123628616333, 'eval_runtime': 10.3859, 'eval_samples_per_second': 96.188, 'eval_steps_per_second': 6.066, 'epoch': 0.64}
{'loss': 1.6267, 'grad_norm': 0.03919534757733345, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5596504211425781, 'eval_runtime': 10.4364, 'eval_samples_per_second': 95.722, 'eval_steps_per_second': 6.037, 'epoch': 0.68}
{'loss': 1.6274, 'grad_norm': 0.04198610410094261, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5519039630889893, 'eval_runtime': 10.3903, 'eval_samples_per_second': 96.147, 'eval_steps_per_second': 6.063, 'epoch': 0.72}
{'loss': 1.6137, 'grad_norm': 0.039692532271146774, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5442150831222534, 'eval_runtime': 10.3615, 'eval_samples_per_second': 96.415, 'eval_steps_per_second': 6.08, 'epoch': 0.76}
{'loss': 1.5703, 'grad_norm': 0.03621847555041313, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5371145009994507, 'eval_runtime': 10.3759, 'eval_samples_per_second': 96.281, 'eval_steps_per_second': 6.072, 'epoch': 0.8}
{'loss': 1.532, 'grad_norm': 0.03791064769029617, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.531172513961792, 'eval_runtime': 10.4583, 'eval_samples_per_second': 95.522, 'eval_steps_per_second': 6.024, 'epoch': 0.84}
{'loss': 1.5222, 'grad_norm': 0.05113544687628746, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5290085077285767, 'eval_runtime': 10.4186, 'eval_samples_per_second': 95.886, 'eval_steps_per_second': 6.047, 'epoch': 0.88}
{'loss': 1.5456, 'grad_norm': 0.04493623599410057, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5269882678985596, 'eval_runtime': 10.4413, 'eval_samples_per_second': 95.677, 'eval_steps_per_second': 6.034, 'epoch': 0.92}
{'loss': 1.5827, 'grad_norm': 0.04813375324010849, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5257946252822876, 'eval_runtime': 10.4495, 'eval_samples_per_second': 95.602, 'eval_steps_per_second': 6.029, 'epoch': 0.96}
{'loss': 1.5573, 'grad_norm': 0.04334327206015587, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5251387357711792, 'eval_runtime': 10.4462, 'eval_samples_per_second': 95.633, 'eval_steps_per_second': 6.031, 'epoch': 1.0}
{'train_runtime': 528.9052, 'train_samples_per_second': 18.901, 'train_steps_per_second': 1.182, 'train_loss': 1.761249639892578, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0598058700561523, 2.1332614421844482, 1.8616465330123901, 1.7627683877944946, 1.7246756553649902, 1.699098825454712, 1.682417392730713, 1.6671113967895508, 1.6571016311645508, 1.6438493728637695, 1.6315467357635498, 1.6166398525238037, 1.590681791305542, 1.5825499296188354, 1.5748708248138428, 1.5666123628616333, 1.5596504211425781, 1.5519039630889893, 1.5442150831222534, 1.5371145009994507, 1.531172513961792, 1.5290085077285767, 1.5269882678985596, 1.5257946252822876, 1.5251387357711792], 'performance': [0.49, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:49,  1.74it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:17, 21.59it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:09, 39.58it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 52.49it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 63.34it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 72.35it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 81.51it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 92.10it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 95.58it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 98.76it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 102.56it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 110.40it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 121.03it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 119.82it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 125.04it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 123.92it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 123.39it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 125.70it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 128.44it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 136.23it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 136.14it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 136.91it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 147.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.61it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.4215773344039917
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246, 1.4188897609710693, 1.3949439525604248, 1.425980567932129, 1.4268748760223389, 1.4215773344039917]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8111 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 1.0164983802252996
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.5445305254791254
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.5449824469369661
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.5748456268917395
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.5897345114838949
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4592, dtype=torch.float64), 0, tensor(0.5331, dtype=torch.float64), 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(2.1402e-16, dtype=torch.float64), tensor(7.5466e-17, dtype=torch.float64), tensor(0.4592, dtype=torch.float64), tensor(0.0040, dtype=torch.float64), tensor(0.5331, dtype=torch.float64), tensor(0.0037, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.459
  sciq: 0
  triviaqa: 0.533
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9922
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  992
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.02it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.74it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.58it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.79it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.28it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.11it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.15it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.29it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.37it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.80it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.32it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.69it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.81it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.71it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.01it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.45it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.38it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.77it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.19it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.26it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.88it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.3332, 'grad_norm': 0.22106312215328217, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.3152828216552734, 'eval_runtime': 10.9091, 'eval_samples_per_second': 90.933, 'eval_steps_per_second': 5.683, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.00it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.68it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.40it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.63it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.26it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.09it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.13it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.31it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.28it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.73it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.25it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.66it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.79it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.60it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.97it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.56it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.50it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.88it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.30it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.43it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.12it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.88it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.5864, 'grad_norm': 0.16296423971652985, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.0070414543151855, 'eval_runtime': 10.9271, 'eval_samples_per_second': 90.783, 'eval_steps_per_second': 5.674, 'epoch': 0.08}
{'loss': 1.8721, 'grad_norm': 0.05894700810313225, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.7482304573059082, 'eval_runtime': 10.9482, 'eval_samples_per_second': 90.609, 'eval_steps_per_second': 5.663, 'epoch': 0.12}
{'loss': 1.7391, 'grad_norm': 0.04900297895073891, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.6336822509765625, 'eval_runtime': 10.9516, 'eval_samples_per_second': 90.58, 'eval_steps_per_second': 5.661, 'epoch': 0.16}
{'loss': 1.6625, 'grad_norm': 0.04249859228730202, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.6111459732055664, 'eval_runtime': 10.9786, 'eval_samples_per_second': 90.357, 'eval_steps_per_second': 5.647, 'epoch': 0.2}
{'loss': 1.5989, 'grad_norm': 0.05262173339724541, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.5943586826324463, 'eval_runtime': 11.0203, 'eval_samples_per_second': 90.015, 'eval_steps_per_second': 5.626, 'epoch': 0.24}
{'loss': 1.6486, 'grad_norm': 0.04010752588510513, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.5858676433563232, 'eval_runtime': 10.9699, 'eval_samples_per_second': 90.429, 'eval_steps_per_second': 5.652, 'epoch': 0.28}
{'loss': 1.6088, 'grad_norm': 0.03699367865920067, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.5778387784957886, 'eval_runtime': 10.9717, 'eval_samples_per_second': 90.414, 'eval_steps_per_second': 5.651, 'epoch': 0.32}
{'loss': 1.5761, 'grad_norm': 0.05802933871746063, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.5729224681854248, 'eval_runtime': 10.954, 'eval_samples_per_second': 90.561, 'eval_steps_per_second': 5.66, 'epoch': 0.36}
{'loss': 1.5925, 'grad_norm': 0.03807555139064789, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.565192461013794, 'eval_runtime': 10.9141, 'eval_samples_per_second': 90.892, 'eval_steps_per_second': 5.681, 'epoch': 0.4}
{'loss': 1.56, 'grad_norm': 0.042161475867033005, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.5592509508132935, 'eval_runtime': 10.9142, 'eval_samples_per_second': 90.891, 'eval_steps_per_second': 5.681, 'epoch': 0.44}
{'loss': 1.5683, 'grad_norm': 0.04497556388378143, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.5545604228973389, 'eval_runtime': 10.9187, 'eval_samples_per_second': 90.853, 'eval_steps_per_second': 5.678, 'epoch': 0.48}
{'loss': 1.5611, 'grad_norm': 0.04525428265333176, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.5507487058639526, 'eval_runtime': 10.9306, 'eval_samples_per_second': 90.755, 'eval_steps_per_second': 5.672, 'epoch': 0.52}
{'loss': 1.5854, 'grad_norm': 0.0379195362329483, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.5474134683609009, 'eval_runtime': 10.918, 'eval_samples_per_second': 90.859, 'eval_steps_per_second': 5.679, 'epoch': 0.56}
{'loss': 1.5558, 'grad_norm': 0.03846033290028572, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.5430068969726562, 'eval_runtime': 10.9124, 'eval_samples_per_second': 90.906, 'eval_steps_per_second': 5.682, 'epoch': 0.6}
{'loss': 1.5774, 'grad_norm': 0.04336386173963547, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.5395987033843994, 'eval_runtime': 10.9359, 'eval_samples_per_second': 90.71, 'eval_steps_per_second': 5.669, 'epoch': 0.64}
{'loss': 1.5433, 'grad_norm': 0.04046965017914772, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.5363420248031616, 'eval_runtime': 10.9538, 'eval_samples_per_second': 90.562, 'eval_steps_per_second': 5.66, 'epoch': 0.68}
{'loss': 1.5554, 'grad_norm': 0.04211629927158356, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.5338995456695557, 'eval_runtime': 10.9471, 'eval_samples_per_second': 90.618, 'eval_steps_per_second': 5.664, 'epoch': 0.72}
{'loss': 1.5318, 'grad_norm': 0.03699738159775734, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.531629204750061, 'eval_runtime': 10.9581, 'eval_samples_per_second': 90.527, 'eval_steps_per_second': 5.658, 'epoch': 0.76}
{'loss': 1.4926, 'grad_norm': 0.0455780066549778, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.5297470092773438, 'eval_runtime': 10.9187, 'eval_samples_per_second': 90.853, 'eval_steps_per_second': 5.678, 'epoch': 0.81}
{'loss': 1.5047, 'grad_norm': 0.04669532924890518, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.5286928415298462, 'eval_runtime': 10.9067, 'eval_samples_per_second': 90.953, 'eval_steps_per_second': 5.685, 'epoch': 0.85}
{'loss': 1.5646, 'grad_norm': 0.04018953815102577, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.5270311832427979, 'eval_runtime': 10.934, 'eval_samples_per_second': 90.726, 'eval_steps_per_second': 5.67, 'epoch': 0.89}
{'loss': 1.5216, 'grad_norm': 0.05124396085739136, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.5257422924041748, 'eval_runtime': 10.9398, 'eval_samples_per_second': 90.678, 'eval_steps_per_second': 5.667, 'epoch': 0.93}
{'loss': 1.5423, 'grad_norm': 0.04667358100414276, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.5250954627990723, 'eval_runtime': 11.0352, 'eval_samples_per_second': 89.894, 'eval_steps_per_second': 5.618, 'epoch': 0.97}
{'train_runtime': 538.8877, 'train_samples_per_second': 18.412, 'train_steps_per_second': 1.152, 'train_loss': 1.7398578318228852, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3152828216552734, 2.0070414543151855, 1.7482304573059082, 1.6336822509765625, 1.6111459732055664, 1.5943586826324463, 1.5858676433563232, 1.5778387784957886, 1.5729224681854248, 1.565192461013794, 1.5592509508132935, 1.5545604228973389, 1.5507487058639526, 1.5474134683609009, 1.5430068969726562, 1.5395987033843994, 1.5363420248031616, 1.5338995456695557, 1.531629204750061, 1.5297470092773438, 1.5286928415298462, 1.5270311832427979, 1.5257422924041748, 1.5250954627990723], 'performance': [0.48, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:58,  3.36it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 36.55it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 55.72it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 65.26it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 72.50it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 78.38it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 84.85it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 93.10it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 94.72it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 96.26it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 99.70it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.35it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.59it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.55it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.11it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 116.92it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.43it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.82it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.15it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.30it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.21it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.78it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 105.10it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4272050857543945
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246, 1.4188897609710693, 1.3949439525604248, 1.425980567932129, 1.4268748760223389, 1.4215773344039917, 1.4272050857543945]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0174 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 1.0243062233860076
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.9949676035216314
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.4877591474665296
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.6510338108779804
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 1.2090911741859034
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4535, dtype=torch.float64), tensor(0.5465, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 1, 1, 1, 1, 128, 4.789372940582841e-19, 1.4800000190734874, 0]
normalized proposed parameters for next round by BO: [tensor(7.5359e-17, dtype=torch.float64), tensor(3.3153e-17, dtype=torch.float64), tensor(0.4535, dtype=torch.float64), tensor(0.5465, dtype=torch.float64), tensor(2.0291e-17, dtype=torch.float64), tensor(3.6896e-17, dtype=torch.float64), tensor(5.5319e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8381e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(4.7894e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.454
  sciq: 0.546
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.789372940582841e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (1.4800000190734874,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  4.789372940582841e-19
lora alpha:  1.4800000190734874
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 52.46it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 85.27it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 95.89it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 102.93it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:03, 107.33it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 118.74it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:01<00:02, 119.57it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 128.66it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 128.71it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 130.03it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 131.35it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 137.56it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 140.39it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 140.90it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 144.80it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 145.04it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 149.16it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 161.17it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 162.93it/s]Running loglikelihood requests:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 336/400 [00:02<00:00, 179.23it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 172.86it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 184.18it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 143.30it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.2955, 'grad_norm': 0.18277624249458313, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.296144723892212, 'eval_runtime': 11.1753, 'eval_samples_per_second': 89.394, 'eval_steps_per_second': 5.637, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 52.57it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 85.16it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 95.60it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 102.67it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:03, 107.11it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 118.56it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:01<00:02, 119.78it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 128.77it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 128.71it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 129.97it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 131.35it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 137.52it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 140.51it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 141.21it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 145.43it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 145.93it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 149.70it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 161.58it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 163.18it/s]Running loglikelihood requests:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 336/400 [00:02<00:00, 179.43it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 172.87it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 184.11it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 143.37it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.4911, 'grad_norm': 0.17122891545295715, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.9031744003295898, 'eval_runtime': 11.2643, 'eval_samples_per_second': 88.687, 'eval_steps_per_second': 5.593, 'epoch': 0.08}
{'loss': 1.7591, 'grad_norm': 0.06665471941232681, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6502139568328857, 'eval_runtime': 11.2784, 'eval_samples_per_second': 88.576, 'eval_steps_per_second': 5.586, 'epoch': 0.12}
{'loss': 1.5577, 'grad_norm': 0.058173906058073044, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.547533631324768, 'eval_runtime': 11.2964, 'eval_samples_per_second': 88.435, 'eval_steps_per_second': 5.577, 'epoch': 0.16}
{'loss': 1.5273, 'grad_norm': 0.054043371230363846, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5224418640136719, 'eval_runtime': 11.2943, 'eval_samples_per_second': 88.452, 'eval_steps_per_second': 5.578, 'epoch': 0.2}
{'loss': 1.5303, 'grad_norm': 0.04385902360081673, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5092151165008545, 'eval_runtime': 11.2525, 'eval_samples_per_second': 88.781, 'eval_steps_per_second': 5.599, 'epoch': 0.24}
{'loss': 1.477, 'grad_norm': 0.04092453047633171, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5012030601501465, 'eval_runtime': 11.1959, 'eval_samples_per_second': 89.229, 'eval_steps_per_second': 5.627, 'epoch': 0.28}
{'loss': 1.4691, 'grad_norm': 0.04071574658155441, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4929847717285156, 'eval_runtime': 11.2804, 'eval_samples_per_second': 88.561, 'eval_steps_per_second': 5.585, 'epoch': 0.32}
{'loss': 1.5027, 'grad_norm': 0.044250402599573135, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4910420179367065, 'eval_runtime': 11.2985, 'eval_samples_per_second': 88.419, 'eval_steps_per_second': 5.576, 'epoch': 0.36}
{'loss': 1.4815, 'grad_norm': 0.047142740339040756, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4790385961532593, 'eval_runtime': 11.2514, 'eval_samples_per_second': 88.789, 'eval_steps_per_second': 5.599, 'epoch': 0.4}
{'loss': 1.5122, 'grad_norm': 0.043437179177999496, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4736907482147217, 'eval_runtime': 11.2284, 'eval_samples_per_second': 88.971, 'eval_steps_per_second': 5.611, 'epoch': 0.44}
{'loss': 1.4926, 'grad_norm': 0.05785699933767319, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4703693389892578, 'eval_runtime': 11.1985, 'eval_samples_per_second': 89.208, 'eval_steps_per_second': 5.626, 'epoch': 0.48}
{'loss': 1.4626, 'grad_norm': 0.04201984032988548, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4669281244277954, 'eval_runtime': 11.1988, 'eval_samples_per_second': 89.206, 'eval_steps_per_second': 5.626, 'epoch': 0.52}
{'loss': 1.4875, 'grad_norm': 0.04418089985847473, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4627248048782349, 'eval_runtime': 11.2306, 'eval_samples_per_second': 88.954, 'eval_steps_per_second': 5.61, 'epoch': 0.56}
{'loss': 1.4582, 'grad_norm': 0.05053144320845604, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.45841646194458, 'eval_runtime': 11.2522, 'eval_samples_per_second': 88.783, 'eval_steps_per_second': 5.599, 'epoch': 0.6}
{'loss': 1.4906, 'grad_norm': 0.06544102728366852, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.455581545829773, 'eval_runtime': 11.2193, 'eval_samples_per_second': 89.043, 'eval_steps_per_second': 5.615, 'epoch': 0.64}
{'loss': 1.4768, 'grad_norm': 0.05001678317785263, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4546185731887817, 'eval_runtime': 11.2065, 'eval_samples_per_second': 89.145, 'eval_steps_per_second': 5.622, 'epoch': 0.68}
{'loss': 1.5074, 'grad_norm': 0.047613564878702164, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4504338502883911, 'eval_runtime': 11.2063, 'eval_samples_per_second': 89.147, 'eval_steps_per_second': 5.622, 'epoch': 0.72}
{'loss': 1.4921, 'grad_norm': 0.04839593544602394, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4475390911102295, 'eval_runtime': 11.199, 'eval_samples_per_second': 89.204, 'eval_steps_per_second': 5.626, 'epoch': 0.76}
{'loss': 1.4528, 'grad_norm': 0.044313281774520874, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.445427656173706, 'eval_runtime': 11.2067, 'eval_samples_per_second': 89.143, 'eval_steps_per_second': 5.622, 'epoch': 0.8}
{'loss': 1.4874, 'grad_norm': 0.047559771686792374, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4430633783340454, 'eval_runtime': 11.2043, 'eval_samples_per_second': 89.162, 'eval_steps_per_second': 5.623, 'epoch': 0.84}
{'loss': 1.4874, 'grad_norm': 0.049174245446920395, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.441721796989441, 'eval_runtime': 11.2215, 'eval_samples_per_second': 89.026, 'eval_steps_per_second': 5.614, 'epoch': 0.88}
{'loss': 1.4506, 'grad_norm': 0.05368571728467941, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4404600858688354, 'eval_runtime': 11.2147, 'eval_samples_per_second': 89.079, 'eval_steps_per_second': 5.618, 'epoch': 0.92}
{'loss': 1.4955, 'grad_norm': 0.05112680420279503, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.439473271369934, 'eval_runtime': 11.2078, 'eval_samples_per_second': 89.135, 'eval_steps_per_second': 5.621, 'epoch': 0.96}
{'loss': 1.5023, 'grad_norm': 0.047214433550834656, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.439445972442627, 'eval_runtime': 11.1881, 'eval_samples_per_second': 89.292, 'eval_steps_per_second': 5.631, 'epoch': 1.0}
{'train_runtime': 582.2932, 'train_samples_per_second': 17.172, 'train_steps_per_second': 1.073, 'train_loss': 1.6538967895507812, 'epoch': 1.0}
train_results:  {'eval_loss': [3.296144723892212, 1.9031744003295898, 1.6502139568328857, 1.547533631324768, 1.5224418640136719, 1.5092151165008545, 1.5012030601501465, 1.4929847717285156, 1.4910420179367065, 1.4790385961532593, 1.4736907482147217, 1.4703693389892578, 1.4669281244277954, 1.4627248048782349, 1.45841646194458, 1.455581545829773, 1.4546185731887817, 1.4504338502883911, 1.4475390911102295, 1.445427656173706, 1.4430633783340454, 1.441721796989441, 1.4404600858688354, 1.439473271369934, 1.439445972442627], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:28,  4.53it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 46.66it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 63.88it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 70.96it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 76.21it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 80.75it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 86.29it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 93.58it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 94.54it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 95.38it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.23it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 104.74it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 113.79it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 111.89it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 116.72it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 115.57it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 115.00it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 117.16it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 119.39it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 126.39it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 126.10it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 126.71it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 136.23it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 107.01it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4215097427368164
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.5880000000000001
BO observations:  [1.0514142513275146, 1.0904109477996826, 1.4104732275009155, 0.4789614677429199, 1.2945060729980469, 0.6097627878189087, 1.414524793624878, 1.2918140888214111, 1.424363136291504, 1.4244529008865356, 1.4275639057159424, 1.4248653650283813, 1.3060916662216187, 1.4254796504974365, 1.423478126525879, 1.4052627086639404, 1.4027016162872314, 1.419787883758545, 1.4275535345077515, 1.4265819787979126, 1.426941156387329, 1.4269388914108276, 1.4230360984802246, 1.4188897609710693, 1.3949439525604248, 1.425980567932129, 1.4268748760223389, 1.4215773344039917, 1.4272050857543945, 1.4215097427368164]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2211 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 1.0530548524247496
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.8367589516173057
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 1.0829354665257627
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.7054719891737755
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.6341684446911864
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1735, dtype=torch.float64), 0, tensor(0.4718, dtype=torch.float64), 0, tensor(0.1628, dtype=torch.float64), tensor(0.1919, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734883, 0]
normalized proposed parameters for next round by BO: [tensor(0.1735, dtype=torch.float64), tensor(3.4238e-17, dtype=torch.float64), tensor(0.4718, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1628, dtype=torch.float64), tensor(0.1919, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6966e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.504, 0.504, 0.504, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5775000000000001, 0.5775000000000001, 0.5775000000000001, 0.5775000000000001, 0.5775000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/arc_challenge/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 17, 1, 0, 1, 1, 0, 59, 0.00445603671922955, 41, 0]
Checking history sample input_X_between_0_1:  [0.09538950919256631, 0.2712278119287982, 0.31701378787803625, 0.061967410356890185, 0.05007035673700025, 0.02708318701312588, 0.08771244884380049, 0.06222571560636049, 0.027309772443421837, 0.53125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4609375, 0.044560367192295496, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.47
Checking history sample input_X:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 22, 1, 1, 1, 0, 0, 61, 0.08406775506720655, 5, 1]
Checking history sample input_X_between_0_1:  [0.050704810733391815, 0.00537728164685224, 0.10598705354838227, 0.3119172453867198, 0.05277008509654292, 0.037936409116012475, 0.03560414503943689, 0.0669123083357019, 0.3327906610969599, 0.6875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4765625, 0.8406775506720655, 0.10416666666666667, 1.0]
Checking history sample performance at 625 steps:  0.47
Checking history sample input_X:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 15, 1, 1, 1, 1, 0, 24, 0.05173325143341287, 28, 0]
Checking history sample input_X_between_0_1:  [0.0895050951327471, 0.09161735962464355, 0.04632782788018367, 0.016562946990237915, 0.217309628399583, 0.034512545276229364, 0.008617474762716452, 0.4862777752107909, 0.009269346722867864, 0.46875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 0.5173325143341286, 0.5833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 1, 0, 1, 1, 0, 0, 114, 0.021033555465481826, 23, 1]
Checking history sample input_X_between_0_1:  [0.017748269501875667, 0.11687752335093718, 0.028118463537739658, 0.10734563326210614, 0.3142607229066349, 0.12553038848089512, 0.04073299503229199, 0.17570775115041296, 0.07367825277710642, 0.03125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.890625, 0.21033555465481824, 0.4791666666666667, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 27, 1, 1, 1, 1, 1, 70, 0.005963494977880357, 46, 1]
Checking history sample input_X_between_0_1:  [0.12410838877266096, 0.06117925264621778, 0.044338867347763246, 0.09546282462057763, 0.09443990037677061, 0.03593784463205927, 0.06869599633131632, 0.40441517592800974, 0.07142174934462434, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.05963494977880357, 0.9583333333333334, 1.0]
Checking history sample performance at 625 steps:  0.46
Checking history sample input_X:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 12, 1, 0, 1, 0, 0, 3, 0.017954470577765235, 12, 1]
Checking history sample input_X_between_0_1:  [0.11348509166286383, 0.20323374865030203, 0.02009526470276355, 0.006825707778390264, 0.25056026482992044, 0.14814888951091681, 0.18532430309665107, 0.026376980791021545, 0.04594974897717038, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0234375, 0.17954470577765233, 0.25, 1.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 18, 1, 0, 0, 0, 1, 5, 0.028541773579282805, 35, 0]
Checking history sample input_X_between_0_1:  [0.029155635359388223, 0.23709426672943262, 0.15315858728043213, 0.005197070447629008, 0.1697495261865987, 0.10897619331108603, 0.04683658579543691, 0.015640161225496885, 0.23419197366449945, 0.5625, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0390625, 0.28541773579282803, 0.7291666666666666, 0.0]
Checking history sample performance at 625 steps:  0.4
Checking history sample input_X:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 24, 0, 1, 0, 1, 0, 55, 0.08420649661865921, 22, 0]
Checking history sample input_X_between_0_1:  [0.06135203497180341, 0.07352861297854503, 0.08130110177670519, 0.2931885223248111, 0.30473202473353095, 0.09294998809340899, 0.0022643596177021495, 0.0769366511032376, 0.013746704400255574, 0.75, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4296875, 0.8420649661865921, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 30, 1, 0, 1, 1, 0, 5, 0.09059974254780842, 29, 1]
Checking history sample input_X_between_0_1:  [0.09433070340764844, 0.02135594440126927, 0.02739850343944074, 0.2302625930299135, 0.1530804122102203, 0.0010274007132643607, 0.05148589317231848, 0.30740284958207376, 0.11365570004385106, 0.9375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0390625, 0.9059974254780842, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.44
Checking history sample input_X:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 20, 1, 1, 0, 1, 0, 27, 0.07188580123073206, 22, 0]
Checking history sample input_X_between_0_1:  [0.3516953059776763, 0.06840550946879465, 0.03125014621167114, 0.14067519419415084, 0.004825372808415826, 0.029311348781288524, 0.13867064773433269, 0.15709436924728773, 0.07807210557638233, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2109375, 0.7188580123073205, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 8, 0, 0, 0, 1, 1, 31, 0.06288146497812035, 13, 1]
Checking history sample input_X_between_0_1:  [0.369129511777336, 0.00046748078297839375, 0.04621144728371157, 0.11862054158159098, 0.11007022974781519, 0.23409182939453932, 0.051431433846834906, 0.003511722640413916, 0.0664658029447798, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2421875, 0.6288146497812034, 0.2708333333333333, 1.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 32, 1, 1, 0, 1, 1, 114, 0.08252011949389138, 13, 0]
Checking history sample input_X_between_0_1:  [0.025888810118539243, 0.11203796722698516, 0.07901174504826568, 0.16402074581761344, 0.07655341341163782, 0.07570170647494788, 0.0918396068663539, 0.26568926511575225, 0.10925673991990464, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.890625, 0.8252011949389138, 0.2708333333333333, 0.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 11, 0, 1, 0, 0, 1, 100, 0.09203832421630076, 16, 1]
Checking history sample input_X_between_0_1:  [0.10063338762470554, 0.26798662137905255, 0.0885405593912017, 0.05978857885411811, 0.19892821422203225, 0.201678923124618, 0.029038040659988593, 0.015041641401000339, 0.03836403334328297, 0.34375, 0.0, 1.0, 0.0, 0.0, 1.0, 0.78125, 0.9203832421630076, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 31, 1, 0, 1, 1, 1, 60, 0.008025447056787238, 26, 0]
Checking history sample input_X_between_0_1:  [0.04068697763713428, 0.05328651380810958, 0.2788810076528015, 0.1777460538365553, 0.22909326401803112, 0.09699391042878978, 0.08877605882407394, 0.032938688261685, 0.0015975255328196276, 0.96875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.46875, 0.08025447056787237, 0.5416666666666666, 0.0]
Checking history sample performance at 625 steps:  0.57
Checking history sample input_X:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 7, 0, 0, 1, 0, 0, 10, 0.06890300584266877, 3, 1]
Checking history sample input_X_between_0_1:  [0.1777356311391792, 0.01942636533555801, 0.12571175960230482, 0.05177029176329094, 0.2929368887015221, 0.08479894803999374, 0.02279430745696025, 0.16427448487567686, 0.060551323085513926, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.078125, 0.6890300584266876, 0.0625, 1.0]
Checking history sample performance at 625 steps:  0.49
Checking history sample input_X:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 1, 1, 1, 0, 1, 0, 28, 0.0879909943648135, 16, 0]
Checking history sample input_X_between_0_1:  [0.004782440649876434, 0.192014178858742, 0.015091754483879359, 0.09553645696158183, 0.0682234299769642, 0.023052083070234066, 0.06892905642724492, 0.09502326815470577, 0.43734733141677146, 0.03125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21875, 0.879909943648135, 0.3333333333333333, 0.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 1, 1, 0, 0, 1, 0, 47, 0.04979780998318395, 37, 0]
Checking history sample input_X_between_0_1:  [0.0698570517363598, 0.05554379915664428, 0.014618425914918956, 0.15003048941235453, 0.31445406099163825, 0.18597132962742952, 0.03361772529200811, 0.03998019877035168, 0.13592691909829488, 0.03125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3671875, 0.4979780998318395, 0.7708333333333334, 0.0]
Checking history sample performance at 625 steps:  0.5
Checking history sample input_X:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 30, 1, 0, 1, 0, 0, 86, 0.08085387005284786, 6, 1]
Checking history sample input_X_between_0_1:  [0.08588462815372222, 0.2444441247666556, 0.01784143969460146, 0.13059252717321798, 0.07115971446539679, 0.0758383099369087, 0.12001597809341633, 0.23658431051486153, 0.01763896720121921, 0.9375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.671875, 0.8085387005284785, 0.125, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 31, 0, 1, 1, 1, 1, 84, 0.015776955723827136, 41, 1]
Checking history sample input_X_between_0_1:  [0.26815296878546835, 0.1435877702371542, 0.15836797839176875, 0.0916665772117945, 0.04202039727550052, 6.612392036271934e-05, 0.06761639288784022, 0.1737705426583011, 0.05475124863180973, 0.96875, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.15776955723827135, 0.8541666666666666, 1.0]
Checking history sample performance at 625 steps:  0.43
Checking history sample input_X:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 24, 0, 1, 1, 0, 1, 1, 0.07052967196513893, 22, 0]
Checking history sample input_X_between_0_1:  [0.06362861638513699, 0.01998030434303658, 0.006509677702868549, 0.05319755110873162, 0.42028129587257773, 0.05710772864593268, 0.03244111545817009, 0.2421587264888914, 0.1046949839946544, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.7052967196513893, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.46
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6836 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.603321762549632
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.603366961800696
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.6018943351770696
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.6033736650693575
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.6033736595345878
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, 0, tensor(0.3410, dtype=torch.float64), tensor(0.2671, dtype=torch.float64), tensor(0.1793, dtype=torch.float64), tensor(0.1508, dtype=torch.float64), tensor(0.0589, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 68, 0.0017964660814157977, 2.045682658071251, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3410, dtype=torch.float64), tensor(0.2671, dtype=torch.float64), tensor(0.1793, dtype=torch.float64), tensor(0.1508, dtype=torch.float64), tensor(0.0589, dtype=torch.float64), tensor(0.0030, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5289, dtype=torch.float64), tensor(0.0180, dtype=torch.float64), tensor(0.0426, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.341
  sciq: 0.267
  triviaqa: 0.179
  truthfulqa_gen: 0.151
  wikitext: 0.059
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (68,)
  lora_dropout: (0.0017964660814157977,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (2.045682658071251,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  68
lora dropout:  0.0017964660814157977
lora alpha:  2.045682658071251
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 138,149,888 || all params: 8,168,411,136 || trainable%: 1.6913
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9967
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  996
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 51.86it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 84.27it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 94.79it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 101.79it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:03, 106.17it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 117.60it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:01<00:02, 118.91it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 127.68it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:02, 127.43it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 128.66it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 130.04it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 136.12it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 139.32it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 140.11it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 144.20it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 144.78it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 148.53it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 160.91it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 162.97it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 339/400 [00:02<00:00, 187.82it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 170.73it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 183.25it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 142.56it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.3326, 'grad_norm': 0.38712653517723083, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.2182974815368652, 'eval_runtime': 11.1909, 'eval_samples_per_second': 89.001, 'eval_steps_per_second': 5.63, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 51.88it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 84.15it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 94.53it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 101.53it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:03, 105.83it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 117.13it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:01<00:02, 118.36it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 126.98it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:02, 126.89it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 128.04it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 129.48it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 135.60it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 138.75it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 139.51it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 143.58it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 144.07it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 147.72it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 160.16it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 162.24it/s]Running loglikelihood requests:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 336/400 [00:02<00:00, 178.65it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 172.65it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 184.28it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 142.02it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.4783, 'grad_norm': 0.19521954655647278, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.9160300493240356, 'eval_runtime': 11.2719, 'eval_samples_per_second': 88.361, 'eval_steps_per_second': 5.589, 'epoch': 0.08}
{'loss': 1.7709, 'grad_norm': 0.07633158564567566, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.6730948686599731, 'eval_runtime': 11.2726, 'eval_samples_per_second': 88.356, 'eval_steps_per_second': 5.589, 'epoch': 0.12}
{'loss': 1.6219, 'grad_norm': 0.11790452152490616, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.577280879020691, 'eval_runtime': 11.3715, 'eval_samples_per_second': 87.587, 'eval_steps_per_second': 5.54, 'epoch': 0.16}
{'loss': 1.5705, 'grad_norm': 0.08372952789068222, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.5427231788635254, 'eval_runtime': 11.367, 'eval_samples_per_second': 87.622, 'eval_steps_per_second': 5.542, 'epoch': 0.2}
{'loss': 1.5956, 'grad_norm': 0.06012473255395889, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.5256786346435547, 'eval_runtime': 11.3722, 'eval_samples_per_second': 87.582, 'eval_steps_per_second': 5.54, 'epoch': 0.24}
{'loss': 1.5631, 'grad_norm': 0.07603900879621506, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.512567400932312, 'eval_runtime': 11.321, 'eval_samples_per_second': 87.978, 'eval_steps_per_second': 5.565, 'epoch': 0.28}
{'loss': 1.5131, 'grad_norm': 0.0816558375954628, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.5052803754806519, 'eval_runtime': 11.3648, 'eval_samples_per_second': 87.639, 'eval_steps_per_second': 5.543, 'epoch': 0.32}
{'loss': 1.477, 'grad_norm': 0.07745488733053207, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.493526816368103, 'eval_runtime': 11.294, 'eval_samples_per_second': 88.188, 'eval_steps_per_second': 5.578, 'epoch': 0.36}
{'loss': 1.5447, 'grad_norm': 0.06997029483318329, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.4878145456314087, 'eval_runtime': 11.2943, 'eval_samples_per_second': 88.186, 'eval_steps_per_second': 5.578, 'epoch': 0.4}
{'loss': 1.4946, 'grad_norm': 0.08341999351978302, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.4806158542633057, 'eval_runtime': 11.3392, 'eval_samples_per_second': 87.837, 'eval_steps_per_second': 5.556, 'epoch': 0.44}
{'loss': 1.4626, 'grad_norm': 0.07560066878795624, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.4732002019882202, 'eval_runtime': 11.2886, 'eval_samples_per_second': 88.231, 'eval_steps_per_second': 5.581, 'epoch': 0.48}
{'loss': 1.5261, 'grad_norm': 0.05991780757904053, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.465397596359253, 'eval_runtime': 11.2459, 'eval_samples_per_second': 88.565, 'eval_steps_per_second': 5.602, 'epoch': 0.52}
{'loss': 1.4362, 'grad_norm': 0.0784478634595871, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.4616360664367676, 'eval_runtime': 11.2328, 'eval_samples_per_second': 88.669, 'eval_steps_per_second': 5.609, 'epoch': 0.56}
{'loss': 1.454, 'grad_norm': 0.101336769759655, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.4594053030014038, 'eval_runtime': 11.2192, 'eval_samples_per_second': 88.776, 'eval_steps_per_second': 5.615, 'epoch': 0.6}
{'loss': 1.4032, 'grad_norm': 0.06640595197677612, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.4535969495773315, 'eval_runtime': 11.2223, 'eval_samples_per_second': 88.752, 'eval_steps_per_second': 5.614, 'epoch': 0.64}
{'loss': 1.4482, 'grad_norm': 0.08014295995235443, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.4509109258651733, 'eval_runtime': 11.2155, 'eval_samples_per_second': 88.806, 'eval_steps_per_second': 5.617, 'epoch': 0.68}
{'loss': 1.4854, 'grad_norm': 0.07851972430944443, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.446466326713562, 'eval_runtime': 11.3049, 'eval_samples_per_second': 88.103, 'eval_steps_per_second': 5.573, 'epoch': 0.72}
{'loss': 1.4309, 'grad_norm': 0.07202331721782684, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.4437334537506104, 'eval_runtime': 11.355, 'eval_samples_per_second': 87.715, 'eval_steps_per_second': 5.548, 'epoch': 0.76}
{'loss': 1.4729, 'grad_norm': 0.07331209629774094, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.4418060779571533, 'eval_runtime': 11.2837, 'eval_samples_per_second': 88.269, 'eval_steps_per_second': 5.583, 'epoch': 0.8}
{'loss': 1.4412, 'grad_norm': 0.07467997074127197, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.440178632736206, 'eval_runtime': 11.2543, 'eval_samples_per_second': 88.5, 'eval_steps_per_second': 5.598, 'epoch': 0.84}
{'loss': 1.4657, 'grad_norm': 0.08919857442378998, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.4372152090072632, 'eval_runtime': 11.2012, 'eval_samples_per_second': 88.919, 'eval_steps_per_second': 5.624, 'epoch': 0.88}
{'loss': 1.4542, 'grad_norm': 0.0740765929222107, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.4352293014526367, 'eval_runtime': 11.1956, 'eval_samples_per_second': 88.964, 'eval_steps_per_second': 5.627, 'epoch': 0.92}
{'loss': 1.4433, 'grad_norm': 0.07520146667957306, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.4342198371887207, 'eval_runtime': 11.2021, 'eval_samples_per_second': 88.912, 'eval_steps_per_second': 5.624, 'epoch': 0.96}
{'train_runtime': 565.7796, 'train_samples_per_second': 17.616, 'train_steps_per_second': 1.101, 'train_loss': 1.654725333469446, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2182974815368652, 1.9160300493240356, 1.6730948686599731, 1.577280879020691, 1.5427231788635254, 1.5256786346435547, 1.512567400932312, 1.5052803754806519, 1.493526816368103, 1.4878145456314087, 1.4806158542633057, 1.4732002019882202, 1.465397596359253, 1.4616360664367676, 1.4594053030014038, 1.4535969495773315, 1.4509109258651733, 1.446466326713562, 1.4437334537506104, 1.4418060779571533, 1.440178632736206, 1.4372152090072632, 1.4352293014526367, 1.4342198371887207], 'performance': [0.48, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:29,  4.46it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 45.98it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 63.02it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 69.93it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 75.13it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 79.78it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 85.31it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 92.70it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 93.70it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 94.49it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 97.35it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 103.89it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 113.07it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 110.89it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 115.64it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 114.07it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 113.53it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 115.54it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 117.81it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 124.90it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 124.66it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 125.22it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 134.63it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 105.83it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.0514811277389526
current iteration best possible performance (full train run):  0.462
max performance so far:  0.462
BO observations:  [1.0514811277389526]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5299 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.0216961106991282
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.9193644568481933
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.0217251915509027
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.021700016808969
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.0219550353114046
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4373, dtype=torch.float64), tensor(0.3040, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.2465, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 1, 0, 77, 9.207185256757252e-05, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(5.1564e-17, dtype=torch.float64), tensor(7.3252e-20, dtype=torch.float64), tensor(0.4373, dtype=torch.float64), tensor(0.3040, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.2465, dtype=torch.float64), tensor(1.0243e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1625e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5993, dtype=torch.float64), tensor(0.0009, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.437
  sciq: 0.304
  triviaqa: 0.012
  truthfulqa_gen: 0.247
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (9.207185256757252e-05,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  77
lora dropout:  9.207185256757252e-05
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 78,217,216 || all params: 8,108,478,464 || trainable%: 0.9646
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.15it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.80it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.01it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.34it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.36it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 124.85it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 125.43it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.25it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.78it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.25it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.32it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 143.59it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 147.64it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 148.96it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 153.89it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 152.49it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 156.90it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 170.49it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 180.54it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 185.36it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 196.17it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 151.10it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.358, 'grad_norm': 0.4765649139881134, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.6094071865081787, 'eval_runtime': 10.5423, 'eval_samples_per_second': 94.762, 'eval_steps_per_second': 5.976, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.92it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 89.81it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.12it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.74it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.57it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.65it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.86it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.44it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.70it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.09it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.80it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.33it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 149.46it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.27it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 154.54it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 154.96it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 159.01it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 172.01it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 181.57it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 186.06it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 196.85it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.56it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.7809, 'grad_norm': 0.32609623670578003, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.1420772075653076, 'eval_runtime': 10.5553, 'eval_samples_per_second': 94.644, 'eval_steps_per_second': 5.969, 'epoch': 0.08}
{'loss': 1.8862, 'grad_norm': 0.102608323097229, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7421374320983887, 'eval_runtime': 10.5445, 'eval_samples_per_second': 94.742, 'eval_steps_per_second': 5.975, 'epoch': 0.12}
{'loss': 1.6897, 'grad_norm': 0.1006406918168068, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6143275499343872, 'eval_runtime': 10.545, 'eval_samples_per_second': 94.737, 'eval_steps_per_second': 5.974, 'epoch': 0.16}
{'loss': 1.5344, 'grad_norm': 0.10678588598966599, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5741620063781738, 'eval_runtime': 10.5364, 'eval_samples_per_second': 94.814, 'eval_steps_per_second': 5.979, 'epoch': 0.2}
{'loss': 1.5846, 'grad_norm': 0.08223886787891388, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5566281080245972, 'eval_runtime': 10.5477, 'eval_samples_per_second': 94.712, 'eval_steps_per_second': 5.973, 'epoch': 0.24}
{'loss': 1.5118, 'grad_norm': 0.08409365266561508, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5455260276794434, 'eval_runtime': 10.4875, 'eval_samples_per_second': 95.256, 'eval_steps_per_second': 6.007, 'epoch': 0.28}
{'loss': 1.5101, 'grad_norm': 0.09050948172807693, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5346122980117798, 'eval_runtime': 10.5141, 'eval_samples_per_second': 95.015, 'eval_steps_per_second': 5.992, 'epoch': 0.32}
{'loss': 1.5263, 'grad_norm': 0.08581552654504776, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5339422225952148, 'eval_runtime': 10.5025, 'eval_samples_per_second': 95.12, 'eval_steps_per_second': 5.999, 'epoch': 0.36}
{'loss': 1.498, 'grad_norm': 0.08314171433448792, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.522510290145874, 'eval_runtime': 10.4987, 'eval_samples_per_second': 95.154, 'eval_steps_per_second': 6.001, 'epoch': 0.4}
{'loss': 1.5159, 'grad_norm': 0.09470705687999725, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5157532691955566, 'eval_runtime': 10.5644, 'eval_samples_per_second': 94.563, 'eval_steps_per_second': 5.963, 'epoch': 0.44}
{'loss': 1.4933, 'grad_norm': 0.09206317365169525, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5140306949615479, 'eval_runtime': 10.5419, 'eval_samples_per_second': 94.765, 'eval_steps_per_second': 5.976, 'epoch': 0.48}
{'loss': 1.5179, 'grad_norm': 0.09113167971372604, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.508909821510315, 'eval_runtime': 10.5048, 'eval_samples_per_second': 95.099, 'eval_steps_per_second': 5.997, 'epoch': 0.52}
{'loss': 1.4999, 'grad_norm': 0.08241880685091019, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5050886869430542, 'eval_runtime': 10.5814, 'eval_samples_per_second': 94.411, 'eval_steps_per_second': 5.954, 'epoch': 0.56}
{'loss': 1.4666, 'grad_norm': 0.08044339716434479, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5023229122161865, 'eval_runtime': 10.5746, 'eval_samples_per_second': 94.471, 'eval_steps_per_second': 5.958, 'epoch': 0.6}
{'loss': 1.5091, 'grad_norm': 0.08650484681129456, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5001204013824463, 'eval_runtime': 10.5501, 'eval_samples_per_second': 94.691, 'eval_steps_per_second': 5.972, 'epoch': 0.64}
{'loss': 1.4892, 'grad_norm': 0.09368825703859329, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4984325170516968, 'eval_runtime': 10.5518, 'eval_samples_per_second': 94.675, 'eval_steps_per_second': 5.971, 'epoch': 0.68}
{'loss': 1.5084, 'grad_norm': 0.08034498244524002, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4953664541244507, 'eval_runtime': 10.5283, 'eval_samples_per_second': 94.887, 'eval_steps_per_second': 5.984, 'epoch': 0.72}
{'loss': 1.4824, 'grad_norm': 0.08385027199983597, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4928622245788574, 'eval_runtime': 10.5227, 'eval_samples_per_second': 94.937, 'eval_steps_per_second': 5.987, 'epoch': 0.76}
{'loss': 1.5192, 'grad_norm': 0.08647704124450684, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4917916059494019, 'eval_runtime': 10.4658, 'eval_samples_per_second': 95.454, 'eval_steps_per_second': 6.02, 'epoch': 0.8}
{'loss': 1.4856, 'grad_norm': 0.08417945355176926, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.490057110786438, 'eval_runtime': 10.4697, 'eval_samples_per_second': 95.419, 'eval_steps_per_second': 6.017, 'epoch': 0.84}
{'loss': 1.4941, 'grad_norm': 0.09831854701042175, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4892817735671997, 'eval_runtime': 10.4676, 'eval_samples_per_second': 95.438, 'eval_steps_per_second': 6.019, 'epoch': 0.88}
{'loss': 1.4472, 'grad_norm': 0.07735899835824966, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4881199598312378, 'eval_runtime': 10.4699, 'eval_samples_per_second': 95.416, 'eval_steps_per_second': 6.017, 'epoch': 0.92}
{'loss': 1.5363, 'grad_norm': 0.0685814619064331, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4873547554016113, 'eval_runtime': 10.475, 'eval_samples_per_second': 95.37, 'eval_steps_per_second': 6.014, 'epoch': 0.96}
{'loss': 1.5133, 'grad_norm': 0.10546590387821198, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.486965537071228, 'eval_runtime': 10.4717, 'eval_samples_per_second': 95.4, 'eval_steps_per_second': 6.016, 'epoch': 1.0}
{'train_runtime': 547.7654, 'train_samples_per_second': 18.254, 'train_steps_per_second': 1.141, 'train_loss': 1.6943429382324218, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6094071865081787, 2.1420772075653076, 1.7421374320983887, 1.6143275499343872, 1.5741620063781738, 1.5566281080245972, 1.5455260276794434, 1.5346122980117798, 1.5339422225952148, 1.522510290145874, 1.5157532691955566, 1.5140306949615479, 1.508909821510315, 1.5050886869430542, 1.5023229122161865, 1.5001204013824463, 1.4984325170516968, 1.4953664541244507, 1.4928622245788574, 1.4917916059494019, 1.490057110786438, 1.4892817735671997, 1.4881199598312378, 1.4873547554016113, 1.486965537071228], 'performance': [0.48, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<04:34,  1.45it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:16, 23.59it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:08, 41.86it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 54.20it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 64.36it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 72.79it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 81.44it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 91.45it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 94.46it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 97.23it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 101.51it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 109.17it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 119.05it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.58it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.97it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 121.72it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 120.72it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 122.73it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.50it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.77it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 133.00it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 133.97it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 144.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 98.79it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.0904102325439453
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.4935
BO observations:  [1.0514811277389526, 1.0904102325439453]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.4928 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.0201282992599783
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 1.0166128066799551
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.067864110315313
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.9225655829360899
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.7757652949714529
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4139, dtype=torch.float64), tensor(0.1439, dtype=torch.float64), 0, 0, tensor(0.4422, dtype=torch.float64), 0, 0, 32, 1, 1, 0, 1, 0, 128, 5.637851296924623e-19, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.8302e-18, dtype=torch.float64), tensor(0.4139, dtype=torch.float64), tensor(0.1439, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.4604e-17, dtype=torch.float64), tensor(0.4422, dtype=torch.float64), tensor(3.4630e-17, dtype=torch.float64), tensor(3.5127e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.6379e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.414
  sciq: 0.144
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.442
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.637851296924623e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  5.637851296924623e-19
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.72it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 95.62it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 107.60it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 115.71it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 120.74it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 133.29it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 133.28it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 143.62it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 144.23it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 146.04it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 147.92it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 154.87it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 157.90it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 156.07it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 161.49it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 166.22it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 177.02it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 186.82it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 191.80it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 204.00it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 160.57it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.8718, 'grad_norm': 0.2678644359111786, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.4308176040649414, 'eval_runtime': 10.0439, 'eval_samples_per_second': 99.463, 'eval_steps_per_second': 6.272, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 59.30it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 96.26it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 108.02it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 116.16it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 121.30it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 134.11it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 135.44it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 145.52it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 145.59it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 147.18it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 149.09it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 156.28it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 159.91it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:01, 161.09it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 165.38it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 172.22it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 183.19it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 190.35it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 203.49it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 209.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 162.70it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.8579, 'grad_norm': 0.15773291885852814, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 2.359253406524658, 'eval_runtime': 9.9974, 'eval_samples_per_second': 99.926, 'eval_steps_per_second': 6.302, 'epoch': 0.08}
{'loss': 2.1729, 'grad_norm': 0.06001565232872963, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0230329036712646, 'eval_runtime': 9.9977, 'eval_samples_per_second': 99.923, 'eval_steps_per_second': 6.301, 'epoch': 0.12}
{'loss': 1.9425, 'grad_norm': 0.07458411902189255, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9276413917541504, 'eval_runtime': 10.1021, 'eval_samples_per_second': 98.891, 'eval_steps_per_second': 6.236, 'epoch': 0.16}
{'loss': 1.8992, 'grad_norm': 0.06347795575857162, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8568528890609741, 'eval_runtime': 10.0881, 'eval_samples_per_second': 99.028, 'eval_steps_per_second': 6.245, 'epoch': 0.2}
{'loss': 1.8689, 'grad_norm': 0.0825183242559433, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8318445682525635, 'eval_runtime': 10.0431, 'eval_samples_per_second': 99.472, 'eval_steps_per_second': 6.273, 'epoch': 0.24}
{'loss': 1.8559, 'grad_norm': 0.10337620228528976, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8183518648147583, 'eval_runtime': 10.0338, 'eval_samples_per_second': 99.563, 'eval_steps_per_second': 6.279, 'epoch': 0.28}
{'loss': 1.8648, 'grad_norm': 0.0538092739880085, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8059686422348022, 'eval_runtime': 10.0135, 'eval_samples_per_second': 99.765, 'eval_steps_per_second': 6.292, 'epoch': 0.32}
{'loss': 1.7929, 'grad_norm': 0.06038210541009903, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8002090454101562, 'eval_runtime': 10.0121, 'eval_samples_per_second': 99.779, 'eval_steps_per_second': 6.292, 'epoch': 0.36}
{'loss': 1.7954, 'grad_norm': 0.07627476006746292, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7930724620819092, 'eval_runtime': 9.9863, 'eval_samples_per_second': 100.037, 'eval_steps_per_second': 6.309, 'epoch': 0.4}
{'loss': 1.7974, 'grad_norm': 0.07700379937887192, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7875272035598755, 'eval_runtime': 9.9856, 'eval_samples_per_second': 100.044, 'eval_steps_per_second': 6.309, 'epoch': 0.44}
{'loss': 1.8163, 'grad_norm': 0.05411563813686371, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7830406427383423, 'eval_runtime': 9.9922, 'eval_samples_per_second': 99.978, 'eval_steps_per_second': 6.305, 'epoch': 0.48}
{'loss': 1.7766, 'grad_norm': 0.06649499386548996, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7785735130310059, 'eval_runtime': 10.0084, 'eval_samples_per_second': 99.816, 'eval_steps_per_second': 6.295, 'epoch': 0.52}
{'loss': 1.7881, 'grad_norm': 0.061890896409749985, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.774848222732544, 'eval_runtime': 9.988, 'eval_samples_per_second': 100.02, 'eval_steps_per_second': 6.308, 'epoch': 0.56}
{'loss': 1.8088, 'grad_norm': 0.07147623598575592, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.771234393119812, 'eval_runtime': 9.9917, 'eval_samples_per_second': 99.983, 'eval_steps_per_second': 6.305, 'epoch': 0.6}
{'loss': 1.8173, 'grad_norm': 0.05368302762508392, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7693625688552856, 'eval_runtime': 9.9728, 'eval_samples_per_second': 100.172, 'eval_steps_per_second': 6.317, 'epoch': 0.64}
{'loss': 1.8365, 'grad_norm': 0.07450719177722931, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7664222717285156, 'eval_runtime': 9.978, 'eval_samples_per_second': 100.121, 'eval_steps_per_second': 6.314, 'epoch': 0.68}
{'loss': 1.8066, 'grad_norm': 0.057172276079654694, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.764443278312683, 'eval_runtime': 9.9695, 'eval_samples_per_second': 100.205, 'eval_steps_per_second': 6.319, 'epoch': 0.72}
{'loss': 1.8031, 'grad_norm': 0.0789378210902214, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7624270915985107, 'eval_runtime': 9.9747, 'eval_samples_per_second': 100.153, 'eval_steps_per_second': 6.316, 'epoch': 0.76}
{'loss': 1.7653, 'grad_norm': 0.09226442873477936, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7609699964523315, 'eval_runtime': 9.9788, 'eval_samples_per_second': 100.112, 'eval_steps_per_second': 6.313, 'epoch': 0.8}
{'loss': 1.7673, 'grad_norm': 0.06707780063152313, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.759474754333496, 'eval_runtime': 9.9799, 'eval_samples_per_second': 100.101, 'eval_steps_per_second': 6.313, 'epoch': 0.84}
{'loss': 1.7852, 'grad_norm': 0.06837653368711472, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7582426071166992, 'eval_runtime': 9.9821, 'eval_samples_per_second': 100.079, 'eval_steps_per_second': 6.311, 'epoch': 0.88}
{'loss': 1.769, 'grad_norm': 0.05394739657640457, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.757590413093567, 'eval_runtime': 9.9802, 'eval_samples_per_second': 100.098, 'eval_steps_per_second': 6.312, 'epoch': 0.92}
{'loss': 1.7612, 'grad_norm': 0.07579664885997772, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7565966844558716, 'eval_runtime': 9.9738, 'eval_samples_per_second': 100.162, 'eval_steps_per_second': 6.317, 'epoch': 0.96}
{'loss': 1.7843, 'grad_norm': 0.07141045480966568, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.7566410303115845, 'eval_runtime': 9.9814, 'eval_samples_per_second': 100.086, 'eval_steps_per_second': 6.312, 'epoch': 1.0}
{'train_runtime': 520.3419, 'train_samples_per_second': 19.216, 'train_steps_per_second': 1.201, 'train_loss': 1.9522129333496094, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4308176040649414, 2.359253406524658, 2.0230329036712646, 1.9276413917541504, 1.8568528890609741, 1.8318445682525635, 1.8183518648147583, 1.8059686422348022, 1.8002090454101562, 1.7930724620819092, 1.7875272035598755, 1.7830406427383423, 1.7785735130310059, 1.774848222732544, 1.771234393119812, 1.7693625688552856, 1.7664222717285156, 1.764443278312683, 1.7624270915985107, 1.7609699964523315, 1.759474754333496, 1.7582426071166992, 1.757590413093567, 1.7565966844558716, 1.7566410303115845], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:19,  5.00it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 45.74it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 66.07it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 75.06it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 81.75it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 87.50it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 94.07it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 102.47it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 103.87it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 105.31it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 108.76it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 116.04it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 126.25it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 124.17it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 129.49it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 128.14it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 127.64it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 129.67it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 132.24it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 139.78it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 139.50it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 139.97it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 150.09it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 117.01it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4104036092758179
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.4935
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4719 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 1.2534901817488717
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.2193321732961788
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 1.2190115237623655
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.2317447219091733
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.0242236662006985
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4483, dtype=torch.float64), 0, 0, 0, tensor(0.5517, dtype=torch.float64), 0, 0, 1, 0, 1, 0, 1, 0, 2, 0.09999999999999962, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(8.6227e-17, dtype=torch.float64), tensor(1.1973e-17, dtype=torch.float64), tensor(0.4483, dtype=torch.float64), tensor(5.5599e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.7312e-17, dtype=torch.float64), tensor(0.5517, dtype=torch.float64), tensor(7.3608e-18, dtype=torch.float64), tensor(3.2855e-18, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.448
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.552
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09999999999999962,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.09999999999999962
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 47,104 || all params: 8,030,308,352 || trainable%: 0.0006
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.40it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 110.78it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.23it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.82it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.16it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.16it/s]Running loglikelihood requests:  28%|‚ñà‚ñà‚ñä       | 114/400 [00:00<00:01, 172.96it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 165.39it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 169.07it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 172.63it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.62it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 185.25it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 191.34it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 200.12it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 213.20it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 219.25it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 236.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 190.20it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.8536, 'grad_norm': 0.23447075486183167, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.913666248321533, 'eval_runtime': 8.8044, 'eval_samples_per_second': 113.467, 'eval_steps_per_second': 7.156, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.85it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 110.81it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.62it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.11it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 139.84it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.20it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 155.96it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 167.76it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 169.94it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.13it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.68it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 186.04it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 191.69it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 199.65it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 212.84it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 222.52it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 239.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 189.71it/s]
Evaluation performance at step 50: 0.5
{'loss': 3.8041, 'grad_norm': 1.0462712049484253, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 3.4971415996551514, 'eval_runtime': 8.8292, 'eval_samples_per_second': 113.147, 'eval_steps_per_second': 7.135, 'epoch': 0.08}
{'loss': 3.2801, 'grad_norm': 0.5437338352203369, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 3.0898053646087646, 'eval_runtime': 8.9317, 'eval_samples_per_second': 111.848, 'eval_steps_per_second': 7.053, 'epoch': 0.12}
{'loss': 2.9592, 'grad_norm': 0.6160930395126343, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.8628768920898438, 'eval_runtime': 8.9569, 'eval_samples_per_second': 111.534, 'eval_steps_per_second': 7.034, 'epoch': 0.16}
{'loss': 2.7674, 'grad_norm': 0.5336401462554932, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.726053237915039, 'eval_runtime': 8.9627, 'eval_samples_per_second': 111.462, 'eval_steps_per_second': 7.029, 'epoch': 0.2}
{'loss': 2.6884, 'grad_norm': 0.5100666284561157, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.6415421962738037, 'eval_runtime': 8.966, 'eval_samples_per_second': 111.421, 'eval_steps_per_second': 7.027, 'epoch': 0.24}
{'loss': 2.6025, 'grad_norm': 0.4513648748397827, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.581173896789551, 'eval_runtime': 8.9687, 'eval_samples_per_second': 111.388, 'eval_steps_per_second': 7.024, 'epoch': 0.28}
{'loss': 2.5486, 'grad_norm': 0.509470522403717, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.5364978313446045, 'eval_runtime': 8.961, 'eval_samples_per_second': 111.484, 'eval_steps_per_second': 7.031, 'epoch': 0.32}
{'loss': 2.5068, 'grad_norm': 0.6200793981552124, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.5025603771209717, 'eval_runtime': 8.9317, 'eval_samples_per_second': 111.849, 'eval_steps_per_second': 7.054, 'epoch': 0.36}
{'loss': 2.5083, 'grad_norm': 0.389812171459198, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.477482318878174, 'eval_runtime': 8.9555, 'eval_samples_per_second': 111.552, 'eval_steps_per_second': 7.035, 'epoch': 0.4}
{'loss': 2.4438, 'grad_norm': 0.5205860137939453, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.4576735496520996, 'eval_runtime': 8.9182, 'eval_samples_per_second': 112.018, 'eval_steps_per_second': 7.064, 'epoch': 0.44}
{'loss': 2.4563, 'grad_norm': 0.39385712146759033, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.4427411556243896, 'eval_runtime': 8.9119, 'eval_samples_per_second': 112.098, 'eval_steps_per_second': 7.069, 'epoch': 0.48}
{'loss': 2.398, 'grad_norm': 0.3996930718421936, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.429649829864502, 'eval_runtime': 8.95, 'eval_samples_per_second': 111.62, 'eval_steps_per_second': 7.039, 'epoch': 0.52}
{'loss': 2.3957, 'grad_norm': 0.37749966979026794, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.41896390914917, 'eval_runtime': 8.9207, 'eval_samples_per_second': 111.987, 'eval_steps_per_second': 7.062, 'epoch': 0.56}
{'loss': 2.4633, 'grad_norm': 0.47743529081344604, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.411442279815674, 'eval_runtime': 8.9214, 'eval_samples_per_second': 111.978, 'eval_steps_per_second': 7.062, 'epoch': 0.6}
{'loss': 2.4124, 'grad_norm': 0.3295978605747223, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.4044978618621826, 'eval_runtime': 8.9324, 'eval_samples_per_second': 111.84, 'eval_steps_per_second': 7.053, 'epoch': 0.64}
{'loss': 2.4222, 'grad_norm': 0.399691641330719, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.398503065109253, 'eval_runtime': 8.9547, 'eval_samples_per_second': 111.562, 'eval_steps_per_second': 7.035, 'epoch': 0.68}
{'loss': 2.387, 'grad_norm': 0.28976723551750183, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.393939733505249, 'eval_runtime': 8.9249, 'eval_samples_per_second': 111.934, 'eval_steps_per_second': 7.059, 'epoch': 0.72}
{'loss': 2.3672, 'grad_norm': 0.34651947021484375, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.3906543254852295, 'eval_runtime': 8.9312, 'eval_samples_per_second': 111.855, 'eval_steps_per_second': 7.054, 'epoch': 0.76}
{'loss': 2.4061, 'grad_norm': 0.5268512964248657, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.387359857559204, 'eval_runtime': 8.9345, 'eval_samples_per_second': 111.813, 'eval_steps_per_second': 7.051, 'epoch': 0.8}
{'loss': 2.3823, 'grad_norm': 0.3189257085323334, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.3848483562469482, 'eval_runtime': 8.922, 'eval_samples_per_second': 111.97, 'eval_steps_per_second': 7.061, 'epoch': 0.84}
{'loss': 2.3662, 'grad_norm': 0.37421682476997375, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.382946252822876, 'eval_runtime': 8.9285, 'eval_samples_per_second': 111.889, 'eval_steps_per_second': 7.056, 'epoch': 0.88}
{'loss': 2.3612, 'grad_norm': 0.42768874764442444, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.381828546524048, 'eval_runtime': 8.9134, 'eval_samples_per_second': 112.078, 'eval_steps_per_second': 7.068, 'epoch': 0.92}
{'loss': 2.3553, 'grad_norm': 0.3534020483493805, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.381023645401001, 'eval_runtime': 8.8823, 'eval_samples_per_second': 112.471, 'eval_steps_per_second': 7.093, 'epoch': 0.96}
{'loss': 2.3418, 'grad_norm': 0.29096683859825134, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.3803465366363525, 'eval_runtime': 8.845, 'eval_samples_per_second': 112.945, 'eval_steps_per_second': 7.123, 'epoch': 1.0}
{'train_runtime': 357.4529, 'train_samples_per_second': 27.973, 'train_steps_per_second': 1.748, 'train_loss': 2.6191064025878905, 'epoch': 1.0}
train_results:  {'eval_loss': [3.913666248321533, 3.4971415996551514, 3.0898053646087646, 2.8628768920898438, 2.726053237915039, 2.6415421962738037, 2.581173896789551, 2.5364978313446045, 2.5025603771209717, 2.477482318878174, 2.4576735496520996, 2.4427411556243896, 2.429649829864502, 2.41896390914917, 2.411442279815674, 2.4044978618621826, 2.398503065109253, 2.393939733505249, 2.3906543254852295, 2.387359857559204, 2.3848483562469482, 2.382946252822876, 2.381828546524048, 2.381023645401001, 2.3803465366363525], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:49,  1.74it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:18, 21.05it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:09, 40.48it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 55.57it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 68.47it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 79.89it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 91.49it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 104.61it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 109.59it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 113.66it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:01, 119.44it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 128.99it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 141.65it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 140.24it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 146.93it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 145.77it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 145.21it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 148.45it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 150.43it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 159.60it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 344/400 [00:03<00:00, 162.38it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 169.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 111.97it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  0.4789681136608124
current iteration best possible performance (full train run):  0.5670000000000001
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3712 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.05570132063994
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 1.134620254921502
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.1484359394403723
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 1.133663773430608
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.8811711643577615
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5481, dtype=torch.float64), 0, 0, 0, tensor(0.4519, dtype=torch.float64), 0, 0, 1, 0, 1, 0, 1, 0, 128, 3.3492523426207303e-18, 1.4800000190734923, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.5633e-17, dtype=torch.float64), tensor(0.5481, dtype=torch.float64), tensor(2.2867e-16, dtype=torch.float64), tensor(1.0513e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4519, dtype=torch.float64), tensor(1.5010e-16, dtype=torch.float64), tensor(1.1016e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(3.3493e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.548
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.452
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.3492523426207303e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734923,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  3.3492523426207303e-18
lora alpha:  1.4800000190734923
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,014,656 || all params: 8,033,275,904 || trainable%: 0.0375
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.55it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 110.92it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.66it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.66it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.80it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.21it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 158.69it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 169.70it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 172.64it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 180.38it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 184.98it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 190.68it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 193.17it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 209.31it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 213.86it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 230.57it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 244.26it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.58it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.0333, 'grad_norm': 0.04536978900432587, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.9925272464752197, 'eval_runtime': 8.7745, 'eval_samples_per_second': 113.853, 'eval_steps_per_second': 7.18, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.11it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 110.98it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.47it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.16it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.28it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.40it/s]Running loglikelihood requests:  28%|‚ñà‚ñà‚ñä       | 111/400 [00:00<00:01, 163.01it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 167.02it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 169.65it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.18it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.85it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 186.92it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 192.73it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 201.09it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 214.10it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 223.73it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 240.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.04it/s]
Evaluation performance at step 50: 0.51
{'loss': 3.7843, 'grad_norm': 0.12529252469539642, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 3.4378695487976074, 'eval_runtime': 8.7893, 'eval_samples_per_second': 113.661, 'eval_steps_per_second': 7.168, 'epoch': 0.08}
{'loss': 3.1584, 'grad_norm': 0.08905159682035446, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.9550673961639404, 'eval_runtime': 8.808, 'eval_samples_per_second': 113.419, 'eval_steps_per_second': 7.153, 'epoch': 0.12}
{'loss': 2.8328, 'grad_norm': 0.0788322389125824, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.7439262866973877, 'eval_runtime': 8.8649, 'eval_samples_per_second': 112.692, 'eval_steps_per_second': 7.107, 'epoch': 0.16}
{'loss': 2.7115, 'grad_norm': 0.06364016979932785, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.6053619384765625, 'eval_runtime': 8.8884, 'eval_samples_per_second': 112.394, 'eval_steps_per_second': 7.088, 'epoch': 0.2}
{'loss': 2.5433, 'grad_norm': 0.06459949910640717, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.506838798522949, 'eval_runtime': 8.915, 'eval_samples_per_second': 112.059, 'eval_steps_per_second': 7.067, 'epoch': 0.24}
{'loss': 2.4863, 'grad_norm': 0.07293883711099625, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.434753656387329, 'eval_runtime': 8.9071, 'eval_samples_per_second': 112.157, 'eval_steps_per_second': 7.073, 'epoch': 0.28}
{'loss': 2.4088, 'grad_norm': 0.06272012740373611, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.3789443969726562, 'eval_runtime': 8.8712, 'eval_samples_per_second': 112.612, 'eval_steps_per_second': 7.102, 'epoch': 0.32}
{'loss': 2.3538, 'grad_norm': 0.07156778126955032, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.3394596576690674, 'eval_runtime': 8.8717, 'eval_samples_per_second': 112.606, 'eval_steps_per_second': 7.101, 'epoch': 0.36}
{'loss': 2.3348, 'grad_norm': 0.06209779158234596, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.3114662170410156, 'eval_runtime': 8.8812, 'eval_samples_per_second': 112.485, 'eval_steps_per_second': 7.094, 'epoch': 0.4}
{'loss': 2.3239, 'grad_norm': 0.06635633856058121, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.2905797958374023, 'eval_runtime': 8.9016, 'eval_samples_per_second': 112.227, 'eval_steps_per_second': 7.077, 'epoch': 0.44}
{'loss': 2.2844, 'grad_norm': 0.08910027891397476, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.2757625579833984, 'eval_runtime': 9.0042, 'eval_samples_per_second': 110.948, 'eval_steps_per_second': 6.997, 'epoch': 0.48}
{'loss': 2.2466, 'grad_norm': 0.06683449447154999, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.2624075412750244, 'eval_runtime': 9.0271, 'eval_samples_per_second': 110.667, 'eval_steps_per_second': 6.979, 'epoch': 0.52}
{'loss': 2.2466, 'grad_norm': 0.1153833270072937, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.2519102096557617, 'eval_runtime': 9.011, 'eval_samples_per_second': 110.864, 'eval_steps_per_second': 6.991, 'epoch': 0.56}
{'loss': 2.2804, 'grad_norm': 0.06549880653619766, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.2429401874542236, 'eval_runtime': 9.0112, 'eval_samples_per_second': 110.863, 'eval_steps_per_second': 6.991, 'epoch': 0.6}
{'loss': 2.2455, 'grad_norm': 0.08097810298204422, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.23695707321167, 'eval_runtime': 8.9963, 'eval_samples_per_second': 111.046, 'eval_steps_per_second': 7.003, 'epoch': 0.64}
{'loss': 2.2516, 'grad_norm': 0.06154154986143112, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.230998992919922, 'eval_runtime': 9.0068, 'eval_samples_per_second': 110.916, 'eval_steps_per_second': 6.995, 'epoch': 0.68}
{'loss': 2.2196, 'grad_norm': 0.08345581591129303, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.225677728652954, 'eval_runtime': 9.008, 'eval_samples_per_second': 110.901, 'eval_steps_per_second': 6.994, 'epoch': 0.72}
{'loss': 2.2338, 'grad_norm': 0.07914330810308456, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.222323417663574, 'eval_runtime': 9.0361, 'eval_samples_per_second': 110.557, 'eval_steps_per_second': 6.972, 'epoch': 0.76}
{'loss': 2.2774, 'grad_norm': 0.08234599232673645, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.218872308731079, 'eval_runtime': 8.9895, 'eval_samples_per_second': 111.13, 'eval_steps_per_second': 7.008, 'epoch': 0.8}
{'loss': 2.2094, 'grad_norm': 0.0696631446480751, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.216238498687744, 'eval_runtime': 8.9407, 'eval_samples_per_second': 111.737, 'eval_steps_per_second': 7.046, 'epoch': 0.84}
{'loss': 2.2154, 'grad_norm': 0.07118851691484451, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.2144386768341064, 'eval_runtime': 8.9292, 'eval_samples_per_second': 111.88, 'eval_steps_per_second': 7.055, 'epoch': 0.88}
{'loss': 2.2138, 'grad_norm': 0.06776473671197891, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.2132463455200195, 'eval_runtime': 8.9482, 'eval_samples_per_second': 111.642, 'eval_steps_per_second': 7.041, 'epoch': 0.92}
{'loss': 2.2134, 'grad_norm': 0.0623086616396904, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.2125308513641357, 'eval_runtime': 8.9619, 'eval_samples_per_second': 111.472, 'eval_steps_per_second': 7.03, 'epoch': 0.96}
{'loss': 2.198, 'grad_norm': 0.07504790276288986, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.212184429168701, 'eval_runtime': 8.9434, 'eval_samples_per_second': 111.702, 'eval_steps_per_second': 7.044, 'epoch': 1.0}
{'train_runtime': 349.9132, 'train_samples_per_second': 28.576, 'train_steps_per_second': 1.786, 'train_loss': 2.4922928955078123, 'epoch': 1.0}
train_results:  {'eval_loss': [3.9925272464752197, 3.4378695487976074, 2.9550673961639404, 2.7439262866973877, 2.6053619384765625, 2.506838798522949, 2.434753656387329, 2.3789443969726562, 2.3394596576690674, 2.3114662170410156, 2.2905797958374023, 2.2757625579833984, 2.2624075412750244, 2.2519102096557617, 2.2429401874542236, 2.23695707321167, 2.230998992919922, 2.225677728652954, 2.222323417663574, 2.218872308731079, 2.216238498687744, 2.2144386768341064, 2.2132463455200195, 2.2125308513641357, 2.212184429168701], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:29,  2.67it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:09, 39.51it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 63.29it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 76.39it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 85.96it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 93.94it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 102.43it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 113.09it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 115.69it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 118.03it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 122.74it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 131.21it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 143.18it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 141.01it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 147.11it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 145.47it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 144.76it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 147.39it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 149.56it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 158.46it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:02<00:00, 158.50it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 374/400 [00:03<00:00, 201.51it/s]Running loglikelihood requests:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 395/400 [00:03<00:00, 186.30it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 126.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.2944432497024536
current iteration best possible performance (full train run):  0.546
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2026 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.1320072038675204
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 1.1741887320879216
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.8220157705362853
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 1.0762859649638707
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.13911208650277
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4368, dtype=torch.float64), 0, 0, 0, tensor(0.5632, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 2, 0.0, 1.4800000190734899, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.7678e-17, dtype=torch.float64), tensor(0.4368, dtype=torch.float64), tensor(1.6695e-16, dtype=torch.float64), tensor(3.5181e-17, dtype=torch.float64), tensor(5.8818e-18, dtype=torch.float64), tensor(0.5632, dtype=torch.float64), tensor(1.7286e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.437
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.563
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734899,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  1.4800000190734899
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,507,328 || all params: 8,031,768,576 || trainable%: 0.0188
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 61.58it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 99.78it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 112.20it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 120.74it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 125.94it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 139.11it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 140.60it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 150.96it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 151.00it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 152.80it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 154.77it/s]Running loglikelihood requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 195/400 [00:01<00:01, 165.02it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 165.62it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 170.36it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 172.00it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 186.46it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 189.66it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 204.17it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 212.99it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 169.49it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.8278, 'grad_norm': 1.292918086051941, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3505210876464844, 'eval_runtime': 9.6997, 'eval_samples_per_second': 102.993, 'eval_steps_per_second': 6.495, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 61.24it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 99.46it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 111.92it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 120.48it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 125.93it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 139.42it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 140.84it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 150.58it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 150.86it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 152.75it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 177/400 [00:01<00:01, 157.69it/s]Running loglikelihood requests:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 198/400 [00:01<00:01, 172.95it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 162.84it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 168.44it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 170.60it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 185.35it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 188.83it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 203.33it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 213.30it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 169.48it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.8299, 'grad_norm': 1.4094038009643555, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 2.3815579414367676, 'eval_runtime': 9.6745, 'eval_samples_per_second': 103.261, 'eval_steps_per_second': 6.512, 'epoch': 0.08}
{'loss': 2.2665, 'grad_norm': 0.46600890159606934, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1087403297424316, 'eval_runtime': 9.6484, 'eval_samples_per_second': 103.541, 'eval_steps_per_second': 6.53, 'epoch': 0.12}
{'loss': 2.0364, 'grad_norm': 0.6291118860244751, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.0014967918395996, 'eval_runtime': 9.6672, 'eval_samples_per_second': 103.339, 'eval_steps_per_second': 6.517, 'epoch': 0.16}
{'loss': 2.0316, 'grad_norm': 0.5735255479812622, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9404932260513306, 'eval_runtime': 9.6789, 'eval_samples_per_second': 103.214, 'eval_steps_per_second': 6.509, 'epoch': 0.2}
{'loss': 1.9743, 'grad_norm': 0.7017362713813782, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9152988195419312, 'eval_runtime': 9.6955, 'eval_samples_per_second': 103.037, 'eval_steps_per_second': 6.498, 'epoch': 0.24}
{'loss': 1.9328, 'grad_norm': 0.6148040294647217, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9016213417053223, 'eval_runtime': 9.709, 'eval_samples_per_second': 102.894, 'eval_steps_per_second': 6.489, 'epoch': 0.28}
{'loss': 1.9189, 'grad_norm': 0.7659466862678528, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.893925428390503, 'eval_runtime': 9.7016, 'eval_samples_per_second': 102.972, 'eval_steps_per_second': 6.494, 'epoch': 0.32}
{'loss': 1.9468, 'grad_norm': 0.65730220079422, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8873590230941772, 'eval_runtime': 9.6801, 'eval_samples_per_second': 103.201, 'eval_steps_per_second': 6.508, 'epoch': 0.36}
{'loss': 1.93, 'grad_norm': 0.4662858545780182, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8799679279327393, 'eval_runtime': 9.6879, 'eval_samples_per_second': 103.118, 'eval_steps_per_second': 6.503, 'epoch': 0.4}
{'loss': 1.8949, 'grad_norm': 0.5457552671432495, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.874761939048767, 'eval_runtime': 9.6887, 'eval_samples_per_second': 103.11, 'eval_steps_per_second': 6.502, 'epoch': 0.44}
{'loss': 1.8987, 'grad_norm': 0.48201748728752136, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8697072267532349, 'eval_runtime': 9.6953, 'eval_samples_per_second': 103.04, 'eval_steps_per_second': 6.498, 'epoch': 0.48}
{'loss': 1.9044, 'grad_norm': 0.4936510920524597, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8671938180923462, 'eval_runtime': 9.6904, 'eval_samples_per_second': 103.092, 'eval_steps_per_second': 6.501, 'epoch': 0.52}
{'loss': 1.8747, 'grad_norm': 0.46265944838523865, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8640968799591064, 'eval_runtime': 9.7131, 'eval_samples_per_second': 102.851, 'eval_steps_per_second': 6.486, 'epoch': 0.56}
{'loss': 1.8584, 'grad_norm': 0.46809837222099304, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8611282110214233, 'eval_runtime': 9.7706, 'eval_samples_per_second': 102.246, 'eval_steps_per_second': 6.448, 'epoch': 0.6}
{'loss': 1.8696, 'grad_norm': 0.6382030844688416, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.858451008796692, 'eval_runtime': 9.8311, 'eval_samples_per_second': 101.616, 'eval_steps_per_second': 6.408, 'epoch': 0.64}
{'loss': 1.8727, 'grad_norm': 0.6351025700569153, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8573859930038452, 'eval_runtime': 9.8187, 'eval_samples_per_second': 101.745, 'eval_steps_per_second': 6.416, 'epoch': 0.68}
{'loss': 1.8662, 'grad_norm': 0.5240291357040405, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8548362255096436, 'eval_runtime': 9.8136, 'eval_samples_per_second': 101.798, 'eval_steps_per_second': 6.42, 'epoch': 0.72}
{'loss': 1.8724, 'grad_norm': 0.5862846374511719, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.852201223373413, 'eval_runtime': 9.8046, 'eval_samples_per_second': 101.891, 'eval_steps_per_second': 6.426, 'epoch': 0.76}
{'loss': 1.8226, 'grad_norm': 0.6339646577835083, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8511308431625366, 'eval_runtime': 9.7915, 'eval_samples_per_second': 102.028, 'eval_steps_per_second': 6.434, 'epoch': 0.8}
{'loss': 1.8596, 'grad_norm': 0.5090514421463013, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8498769998550415, 'eval_runtime': 9.8071, 'eval_samples_per_second': 101.865, 'eval_steps_per_second': 6.424, 'epoch': 0.84}
{'loss': 1.9106, 'grad_norm': 0.4690591096878052, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8490039110183716, 'eval_runtime': 9.8303, 'eval_samples_per_second': 101.624, 'eval_steps_per_second': 6.409, 'epoch': 0.88}
{'loss': 1.8602, 'grad_norm': 0.47222191095352173, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8478913307189941, 'eval_runtime': 9.8468, 'eval_samples_per_second': 101.454, 'eval_steps_per_second': 6.398, 'epoch': 0.92}
{'loss': 1.8387, 'grad_norm': 0.6625919938087463, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8473095893859863, 'eval_runtime': 9.8433, 'eval_samples_per_second': 101.491, 'eval_steps_per_second': 6.4, 'epoch': 0.96}
{'loss': 1.8766, 'grad_norm': 0.6145520210266113, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8472161293029785, 'eval_runtime': 9.8332, 'eval_samples_per_second': 101.595, 'eval_steps_per_second': 6.407, 'epoch': 1.0}
{'train_runtime': 497.2052, 'train_samples_per_second': 20.11, 'train_steps_per_second': 1.257, 'train_loss': 2.031012225341797, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3505210876464844, 2.3815579414367676, 2.1087403297424316, 2.0014967918395996, 1.9404932260513306, 1.9152988195419312, 1.9016213417053223, 1.893925428390503, 1.8873590230941772, 1.8799679279327393, 1.874761939048767, 1.8697072267532349, 1.8671938180923462, 1.8640968799591064, 1.8611282110214233, 1.858451008796692, 1.8573859930038452, 1.8548362255096436, 1.852201223373413, 1.8511308431625366, 1.8498769998550415, 1.8490039110183716, 1.8478913307189941, 1.8473095893859863, 1.8472161293029785], 'performance': [0.49, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:17,  5.16it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 52.94it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 72.27it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 80.10it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 85.95it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 90.84it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 97.15it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 105.97it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 107.07it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 108.34it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 111.86it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 119.13it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 129.80it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 127.27it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 132.41it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 130.97it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 130.30it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 132.32it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 134.88it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 142.91it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 142.03it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 142.77it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 153.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 121.12it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  0.609687864780426
current iteration best possible performance (full train run):  0.504
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9365 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.0504287947380442
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 1.0746020628267863
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.0414865181591773
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.9758707129110332
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.0898728006835623
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4319, dtype=torch.float64), 0, tensor(0.5274, dtype=torch.float64), 0, tensor(0.0406, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 1, 0, 128, 6.93889390390408e-19, 1.480000019073492, 0]
normalized proposed parameters for next round by BO: [tensor(4.6412e-17, dtype=torch.float64), tensor(2.6140e-16, dtype=torch.float64), tensor(0.4319, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5274, dtype=torch.float64), tensor(8.1452e-17, dtype=torch.float64), tensor(0.0406, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.432
  sciq: 0
  triviaqa: 0.527
  truthfulqa_gen: 0
  wikitext: 0.041
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (6.93889390390408e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (1.480000019073492,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  6.93889390390408e-19
lora alpha:  1.480000019073492
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 63.42it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 102.36it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 115.37it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 124.53it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 130.15it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 143.95it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 145.34it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 156.21it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 156.26it/s]Running loglikelihood requests:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 165/400 [00:01<00:01, 171.93it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 156.95it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 168.72it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 171.42it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 176.81it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 184.47it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 196.05it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 204.37it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 219.54it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 175.37it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3704, 'grad_norm': 0.3077560365200043, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.7805137634277344, 'eval_runtime': 9.515, 'eval_samples_per_second': 104.992, 'eval_steps_per_second': 6.621, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 63.25it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 102.53it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 115.54it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 124.45it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 130.01it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 143.53it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 145.05it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 155.72it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 155.92it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 158.43it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 165.90it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 170.48it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 175.40it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 177.38it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 192.07it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 195.35it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 210.47it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 220.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 175.36it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.9264, 'grad_norm': 0.24645571410655975, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.3474338054656982, 'eval_runtime': 9.4582, 'eval_samples_per_second': 105.623, 'eval_steps_per_second': 6.661, 'epoch': 0.08}
{'loss': 1.9997, 'grad_norm': 0.0986805334687233, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8661917448043823, 'eval_runtime': 9.5207, 'eval_samples_per_second': 104.93, 'eval_steps_per_second': 6.617, 'epoch': 0.12}
{'loss': 1.7762, 'grad_norm': 0.09649423509836197, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7399183511734009, 'eval_runtime': 9.5565, 'eval_samples_per_second': 104.536, 'eval_steps_per_second': 6.592, 'epoch': 0.16}
{'loss': 1.7041, 'grad_norm': 0.06038503721356392, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6911381483078003, 'eval_runtime': 9.5781, 'eval_samples_per_second': 104.301, 'eval_steps_per_second': 6.578, 'epoch': 0.2}
{'loss': 1.6955, 'grad_norm': 0.061757370829582214, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6670540571212769, 'eval_runtime': 9.558, 'eval_samples_per_second': 104.519, 'eval_steps_per_second': 6.591, 'epoch': 0.24}
{'loss': 1.6513, 'grad_norm': 0.055969543755054474, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.653252124786377, 'eval_runtime': 9.5361, 'eval_samples_per_second': 104.76, 'eval_steps_per_second': 6.606, 'epoch': 0.28}
{'loss': 1.6484, 'grad_norm': 0.07237762957811356, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6414573192596436, 'eval_runtime': 9.5892, 'eval_samples_per_second': 104.18, 'eval_steps_per_second': 6.57, 'epoch': 0.32}
{'loss': 1.6764, 'grad_norm': 0.05240052938461304, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6339088678359985, 'eval_runtime': 9.5758, 'eval_samples_per_second': 104.326, 'eval_steps_per_second': 6.579, 'epoch': 0.36}
{'loss': 1.6264, 'grad_norm': 0.06135120615363121, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.627246618270874, 'eval_runtime': 9.5785, 'eval_samples_per_second': 104.296, 'eval_steps_per_second': 6.577, 'epoch': 0.4}
{'loss': 1.6589, 'grad_norm': 0.05547728389501572, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.623624324798584, 'eval_runtime': 9.5714, 'eval_samples_per_second': 104.374, 'eval_steps_per_second': 6.582, 'epoch': 0.44}
{'loss': 1.6181, 'grad_norm': 0.05465035140514374, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6175559759140015, 'eval_runtime': 9.5539, 'eval_samples_per_second': 104.565, 'eval_steps_per_second': 6.594, 'epoch': 0.48}
{'loss': 1.6087, 'grad_norm': 0.0635196790099144, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6134103536605835, 'eval_runtime': 9.555, 'eval_samples_per_second': 104.553, 'eval_steps_per_second': 6.593, 'epoch': 0.52}
{'loss': 1.6116, 'grad_norm': 0.06675484031438828, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6091657876968384, 'eval_runtime': 9.5584, 'eval_samples_per_second': 104.516, 'eval_steps_per_second': 6.591, 'epoch': 0.56}
{'loss': 1.6295, 'grad_norm': 0.06662451475858688, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6072874069213867, 'eval_runtime': 9.5501, 'eval_samples_per_second': 104.606, 'eval_steps_per_second': 6.597, 'epoch': 0.6}
{'loss': 1.6433, 'grad_norm': 0.05328962579369545, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6041648387908936, 'eval_runtime': 9.5627, 'eval_samples_per_second': 104.468, 'eval_steps_per_second': 6.588, 'epoch': 0.64}
{'loss': 1.6, 'grad_norm': 0.06636517494916916, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6016011238098145, 'eval_runtime': 9.5603, 'eval_samples_per_second': 104.495, 'eval_steps_per_second': 6.59, 'epoch': 0.68}
{'loss': 1.6353, 'grad_norm': 0.06365875899791718, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5990121364593506, 'eval_runtime': 9.5524, 'eval_samples_per_second': 104.582, 'eval_steps_per_second': 6.595, 'epoch': 0.72}
{'loss': 1.644, 'grad_norm': 0.06830127537250519, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5965476036071777, 'eval_runtime': 9.5528, 'eval_samples_per_second': 104.577, 'eval_steps_per_second': 6.595, 'epoch': 0.76}
{'loss': 1.5871, 'grad_norm': 0.07042444497346878, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5950605869293213, 'eval_runtime': 9.5031, 'eval_samples_per_second': 105.124, 'eval_steps_per_second': 6.629, 'epoch': 0.8}
{'loss': 1.6131, 'grad_norm': 0.0641452819108963, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5939059257507324, 'eval_runtime': 9.4958, 'eval_samples_per_second': 105.204, 'eval_steps_per_second': 6.635, 'epoch': 0.84}
{'loss': 1.5706, 'grad_norm': 0.06616801768541336, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5938624143600464, 'eval_runtime': 9.5011, 'eval_samples_per_second': 105.146, 'eval_steps_per_second': 6.631, 'epoch': 0.88}
{'loss': 1.5771, 'grad_norm': 0.061428576707839966, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5931531190872192, 'eval_runtime': 9.5002, 'eval_samples_per_second': 105.155, 'eval_steps_per_second': 6.631, 'epoch': 0.92}
{'loss': 1.6115, 'grad_norm': 0.06610238552093506, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5920108556747437, 'eval_runtime': 9.5029, 'eval_samples_per_second': 105.126, 'eval_steps_per_second': 6.63, 'epoch': 0.96}
{'loss': 1.6421, 'grad_norm': 0.07606370002031326, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5914608240127563, 'eval_runtime': 9.5007, 'eval_samples_per_second': 105.15, 'eval_steps_per_second': 6.631, 'epoch': 1.0}
{'train_runtime': 480.2186, 'train_samples_per_second': 20.822, 'train_steps_per_second': 1.301, 'train_loss': 1.813034002685547, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7805137634277344, 2.3474338054656982, 1.8661917448043823, 1.7399183511734009, 1.6911381483078003, 1.6670540571212769, 1.653252124786377, 1.6414573192596436, 1.6339088678359985, 1.627246618270874, 1.623624324798584, 1.6175559759140015, 1.6134103536605835, 1.6091657876968384, 1.6072874069213867, 1.6041648387908936, 1.6016011238098145, 1.5990121364593506, 1.5965476036071777, 1.5950605869293213, 1.5939059257507324, 1.5938624143600464, 1.5931531190872192, 1.5920108556747437, 1.5914608240127563], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:14,  5.36it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:06, 54.80it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:04, 75.16it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 83.51it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 89.26it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 94.33it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 100.89it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 109.67it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 110.85it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 112.45it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 116.11it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 122.89it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:01<00:01, 134.29it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 132.31it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 137.97it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 136.58it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 135.81it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 138.22it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 141.09it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 149.35it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:02<00:00, 148.54it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 149.42it/s]Running loglikelihood requests:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 381/400 [00:03<00:00, 168.87it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 126.07it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4146111011505127
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6640 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.0514762873453263
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.8645966060359213
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.0139768089041707
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.8500012356811755
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.6667506936788072
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5783, dtype=torch.float64), 0, tensor(0.4217, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 128, 4.917570893892482e-18, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(0.5783, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4217, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2048e-16, dtype=torch.float64), tensor(1.9546e-16, dtype=torch.float64), tensor(1.4299e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0449e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(4.9176e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.578
  gsm8k: 0
  rowan_hellaswag: 0.422
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.917570893892482e-18,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  4.917570893892482e-18
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,014,656 || all params: 8,033,275,904 || trainable%: 0.0375
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.10it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.23it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.87it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.44it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.57it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.79it/s]Running loglikelihood requests:  28%|‚ñà‚ñà‚ñä       | 114/400 [00:00<00:01, 172.67it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 164.94it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 168.07it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 171.92it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.23it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 185.81it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 191.71it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 200.65it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 214.14it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 223.98it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 240.98it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.23it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.4841, 'grad_norm': 0.06616929918527603, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 4.441555023193359, 'eval_runtime': 8.8491, 'eval_samples_per_second': 112.893, 'eval_steps_per_second': 7.119, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.26it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 110.56it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.00it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 133.81it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 140.24it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 155.25it/s]Running loglikelihood requests:  28%|‚ñà‚ñà‚ñä       | 110/400 [00:00<00:01, 159.79it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 167.64it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 170.05it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.23it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 183.84it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 186.49it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 192.02it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 200.02it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 212.83it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 222.71it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 239.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 190.41it/s]
Evaluation performance at step 50: 0.51
{'loss': 4.0927, 'grad_norm': 0.17640438675880432, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 3.487419366836548, 'eval_runtime': 8.9081, 'eval_samples_per_second': 112.146, 'eval_steps_per_second': 7.072, 'epoch': 0.08}
{'loss': 3.1199, 'grad_norm': 0.1297653317451477, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.830260992050171, 'eval_runtime': 8.9385, 'eval_samples_per_second': 111.764, 'eval_steps_per_second': 7.048, 'epoch': 0.12}
{'loss': 2.6589, 'grad_norm': 0.1263004094362259, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.5143556594848633, 'eval_runtime': 8.9822, 'eval_samples_per_second': 111.22, 'eval_steps_per_second': 7.014, 'epoch': 0.16}
{'loss': 2.4253, 'grad_norm': 0.08921798318624496, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.340283155441284, 'eval_runtime': 8.9996, 'eval_samples_per_second': 111.004, 'eval_steps_per_second': 7.0, 'epoch': 0.2}
{'loss': 2.2772, 'grad_norm': 0.07189640402793884, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.2201991081237793, 'eval_runtime': 9.0146, 'eval_samples_per_second': 110.82, 'eval_steps_per_second': 6.989, 'epoch': 0.24}
{'loss': 2.204, 'grad_norm': 0.08252346515655518, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1366584300994873, 'eval_runtime': 9.0267, 'eval_samples_per_second': 110.672, 'eval_steps_per_second': 6.979, 'epoch': 0.28}
{'loss': 2.1415, 'grad_norm': 0.06155054643750191, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.072462320327759, 'eval_runtime': 9.0494, 'eval_samples_per_second': 110.394, 'eval_steps_per_second': 6.962, 'epoch': 0.32}
{'loss': 2.007, 'grad_norm': 0.07728933542966843, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0202627182006836, 'eval_runtime': 9.0364, 'eval_samples_per_second': 110.553, 'eval_steps_per_second': 6.972, 'epoch': 0.36}
{'loss': 1.9992, 'grad_norm': 0.061075083911418915, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.978759765625, 'eval_runtime': 9.0394, 'eval_samples_per_second': 110.516, 'eval_steps_per_second': 6.969, 'epoch': 0.4}
{'loss': 1.968, 'grad_norm': 0.07707275450229645, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.945582628250122, 'eval_runtime': 9.0288, 'eval_samples_per_second': 110.646, 'eval_steps_per_second': 6.978, 'epoch': 0.44}
{'loss': 1.9495, 'grad_norm': 0.06429802626371384, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9181489944458008, 'eval_runtime': 9.0462, 'eval_samples_per_second': 110.433, 'eval_steps_per_second': 6.964, 'epoch': 0.48}
{'loss': 1.8466, 'grad_norm': 0.06428336352109909, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8953663110733032, 'eval_runtime': 9.0519, 'eval_samples_per_second': 110.363, 'eval_steps_per_second': 6.96, 'epoch': 0.52}
{'loss': 1.8728, 'grad_norm': 0.0701005607843399, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.876773715019226, 'eval_runtime': 9.0292, 'eval_samples_per_second': 110.642, 'eval_steps_per_second': 6.977, 'epoch': 0.56}
{'loss': 1.8953, 'grad_norm': 0.07189369946718216, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.861640214920044, 'eval_runtime': 9.0277, 'eval_samples_per_second': 110.659, 'eval_steps_per_second': 6.979, 'epoch': 0.6}
{'loss': 1.8466, 'grad_norm': 0.06581198424100876, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8489958047866821, 'eval_runtime': 9.02, 'eval_samples_per_second': 110.754, 'eval_steps_per_second': 6.984, 'epoch': 0.64}
{'loss': 1.8493, 'grad_norm': 0.06579314172267914, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.838711142539978, 'eval_runtime': 8.9969, 'eval_samples_per_second': 111.038, 'eval_steps_per_second': 7.002, 'epoch': 0.68}
{'loss': 1.796, 'grad_norm': 0.06018645688891411, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.830195426940918, 'eval_runtime': 8.9831, 'eval_samples_per_second': 111.209, 'eval_steps_per_second': 7.013, 'epoch': 0.72}
{'loss': 1.8034, 'grad_norm': 0.0674145296216011, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8232600688934326, 'eval_runtime': 8.9762, 'eval_samples_per_second': 111.294, 'eval_steps_per_second': 7.019, 'epoch': 0.76}
{'loss': 1.8064, 'grad_norm': 0.07282348722219467, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8176648616790771, 'eval_runtime': 8.9457, 'eval_samples_per_second': 111.674, 'eval_steps_per_second': 7.043, 'epoch': 0.8}
{'loss': 1.8051, 'grad_norm': 0.057913780212402344, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8131039142608643, 'eval_runtime': 8.9416, 'eval_samples_per_second': 111.726, 'eval_steps_per_second': 7.046, 'epoch': 0.84}
{'loss': 1.819, 'grad_norm': 0.06794779002666473, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8097467422485352, 'eval_runtime': 8.9405, 'eval_samples_per_second': 111.739, 'eval_steps_per_second': 7.047, 'epoch': 0.88}
{'loss': 1.8037, 'grad_norm': 0.06302817910909653, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8073229789733887, 'eval_runtime': 8.9423, 'eval_samples_per_second': 111.716, 'eval_steps_per_second': 7.045, 'epoch': 0.92}
{'loss': 1.7646, 'grad_norm': 0.06604842841625214, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8060213327407837, 'eval_runtime': 8.9221, 'eval_samples_per_second': 111.97, 'eval_steps_per_second': 7.061, 'epoch': 0.96}
{'loss': 1.7692, 'grad_norm': 0.0649733766913414, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8053654432296753, 'eval_runtime': 8.9355, 'eval_samples_per_second': 111.801, 'eval_steps_per_second': 7.051, 'epoch': 1.0}
{'train_runtime': 357.1758, 'train_samples_per_second': 27.995, 'train_steps_per_second': 1.75, 'train_loss': 2.2001995361328124, 'epoch': 1.0}
train_results:  {'eval_loss': [4.441555023193359, 3.487419366836548, 2.830260992050171, 2.5143556594848633, 2.340283155441284, 2.2201991081237793, 2.1366584300994873, 2.072462320327759, 2.0202627182006836, 1.978759765625, 1.945582628250122, 1.9181489944458008, 1.8953663110733032, 1.876773715019226, 1.861640214920044, 1.8489958047866821, 1.838711142539978, 1.830195426940918, 1.8232600688934326, 1.8176648616790771, 1.8131039142608643, 1.8097467422485352, 1.8073229789733887, 1.8060213327407837, 1.8053654432296753], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:41,  3.92it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:07, 49.14it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 72.52it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 83.44it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 91.58it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 98.71it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 106.61it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 116.82it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 118.27it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 120.22it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 123.99it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 132.45it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:01<00:01, 144.52it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 141.86it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 148.46it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 147.11it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 146.28it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 149.09it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 151.61it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 160.44it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 344/400 [00:02<00:00, 162.83it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 374/400 [00:02<00:00, 202.05it/s]Running loglikelihood requests:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 395/400 [00:03<00:00, 186.61it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 132.04it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.2918784618377686
current iteration best possible performance (full train run):  0.5670000000000001
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5213 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 1.1098799490755917
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 1.0417947794738496
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.039874668942592
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.0024319204502803
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.0042569283847647
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5240, dtype=torch.float64), tensor(0.4410, dtype=torch.float64), 0, 0, 0, tensor(0.0350, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.7755575615628915e-18, 1.4800000190734908, 0]
normalized proposed parameters for next round by BO: [tensor(1.3773e-16, dtype=torch.float64), tensor(0.5240, dtype=torch.float64), tensor(0.4410, dtype=torch.float64), tensor(2.9642e-17, dtype=torch.float64), tensor(8.3125e-17, dtype=torch.float64), tensor(7.9081e-17, dtype=torch.float64), tensor(0.0350, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.7756e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.524
  rowan_hellaswag: 0.441
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.035
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.7755575615628915e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734908,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.7755575615628915e-18
lora alpha:  1.4800000190734908
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.01it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.65it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.60it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.38it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.15it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.95it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.10it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.24it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.30it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.73it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.25it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.70it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.80it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.51it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.76it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.23it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.25it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.66it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.90it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 180.83it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.72it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.2153, 'grad_norm': 0.13572373986244202, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 2.550455331802368, 'eval_runtime': 11.0805, 'eval_samples_per_second': 90.158, 'eval_steps_per_second': 5.686, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.84it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.49it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.19it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.41it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 109.90it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.59it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.75it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.91it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.71it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.33it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.05it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.61it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.86it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.75it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.10it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.73it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.76it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.13it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.56it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.68it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.43it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.80it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.0809, 'grad_norm': 0.12660561501979828, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 1.6876178979873657, 'eval_runtime': 11.0868, 'eval_samples_per_second': 90.107, 'eval_steps_per_second': 5.682, 'epoch': 0.08}
{'loss': 1.5165, 'grad_norm': 0.051298726350069046, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.444799780845642, 'eval_runtime': 11.1216, 'eval_samples_per_second': 89.825, 'eval_steps_per_second': 5.665, 'epoch': 0.12}
{'loss': 1.4321, 'grad_norm': 0.03094172291457653, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3661259412765503, 'eval_runtime': 11.159, 'eval_samples_per_second': 89.524, 'eval_steps_per_second': 5.646, 'epoch': 0.16}
{'loss': 1.4087, 'grad_norm': 0.03432314470410347, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3467339277267456, 'eval_runtime': 11.1606, 'eval_samples_per_second': 89.511, 'eval_steps_per_second': 5.645, 'epoch': 0.2}
{'loss': 1.367, 'grad_norm': 0.035783715546131134, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3351877927780151, 'eval_runtime': 11.1624, 'eval_samples_per_second': 89.497, 'eval_steps_per_second': 5.644, 'epoch': 0.24}
{'loss': 1.3187, 'grad_norm': 0.041815899312496185, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3262594938278198, 'eval_runtime': 11.1008, 'eval_samples_per_second': 89.993, 'eval_steps_per_second': 5.675, 'epoch': 0.28}
{'loss': 1.3224, 'grad_norm': 0.03216731920838356, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.317162036895752, 'eval_runtime': 11.0926, 'eval_samples_per_second': 90.06, 'eval_steps_per_second': 5.679, 'epoch': 0.32}
{'loss': 1.3188, 'grad_norm': 0.030604131519794464, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.311627984046936, 'eval_runtime': 11.2265, 'eval_samples_per_second': 88.986, 'eval_steps_per_second': 5.612, 'epoch': 0.36}
{'loss': 1.3604, 'grad_norm': 0.031479064375162125, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3044971227645874, 'eval_runtime': 11.2355, 'eval_samples_per_second': 88.915, 'eval_steps_per_second': 5.607, 'epoch': 0.4}
{'loss': 1.2998, 'grad_norm': 0.030789442360401154, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2983055114746094, 'eval_runtime': 11.2551, 'eval_samples_per_second': 88.76, 'eval_steps_per_second': 5.597, 'epoch': 0.44}
{'loss': 1.3166, 'grad_norm': 0.030425406992435455, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2946456670761108, 'eval_runtime': 11.2458, 'eval_samples_per_second': 88.833, 'eval_steps_per_second': 5.602, 'epoch': 0.48}
{'loss': 1.2727, 'grad_norm': 0.034223806113004684, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2910295724868774, 'eval_runtime': 11.244, 'eval_samples_per_second': 88.847, 'eval_steps_per_second': 5.603, 'epoch': 0.52}
{'loss': 1.2835, 'grad_norm': 0.03411872312426567, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2869627475738525, 'eval_runtime': 11.2467, 'eval_samples_per_second': 88.826, 'eval_steps_per_second': 5.602, 'epoch': 0.56}
{'loss': 1.3332, 'grad_norm': 0.03649905323982239, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2835458517074585, 'eval_runtime': 11.2431, 'eval_samples_per_second': 88.855, 'eval_steps_per_second': 5.603, 'epoch': 0.6}
{'loss': 1.2583, 'grad_norm': 0.03819737955927849, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2818739414215088, 'eval_runtime': 11.2334, 'eval_samples_per_second': 88.931, 'eval_steps_per_second': 5.608, 'epoch': 0.64}
{'loss': 1.3103, 'grad_norm': 0.03373166173696518, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.276493787765503, 'eval_runtime': 11.188, 'eval_samples_per_second': 89.292, 'eval_steps_per_second': 5.631, 'epoch': 0.68}
{'loss': 1.2997, 'grad_norm': 0.03759166598320007, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2737531661987305, 'eval_runtime': 11.1766, 'eval_samples_per_second': 89.383, 'eval_steps_per_second': 5.637, 'epoch': 0.72}
{'loss': 1.2901, 'grad_norm': 0.03694826364517212, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2713841199874878, 'eval_runtime': 11.1696, 'eval_samples_per_second': 89.439, 'eval_steps_per_second': 5.64, 'epoch': 0.76}
{'loss': 1.2628, 'grad_norm': 0.03323139622807503, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2696690559387207, 'eval_runtime': 11.1699, 'eval_samples_per_second': 89.437, 'eval_steps_per_second': 5.64, 'epoch': 0.8}
{'loss': 1.2543, 'grad_norm': 0.03459535911679268, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.267203688621521, 'eval_runtime': 11.1604, 'eval_samples_per_second': 89.513, 'eval_steps_per_second': 5.645, 'epoch': 0.84}
{'loss': 1.2681, 'grad_norm': 0.03629826381802559, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2658034563064575, 'eval_runtime': 11.0958, 'eval_samples_per_second': 90.034, 'eval_steps_per_second': 5.678, 'epoch': 0.88}
{'loss': 1.3003, 'grad_norm': 0.03708310052752495, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2643705606460571, 'eval_runtime': 11.0918, 'eval_samples_per_second': 90.067, 'eval_steps_per_second': 5.68, 'epoch': 0.92}
{'loss': 1.2326, 'grad_norm': 0.04712265729904175, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.263717770576477, 'eval_runtime': 11.0949, 'eval_samples_per_second': 90.041, 'eval_steps_per_second': 5.678, 'epoch': 0.96}
{'loss': 1.2825, 'grad_norm': 0.03938804939389229, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2635517120361328, 'eval_runtime': 11.106, 'eval_samples_per_second': 89.951, 'eval_steps_per_second': 5.673, 'epoch': 1.0}
{'train_runtime': 567.9974, 'train_samples_per_second': 17.604, 'train_steps_per_second': 1.1, 'train_loss': 1.4242192413330077, 'epoch': 1.0}
train_results:  {'eval_loss': [2.550455331802368, 1.6876178979873657, 1.444799780845642, 1.3661259412765503, 1.3467339277267456, 1.3351877927780151, 1.3262594938278198, 1.317162036895752, 1.311627984046936, 1.3044971227645874, 1.2983055114746094, 1.2946456670761108, 1.2910295724868774, 1.2869627475738525, 1.2835458517074585, 1.2818739414215088, 1.276493787765503, 1.2737531661987305, 1.2713841199874878, 1.2696690559387207, 1.267203688621521, 1.2658034563064575, 1.2643705606460571, 1.263717770576477, 1.2635517120361328], 'performance': [0.48, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:35,  1.85it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:14, 27.16it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 45.77it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 57.15it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 65.99it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 73.26it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 80.97it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 90.08it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 92.40it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 94.60it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.42it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.41it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.03it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.35it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.27it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 117.39it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.86it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 119.20it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.43it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.27it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.29it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.89it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.15it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4244532585144043
current iteration best possible performance (full train run):  0.462
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0786 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.9901946676496548
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9490723861617091
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.2232471089815053
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.8688992373931362
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.8057587079257944
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4369, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5631, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.8679e-16, dtype=torch.float64), tensor(0.4369, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.1238e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.8816e-17, dtype=torch.float64), tensor(0.5631, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.437
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.563
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.93it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.69it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.56it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.98it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.64it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.51it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.66it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.92it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.94it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.31it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.79it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.16it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.20it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.04it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.35it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.83it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.82it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.22it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.61it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.77it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.49it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.29it/s]
Evaluation performance at step 25: 0.5
{'loss': 3.7296, 'grad_norm': 0.19113388657569885, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 2.906388759613037, 'eval_runtime': 11.0359, 'eval_samples_per_second': 90.522, 'eval_steps_per_second': 5.709, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.85it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.35it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.13it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.51it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.15it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.90it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.86it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.65it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.55it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 132.52it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 133.78it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 140.29it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 143.58it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 144.72it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.29it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.06it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.25it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.70it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.15it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.27it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.77it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.33it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.3579, 'grad_norm': 0.14679986238479614, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.9115110635757446, 'eval_runtime': 11.0528, 'eval_samples_per_second': 90.384, 'eval_steps_per_second': 5.7, 'epoch': 0.08}
{'loss': 1.7597, 'grad_norm': 0.055002354085445404, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6980208158493042, 'eval_runtime': 11.0637, 'eval_samples_per_second': 90.295, 'eval_steps_per_second': 5.694, 'epoch': 0.12}
{'loss': 1.6154, 'grad_norm': 0.04692400246858597, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6016998291015625, 'eval_runtime': 11.1447, 'eval_samples_per_second': 89.639, 'eval_steps_per_second': 5.653, 'epoch': 0.16}
{'loss': 1.588, 'grad_norm': 0.03524543344974518, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5700702667236328, 'eval_runtime': 11.2044, 'eval_samples_per_second': 89.162, 'eval_steps_per_second': 5.623, 'epoch': 0.2}
{'loss': 1.5977, 'grad_norm': 0.036262497305870056, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5550910234451294, 'eval_runtime': 11.1864, 'eval_samples_per_second': 89.305, 'eval_steps_per_second': 5.632, 'epoch': 0.24}
{'loss': 1.5233, 'grad_norm': 0.038564834743738174, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.546356439590454, 'eval_runtime': 11.217, 'eval_samples_per_second': 89.061, 'eval_steps_per_second': 5.616, 'epoch': 0.28}
{'loss': 1.552, 'grad_norm': 0.037107426673173904, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5376722812652588, 'eval_runtime': 11.2463, 'eval_samples_per_second': 88.829, 'eval_steps_per_second': 5.602, 'epoch': 0.32}
{'loss': 1.5675, 'grad_norm': 0.03735455498099327, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5318719148635864, 'eval_runtime': 11.2431, 'eval_samples_per_second': 88.854, 'eval_steps_per_second': 5.603, 'epoch': 0.36}
{'loss': 1.5237, 'grad_norm': 0.03334154188632965, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5236941576004028, 'eval_runtime': 11.2466, 'eval_samples_per_second': 88.827, 'eval_steps_per_second': 5.602, 'epoch': 0.4}
{'loss': 1.5424, 'grad_norm': 0.0456080436706543, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5184118747711182, 'eval_runtime': 11.2004, 'eval_samples_per_second': 89.194, 'eval_steps_per_second': 5.625, 'epoch': 0.44}
{'loss': 1.5106, 'grad_norm': 0.03888174146413803, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5125199556350708, 'eval_runtime': 11.1745, 'eval_samples_per_second': 89.4, 'eval_steps_per_second': 5.638, 'epoch': 0.48}
{'loss': 1.5341, 'grad_norm': 0.035641737282276154, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5100398063659668, 'eval_runtime': 11.1721, 'eval_samples_per_second': 89.419, 'eval_steps_per_second': 5.639, 'epoch': 0.52}
{'loss': 1.5304, 'grad_norm': 0.03674953058362007, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5060123205184937, 'eval_runtime': 11.1452, 'eval_samples_per_second': 89.635, 'eval_steps_per_second': 5.653, 'epoch': 0.56}
{'loss': 1.5097, 'grad_norm': 0.038135603070259094, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5012454986572266, 'eval_runtime': 11.1331, 'eval_samples_per_second': 89.732, 'eval_steps_per_second': 5.659, 'epoch': 0.6}
{'loss': 1.5356, 'grad_norm': 0.04184645414352417, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4985536336898804, 'eval_runtime': 11.0732, 'eval_samples_per_second': 90.218, 'eval_steps_per_second': 5.689, 'epoch': 0.64}
{'loss': 1.529, 'grad_norm': 0.03863530978560448, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4939714670181274, 'eval_runtime': 11.0714, 'eval_samples_per_second': 90.232, 'eval_steps_per_second': 5.69, 'epoch': 0.68}
{'loss': 1.5004, 'grad_norm': 0.046876415610313416, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4919776916503906, 'eval_runtime': 11.0652, 'eval_samples_per_second': 90.283, 'eval_steps_per_second': 5.694, 'epoch': 0.72}
{'loss': 1.519, 'grad_norm': 0.03860941901803017, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4896444082260132, 'eval_runtime': 11.0632, 'eval_samples_per_second': 90.299, 'eval_steps_per_second': 5.695, 'epoch': 0.76}
{'loss': 1.5074, 'grad_norm': 0.041693586856126785, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4880750179290771, 'eval_runtime': 11.0831, 'eval_samples_per_second': 90.137, 'eval_steps_per_second': 5.684, 'epoch': 0.8}
{'loss': 1.5184, 'grad_norm': 0.03771398961544037, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4859269857406616, 'eval_runtime': 11.0951, 'eval_samples_per_second': 90.039, 'eval_steps_per_second': 5.678, 'epoch': 0.84}
{'loss': 1.5249, 'grad_norm': 0.03712301328778267, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4845144748687744, 'eval_runtime': 11.2088, 'eval_samples_per_second': 89.127, 'eval_steps_per_second': 5.621, 'epoch': 0.88}
{'loss': 1.4829, 'grad_norm': 0.036560945212841034, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4828866720199585, 'eval_runtime': 11.1859, 'eval_samples_per_second': 89.309, 'eval_steps_per_second': 5.632, 'epoch': 0.92}
{'loss': 1.5134, 'grad_norm': 0.042464930564165115, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4821943044662476, 'eval_runtime': 11.1586, 'eval_samples_per_second': 89.528, 'eval_steps_per_second': 5.646, 'epoch': 0.96}
{'loss': 1.5272, 'grad_norm': 0.0450446717441082, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4818847179412842, 'eval_runtime': 11.1739, 'eval_samples_per_second': 89.405, 'eval_steps_per_second': 5.638, 'epoch': 1.0}
{'train_runtime': 558.1996, 'train_samples_per_second': 17.913, 'train_steps_per_second': 1.12, 'train_loss': 1.664003253173828, 'epoch': 1.0}
train_results:  {'eval_loss': [2.906388759613037, 1.9115110635757446, 1.6980208158493042, 1.6016998291015625, 1.5700702667236328, 1.5550910234451294, 1.546356439590454, 1.5376722812652588, 1.5318719148635864, 1.5236941576004028, 1.5184118747711182, 1.5125199556350708, 1.5100398063659668, 1.5060123205184937, 1.5012454986572266, 1.4985536336898804, 1.4939714670181274, 1.4919776916503906, 1.4896444082260132, 1.4880750179290771, 1.4859269857406616, 1.4845144748687744, 1.4828866720199585, 1.4821943044662476, 1.4818847179412842], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.60it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.37it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.80it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.89it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.40it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 82.21it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.83it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.23it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.24it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.38it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.53it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 107.03it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 116.41it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.65it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 119.43it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.89it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 117.05it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 119.14it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.55it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.80it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.60it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.91it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.69it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 109.04it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4243788719177246
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2365 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.7451187252867622
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.9207475747576732
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 1.0125176073657842
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 1.1634005550708986
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 1.0512959782010407
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4441, dtype=torch.float64), tensor(0.5559, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.5862029558109827e-17, 1.4800000190735039, 0]
normalized proposed parameters for next round by BO: [tensor(1.4587e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4441, dtype=torch.float64), tensor(0.5559, dtype=torch.float64), tensor(5.1219e-16, dtype=torch.float64), tensor(2.6776e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5672e-16, dtype=torch.float64), tensor(1.6011e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.5862e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.444
  sciq: 0.556
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.5862029558109827e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190735039,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.5862029558109827e-17
lora alpha:  1.4800000190735039
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.11it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.65it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.49it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.77it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.48it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.42it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.56it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.84it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.82it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.13it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.55it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.02it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.12it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.88it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.79it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 149.96it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.04it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.60it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.18it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.38it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.05it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.3413, 'grad_norm': 0.2719930112361908, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.351637125015259, 'eval_runtime': 10.9679, 'eval_samples_per_second': 91.084, 'eval_steps_per_second': 5.744, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.02it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.57it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.27it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.50it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.09it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.93it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.20it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.37it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.37it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.62it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.99it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.37it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.44it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.46it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.87it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.57it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.57it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.91it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.30it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.45it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.27it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.84it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.5607, 'grad_norm': 0.21092571318149567, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.9690767526626587, 'eval_runtime': 10.9848, 'eval_samples_per_second': 90.944, 'eval_steps_per_second': 5.735, 'epoch': 0.08}
{'loss': 1.7439, 'grad_norm': 0.06578677147626877, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6869856119155884, 'eval_runtime': 11.0127, 'eval_samples_per_second': 90.713, 'eval_steps_per_second': 5.721, 'epoch': 0.12}
{'loss': 1.6086, 'grad_norm': 0.05178926885128021, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5784647464752197, 'eval_runtime': 11.0327, 'eval_samples_per_second': 90.549, 'eval_steps_per_second': 5.71, 'epoch': 0.16}
{'loss': 1.5164, 'grad_norm': 0.04867275059223175, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.553353190422058, 'eval_runtime': 11.0478, 'eval_samples_per_second': 90.426, 'eval_steps_per_second': 5.703, 'epoch': 0.2}
{'loss': 1.4958, 'grad_norm': 0.039134372025728226, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5407791137695312, 'eval_runtime': 11.0575, 'eval_samples_per_second': 90.346, 'eval_steps_per_second': 5.698, 'epoch': 0.24}
{'loss': 1.4659, 'grad_norm': 0.03961668908596039, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5315862894058228, 'eval_runtime': 11.0526, 'eval_samples_per_second': 90.386, 'eval_steps_per_second': 5.7, 'epoch': 0.28}
{'loss': 1.532, 'grad_norm': 0.039015237241983414, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5244959592819214, 'eval_runtime': 11.0283, 'eval_samples_per_second': 90.585, 'eval_steps_per_second': 5.713, 'epoch': 0.32}
{'loss': 1.5024, 'grad_norm': 0.036800868809223175, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5187067985534668, 'eval_runtime': 11.1026, 'eval_samples_per_second': 89.979, 'eval_steps_per_second': 5.674, 'epoch': 0.36}
{'loss': 1.475, 'grad_norm': 0.03592824190855026, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5121169090270996, 'eval_runtime': 11.0932, 'eval_samples_per_second': 90.056, 'eval_steps_per_second': 5.679, 'epoch': 0.4}
{'loss': 1.5205, 'grad_norm': 0.0413140244781971, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5052342414855957, 'eval_runtime': 11.097, 'eval_samples_per_second': 90.025, 'eval_steps_per_second': 5.677, 'epoch': 0.44}
{'loss': 1.4653, 'grad_norm': 0.04182770475745201, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5017915964126587, 'eval_runtime': 11.0721, 'eval_samples_per_second': 90.227, 'eval_steps_per_second': 5.69, 'epoch': 0.48}
{'loss': 1.4759, 'grad_norm': 0.042174484580755234, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.495879888534546, 'eval_runtime': 11.0782, 'eval_samples_per_second': 90.177, 'eval_steps_per_second': 5.687, 'epoch': 0.52}
{'loss': 1.4773, 'grad_norm': 0.04674036055803299, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4941502809524536, 'eval_runtime': 11.1077, 'eval_samples_per_second': 89.937, 'eval_steps_per_second': 5.672, 'epoch': 0.56}
{'loss': 1.4986, 'grad_norm': 0.0387628972530365, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4896456003189087, 'eval_runtime': 11.1085, 'eval_samples_per_second': 89.931, 'eval_steps_per_second': 5.671, 'epoch': 0.6}
{'loss': 1.4907, 'grad_norm': 0.04234634339809418, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4855951070785522, 'eval_runtime': 11.0935, 'eval_samples_per_second': 90.053, 'eval_steps_per_second': 5.679, 'epoch': 0.64}
{'loss': 1.4903, 'grad_norm': 0.04720249027013779, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4831130504608154, 'eval_runtime': 11.0762, 'eval_samples_per_second': 90.194, 'eval_steps_per_second': 5.688, 'epoch': 0.68}
{'loss': 1.4686, 'grad_norm': 0.042768653482198715, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4807524681091309, 'eval_runtime': 11.0542, 'eval_samples_per_second': 90.373, 'eval_steps_per_second': 5.699, 'epoch': 0.72}
{'loss': 1.4986, 'grad_norm': 0.044635213911533356, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4778944253921509, 'eval_runtime': 11.0595, 'eval_samples_per_second': 90.329, 'eval_steps_per_second': 5.696, 'epoch': 0.76}
{'loss': 1.4668, 'grad_norm': 0.04624049738049507, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4754470586776733, 'eval_runtime': 11.0599, 'eval_samples_per_second': 90.326, 'eval_steps_per_second': 5.696, 'epoch': 0.8}
{'loss': 1.4784, 'grad_norm': 0.03767406567931175, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4738794565200806, 'eval_runtime': 11.081, 'eval_samples_per_second': 90.155, 'eval_steps_per_second': 5.685, 'epoch': 0.84}
{'loss': 1.4607, 'grad_norm': 0.04658488556742668, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4722110033035278, 'eval_runtime': 11.0791, 'eval_samples_per_second': 90.17, 'eval_steps_per_second': 5.686, 'epoch': 0.88}
{'loss': 1.4782, 'grad_norm': 0.043165210634469986, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.471719741821289, 'eval_runtime': 11.0843, 'eval_samples_per_second': 90.127, 'eval_steps_per_second': 5.684, 'epoch': 0.92}
{'loss': 1.4913, 'grad_norm': 0.04469616338610649, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.470629334449768, 'eval_runtime': 11.0808, 'eval_samples_per_second': 90.156, 'eval_steps_per_second': 5.686, 'epoch': 0.96}
{'loss': 1.5008, 'grad_norm': 0.04926753044128418, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4703525304794312, 'eval_runtime': 11.0807, 'eval_samples_per_second': 90.157, 'eval_steps_per_second': 5.686, 'epoch': 1.0}
{'train_runtime': 568.9476, 'train_samples_per_second': 17.575, 'train_steps_per_second': 1.099, 'train_loss': 1.660158270263672, 'epoch': 1.0}
train_results:  {'eval_loss': [3.351637125015259, 1.9690767526626587, 1.6869856119155884, 1.5784647464752197, 1.553353190422058, 1.5407791137695312, 1.5315862894058228, 1.5244959592819214, 1.5187067985534668, 1.5121169090270996, 1.5052342414855957, 1.5017915964126587, 1.495879888534546, 1.4941502809524536, 1.4896456003189087, 1.4855951070785522, 1.4831130504608154, 1.4807524681091309, 1.4778944253921509, 1.4754470586776733, 1.4738794565200806, 1.4722110033035278, 1.471719741821289, 1.470629334449768, 1.4703525304794312], 'performance': [0.49, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:56,  2.25it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:11, 32.61it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 51.71it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 62.23it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 70.06it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 76.59it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 83.48it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 92.16it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 93.82it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 95.60it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 99.07it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.60it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.20it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.71it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.75it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.51it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.83it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.73it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.96it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.17it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.06it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.64it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 102.47it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4275758266448975
current iteration best possible performance (full train run):  0.4935
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6424 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.8194764565304877
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 1.0171258973314536
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.9490602494400443
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 1.0088809293844392
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.9498190364958079
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.1132, dtype=torch.float64), tensor(0.4564, dtype=torch.float64), tensor(0.1920, dtype=torch.float64), 0, 0, 0, tensor(0.0476, dtype=torch.float64), tensor(0.1908, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734939, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.1132, dtype=torch.float64), tensor(0.4564, dtype=torch.float64), tensor(0.1920, dtype=torch.float64), tensor(1.8863e-16, dtype=torch.float64), tensor(8.7357e-17, dtype=torch.float64), tensor(1.5048e-17, dtype=torch.float64), tensor(0.0476, dtype=torch.float64), tensor(0.1908, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.113
  rowan_hellaswag: 0.456
  sciq: 0.192
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.048
  arc_challenge: 0.191

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734939,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734939
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.87it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.48it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.46it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.89it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.26it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.04it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.11it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.28it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.20it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.66it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.22it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.79it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.97it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.89it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.29it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.79it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.78it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.18it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.63it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.01it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.92it/s]
Evaluation performance at step 25: 0.5
{'loss': 3.8515, 'grad_norm': 0.16312682628631592, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.019768238067627, 'eval_runtime': 10.9695, 'eval_samples_per_second': 91.071, 'eval_steps_per_second': 5.743, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.05it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.67it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.43it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.60it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.10it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.92it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.02it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.31it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.26it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.51it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.93it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.46it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.54it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.33it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.75it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.38it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.59it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.95it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.35it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.36it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.15it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.80it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.3825, 'grad_norm': 0.15656690299510956, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.906858205795288, 'eval_runtime': 10.9897, 'eval_samples_per_second': 90.903, 'eval_steps_per_second': 5.733, 'epoch': 0.08}
{'loss': 1.7038, 'grad_norm': 0.06087024137377739, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6331111192703247, 'eval_runtime': 11.0238, 'eval_samples_per_second': 90.622, 'eval_steps_per_second': 5.715, 'epoch': 0.12}
{'loss': 1.524, 'grad_norm': 0.040660277009010315, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5293523073196411, 'eval_runtime': 11.1061, 'eval_samples_per_second': 89.951, 'eval_steps_per_second': 5.673, 'epoch': 0.16}
{'loss': 1.5285, 'grad_norm': 0.04838935658335686, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5020338296890259, 'eval_runtime': 11.1053, 'eval_samples_per_second': 89.957, 'eval_steps_per_second': 5.673, 'epoch': 0.2}
{'loss': 1.4863, 'grad_norm': 0.037359148263931274, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.482008457183838, 'eval_runtime': 11.1206, 'eval_samples_per_second': 89.833, 'eval_steps_per_second': 5.665, 'epoch': 0.24}
{'loss': 1.4386, 'grad_norm': 0.03934667631983757, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4709017276763916, 'eval_runtime': 11.1834, 'eval_samples_per_second': 89.329, 'eval_steps_per_second': 5.633, 'epoch': 0.28}
{'loss': 1.475, 'grad_norm': 0.04297395795583725, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4630948305130005, 'eval_runtime': 11.1621, 'eval_samples_per_second': 89.499, 'eval_steps_per_second': 5.644, 'epoch': 0.32}
{'loss': 1.4035, 'grad_norm': 0.03834443911910057, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.454716444015503, 'eval_runtime': 11.1823, 'eval_samples_per_second': 89.338, 'eval_steps_per_second': 5.634, 'epoch': 0.36}
{'loss': 1.4462, 'grad_norm': 0.03934570029377937, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.449006199836731, 'eval_runtime': 11.1867, 'eval_samples_per_second': 89.303, 'eval_steps_per_second': 5.632, 'epoch': 0.4}
{'loss': 1.3907, 'grad_norm': 0.03689882904291153, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4448063373565674, 'eval_runtime': 11.1842, 'eval_samples_per_second': 89.323, 'eval_steps_per_second': 5.633, 'epoch': 0.44}
{'loss': 1.4284, 'grad_norm': 0.0445011630654335, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4392441511154175, 'eval_runtime': 11.1598, 'eval_samples_per_second': 89.518, 'eval_steps_per_second': 5.645, 'epoch': 0.48}
{'loss': 1.4119, 'grad_norm': 0.04244369640946388, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4348407983779907, 'eval_runtime': 11.1787, 'eval_samples_per_second': 89.367, 'eval_steps_per_second': 5.636, 'epoch': 0.52}
{'loss': 1.453, 'grad_norm': 0.04260019585490227, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4312037229537964, 'eval_runtime': 11.1497, 'eval_samples_per_second': 89.599, 'eval_steps_per_second': 5.65, 'epoch': 0.56}
{'loss': 1.4413, 'grad_norm': 0.038286738097667694, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4270901679992676, 'eval_runtime': 11.1108, 'eval_samples_per_second': 89.912, 'eval_steps_per_second': 5.67, 'epoch': 0.6}
{'loss': 1.404, 'grad_norm': 0.05165954306721687, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4232732057571411, 'eval_runtime': 11.1709, 'eval_samples_per_second': 89.429, 'eval_steps_per_second': 5.64, 'epoch': 0.64}
{'loss': 1.3551, 'grad_norm': 0.04430444538593292, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4219473600387573, 'eval_runtime': 11.1214, 'eval_samples_per_second': 89.827, 'eval_steps_per_second': 5.665, 'epoch': 0.68}
{'loss': 1.3972, 'grad_norm': 0.0432184599339962, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4184894561767578, 'eval_runtime': 11.1022, 'eval_samples_per_second': 89.982, 'eval_steps_per_second': 5.675, 'epoch': 0.72}
{'loss': 1.445, 'grad_norm': 0.043560370802879333, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4157177209854126, 'eval_runtime': 11.138, 'eval_samples_per_second': 89.693, 'eval_steps_per_second': 5.656, 'epoch': 0.76}
{'loss': 1.4135, 'grad_norm': 0.04334810748696327, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4140625, 'eval_runtime': 11.1082, 'eval_samples_per_second': 89.934, 'eval_steps_per_second': 5.672, 'epoch': 0.8}
{'loss': 1.3839, 'grad_norm': 0.04534861445426941, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.411484956741333, 'eval_runtime': 11.0997, 'eval_samples_per_second': 90.002, 'eval_steps_per_second': 5.676, 'epoch': 0.84}
{'loss': 1.3893, 'grad_norm': 0.044783156365156174, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.409536600112915, 'eval_runtime': 11.092, 'eval_samples_per_second': 90.065, 'eval_steps_per_second': 5.68, 'epoch': 0.88}
{'loss': 1.37, 'grad_norm': 0.04260525107383728, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.408487319946289, 'eval_runtime': 11.0924, 'eval_samples_per_second': 90.061, 'eval_steps_per_second': 5.68, 'epoch': 0.92}
{'loss': 1.3987, 'grad_norm': 0.039908502250909805, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4079625606536865, 'eval_runtime': 11.0031, 'eval_samples_per_second': 90.792, 'eval_steps_per_second': 5.726, 'epoch': 0.96}
{'loss': 1.4265, 'grad_norm': 0.041126035153865814, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4075618982315063, 'eval_runtime': 11.0013, 'eval_samples_per_second': 90.808, 'eval_steps_per_second': 5.727, 'epoch': 1.0}
{'train_runtime': 558.7266, 'train_samples_per_second': 17.894, 'train_steps_per_second': 1.119, 'train_loss': 1.5739321655273437, 'epoch': 1.0}
train_results:  {'eval_loss': [3.019768238067627, 1.906858205795288, 1.6331111192703247, 1.5293523073196411, 1.5020338296890259, 1.482008457183838, 1.4709017276763916, 1.4630948305130005, 1.454716444015503, 1.449006199836731, 1.4448063373565674, 1.4392441511154175, 1.4348407983779907, 1.4312037229537964, 1.4270901679992676, 1.4232732057571411, 1.4219473600387573, 1.4184894561767578, 1.4157177209854126, 1.4140625, 1.411484956741333, 1.409536600112915, 1.408487319946289, 1.4079625606536865, 1.4075618982315063], 'performance': [0.5, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<04:41,  1.42it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:20, 18.81it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:10, 35.12it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:07, 47.24it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 57.54it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 66.29it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:03, 75.36it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 85.55it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 89.09it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 92.14it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 96.65it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 104.21it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 114.19it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 112.77it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 117.77it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 117.02it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.65it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 119.01it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.94it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.10it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 128.22it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 129.05it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 138.76it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 91.92it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4248547554016113
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8129 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.9876776085980787
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.7364385096560965
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 1.0400912151347739
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.8952317226650509
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.991477537947395
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0289, dtype=torch.float64), tensor(0.4354, dtype=torch.float64), tensor(0.0487, dtype=torch.float64), 0, 0, 0, 0, tensor(0.4870, dtype=torch.float64), 1, 0, 0, 1, 1, 1, 128, 1.873501354054952e-19, 1.4800000190734877, 0]
normalized proposed parameters for next round by BO: [tensor(1.5496e-17, dtype=torch.float64), tensor(0.0289, dtype=torch.float64), tensor(0.4354, dtype=torch.float64), tensor(0.0487, dtype=torch.float64), tensor(3.4886e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.5743e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4870, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.8735e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.029
  rowan_hellaswag: 0.435
  sciq: 0.049
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.487

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.873501354054952e-19,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734877,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.873501354054952e-19
lora alpha:  1.4800000190734877
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 7,077,888 || all params: 8,037,339,136 || trainable%: 0.0881
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 68.82it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.40it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 125.48it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 135.36it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 141.39it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 156.45it/s]Running loglikelihood requests:  28%|‚ñà‚ñà‚ñä       | 113/400 [00:00<00:01, 170.05it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 166.33it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 169.71it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 173.44it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 184.34it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 187.36it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 192.97it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 201.46it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 214.55it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 224.14it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:01<00:00, 241.00it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 191.85it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.1653, 'grad_norm': 0.06630741059780121, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 4.037768840789795, 'eval_runtime': 8.8758, 'eval_samples_per_second': 112.554, 'eval_steps_per_second': 7.098, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 69.06it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 111.07it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 124.91it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 134.14it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 139.71it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:01, 154.94it/s]Running loglikelihood requests:  29%|‚ñà‚ñà‚ñâ       | 117/400 [00:00<00:01, 158.07it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:00<00:01, 169.31it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 171.34it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 178.67it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:00, 183.68it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 189.31it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 191.92it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 207.96it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 212.16it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:01<00:00, 229.02it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 242.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 190.48it/s]
Evaluation performance at step 50: 0.49
{'loss': 3.691, 'grad_norm': 0.16453947126865387, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 3.0721635818481445, 'eval_runtime': 8.8938, 'eval_samples_per_second': 112.326, 'eval_steps_per_second': 7.084, 'epoch': 0.08}
{'loss': 2.7185, 'grad_norm': 0.08770598471164703, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.446681022644043, 'eval_runtime': 8.915, 'eval_samples_per_second': 112.059, 'eval_steps_per_second': 7.067, 'epoch': 0.12}
{'loss': 2.2017, 'grad_norm': 0.07673382014036179, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.102815628051758, 'eval_runtime': 8.9602, 'eval_samples_per_second': 111.493, 'eval_steps_per_second': 7.031, 'epoch': 0.16}
{'loss': 1.9815, 'grad_norm': 0.07694617658853531, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.929965615272522, 'eval_runtime': 8.9828, 'eval_samples_per_second': 111.213, 'eval_steps_per_second': 7.013, 'epoch': 0.2}
{'loss': 1.9051, 'grad_norm': 0.05789275839924812, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.847090244293213, 'eval_runtime': 8.9812, 'eval_samples_per_second': 111.233, 'eval_steps_per_second': 7.015, 'epoch': 0.24}
{'loss': 1.8377, 'grad_norm': 0.07293127477169037, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8001856803894043, 'eval_runtime': 8.9986, 'eval_samples_per_second': 111.018, 'eval_steps_per_second': 7.001, 'epoch': 0.28}
{'loss': 1.7827, 'grad_norm': 0.08480659872293472, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7711960077285767, 'eval_runtime': 9.0243, 'eval_samples_per_second': 110.701, 'eval_steps_per_second': 6.981, 'epoch': 0.32}
{'loss': 1.7024, 'grad_norm': 0.06703327596187592, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7507915496826172, 'eval_runtime': 9.0105, 'eval_samples_per_second': 110.87, 'eval_steps_per_second': 6.992, 'epoch': 0.36}
{'loss': 1.6929, 'grad_norm': 0.06167906895279884, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7353992462158203, 'eval_runtime': 9.0358, 'eval_samples_per_second': 110.56, 'eval_steps_per_second': 6.972, 'epoch': 0.4}
{'loss': 1.6735, 'grad_norm': 0.0937875509262085, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.72487473487854, 'eval_runtime': 9.0221, 'eval_samples_per_second': 110.728, 'eval_steps_per_second': 6.983, 'epoch': 0.44}
{'loss': 1.7192, 'grad_norm': 0.07110293954610825, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7159130573272705, 'eval_runtime': 9.015, 'eval_samples_per_second': 110.816, 'eval_steps_per_second': 6.988, 'epoch': 0.48}
{'loss': 1.7305, 'grad_norm': 0.0619187168776989, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7077139616012573, 'eval_runtime': 9.0026, 'eval_samples_per_second': 110.968, 'eval_steps_per_second': 6.998, 'epoch': 0.52}
{'loss': 1.6747, 'grad_norm': 0.06502872705459595, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7005445957183838, 'eval_runtime': 9.0352, 'eval_samples_per_second': 110.568, 'eval_steps_per_second': 6.973, 'epoch': 0.56}
{'loss': 1.6828, 'grad_norm': 0.08777280151844025, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6951509714126587, 'eval_runtime': 9.0499, 'eval_samples_per_second': 110.388, 'eval_steps_per_second': 6.961, 'epoch': 0.6}
{'loss': 1.6103, 'grad_norm': 0.07301783561706543, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.690400242805481, 'eval_runtime': 9.0333, 'eval_samples_per_second': 110.591, 'eval_steps_per_second': 6.974, 'epoch': 0.64}
{'loss': 1.6965, 'grad_norm': 0.06813842058181763, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6863642930984497, 'eval_runtime': 9.0517, 'eval_samples_per_second': 110.367, 'eval_steps_per_second': 6.96, 'epoch': 0.68}
{'loss': 1.667, 'grad_norm': 0.08724015951156616, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6827472448349, 'eval_runtime': 9.0399, 'eval_samples_per_second': 110.51, 'eval_steps_per_second': 6.969, 'epoch': 0.72}
{'loss': 1.7097, 'grad_norm': 0.08206702768802643, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6800678968429565, 'eval_runtime': 9.0416, 'eval_samples_per_second': 110.489, 'eval_steps_per_second': 6.968, 'epoch': 0.76}
{'loss': 1.604, 'grad_norm': 0.07696446031332016, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6777414083480835, 'eval_runtime': 9.0407, 'eval_samples_per_second': 110.501, 'eval_steps_per_second': 6.969, 'epoch': 0.8}
{'loss': 1.6449, 'grad_norm': 0.06524311006069183, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6755428314208984, 'eval_runtime': 9.0515, 'eval_samples_per_second': 110.368, 'eval_steps_per_second': 6.96, 'epoch': 0.84}
{'loss': 1.6582, 'grad_norm': 0.07694113254547119, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6745226383209229, 'eval_runtime': 9.0333, 'eval_samples_per_second': 110.591, 'eval_steps_per_second': 6.974, 'epoch': 0.88}
{'loss': 1.7218, 'grad_norm': 0.06254731863737106, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6731643676757812, 'eval_runtime': 9.1122, 'eval_samples_per_second': 109.633, 'eval_steps_per_second': 6.914, 'epoch': 0.92}
{'loss': 1.638, 'grad_norm': 0.06902499496936798, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6724556684494019, 'eval_runtime': 9.0865, 'eval_samples_per_second': 109.944, 'eval_steps_per_second': 6.933, 'epoch': 0.96}
{'loss': 1.691, 'grad_norm': 0.0825737714767456, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.672292709350586, 'eval_runtime': 9.1, 'eval_samples_per_second': 109.78, 'eval_steps_per_second': 6.923, 'epoch': 1.0}
{'train_runtime': 357.9276, 'train_samples_per_second': 27.93, 'train_steps_per_second': 1.746, 'train_loss': 1.9520318359375, 'epoch': 1.0}
train_results:  {'eval_loss': [4.037768840789795, 3.0721635818481445, 2.446681022644043, 2.102815628051758, 1.929965615272522, 1.847090244293213, 1.8001856803894043, 1.7711960077285767, 1.7507915496826172, 1.7353992462158203, 1.72487473487854, 1.7159130573272705, 1.7077139616012573, 1.7005445957183838, 1.6951509714126587, 1.690400242805481, 1.6863642930984497, 1.6827472448349, 1.6800678968429565, 1.6777414083480835, 1.6755428314208984, 1.6745226383209229, 1.6731643676757812, 1.6724556684494019, 1.672292709350586], 'performance': [0.48, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<04:42,  1.41it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:15, 25.37it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:07, 46.08it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 60.94it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 72.90it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 83.48it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 94.24it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 106.43it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 110.78it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 114.26it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:01, 119.67it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 128.72it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 140.25it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 138.87it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 145.10it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:00, 143.94it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 143.99it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 146.83it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 148.78it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 157.88it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 157.00it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 157.55it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 190.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 113.42it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.3060258626937866
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.7244 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.9788133453135615
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.7826654955516285
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.814912961419327
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.7487178922367775
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8548368020011394
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0911, dtype=torch.float64), tensor(0.4623, dtype=torch.float64), tensor(0.1559, dtype=torch.float64), 0, 0, tensor(0.2173, dtype=torch.float64), tensor(0.0733, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 128, 0.06922330648106288, 1.4800000190734866, 0]
normalized proposed parameters for next round by BO: [tensor(1.2738e-16, dtype=torch.float64), tensor(0.0911, dtype=torch.float64), tensor(0.4623, dtype=torch.float64), tensor(0.1559, dtype=torch.float64), tensor(1.9428e-17, dtype=torch.float64), tensor(3.8311e-17, dtype=torch.float64), tensor(0.2173, dtype=torch.float64), tensor(0.0733, dtype=torch.float64), tensor(2.9301e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6922, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.091
  rowan_hellaswag: 0.462
  sciq: 0.156
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.217
  mmlu: 0.073
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06922330648106288,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.06922330648106288
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.92it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.88it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.84it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.20it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.90it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.83it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.00it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.14it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.10it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.43it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.92it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.27it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.23it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.07it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.49it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.08it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.16it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.38it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.55it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.47it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.44it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.8714, 'grad_norm': 0.12553997337818146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.0667672157287598, 'eval_runtime': 10.9717, 'eval_samples_per_second': 91.053, 'eval_steps_per_second': 5.742, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.73it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.40it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.17it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.55it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.24it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.04it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.26it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.49it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.43it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.75it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.16it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.48it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.74it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.66it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.94it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.52it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.36it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.66it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.05it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.21it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.78it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.5051, 'grad_norm': 0.142137810587883, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.067084789276123, 'eval_runtime': 10.9477, 'eval_samples_per_second': 91.252, 'eval_steps_per_second': 5.755, 'epoch': 0.08}
{'loss': 1.9543, 'grad_norm': 0.08430229127407074, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8103313446044922, 'eval_runtime': 11.0566, 'eval_samples_per_second': 90.353, 'eval_steps_per_second': 5.698, 'epoch': 0.12}
{'loss': 1.776, 'grad_norm': 0.05260980501770973, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.724722981452942, 'eval_runtime': 11.0813, 'eval_samples_per_second': 90.152, 'eval_steps_per_second': 5.685, 'epoch': 0.16}
{'loss': 1.7043, 'grad_norm': 0.06359557062387466, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6745631694793701, 'eval_runtime': 11.0846, 'eval_samples_per_second': 90.125, 'eval_steps_per_second': 5.684, 'epoch': 0.2}
{'loss': 1.6511, 'grad_norm': 0.0414079912006855, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.652923345565796, 'eval_runtime': 11.071, 'eval_samples_per_second': 90.235, 'eval_steps_per_second': 5.691, 'epoch': 0.24}
{'loss': 1.6662, 'grad_norm': 0.0532015785574913, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6397815942764282, 'eval_runtime': 11.0139, 'eval_samples_per_second': 90.704, 'eval_steps_per_second': 5.72, 'epoch': 0.28}
{'loss': 1.6582, 'grad_norm': 0.03998300060629845, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6313728094100952, 'eval_runtime': 11.0128, 'eval_samples_per_second': 90.713, 'eval_steps_per_second': 5.721, 'epoch': 0.32}
{'loss': 1.6793, 'grad_norm': 0.04091678932309151, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6208665370941162, 'eval_runtime': 11.0185, 'eval_samples_per_second': 90.666, 'eval_steps_per_second': 5.718, 'epoch': 0.36}
{'loss': 1.6589, 'grad_norm': 0.04967331513762474, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6130422353744507, 'eval_runtime': 11.0111, 'eval_samples_per_second': 90.727, 'eval_steps_per_second': 5.722, 'epoch': 0.4}
{'loss': 1.5678, 'grad_norm': 0.041386183351278305, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6069508790969849, 'eval_runtime': 11.0092, 'eval_samples_per_second': 90.742, 'eval_steps_per_second': 5.722, 'epoch': 0.44}
{'loss': 1.6206, 'grad_norm': 0.04720528796315193, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6004172563552856, 'eval_runtime': 11.0328, 'eval_samples_per_second': 90.548, 'eval_steps_per_second': 5.71, 'epoch': 0.48}
{'loss': 1.5944, 'grad_norm': 0.05547554790973663, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5959362983703613, 'eval_runtime': 11.0244, 'eval_samples_per_second': 90.617, 'eval_steps_per_second': 5.715, 'epoch': 0.52}
{'loss': 1.6087, 'grad_norm': 0.04512094706296921, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5915818214416504, 'eval_runtime': 11.0298, 'eval_samples_per_second': 90.573, 'eval_steps_per_second': 5.712, 'epoch': 0.56}
{'loss': 1.633, 'grad_norm': 0.044875018298625946, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5887891054153442, 'eval_runtime': 11.079, 'eval_samples_per_second': 90.171, 'eval_steps_per_second': 5.686, 'epoch': 0.6}
{'loss': 1.5793, 'grad_norm': 0.04279259219765663, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.584163784980774, 'eval_runtime': 11.1517, 'eval_samples_per_second': 89.583, 'eval_steps_per_second': 5.649, 'epoch': 0.64}
{'loss': 1.6133, 'grad_norm': 0.04223514348268509, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5820931196212769, 'eval_runtime': 11.1015, 'eval_samples_per_second': 89.988, 'eval_steps_per_second': 5.675, 'epoch': 0.68}
{'loss': 1.5758, 'grad_norm': 0.04134587198495865, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5781883001327515, 'eval_runtime': 11.1304, 'eval_samples_per_second': 89.754, 'eval_steps_per_second': 5.66, 'epoch': 0.72}
{'loss': 1.6127, 'grad_norm': 0.05642257258296013, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5762016773223877, 'eval_runtime': 11.0961, 'eval_samples_per_second': 90.032, 'eval_steps_per_second': 5.678, 'epoch': 0.76}
{'loss': 1.5727, 'grad_norm': 0.04997541755437851, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.57487952709198, 'eval_runtime': 11.0715, 'eval_samples_per_second': 90.232, 'eval_steps_per_second': 5.69, 'epoch': 0.8}
{'loss': 1.57, 'grad_norm': 0.044463880360126495, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.572463870048523, 'eval_runtime': 11.0643, 'eval_samples_per_second': 90.291, 'eval_steps_per_second': 5.694, 'epoch': 0.84}
{'loss': 1.6033, 'grad_norm': 0.04280637949705124, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.571244239807129, 'eval_runtime': 11.0245, 'eval_samples_per_second': 90.617, 'eval_steps_per_second': 5.715, 'epoch': 0.88}
{'loss': 1.5684, 'grad_norm': 0.043745413422584534, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5700476169586182, 'eval_runtime': 11.0618, 'eval_samples_per_second': 90.311, 'eval_steps_per_second': 5.695, 'epoch': 0.92}
{'loss': 1.6234, 'grad_norm': 0.04480917379260063, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5691478252410889, 'eval_runtime': 11.0821, 'eval_samples_per_second': 90.145, 'eval_steps_per_second': 5.685, 'epoch': 0.96}
{'loss': 1.6094, 'grad_norm': 0.046540576964616776, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5685961246490479, 'eval_runtime': 11.1234, 'eval_samples_per_second': 89.811, 'eval_steps_per_second': 5.664, 'epoch': 1.0}
{'train_runtime': 569.8276, 'train_samples_per_second': 17.547, 'train_steps_per_second': 1.097, 'train_loss': 1.7631045654296875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0667672157287598, 2.067084789276123, 1.8103313446044922, 1.724722981452942, 1.6745631694793701, 1.652923345565796, 1.6397815942764282, 1.6313728094100952, 1.6208665370941162, 1.6130422353744507, 1.6069508790969849, 1.6004172563552856, 1.5959362983703613, 1.5915818214416504, 1.5887891054153442, 1.584163784980774, 1.5820931196212769, 1.5781883001327515, 1.5762016773223877, 1.57487952709198, 1.572463870048523, 1.571244239807129, 1.5700476169586182, 1.5691478252410889, 1.5685961246490479], 'performance': [0.48, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:27,  4.57it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 46.95it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.36it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.41it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 76.95it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 81.74it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.27it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 94.72it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.82it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.00it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.13it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.75it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.99it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.70it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.01it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.09it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.64it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.82it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.11it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.02it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.76it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.10it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 137.81it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.38it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4257614612579346
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7727 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.8762281013561504
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.9755363179314205
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.960562332609287
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.5921970599131843
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8600039737344535
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4672, dtype=torch.float64), 0, 0, 0, tensor(0.5328, dtype=torch.float64), 0, 0, 32, 0, 0, 1, 1, 1, 128, 1.0376454246028031e-17, 1.4800000190734866, 0]
normalized proposed parameters for next round by BO: [tensor(3.9897e-17, dtype=torch.float64), tensor(9.2549e-18, dtype=torch.float64), tensor(0.4672, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.4376e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5328, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0376e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.467
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.533
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.0376454246028031e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.0376454246028031e-17
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.45it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.32it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 100.19it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.25it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.28it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.89it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 126.98it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.72it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.03it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 137.48it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.44it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 144.96it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 148.68it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 150.15it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.06it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.83it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 282/400 [00:02<00:00, 174.30it/s]Running loglikelihood requests:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 300/400 [00:02<00:00, 174.70it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.96it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 183.93it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 193.02it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.21it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.7728, 'grad_norm': 0.13586558401584625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.181796073913574, 'eval_runtime': 10.6554, 'eval_samples_per_second': 93.755, 'eval_steps_per_second': 5.912, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 55.76it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.58it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 101.45it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 108.90it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 113.63it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 125.87it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.27it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 136.94it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 136.97it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.40it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 139.94it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 146.83it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.17it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.02it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 155.40it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 155.80it/s]Running loglikelihood requests:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 279/400 [00:02<00:00, 165.63it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 170.87it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 180.63it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 184.89it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 195.15it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 152.78it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.6756, 'grad_norm': 0.14983989298343658, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 2.280456066131592, 'eval_runtime': 10.7136, 'eval_samples_per_second': 93.246, 'eval_steps_per_second': 5.88, 'epoch': 0.08}
{'loss': 2.1488, 'grad_norm': 0.05670357868075371, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0392260551452637, 'eval_runtime': 10.7128, 'eval_samples_per_second': 93.253, 'eval_steps_per_second': 5.881, 'epoch': 0.12}
{'loss': 1.9903, 'grad_norm': 0.08333101123571396, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9559657573699951, 'eval_runtime': 10.7587, 'eval_samples_per_second': 92.855, 'eval_steps_per_second': 5.856, 'epoch': 0.16}
{'loss': 1.9666, 'grad_norm': 0.06673047691583633, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.908753514289856, 'eval_runtime': 10.7515, 'eval_samples_per_second': 92.917, 'eval_steps_per_second': 5.86, 'epoch': 0.2}
{'loss': 1.9045, 'grad_norm': 0.052476707845926285, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8929234743118286, 'eval_runtime': 10.6858, 'eval_samples_per_second': 93.489, 'eval_steps_per_second': 5.896, 'epoch': 0.24}
{'loss': 1.8813, 'grad_norm': 0.05291886255145073, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.880513072013855, 'eval_runtime': 10.6876, 'eval_samples_per_second': 93.473, 'eval_steps_per_second': 5.895, 'epoch': 0.28}
{'loss': 1.8908, 'grad_norm': 0.050384968519210815, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8688310384750366, 'eval_runtime': 10.6865, 'eval_samples_per_second': 93.482, 'eval_steps_per_second': 5.895, 'epoch': 0.32}
{'loss': 1.8999, 'grad_norm': 0.049396269023418427, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.861072063446045, 'eval_runtime': 10.6796, 'eval_samples_per_second': 93.543, 'eval_steps_per_second': 5.899, 'epoch': 0.36}
{'loss': 1.8829, 'grad_norm': 0.05405525118112564, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8521124124526978, 'eval_runtime': 10.7191, 'eval_samples_per_second': 93.198, 'eval_steps_per_second': 5.877, 'epoch': 0.4}
{'loss': 1.8744, 'grad_norm': 0.05075300857424736, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8447033166885376, 'eval_runtime': 10.6939, 'eval_samples_per_second': 93.418, 'eval_steps_per_second': 5.891, 'epoch': 0.44}
{'loss': 1.878, 'grad_norm': 0.048682134598493576, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8388010263442993, 'eval_runtime': 10.6851, 'eval_samples_per_second': 93.494, 'eval_steps_per_second': 5.896, 'epoch': 0.48}
{'loss': 1.8115, 'grad_norm': 0.05797914043068886, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8330285549163818, 'eval_runtime': 10.6857, 'eval_samples_per_second': 93.489, 'eval_steps_per_second': 5.896, 'epoch': 0.52}
{'loss': 1.8198, 'grad_norm': 0.05900057405233383, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.830954909324646, 'eval_runtime': 10.6725, 'eval_samples_per_second': 93.605, 'eval_steps_per_second': 5.903, 'epoch': 0.56}
{'loss': 1.8926, 'grad_norm': 0.04140531271696091, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8260257244110107, 'eval_runtime': 10.6915, 'eval_samples_per_second': 93.439, 'eval_steps_per_second': 5.893, 'epoch': 0.6}
{'loss': 1.8483, 'grad_norm': 0.06536904722452164, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8214222192764282, 'eval_runtime': 10.7175, 'eval_samples_per_second': 93.212, 'eval_steps_per_second': 5.878, 'epoch': 0.64}
{'loss': 1.8448, 'grad_norm': 0.07835359871387482, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8182332515716553, 'eval_runtime': 10.679, 'eval_samples_per_second': 93.548, 'eval_steps_per_second': 5.899, 'epoch': 0.68}
{'loss': 1.826, 'grad_norm': 0.052326500415802, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8151189088821411, 'eval_runtime': 10.6864, 'eval_samples_per_second': 93.483, 'eval_steps_per_second': 5.895, 'epoch': 0.72}
{'loss': 1.8241, 'grad_norm': 0.07559838145971298, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8116282224655151, 'eval_runtime': 10.6815, 'eval_samples_per_second': 93.526, 'eval_steps_per_second': 5.898, 'epoch': 0.76}
{'loss': 1.8225, 'grad_norm': 0.07488881796598434, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8093068599700928, 'eval_runtime': 10.7156, 'eval_samples_per_second': 93.228, 'eval_steps_per_second': 5.879, 'epoch': 0.8}
{'loss': 1.8035, 'grad_norm': 0.07327442616224289, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8069255352020264, 'eval_runtime': 10.715, 'eval_samples_per_second': 93.233, 'eval_steps_per_second': 5.88, 'epoch': 0.84}
{'loss': 1.8291, 'grad_norm': 0.056838106364011765, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.805673599243164, 'eval_runtime': 10.6481, 'eval_samples_per_second': 93.819, 'eval_steps_per_second': 5.917, 'epoch': 0.88}
{'loss': 1.8548, 'grad_norm': 0.04786129668354988, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.803801417350769, 'eval_runtime': 10.6367, 'eval_samples_per_second': 93.92, 'eval_steps_per_second': 5.923, 'epoch': 0.92}
{'loss': 1.817, 'grad_norm': 0.04308009892702103, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8030552864074707, 'eval_runtime': 10.6341, 'eval_samples_per_second': 93.943, 'eval_steps_per_second': 5.924, 'epoch': 0.96}
{'loss': 1.8216, 'grad_norm': 0.055221207439899445, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8027231693267822, 'eval_runtime': 10.6298, 'eval_samples_per_second': 93.981, 'eval_steps_per_second': 5.927, 'epoch': 1.0}
{'train_runtime': 554.3461, 'train_samples_per_second': 18.037, 'train_steps_per_second': 1.127, 'train_loss': 1.98325703125, 'epoch': 1.0}
train_results:  {'eval_loss': [3.181796073913574, 2.280456066131592, 2.0392260551452637, 1.9559657573699951, 1.908753514289856, 1.8929234743118286, 1.880513072013855, 1.8688310384750366, 1.861072063446045, 1.8521124124526978, 1.8447033166885376, 1.8388010263442993, 1.8330285549163818, 1.830954909324646, 1.8260257244110107, 1.8214222192764282, 1.8182332515716553, 1.8151189088821411, 1.8116282224655151, 1.8093068599700928, 1.8069255352020264, 1.805673599243164, 1.803801417350769, 1.8030552864074707, 1.8027231693267822], 'performance': [0.48, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:37,  1.83it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 28.95it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 48.40it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 60.27it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 69.35it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 76.81it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 84.34it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 93.70it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.99it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 98.15it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 101.99it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 109.28it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 119.14it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 116.78it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.24it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 121.42it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 121.01it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 123.16it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.58it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.56it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 132.55it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 133.07it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 143.11it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 102.89it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.423555612564087
current iteration best possible performance (full train run):  0.399
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0194 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.971386764700003
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 1.1776677405733873
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.7763105548105915
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.7341337813767023
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.5361638825374898
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4662, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5338, dtype=torch.float64), 0, 32, 0, 1, 0, 0, 0, 128, 1.2059691520524628e-19, 1.480000019073487, 1]
normalized proposed parameters for next round by BO: [tensor(5.5759e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4662, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5920e-16, dtype=torch.float64), tensor(1.1558e-17, dtype=torch.float64), tensor(3.6717e-18, dtype=torch.float64), tensor(0.5338, dtype=torch.float64), tensor(1.3785e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.2060e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.466
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.534
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.2059691520524628e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  128
lora dropout:  1.2059691520524628e-19
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 67.64it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 107.90it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 121.16it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 130.97it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 137.21it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 151.97it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 153.48it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/400 [00:00<00:01, 165.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 167.30it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 184/400 [00:01<00:01, 169.80it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 180.18it/s]Running loglikelihood requests:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/400 [00:01<00:00, 182.40it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 187.78it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:01<00:00, 195.66it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:01<00:00, 207.93it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:01<00:00, 216.55it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 232.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 185.93it/s]
Evaluation performance at step 25: 0.5
{'loss': 3.8902, 'grad_norm': 0.2364446222782135, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.6693105697631836, 'eval_runtime': 9.0594, 'eval_samples_per_second': 110.272, 'eval_steps_per_second': 6.954, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:05, 67.33it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 108.09it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:02, 120.82it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 129.99it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 135.91it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 150.48it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 152.20it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 163.44it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 152/400 [00:01<00:01, 164.76it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 167.63it/s]Running loglikelihood requests:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [00:01<00:01, 177.82it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:00, 179.45it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 184.72it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 190.10it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:01<00:00, 202.90it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:01<00:00, 214.31it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 221.17it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 238.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 184.43it/s]
Evaluation performance at step 50: 0.53
{'loss': 3.0458, 'grad_norm': 0.10532286763191223, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 2.5676429271698, 'eval_runtime': 9.1028, 'eval_samples_per_second': 109.747, 'eval_steps_per_second': 6.921, 'epoch': 0.08}
{'loss': 2.3302, 'grad_norm': 0.06586206704378128, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.0798611640930176, 'eval_runtime': 9.1403, 'eval_samples_per_second': 109.296, 'eval_steps_per_second': 6.893, 'epoch': 0.12}
{'loss': 1.9696, 'grad_norm': 0.052911221981048584, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9235299825668335, 'eval_runtime': 9.2108, 'eval_samples_per_second': 108.46, 'eval_steps_per_second': 6.84, 'epoch': 0.16}
{'loss': 1.8935, 'grad_norm': 0.05487599968910217, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8389121294021606, 'eval_runtime': 9.1908, 'eval_samples_per_second': 108.696, 'eval_steps_per_second': 6.855, 'epoch': 0.2}
{'loss': 1.839, 'grad_norm': 0.06459036469459534, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.796521782875061, 'eval_runtime': 9.2058, 'eval_samples_per_second': 108.518, 'eval_steps_per_second': 6.844, 'epoch': 0.24}
{'loss': 1.7525, 'grad_norm': 0.0648277997970581, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7716628313064575, 'eval_runtime': 9.2565, 'eval_samples_per_second': 107.924, 'eval_steps_per_second': 6.806, 'epoch': 0.28}
{'loss': 1.7338, 'grad_norm': 0.0659128725528717, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7529733180999756, 'eval_runtime': 9.2322, 'eval_samples_per_second': 108.208, 'eval_steps_per_second': 6.824, 'epoch': 0.32}
{'loss': 1.7741, 'grad_norm': 0.08276691287755966, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7388602495193481, 'eval_runtime': 9.1954, 'eval_samples_per_second': 108.641, 'eval_steps_per_second': 6.851, 'epoch': 0.36}
{'loss': 1.7191, 'grad_norm': 0.06805187463760376, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7263439893722534, 'eval_runtime': 9.2111, 'eval_samples_per_second': 108.456, 'eval_steps_per_second': 6.84, 'epoch': 0.4}
{'loss': 1.7589, 'grad_norm': 0.07615072280168533, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7147897481918335, 'eval_runtime': 9.2012, 'eval_samples_per_second': 108.573, 'eval_steps_per_second': 6.847, 'epoch': 0.44}
{'loss': 1.7041, 'grad_norm': 0.07275467365980148, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7055717706680298, 'eval_runtime': 9.1927, 'eval_samples_per_second': 108.673, 'eval_steps_per_second': 6.853, 'epoch': 0.48}
{'loss': 1.7236, 'grad_norm': 0.06449234485626221, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7006423473358154, 'eval_runtime': 9.1935, 'eval_samples_per_second': 108.663, 'eval_steps_per_second': 6.853, 'epoch': 0.52}
{'loss': 1.6612, 'grad_norm': 0.06463861465454102, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6947181224822998, 'eval_runtime': 9.2166, 'eval_samples_per_second': 108.391, 'eval_steps_per_second': 6.835, 'epoch': 0.56}
{'loss': 1.6524, 'grad_norm': 0.05927686393260956, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6896181106567383, 'eval_runtime': 9.2072, 'eval_samples_per_second': 108.502, 'eval_steps_per_second': 6.842, 'epoch': 0.6}
{'loss': 1.7229, 'grad_norm': 0.060576461255550385, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6853426694869995, 'eval_runtime': 9.2039, 'eval_samples_per_second': 108.541, 'eval_steps_per_second': 6.845, 'epoch': 0.64}
{'loss': 1.6855, 'grad_norm': 0.06362927705049515, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6813148260116577, 'eval_runtime': 9.1999, 'eval_samples_per_second': 108.588, 'eval_steps_per_second': 6.848, 'epoch': 0.68}
{'loss': 1.6842, 'grad_norm': 0.059495434165000916, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6786640882492065, 'eval_runtime': 9.1889, 'eval_samples_per_second': 108.718, 'eval_steps_per_second': 6.856, 'epoch': 0.72}
{'loss': 1.6986, 'grad_norm': 0.060989830642938614, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6755326986312866, 'eval_runtime': 9.2069, 'eval_samples_per_second': 108.505, 'eval_steps_per_second': 6.843, 'epoch': 0.76}
{'loss': 1.6619, 'grad_norm': 0.06358005851507187, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.672790765762329, 'eval_runtime': 9.2001, 'eval_samples_per_second': 108.586, 'eval_steps_per_second': 6.848, 'epoch': 0.8}
{'loss': 1.72, 'grad_norm': 0.07986459136009216, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.670088768005371, 'eval_runtime': 9.182, 'eval_samples_per_second': 108.8, 'eval_steps_per_second': 6.861, 'epoch': 0.84}
{'loss': 1.6873, 'grad_norm': 0.09178332984447479, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6681889295578003, 'eval_runtime': 9.2035, 'eval_samples_per_second': 108.546, 'eval_steps_per_second': 6.845, 'epoch': 0.88}
{'loss': 1.6764, 'grad_norm': 0.06288225948810577, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.666731595993042, 'eval_runtime': 9.1999, 'eval_samples_per_second': 108.588, 'eval_steps_per_second': 6.848, 'epoch': 0.92}
{'loss': 1.7205, 'grad_norm': 0.06944787502288818, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6656798124313354, 'eval_runtime': 9.1951, 'eval_samples_per_second': 108.645, 'eval_steps_per_second': 6.851, 'epoch': 0.96}
{'loss': 1.6826, 'grad_norm': 0.05608212947845459, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6654239892959595, 'eval_runtime': 9.1855, 'eval_samples_per_second': 108.759, 'eval_steps_per_second': 6.859, 'epoch': 1.0}
{'train_runtime': 468.9844, 'train_samples_per_second': 21.321, 'train_steps_per_second': 1.333, 'train_loss': 1.8955105407714843, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6693105697631836, 2.5676429271698, 2.0798611640930176, 1.9235299825668335, 1.8389121294021606, 1.796521782875061, 1.7716628313064575, 1.7529733180999756, 1.7388602495193481, 1.7263439893722534, 1.7147897481918335, 1.7055717706680298, 1.7006423473358154, 1.6947181224822998, 1.6896181106567383, 1.6853426694869995, 1.6813148260116577, 1.6786640882492065, 1.6755326986312866, 1.672790765762329, 1.670088768005371, 1.6681889295578003, 1.666731595993042, 1.6656798124313354, 1.6654239892959595], 'performance': [0.5, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:21,  2.82it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:10, 36.19it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 59.10it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 72.25it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 82.18it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 90.48it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 98.92it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 109.63it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 112.27it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 114.68it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 119.26it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 127.23it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 139.14it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 137.14it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 142.94it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 141.58it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 141.17it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 143.95it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 146.31it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 155.29it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 155.19it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 155.18it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:03<00:00, 189.18it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 122.30it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4009408950805664
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4815 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.8435566188270887
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.9669292645089815
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.94655354454849
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.6087019667608263
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.9884515984091784
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4548, dtype=torch.float64), tensor(0.5452, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 1.537975277757075e-19, 1.4800000190734885, 0]
normalized proposed parameters for next round by BO: [tensor(3.0301e-17, dtype=torch.float64), tensor(8.5747e-17, dtype=torch.float64), tensor(0.4548, dtype=torch.float64), tensor(0.5452, dtype=torch.float64), tensor(3.5646e-17, dtype=torch.float64), tensor(1.1259e-16, dtype=torch.float64), tensor(1.5947e-17, dtype=torch.float64), tensor(1.5923e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.5380e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.455
  sciq: 0.545
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.537975277757075e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.537975277757075e-19
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.09it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.97it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.15it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.71it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.54it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.82it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.95it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.47it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.46it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.96it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.61it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.38it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.68it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.63it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.09it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.72it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.08it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.14it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.06it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.17it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.96it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.3328, 'grad_norm': 0.20665201544761658, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.342392683029175, 'eval_runtime': 10.656, 'eval_samples_per_second': 93.75, 'eval_steps_per_second': 5.912, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 56.12it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 90.95it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 102.10it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 109.71it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 114.44it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 126.63it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 127.83it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 137.39it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 137.41it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 138.97it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 140.72it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 147.36it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 150.62it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 151.59it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 156.15it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 156.82it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/400 [00:02<00:00, 164.27it/s]Running loglikelihood requests:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 314/400 [00:02<00:00, 175.31it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 182.16it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 195.19it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 200.75it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 153.97it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.5525, 'grad_norm': 0.17190393805503845, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 1.9461690187454224, 'eval_runtime': 10.6202, 'eval_samples_per_second': 94.066, 'eval_steps_per_second': 5.932, 'epoch': 0.08}
{'loss': 1.7681, 'grad_norm': 0.11089327186346054, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6615749597549438, 'eval_runtime': 10.6227, 'eval_samples_per_second': 94.044, 'eval_steps_per_second': 5.931, 'epoch': 0.12}
{'loss': 1.5922, 'grad_norm': 0.05138364061713219, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5532302856445312, 'eval_runtime': 10.6498, 'eval_samples_per_second': 93.805, 'eval_steps_per_second': 5.916, 'epoch': 0.16}
{'loss': 1.5103, 'grad_norm': 0.048626743257045746, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5276775360107422, 'eval_runtime': 10.661, 'eval_samples_per_second': 93.706, 'eval_steps_per_second': 5.909, 'epoch': 0.2}
{'loss': 1.5552, 'grad_norm': 0.038402579724788666, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5141152143478394, 'eval_runtime': 10.6595, 'eval_samples_per_second': 93.719, 'eval_steps_per_second': 5.91, 'epoch': 0.24}
{'loss': 1.5163, 'grad_norm': 0.055560365319252014, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5059715509414673, 'eval_runtime': 10.7226, 'eval_samples_per_second': 93.168, 'eval_steps_per_second': 5.875, 'epoch': 0.28}
{'loss': 1.4858, 'grad_norm': 0.03457121178507805, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4967772960662842, 'eval_runtime': 10.8002, 'eval_samples_per_second': 92.498, 'eval_steps_per_second': 5.833, 'epoch': 0.32}
{'loss': 1.4947, 'grad_norm': 0.04834350571036339, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4934223890304565, 'eval_runtime': 10.7578, 'eval_samples_per_second': 92.863, 'eval_steps_per_second': 5.856, 'epoch': 0.36}
{'loss': 1.4935, 'grad_norm': 0.03898047283291817, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4838849306106567, 'eval_runtime': 10.7845, 'eval_samples_per_second': 92.633, 'eval_steps_per_second': 5.842, 'epoch': 0.4}
{'loss': 1.5095, 'grad_norm': 0.040114909410476685, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.478173851966858, 'eval_runtime': 10.8188, 'eval_samples_per_second': 92.339, 'eval_steps_per_second': 5.823, 'epoch': 0.44}
{'loss': 1.4698, 'grad_norm': 0.04050833359360695, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4727503061294556, 'eval_runtime': 10.8211, 'eval_samples_per_second': 92.32, 'eval_steps_per_second': 5.822, 'epoch': 0.48}
{'loss': 1.5185, 'grad_norm': 0.045542310923337936, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4722200632095337, 'eval_runtime': 10.8182, 'eval_samples_per_second': 92.344, 'eval_steps_per_second': 5.824, 'epoch': 0.52}
{'loss': 1.4977, 'grad_norm': 0.04419508948922157, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.466688632965088, 'eval_runtime': 10.7664, 'eval_samples_per_second': 92.789, 'eval_steps_per_second': 5.852, 'epoch': 0.56}
{'loss': 1.5113, 'grad_norm': 0.03732556104660034, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4620932340621948, 'eval_runtime': 10.7395, 'eval_samples_per_second': 93.021, 'eval_steps_per_second': 5.866, 'epoch': 0.6}
{'loss': 1.5145, 'grad_norm': 0.03901500999927521, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4590237140655518, 'eval_runtime': 10.7573, 'eval_samples_per_second': 92.867, 'eval_steps_per_second': 5.856, 'epoch': 0.64}
{'loss': 1.4716, 'grad_norm': 0.05012078583240509, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4569405317306519, 'eval_runtime': 10.7512, 'eval_samples_per_second': 92.92, 'eval_steps_per_second': 5.86, 'epoch': 0.68}
{'loss': 1.4825, 'grad_norm': 0.049948740750551224, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4535342454910278, 'eval_runtime': 10.7453, 'eval_samples_per_second': 92.97, 'eval_steps_per_second': 5.863, 'epoch': 0.72}
{'loss': 1.4993, 'grad_norm': 0.04563348740339279, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.450716257095337, 'eval_runtime': 10.7508, 'eval_samples_per_second': 92.923, 'eval_steps_per_second': 5.86, 'epoch': 0.76}
{'loss': 1.5167, 'grad_norm': 0.04475580155849457, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4489264488220215, 'eval_runtime': 10.712, 'eval_samples_per_second': 93.26, 'eval_steps_per_second': 5.881, 'epoch': 0.8}
{'loss': 1.4724, 'grad_norm': 0.04550759494304657, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4470804929733276, 'eval_runtime': 10.7455, 'eval_samples_per_second': 92.969, 'eval_steps_per_second': 5.863, 'epoch': 0.84}
{'loss': 1.5049, 'grad_norm': 0.04598510265350342, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4458881616592407, 'eval_runtime': 10.7361, 'eval_samples_per_second': 93.051, 'eval_steps_per_second': 5.868, 'epoch': 0.88}
{'loss': 1.4859, 'grad_norm': 0.04023310914635658, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4447575807571411, 'eval_runtime': 10.75, 'eval_samples_per_second': 92.93, 'eval_steps_per_second': 5.86, 'epoch': 0.92}
{'loss': 1.4868, 'grad_norm': 0.04205229505896568, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4441921710968018, 'eval_runtime': 10.7386, 'eval_samples_per_second': 93.029, 'eval_steps_per_second': 5.867, 'epoch': 0.96}
{'loss': 1.4775, 'grad_norm': 0.04772947356104851, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4439771175384521, 'eval_runtime': 10.7385, 'eval_samples_per_second': 93.03, 'eval_steps_per_second': 5.867, 'epoch': 1.0}
{'train_runtime': 548.2926, 'train_samples_per_second': 18.237, 'train_steps_per_second': 1.14, 'train_loss': 1.6688173278808593, 'epoch': 1.0}
train_results:  {'eval_loss': [3.342392683029175, 1.9461690187454224, 1.6615749597549438, 1.5532302856445312, 1.5276775360107422, 1.5141152143478394, 1.5059715509414673, 1.4967772960662842, 1.4934223890304565, 1.4838849306106567, 1.478173851966858, 1.4727503061294556, 1.4722200632095337, 1.466688632965088, 1.4620932340621948, 1.4590237140655518, 1.4569405317306519, 1.4535342454910278, 1.450716257095337, 1.4489264488220215, 1.4470804929733276, 1.4458881616592407, 1.4447575807571411, 1.4441921710968018, 1.4439771175384521], 'performance': [0.48, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:58,  3.38it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:09, 41.47it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:06, 60.77it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 70.01it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 76.59it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 82.00it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 88.40it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 96.77it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 98.24it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 99.67it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 103.06it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 109.75it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 119.44it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 117.21it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 122.12it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 120.78it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 120.32it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 122.73it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 125.00it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 132.18it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 131.82it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 132.23it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 142.26it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 109.45it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4250919818878174
current iteration best possible performance (full train run):  0.525
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4153 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.673101950886007
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 1.2048696177470013
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 1.2153050242027774
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.5840541110498556
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.8140825126330797
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4606, dtype=torch.float64), tensor(0.4422, dtype=torch.float64), tensor(0.0972, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 0, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.4606, dtype=torch.float64), tensor(0.4422, dtype=torch.float64), tensor(0.0972, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.9837e-17, dtype=torch.float64), tensor(3.7525e-18, dtype=torch.float64), tensor(8.2916e-19, dtype=torch.float64), tensor(4.5984e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.461
  rowan_hellaswag: 0.442
  sciq: 0.097
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 64.81it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 104.68it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 117.66it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 126.24it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 132.15it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 146.28it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 147.69it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 158.71it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñã      | 145/400 [00:01<00:01, 162.04it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 161.58it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 169.35it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 174.11it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 179.46it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 181.57it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 196.40it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 199.84it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 215.13it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 225.87it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 179.25it/s]
Evaluation performance at step 25: 0.49
{'loss': 3.4402, 'grad_norm': 0.07710044831037521, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 2.9950168132781982, 'eval_runtime': 9.4317, 'eval_samples_per_second': 105.919, 'eval_steps_per_second': 6.68, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 64.85it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 104.63it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 117.76it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:02, 126.55it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 131.64it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 146.18it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:01, 147.76it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:00<00:01, 158.56it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñã      | 146/400 [00:01<00:01, 164.85it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 168/400 [00:01<00:01, 160.48it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 168.63it/s]Running loglikelihood requests:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 219/400 [00:01<00:01, 173.72it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:00, 179.02it/s]Running loglikelihood requests:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/400 [00:01<00:00, 181.29it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:01<00:00, 196.15it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:01<00:00, 198.99it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 213.90it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 224.69it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 178.93it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.4189, 'grad_norm': 0.05747893452644348, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 2.008559226989746, 'eval_runtime': 9.4373, 'eval_samples_per_second': 105.857, 'eval_steps_per_second': 6.676, 'epoch': 0.08}
{'loss': 1.7999, 'grad_norm': 0.06579998880624771, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.639294981956482, 'eval_runtime': 9.4639, 'eval_samples_per_second': 105.559, 'eval_steps_per_second': 6.657, 'epoch': 0.12}
{'loss': 1.6088, 'grad_norm': 0.052519429475069046, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5328404903411865, 'eval_runtime': 9.4804, 'eval_samples_per_second': 105.376, 'eval_steps_per_second': 6.645, 'epoch': 0.16}
{'loss': 1.5434, 'grad_norm': 0.03432678431272507, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4912889003753662, 'eval_runtime': 9.5092, 'eval_samples_per_second': 105.056, 'eval_steps_per_second': 6.625, 'epoch': 0.2}
{'loss': 1.4851, 'grad_norm': 0.0306035615503788, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4697480201721191, 'eval_runtime': 9.5593, 'eval_samples_per_second': 104.505, 'eval_steps_per_second': 6.59, 'epoch': 0.24}
{'loss': 1.5225, 'grad_norm': 0.031733546406030655, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4573626518249512, 'eval_runtime': 9.6135, 'eval_samples_per_second': 103.916, 'eval_steps_per_second': 6.553, 'epoch': 0.28}
{'loss': 1.4631, 'grad_norm': 0.032670244574546814, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4451690912246704, 'eval_runtime': 9.6427, 'eval_samples_per_second': 103.602, 'eval_steps_per_second': 6.533, 'epoch': 0.32}
{'loss': 1.4539, 'grad_norm': 0.03720399737358093, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.436269998550415, 'eval_runtime': 9.6428, 'eval_samples_per_second': 103.6, 'eval_steps_per_second': 6.533, 'epoch': 0.36}
{'loss': 1.4526, 'grad_norm': 0.03279097378253937, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4269042015075684, 'eval_runtime': 9.6316, 'eval_samples_per_second': 103.721, 'eval_steps_per_second': 6.541, 'epoch': 0.4}
{'loss': 1.3917, 'grad_norm': 0.03911717236042023, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4195020198822021, 'eval_runtime': 9.6206, 'eval_samples_per_second': 103.839, 'eval_steps_per_second': 6.548, 'epoch': 0.44}
{'loss': 1.41, 'grad_norm': 0.03680017963051796, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4116257429122925, 'eval_runtime': 9.5869, 'eval_samples_per_second': 104.204, 'eval_steps_per_second': 6.571, 'epoch': 0.48}
{'loss': 1.4205, 'grad_norm': 0.032359398901462555, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4055119752883911, 'eval_runtime': 9.6096, 'eval_samples_per_second': 103.958, 'eval_steps_per_second': 6.556, 'epoch': 0.52}
{'loss': 1.4417, 'grad_norm': 0.03632929176092148, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3984196186065674, 'eval_runtime': 9.6112, 'eval_samples_per_second': 103.942, 'eval_steps_per_second': 6.555, 'epoch': 0.56}
{'loss': 1.3769, 'grad_norm': 0.03977148234844208, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3912477493286133, 'eval_runtime': 9.5934, 'eval_samples_per_second': 104.134, 'eval_steps_per_second': 6.567, 'epoch': 0.6}
{'loss': 1.3567, 'grad_norm': 0.042744044214487076, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3832765817642212, 'eval_runtime': 9.5871, 'eval_samples_per_second': 104.203, 'eval_steps_per_second': 6.571, 'epoch': 0.64}
{'loss': 1.4245, 'grad_norm': 0.04693850874900818, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.374348521232605, 'eval_runtime': 9.6134, 'eval_samples_per_second': 103.918, 'eval_steps_per_second': 6.553, 'epoch': 0.68}
{'loss': 1.393, 'grad_norm': 0.04546051844954491, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3630160093307495, 'eval_runtime': 9.6436, 'eval_samples_per_second': 103.592, 'eval_steps_per_second': 6.533, 'epoch': 0.72}
{'loss': 1.3678, 'grad_norm': 0.03980032727122307, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3508286476135254, 'eval_runtime': 9.6323, 'eval_samples_per_second': 103.714, 'eval_steps_per_second': 6.54, 'epoch': 0.76}
{'loss': 1.3486, 'grad_norm': 0.04007875546813011, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3412662744522095, 'eval_runtime': 9.5812, 'eval_samples_per_second': 104.267, 'eval_steps_per_second': 6.575, 'epoch': 0.8}
{'loss': 1.3337, 'grad_norm': 0.041197698563337326, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.336707592010498, 'eval_runtime': 9.5852, 'eval_samples_per_second': 104.223, 'eval_steps_per_second': 6.573, 'epoch': 0.84}
{'loss': 1.3387, 'grad_norm': 0.045925360172986984, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3346518278121948, 'eval_runtime': 9.5623, 'eval_samples_per_second': 104.473, 'eval_steps_per_second': 6.588, 'epoch': 0.88}
{'loss': 1.3439, 'grad_norm': 0.0424727164208889, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3328903913497925, 'eval_runtime': 9.5717, 'eval_samples_per_second': 104.37, 'eval_steps_per_second': 6.582, 'epoch': 0.92}
{'loss': 1.3063, 'grad_norm': 0.035280853509902954, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.331508755683899, 'eval_runtime': 9.5824, 'eval_samples_per_second': 104.253, 'eval_steps_per_second': 6.575, 'epoch': 0.96}
{'loss': 1.3225, 'grad_norm': 0.044096220284700394, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3311827182769775, 'eval_runtime': 9.5634, 'eval_samples_per_second': 104.461, 'eval_steps_per_second': 6.588, 'epoch': 1.0}
{'train_runtime': 475.2373, 'train_samples_per_second': 21.04, 'train_steps_per_second': 1.315, 'train_loss': 1.55058818359375, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9950168132781982, 2.008559226989746, 1.639294981956482, 1.5328404903411865, 1.4912889003753662, 1.4697480201721191, 1.4573626518249512, 1.4451690912246704, 1.436269998550415, 1.4269042015075684, 1.4195020198822021, 1.4116257429122925, 1.4055119752883911, 1.3984196186065674, 1.3912477493286133, 1.3832765817642212, 1.374348521232605, 1.3630160093307495, 1.3508286476135254, 1.3412662744522095, 1.336707592010498, 1.3346518278121948, 1.3328903913497925, 1.331508755683899, 1.3311827182769775], 'performance': [0.49, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:45,  3.80it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 46.84it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 68.77it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 79.19it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:03, 86.92it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 93.69it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:02, 100.80it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 110.19it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 111.55it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 112.90it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:01, 116.66it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:01<00:01, 124.76it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 136.05it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 133.92it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 139.27it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 137.68it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:00, 136.96it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 139.35it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:02<00:00, 141.83it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:02<00:00, 150.39it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:02<00:00, 150.13it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 150.90it/s]Running loglikelihood requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 387/400 [00:03<00:00, 186.77it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 124.47it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.412306308746338
current iteration best possible performance (full train run):  0.462
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3767 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.9392612855700171
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.6649103149930244
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.793816786066656
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 1.0697365259842195
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.7926659055189886
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4506, dtype=torch.float64), 0, 0, 0, tensor(0.5494, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734914, 0]
normalized proposed parameters for next round by BO: [tensor(1.6686e-17, dtype=torch.float64), tensor(1.1843e-17, dtype=torch.float64), tensor(0.4506, dtype=torch.float64), tensor(1.2322e-16, dtype=torch.float64), tensor(5.0091e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5494, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1559e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.451
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.549
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734914,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734914
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.12it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.66it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.50it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.85it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.44it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.42it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.69it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.99it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.98it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.28it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.72it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.14it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.54it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.56it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.05it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.74it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.92it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.25it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.57it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.60it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.29it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.18it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.7417, 'grad_norm': 0.12016753107309341, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.176034927368164, 'eval_runtime': 10.8755, 'eval_samples_per_second': 91.858, 'eval_steps_per_second': 5.793, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.35it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.15it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 99.07it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.37it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.91it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.78it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.94it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.11it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.00it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.29it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.70it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.06it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.18it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.07it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.50it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.13it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.14it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.39it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.54it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.30it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.03it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.40it/s]
Evaluation performance at step 50: 0.49
{'loss': 2.6534, 'grad_norm': 0.16137506067752838, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 2.2824912071228027, 'eval_runtime': 10.9998, 'eval_samples_per_second': 90.82, 'eval_steps_per_second': 5.727, 'epoch': 0.08}
{'loss': 2.1282, 'grad_norm': 0.05662209540605545, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.042158365249634, 'eval_runtime': 10.9996, 'eval_samples_per_second': 90.822, 'eval_steps_per_second': 5.727, 'epoch': 0.12}
{'loss': 2.0281, 'grad_norm': 0.04351715371012688, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.958951473236084, 'eval_runtime': 11.0577, 'eval_samples_per_second': 90.344, 'eval_steps_per_second': 5.697, 'epoch': 0.16}
{'loss': 1.9511, 'grad_norm': 0.055365823209285736, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9087812900543213, 'eval_runtime': 11.092, 'eval_samples_per_second': 90.065, 'eval_steps_per_second': 5.68, 'epoch': 0.2}
{'loss': 1.9252, 'grad_norm': 0.050289519131183624, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8908867835998535, 'eval_runtime': 11.0643, 'eval_samples_per_second': 90.29, 'eval_steps_per_second': 5.694, 'epoch': 0.24}
{'loss': 1.9081, 'grad_norm': 0.0503595769405365, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8794654607772827, 'eval_runtime': 11.0821, 'eval_samples_per_second': 90.145, 'eval_steps_per_second': 5.685, 'epoch': 0.28}
{'loss': 1.8938, 'grad_norm': 0.05829862132668495, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8703064918518066, 'eval_runtime': 11.0207, 'eval_samples_per_second': 90.647, 'eval_steps_per_second': 5.716, 'epoch': 0.32}
{'loss': 1.9059, 'grad_norm': 0.0882015973329544, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8602149486541748, 'eval_runtime': 10.9983, 'eval_samples_per_second': 90.832, 'eval_steps_per_second': 5.728, 'epoch': 0.36}
{'loss': 1.8786, 'grad_norm': 0.046574916690588, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8514642715454102, 'eval_runtime': 10.9952, 'eval_samples_per_second': 90.857, 'eval_steps_per_second': 5.73, 'epoch': 0.4}
{'loss': 1.8386, 'grad_norm': 0.05537677928805351, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8445510864257812, 'eval_runtime': 10.995, 'eval_samples_per_second': 90.86, 'eval_steps_per_second': 5.73, 'epoch': 0.44}
{'loss': 1.8658, 'grad_norm': 0.061856914311647415, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8397424221038818, 'eval_runtime': 11.0, 'eval_samples_per_second': 90.818, 'eval_steps_per_second': 5.727, 'epoch': 0.48}
{'loss': 1.8557, 'grad_norm': 0.13511741161346436, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.833400011062622, 'eval_runtime': 11.002, 'eval_samples_per_second': 90.802, 'eval_steps_per_second': 5.726, 'epoch': 0.52}
{'loss': 1.8526, 'grad_norm': 0.06910621374845505, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8291971683502197, 'eval_runtime': 10.9979, 'eval_samples_per_second': 90.836, 'eval_steps_per_second': 5.728, 'epoch': 0.56}
{'loss': 1.8604, 'grad_norm': 0.05180072784423828, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8256950378417969, 'eval_runtime': 10.9932, 'eval_samples_per_second': 90.875, 'eval_steps_per_second': 5.731, 'epoch': 0.6}
{'loss': 1.853, 'grad_norm': 0.05690065026283264, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8215818405151367, 'eval_runtime': 10.988, 'eval_samples_per_second': 90.918, 'eval_steps_per_second': 5.734, 'epoch': 0.64}
{'loss': 1.8383, 'grad_norm': 0.0765896588563919, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8185909986495972, 'eval_runtime': 10.9831, 'eval_samples_per_second': 90.958, 'eval_steps_per_second': 5.736, 'epoch': 0.68}
{'loss': 1.8255, 'grad_norm': 0.05779816955327988, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8139140605926514, 'eval_runtime': 10.9665, 'eval_samples_per_second': 91.095, 'eval_steps_per_second': 5.745, 'epoch': 0.72}
{'loss': 1.8514, 'grad_norm': 0.05082351341843605, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8112328052520752, 'eval_runtime': 10.9442, 'eval_samples_per_second': 91.281, 'eval_steps_per_second': 5.756, 'epoch': 0.76}
{'loss': 1.8246, 'grad_norm': 0.08321253210306168, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.8085603713989258, 'eval_runtime': 10.9371, 'eval_samples_per_second': 91.341, 'eval_steps_per_second': 5.76, 'epoch': 0.8}
{'loss': 1.8371, 'grad_norm': 0.05015510320663452, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8069109916687012, 'eval_runtime': 10.9497, 'eval_samples_per_second': 91.235, 'eval_steps_per_second': 5.754, 'epoch': 0.84}
{'loss': 1.8437, 'grad_norm': 0.06474368274211884, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.805256962776184, 'eval_runtime': 10.969, 'eval_samples_per_second': 91.075, 'eval_steps_per_second': 5.743, 'epoch': 0.88}
{'loss': 1.8505, 'grad_norm': 0.04615809768438339, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8037793636322021, 'eval_runtime': 10.9909, 'eval_samples_per_second': 90.894, 'eval_steps_per_second': 5.732, 'epoch': 0.92}
{'loss': 1.8357, 'grad_norm': 0.04344502463936806, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.802821159362793, 'eval_runtime': 11.0012, 'eval_samples_per_second': 90.809, 'eval_steps_per_second': 5.727, 'epoch': 0.96}
{'loss': 1.8428, 'grad_norm': 0.052711352705955505, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.80268132686615, 'eval_runtime': 11.0095, 'eval_samples_per_second': 90.74, 'eval_steps_per_second': 5.722, 'epoch': 1.0}
{'train_runtime': 556.5741, 'train_samples_per_second': 17.965, 'train_steps_per_second': 1.123, 'train_loss': 1.9875991760253906, 'epoch': 1.0}
train_results:  {'eval_loss': [3.176034927368164, 2.2824912071228027, 2.042158365249634, 1.958951473236084, 1.9087812900543213, 1.8908867835998535, 1.8794654607772827, 1.8703064918518066, 1.8602149486541748, 1.8514642715454102, 1.8445510864257812, 1.8397424221038818, 1.833400011062622, 1.8291971683502197, 1.8256950378417969, 1.8215818405151367, 1.8185909986495972, 1.8139140605926514, 1.8112328052520752, 1.8085603713989258, 1.8069109916687012, 1.805256962776184, 1.8037793636322021, 1.802821159362793, 1.80268132686615], 'performance': [0.48, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.61it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 28.15it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 46.86it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 58.07it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 66.89it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 74.15it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 81.56it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 90.40it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 92.73it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 94.79it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.60it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.59it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.27it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.81it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.63it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.35it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.84it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 119.04it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.08it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.30it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.28it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.49it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 137.94it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 102.11it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.4260348081588745
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1993 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.9551566699473569
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.6097205838768072
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.7779586156558514
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.7435100698986028
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 1.1081398784429701
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.5466, dtype=torch.float64), tensor(0.4534, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 3.212121977949959e-18, 1.4800000190734903, 0]
normalized proposed parameters for next round by BO: [tensor(3.0590e-17, dtype=torch.float64), tensor(0.5466, dtype=torch.float64), tensor(0.4534, dtype=torch.float64), tensor(1.0216e-17, dtype=torch.float64), tensor(6.3866e-18, dtype=torch.float64), tensor(4.6201e-18, dtype=torch.float64), tensor(3.3614e-05, dtype=torch.float64), tensor(8.2985e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.2121e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.547
  rowan_hellaswag: 0.453
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.212121977949959e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734903,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  3.212121977949959e-18
lora alpha:  1.4800000190734903
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.24it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.02it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.89it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.28it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.75it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.49it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.51it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.65it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.65it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.96it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.51it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.98it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.06it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.91it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.24it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.76it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.78it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.15it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.57it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.69it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.24it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.207, 'grad_norm': 0.14968186616897583, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 2.472046136856079, 'eval_runtime': 11.099, 'eval_samples_per_second': 90.008, 'eval_steps_per_second': 5.676, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.71it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 86.98it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 97.66it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 104.96it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 109.61it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.38it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.65it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.74it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.74it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.15it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.63it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.04it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.20it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.13it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.22it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 149.76it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 153.81it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.04it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.56it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 180.50it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 187.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.01it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.0871, 'grad_norm': 0.12034955620765686, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 1.626658320426941, 'eval_runtime': 11.1694, 'eval_samples_per_second': 89.44, 'eval_steps_per_second': 5.64, 'epoch': 0.08}
{'loss': 1.4843, 'grad_norm': 0.04361123964190483, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3872466087341309, 'eval_runtime': 11.249, 'eval_samples_per_second': 88.808, 'eval_steps_per_second': 5.601, 'epoch': 0.12}
{'loss': 1.4106, 'grad_norm': 0.03623184934258461, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3114129304885864, 'eval_runtime': 11.2527, 'eval_samples_per_second': 88.779, 'eval_steps_per_second': 5.599, 'epoch': 0.16}
{'loss': 1.3404, 'grad_norm': 0.03046458214521408, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2924596071243286, 'eval_runtime': 11.2474, 'eval_samples_per_second': 88.82, 'eval_steps_per_second': 5.601, 'epoch': 0.2}
{'loss': 1.3239, 'grad_norm': 0.03688505291938782, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2790464162826538, 'eval_runtime': 11.2255, 'eval_samples_per_second': 88.994, 'eval_steps_per_second': 5.612, 'epoch': 0.24}
{'loss': 1.313, 'grad_norm': 0.03412877768278122, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2700163125991821, 'eval_runtime': 11.254, 'eval_samples_per_second': 88.769, 'eval_steps_per_second': 5.598, 'epoch': 0.28}
{'loss': 1.3074, 'grad_norm': 0.033579982817173004, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2624744176864624, 'eval_runtime': 11.2711, 'eval_samples_per_second': 88.634, 'eval_steps_per_second': 5.59, 'epoch': 0.32}
{'loss': 1.3008, 'grad_norm': 0.03538210690021515, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2567616701126099, 'eval_runtime': 11.2399, 'eval_samples_per_second': 88.88, 'eval_steps_per_second': 5.605, 'epoch': 0.36}
{'loss': 1.2998, 'grad_norm': 0.03723687678575516, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.251139521598816, 'eval_runtime': 11.2394, 'eval_samples_per_second': 88.883, 'eval_steps_per_second': 5.605, 'epoch': 0.4}
{'loss': 1.2957, 'grad_norm': 0.03269675746560097, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2471145391464233, 'eval_runtime': 11.252, 'eval_samples_per_second': 88.784, 'eval_steps_per_second': 5.599, 'epoch': 0.44}
{'loss': 1.2541, 'grad_norm': 0.037838131189346313, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2422480583190918, 'eval_runtime': 11.2337, 'eval_samples_per_second': 88.929, 'eval_steps_per_second': 5.608, 'epoch': 0.48}
{'loss': 1.286, 'grad_norm': 0.032923709601163864, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2383780479431152, 'eval_runtime': 11.2282, 'eval_samples_per_second': 88.972, 'eval_steps_per_second': 5.611, 'epoch': 0.52}
{'loss': 1.3426, 'grad_norm': 0.03198512643575668, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.23543119430542, 'eval_runtime': 11.2122, 'eval_samples_per_second': 89.099, 'eval_steps_per_second': 5.619, 'epoch': 0.56}
{'loss': 1.2681, 'grad_norm': 0.041704703122377396, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.231912612915039, 'eval_runtime': 11.2238, 'eval_samples_per_second': 89.007, 'eval_steps_per_second': 5.613, 'epoch': 0.6}
{'loss': 1.2663, 'grad_norm': 0.04400169104337692, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.229828119277954, 'eval_runtime': 11.2204, 'eval_samples_per_second': 89.035, 'eval_steps_per_second': 5.615, 'epoch': 0.64}
{'loss': 1.2662, 'grad_norm': 0.035543788224458694, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2259122133255005, 'eval_runtime': 11.2162, 'eval_samples_per_second': 89.067, 'eval_steps_per_second': 5.617, 'epoch': 0.68}
{'loss': 1.2919, 'grad_norm': 0.036535151302814484, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2235597372055054, 'eval_runtime': 11.1408, 'eval_samples_per_second': 89.671, 'eval_steps_per_second': 5.655, 'epoch': 0.72}
{'loss': 1.2306, 'grad_norm': 0.038894735276699066, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.220749020576477, 'eval_runtime': 11.1483, 'eval_samples_per_second': 89.61, 'eval_steps_per_second': 5.651, 'epoch': 0.76}
{'loss': 1.2342, 'grad_norm': 0.035737160593271255, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2194679975509644, 'eval_runtime': 11.1644, 'eval_samples_per_second': 89.481, 'eval_steps_per_second': 5.643, 'epoch': 0.8}
{'loss': 1.3105, 'grad_norm': 0.0348249189555645, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2174897193908691, 'eval_runtime': 11.1334, 'eval_samples_per_second': 89.73, 'eval_steps_per_second': 5.659, 'epoch': 0.84}
{'loss': 1.261, 'grad_norm': 0.038746803998947144, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2157880067825317, 'eval_runtime': 11.1235, 'eval_samples_per_second': 89.81, 'eval_steps_per_second': 5.664, 'epoch': 0.88}
{'loss': 1.2634, 'grad_norm': 0.03884868323802948, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2148305177688599, 'eval_runtime': 11.1159, 'eval_samples_per_second': 89.872, 'eval_steps_per_second': 5.668, 'epoch': 0.92}
{'loss': 1.257, 'grad_norm': 0.036281805485486984, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2140347957611084, 'eval_runtime': 11.1518, 'eval_samples_per_second': 89.582, 'eval_steps_per_second': 5.649, 'epoch': 0.96}
{'loss': 1.2511, 'grad_norm': 0.035966698080301285, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2137696743011475, 'eval_runtime': 11.2256, 'eval_samples_per_second': 88.993, 'eval_steps_per_second': 5.612, 'epoch': 1.0}
{'train_runtime': 577.3252, 'train_samples_per_second': 17.318, 'train_steps_per_second': 1.083, 'train_loss': 1.4061342407226562, 'epoch': 1.0}
train_results:  {'eval_loss': [2.472046136856079, 1.626658320426941, 1.3872466087341309, 1.3114129304885864, 1.2924596071243286, 1.2790464162826538, 1.2700163125991821, 1.2624744176864624, 1.2567616701126099, 1.251139521598816, 1.2471145391464233, 1.2422480583190918, 1.2383780479431152, 1.23543119430542, 1.231912612915039, 1.229828119277954, 1.2259122133255005, 1.2235597372055054, 1.220749020576477, 1.2194679975509644, 1.2174897193908691, 1.2157880067825317, 1.2148305177688599, 1.2140347957611084, 1.2137696743011475], 'performance': [0.48, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:53,  2.30it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:11, 32.94it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 52.15it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 62.67it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 70.39it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 76.74it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 83.42it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 92.02it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 93.70it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 95.41it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.94it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.72it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 114.83it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.28it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.32it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 116.75it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.41it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.57it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.63it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.87it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.98it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.67it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.34it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 102.52it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.4243698120117188
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.2311 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.8070676067968426
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.6456417347861633
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.712156816197835
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.802078807131395
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.5509746526274709
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4433, dtype=torch.float64), tensor(0.1661, dtype=torch.float64), 0, tensor(0.1695, dtype=torch.float64), tensor(0.2187, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.8201697238804387e-17, 1.4800000190734899, 0]
normalized proposed parameters for next round by BO: [tensor(4.4690e-17, dtype=torch.float64), tensor(0.0025, dtype=torch.float64), tensor(0.4433, dtype=torch.float64), tensor(0.1661, dtype=torch.float64), tensor(6.4650e-17, dtype=torch.float64), tensor(0.1695, dtype=torch.float64), tensor(0.2187, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.8202e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.443
  sciq: 0.166
  triviaqa: 0
  truthfulqa_gen: 0.169
  wikitext: 0.219
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.8201697238804387e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734899,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.8201697238804387e-17
lora alpha:  1.4800000190734899
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9973
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  997
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.02it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.51it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.25it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.57it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.14it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.96it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.03it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.35it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.27it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.65it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.09it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.26it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.19it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.04it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.41it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 149.90it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 153.83it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.30it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.87it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 180.81it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.10it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.49it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.1201, 'grad_norm': 0.17365708947181702, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.2876646518707275, 'eval_runtime': 10.999, 'eval_samples_per_second': 90.644, 'eval_steps_per_second': 5.728, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.85it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.45it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.17it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.44it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.00it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.72it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.96it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.03it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.05it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.42it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.92it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.33it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.40it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.30it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.74it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.33it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.34it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.77it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.02it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.09it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.83it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.60it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.6427, 'grad_norm': 0.2368871569633484, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 2.134427070617676, 'eval_runtime': 10.9587, 'eval_samples_per_second': 90.978, 'eval_steps_per_second': 5.749, 'epoch': 0.08}
{'loss': 1.966, 'grad_norm': 0.08748914301395416, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.8619790077209473, 'eval_runtime': 10.9549, 'eval_samples_per_second': 91.009, 'eval_steps_per_second': 5.751, 'epoch': 0.12}
{'loss': 1.8, 'grad_norm': 0.05023408308625221, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.7692668437957764, 'eval_runtime': 10.9831, 'eval_samples_per_second': 90.776, 'eval_steps_per_second': 5.736, 'epoch': 0.16}
{'loss': 1.7728, 'grad_norm': 0.04433135315775871, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.7206512689590454, 'eval_runtime': 10.9623, 'eval_samples_per_second': 90.948, 'eval_steps_per_second': 5.747, 'epoch': 0.2}
{'loss': 1.7309, 'grad_norm': 0.037099484354257584, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.6936341524124146, 'eval_runtime': 11.0077, 'eval_samples_per_second': 90.573, 'eval_steps_per_second': 5.723, 'epoch': 0.24}
{'loss': 1.6868, 'grad_norm': 0.0398949459195137, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.6770024299621582, 'eval_runtime': 10.9959, 'eval_samples_per_second': 90.67, 'eval_steps_per_second': 5.729, 'epoch': 0.28}
{'loss': 1.6671, 'grad_norm': 0.04750587418675423, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.6651129722595215, 'eval_runtime': 10.9667, 'eval_samples_per_second': 90.912, 'eval_steps_per_second': 5.745, 'epoch': 0.32}
{'loss': 1.7108, 'grad_norm': 0.047171544283628464, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.6548336744308472, 'eval_runtime': 11.0295, 'eval_samples_per_second': 90.394, 'eval_steps_per_second': 5.712, 'epoch': 0.36}
{'loss': 1.6713, 'grad_norm': 0.06241343542933464, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.648276448249817, 'eval_runtime': 11.0484, 'eval_samples_per_second': 90.239, 'eval_steps_per_second': 5.702, 'epoch': 0.4}
{'loss': 1.6121, 'grad_norm': 0.05988309904932976, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.6402941942214966, 'eval_runtime': 11.0518, 'eval_samples_per_second': 90.212, 'eval_steps_per_second': 5.7, 'epoch': 0.44}
{'loss': 1.698, 'grad_norm': 0.055320437997579575, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.6340956687927246, 'eval_runtime': 11.0308, 'eval_samples_per_second': 90.384, 'eval_steps_per_second': 5.711, 'epoch': 0.48}
{'loss': 1.6358, 'grad_norm': 0.07058317214250565, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.6295779943466187, 'eval_runtime': 10.9672, 'eval_samples_per_second': 90.908, 'eval_steps_per_second': 5.744, 'epoch': 0.52}
{'loss': 1.6329, 'grad_norm': 0.04842422157526016, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.624023199081421, 'eval_runtime': 10.9492, 'eval_samples_per_second': 91.057, 'eval_steps_per_second': 5.754, 'epoch': 0.56}
{'loss': 1.6376, 'grad_norm': 0.051627159118652344, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 1.6205501556396484, 'eval_runtime': 10.938, 'eval_samples_per_second': 91.15, 'eval_steps_per_second': 5.76, 'epoch': 0.6}
{'loss': 1.5951, 'grad_norm': 0.04739943891763687, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 1.6165919303894043, 'eval_runtime': 10.9506, 'eval_samples_per_second': 91.045, 'eval_steps_per_second': 5.753, 'epoch': 0.64}
{'loss': 1.6513, 'grad_norm': 0.049589116126298904, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 1.6125106811523438, 'eval_runtime': 10.9382, 'eval_samples_per_second': 91.148, 'eval_steps_per_second': 5.76, 'epoch': 0.68}
{'loss': 1.6512, 'grad_norm': 0.05100874975323677, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 1.6096211671829224, 'eval_runtime': 10.9476, 'eval_samples_per_second': 91.07, 'eval_steps_per_second': 5.755, 'epoch': 0.72}
{'loss': 1.6158, 'grad_norm': 0.05264097824692726, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 1.6070139408111572, 'eval_runtime': 10.9542, 'eval_samples_per_second': 91.015, 'eval_steps_per_second': 5.751, 'epoch': 0.76}
{'loss': 1.6394, 'grad_norm': 0.04749593511223793, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 1.6040704250335693, 'eval_runtime': 10.953, 'eval_samples_per_second': 91.025, 'eval_steps_per_second': 5.752, 'epoch': 0.8}
{'loss': 1.69, 'grad_norm': 0.06734798103570938, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 1.6014537811279297, 'eval_runtime': 10.9489, 'eval_samples_per_second': 91.059, 'eval_steps_per_second': 5.754, 'epoch': 0.84}
{'loss': 1.5947, 'grad_norm': 0.045640572905540466, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 1.600114345550537, 'eval_runtime': 10.9493, 'eval_samples_per_second': 91.056, 'eval_steps_per_second': 5.754, 'epoch': 0.88}
{'loss': 1.6138, 'grad_norm': 0.04663786292076111, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 1.59860098361969, 'eval_runtime': 10.9542, 'eval_samples_per_second': 91.015, 'eval_steps_per_second': 5.751, 'epoch': 0.92}
{'loss': 1.6486, 'grad_norm': 0.04849647730588913, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 1.5977568626403809, 'eval_runtime': 10.9533, 'eval_samples_per_second': 91.023, 'eval_steps_per_second': 5.752, 'epoch': 0.96}
{'train_runtime': 550.9757, 'train_samples_per_second': 18.101, 'train_steps_per_second': 1.133, 'train_loss': 1.8124195001064203, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2876646518707275, 2.134427070617676, 1.8619790077209473, 1.7692668437957764, 1.7206512689590454, 1.6936341524124146, 1.6770024299621582, 1.6651129722595215, 1.6548336744308472, 1.648276448249817, 1.6402941942214966, 1.6340956687927246, 1.6295779943466187, 1.624023199081421, 1.6205501556396484, 1.6165919303894043, 1.6125106811523438, 1.6096211671829224, 1.6070139408111572, 1.6040704250335693, 1.6014537811279297, 1.600114345550537, 1.59860098361969, 1.5977568626403809], 'performance': [0.48, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.59it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.25it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.83it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.94it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.41it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 82.14it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.65it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.26it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.07it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.20it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.42it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.98it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.90it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 114.16it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 119.10it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.74it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.93it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 118.87it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.82it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.08it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.19it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.84it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.85it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4268226623535156
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5228 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 1.0986323072261448
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 1.032842744698705
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 1.2884954607651888
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 1.1107055559273147
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 1.0052470198331753
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4449, dtype=torch.float64), 0, 0, tensor(0.5488, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.480000019073491, 0]
normalized proposed parameters for next round by BO: [tensor(3.7256e-17, dtype=torch.float64), tensor(1.3783e-16, dtype=torch.float64), tensor(0.4449, dtype=torch.float64), tensor(0.0018, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5488, dtype=torch.float64), tensor(0.0045, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0082e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.445
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.549
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073491,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.480000019073491
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9936
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  993
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.19it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.96it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.86it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.24it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.83it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.73it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.83it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.05it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.03it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.35it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.72it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.04it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.25it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.06it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.39it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 150.92it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.95it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.38it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.56it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.66it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.48it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.40it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.2846, 'grad_norm': 0.19662976264953613, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3285086154937744, 'eval_runtime': 11.0069, 'eval_samples_per_second': 90.216, 'eval_steps_per_second': 5.724, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.04it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.58it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.35it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.55it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.08it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.92it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.18it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.39it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.50it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.85it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.32it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.70it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.82it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.67it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.95it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.43it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.51it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.88it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.25it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.33it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.03it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.87it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.5575, 'grad_norm': 0.17836099863052368, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.9465082883834839, 'eval_runtime': 10.9618, 'eval_samples_per_second': 90.587, 'eval_steps_per_second': 5.747, 'epoch': 0.08}
{'loss': 1.8036, 'grad_norm': 0.05436659976840019, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.6628568172454834, 'eval_runtime': 10.9945, 'eval_samples_per_second': 90.318, 'eval_steps_per_second': 5.73, 'epoch': 0.12}
{'loss': 1.6011, 'grad_norm': 0.051867272704839706, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.5526881217956543, 'eval_runtime': 11.0131, 'eval_samples_per_second': 90.166, 'eval_steps_per_second': 5.72, 'epoch': 0.16}
{'loss': 1.5585, 'grad_norm': 0.04692346230149269, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.5243985652923584, 'eval_runtime': 11.0242, 'eval_samples_per_second': 90.075, 'eval_steps_per_second': 5.715, 'epoch': 0.2}
{'loss': 1.5177, 'grad_norm': 0.0386107936501503, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.5086464881896973, 'eval_runtime': 11.0354, 'eval_samples_per_second': 89.983, 'eval_steps_per_second': 5.709, 'epoch': 0.24}
{'loss': 1.4485, 'grad_norm': 0.04530777037143707, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.4938280582427979, 'eval_runtime': 11.0214, 'eval_samples_per_second': 90.097, 'eval_steps_per_second': 5.716, 'epoch': 0.28}
{'loss': 1.4794, 'grad_norm': 0.04740738868713379, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.4802687168121338, 'eval_runtime': 11.0727, 'eval_samples_per_second': 89.68, 'eval_steps_per_second': 5.69, 'epoch': 0.32}
{'loss': 1.4542, 'grad_norm': 0.04372953996062279, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.4718149900436401, 'eval_runtime': 11.0731, 'eval_samples_per_second': 89.677, 'eval_steps_per_second': 5.689, 'epoch': 0.36}
{'loss': 1.4537, 'grad_norm': 0.04279208183288574, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.4624398946762085, 'eval_runtime': 11.0817, 'eval_samples_per_second': 89.607, 'eval_steps_per_second': 5.685, 'epoch': 0.4}
{'loss': 1.451, 'grad_norm': 0.04256109520792961, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.451364278793335, 'eval_runtime': 11.0651, 'eval_samples_per_second': 89.742, 'eval_steps_per_second': 5.694, 'epoch': 0.44}
{'loss': 1.465, 'grad_norm': 0.05387187749147415, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.4427706003189087, 'eval_runtime': 11.0545, 'eval_samples_per_second': 89.828, 'eval_steps_per_second': 5.699, 'epoch': 0.48}
{'loss': 1.4314, 'grad_norm': 0.057238463312387466, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.4347875118255615, 'eval_runtime': 11.064, 'eval_samples_per_second': 89.75, 'eval_steps_per_second': 5.694, 'epoch': 0.52}
{'loss': 1.4622, 'grad_norm': 0.05017375200986862, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.424649953842163, 'eval_runtime': 11.0527, 'eval_samples_per_second': 89.842, 'eval_steps_per_second': 5.7, 'epoch': 0.56}
{'loss': 1.4389, 'grad_norm': 0.05210523307323456, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.416420817375183, 'eval_runtime': 11.0622, 'eval_samples_per_second': 89.765, 'eval_steps_per_second': 5.695, 'epoch': 0.6}
{'loss': 1.4292, 'grad_norm': 0.052446700632572174, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.4063611030578613, 'eval_runtime': 11.1066, 'eval_samples_per_second': 89.406, 'eval_steps_per_second': 5.672, 'epoch': 0.64}
{'loss': 1.3626, 'grad_norm': 0.051048409193754196, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.3975313901901245, 'eval_runtime': 11.0602, 'eval_samples_per_second': 89.782, 'eval_steps_per_second': 5.696, 'epoch': 0.68}
{'loss': 1.4093, 'grad_norm': 0.05993884429335594, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.3870375156402588, 'eval_runtime': 11.0607, 'eval_samples_per_second': 89.778, 'eval_steps_per_second': 5.696, 'epoch': 0.72}
{'loss': 1.4221, 'grad_norm': 0.05941810831427574, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.3804457187652588, 'eval_runtime': 11.0806, 'eval_samples_per_second': 89.616, 'eval_steps_per_second': 5.686, 'epoch': 0.76}
{'loss': 1.4325, 'grad_norm': 0.04979743808507919, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.3729397058486938, 'eval_runtime': 11.1568, 'eval_samples_per_second': 89.004, 'eval_steps_per_second': 5.647, 'epoch': 0.81}
{'loss': 1.3729, 'grad_norm': 0.07126631587743759, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.3662354946136475, 'eval_runtime': 11.1547, 'eval_samples_per_second': 89.021, 'eval_steps_per_second': 5.648, 'epoch': 0.85}
{'loss': 1.3601, 'grad_norm': 0.12029948830604553, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.3587268590927124, 'eval_runtime': 11.1847, 'eval_samples_per_second': 88.782, 'eval_steps_per_second': 5.633, 'epoch': 0.89}
{'loss': 1.3928, 'grad_norm': 0.05590183660387993, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.353956937789917, 'eval_runtime': 11.172, 'eval_samples_per_second': 88.883, 'eval_steps_per_second': 5.639, 'epoch': 0.93}
{'loss': 1.3126, 'grad_norm': 0.05808443948626518, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.3509067296981812, 'eval_runtime': 11.1862, 'eval_samples_per_second': 88.77, 'eval_steps_per_second': 5.632, 'epoch': 0.97}
{'train_runtime': 544.605, 'train_samples_per_second': 18.244, 'train_steps_per_second': 1.14, 'train_loss': 1.6134220805145116, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3285086154937744, 1.9465082883834839, 1.6628568172454834, 1.5526881217956543, 1.5243985652923584, 1.5086464881896973, 1.4938280582427979, 1.4802687168121338, 1.4718149900436401, 1.4624398946762085, 1.451364278793335, 1.4427706003189087, 1.4347875118255615, 1.424649953842163, 1.416420817375183, 1.4063611030578613, 1.3975313901901245, 1.3870375156402588, 1.3804457187652588, 1.3729397058486938, 1.3662354946136475, 1.3587268590927124, 1.353956937789917, 1.3509067296981812], 'performance': [0.49, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:27,  4.58it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 46.86it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.31it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 71.35it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 76.78it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 81.36it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.10it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 94.83it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 95.86it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 96.91it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 99.99it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.44it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.86it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.44it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.17it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 116.83it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.01it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.09it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.35it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.45it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.39it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.12it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 137.82it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.20it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4269206523895264
current iteration best possible performance (full train run):  0.441
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6219 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.952470958529
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 1.2682212124965668
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 1.1850199275810456
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.593702064899554
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 1.1913004738657853
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4447, dtype=torch.float64), 0, 0, tensor(0.5553, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.1, 1.4800000190734874, 0]
normalized proposed parameters for next round by BO: [tensor(5.1559e-17, dtype=torch.float64), tensor(1.3211e-17, dtype=torch.float64), tensor(0.4447, dtype=torch.float64), tensor(3.3780e-16, dtype=torch.float64), tensor(7.3223e-17, dtype=torch.float64), tensor(0.5553, dtype=torch.float64), tensor(1.7592e-16, dtype=torch.float64), tensor(5.2065e-17, dtype=torch.float64), tensor(2.9861e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.445
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.555
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734874,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734874
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.03it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.73it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.61it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.98it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.68it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.61it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.76it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.02it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.98it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.23it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.66it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.06it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.17it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.16it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.57it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.15it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.24it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.62it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.88it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.92it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.70it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.41it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.3374, 'grad_norm': 0.2515108585357666, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.3096494674682617, 'eval_runtime': 10.8811, 'eval_samples_per_second': 91.81, 'eval_steps_per_second': 5.79, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.93it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.56it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.41it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.78it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.41it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.24it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.42it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.66it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.71it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.08it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.57it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.01it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.09it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.92it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.29it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.72it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.49it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.39it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.89it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.16it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.03it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.02it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.5365, 'grad_norm': 0.15958914160728455, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.9245017766952515, 'eval_runtime': 10.8936, 'eval_samples_per_second': 91.705, 'eval_steps_per_second': 5.783, 'epoch': 0.08}
{'loss': 1.7722, 'grad_norm': 0.07404953986406326, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6504541635513306, 'eval_runtime': 10.9251, 'eval_samples_per_second': 91.441, 'eval_steps_per_second': 5.767, 'epoch': 0.12}
{'loss': 1.612, 'grad_norm': 0.058086320757865906, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5369346141815186, 'eval_runtime': 10.9509, 'eval_samples_per_second': 91.225, 'eval_steps_per_second': 5.753, 'epoch': 0.16}
{'loss': 1.5082, 'grad_norm': 0.057048406451940536, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.510813593864441, 'eval_runtime': 10.9797, 'eval_samples_per_second': 90.986, 'eval_steps_per_second': 5.738, 'epoch': 0.2}
{'loss': 1.524, 'grad_norm': 0.04475989192724228, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4906785488128662, 'eval_runtime': 10.9849, 'eval_samples_per_second': 90.943, 'eval_steps_per_second': 5.735, 'epoch': 0.24}
{'loss': 1.4685, 'grad_norm': 0.045244134962558746, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.475808024406433, 'eval_runtime': 11.0057, 'eval_samples_per_second': 90.771, 'eval_steps_per_second': 5.724, 'epoch': 0.28}
{'loss': 1.4388, 'grad_norm': 0.044077709317207336, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4646865129470825, 'eval_runtime': 11.0915, 'eval_samples_per_second': 90.069, 'eval_steps_per_second': 5.68, 'epoch': 0.32}
{'loss': 1.4508, 'grad_norm': 0.04170258715748787, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4547549486160278, 'eval_runtime': 11.0873, 'eval_samples_per_second': 90.103, 'eval_steps_per_second': 5.682, 'epoch': 0.36}
{'loss': 1.4686, 'grad_norm': 0.04072066769003868, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4402530193328857, 'eval_runtime': 11.0199, 'eval_samples_per_second': 90.654, 'eval_steps_per_second': 5.717, 'epoch': 0.4}
{'loss': 1.4584, 'grad_norm': 0.051190514117479324, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.429320216178894, 'eval_runtime': 11.0194, 'eval_samples_per_second': 90.658, 'eval_steps_per_second': 5.717, 'epoch': 0.44}
{'loss': 1.4337, 'grad_norm': 0.056998562067747116, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4183939695358276, 'eval_runtime': 11.0225, 'eval_samples_per_second': 90.633, 'eval_steps_per_second': 5.716, 'epoch': 0.48}
{'loss': 1.4625, 'grad_norm': 0.057508211582899094, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4093143939971924, 'eval_runtime': 11.0394, 'eval_samples_per_second': 90.494, 'eval_steps_per_second': 5.707, 'epoch': 0.52}
{'loss': 1.3742, 'grad_norm': 0.0663759708404541, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3995335102081299, 'eval_runtime': 11.021, 'eval_samples_per_second': 90.645, 'eval_steps_per_second': 5.716, 'epoch': 0.56}
{'loss': 1.4103, 'grad_norm': 0.06499971449375153, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3875246047973633, 'eval_runtime': 11.0063, 'eval_samples_per_second': 90.766, 'eval_steps_per_second': 5.724, 'epoch': 0.6}
{'loss': 1.4209, 'grad_norm': 0.05417398735880852, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.375939130783081, 'eval_runtime': 11.0212, 'eval_samples_per_second': 90.643, 'eval_steps_per_second': 5.716, 'epoch': 0.64}
{'loss': 1.3612, 'grad_norm': 0.0713978037238121, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3659660816192627, 'eval_runtime': 11.009, 'eval_samples_per_second': 90.744, 'eval_steps_per_second': 5.723, 'epoch': 0.68}
{'loss': 1.4169, 'grad_norm': 0.06897764652967453, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3562532663345337, 'eval_runtime': 11.0003, 'eval_samples_per_second': 90.815, 'eval_steps_per_second': 5.727, 'epoch': 0.72}
{'loss': 1.3758, 'grad_norm': 0.06575363874435425, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3476091623306274, 'eval_runtime': 11.0031, 'eval_samples_per_second': 90.793, 'eval_steps_per_second': 5.726, 'epoch': 0.76}
{'loss': 1.3751, 'grad_norm': 0.09272905439138412, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3381952047348022, 'eval_runtime': 11.0039, 'eval_samples_per_second': 90.786, 'eval_steps_per_second': 5.725, 'epoch': 0.8}
{'loss': 1.3623, 'grad_norm': 0.07885852456092834, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3299063444137573, 'eval_runtime': 11.0347, 'eval_samples_per_second': 90.533, 'eval_steps_per_second': 5.709, 'epoch': 0.84}
{'loss': 1.3575, 'grad_norm': 0.08471725136041641, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3246008157730103, 'eval_runtime': 11.03, 'eval_samples_per_second': 90.571, 'eval_steps_per_second': 5.712, 'epoch': 0.88}
{'loss': 1.3551, 'grad_norm': 0.07628446817398071, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.319954514503479, 'eval_runtime': 11.1138, 'eval_samples_per_second': 89.889, 'eval_steps_per_second': 5.669, 'epoch': 0.92}
{'loss': 1.394, 'grad_norm': 0.06103629246354103, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.316053032875061, 'eval_runtime': 11.0535, 'eval_samples_per_second': 90.379, 'eval_steps_per_second': 5.7, 'epoch': 0.96}
{'loss': 1.4024, 'grad_norm': 0.08552367985248566, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3149693012237549, 'eval_runtime': 11.0707, 'eval_samples_per_second': 90.238, 'eval_steps_per_second': 5.691, 'epoch': 1.0}
{'train_runtime': 568.1203, 'train_samples_per_second': 17.6, 'train_steps_per_second': 1.1, 'train_loss': 1.6030957214355468, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3096494674682617, 1.9245017766952515, 1.6504541635513306, 1.5369346141815186, 1.510813593864441, 1.4906785488128662, 1.475808024406433, 1.4646865129470825, 1.4547549486160278, 1.4402530193328857, 1.429320216178894, 1.4183939695358276, 1.4093143939971924, 1.3995335102081299, 1.3875246047973633, 1.375939130783081, 1.3659660816192627, 1.3562532663345337, 1.3476091623306274, 1.3381952047348022, 1.3299063444137573, 1.3246008157730103, 1.319954514503479, 1.316053032875061, 1.3149693012237549], 'performance': [0.5, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:27,  1.93it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:14, 27.13it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:08, 45.70it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 57.21it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 66.02it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 73.17it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 80.75it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 89.88it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 92.22it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 94.21it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 97.74it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 104.88it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 114.74it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.34it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.17it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 117.13it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.57it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.74it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.15it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.30it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.32it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.75it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.12it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4264812469482422
current iteration best possible performance (full train run):  0.462
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4093 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 1.0070676532371294
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 1.3273890457014172
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.563048801480616
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.8229076225013674
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.5751923968289367
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4488, dtype=torch.float64), 0, tensor(0.4465, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.1047, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.04466557383162341, 1.480000019073489, 0]
normalized proposed parameters for next round by BO: [tensor(0.4488, dtype=torch.float64), tensor(7.6556e-17, dtype=torch.float64), tensor(0.4465, dtype=torch.float64), tensor(1.0858e-16, dtype=torch.float64), tensor(2.0339e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.0450e-18, dtype=torch.float64), tensor(0.1047, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4467, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.449
  gsm8k: 0
  rowan_hellaswag: 0.447
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.105

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04466557383162341,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073489,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.04466557383162341
lora alpha:  1.480000019073489
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.82it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.35it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 96.71it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 104.28it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 109.25it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.23it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 121.91it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.38it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.49it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 132.90it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.46it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.11it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.30it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.22it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.57it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 149.79it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 153.93it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.20it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 167.89it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.11it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.07it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.17it/s]
Evaluation performance at step 25: 0.51
{'loss': 4.1383, 'grad_norm': 0.19351771473884583, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.51}
{'eval_loss': 3.154510259628296, 'eval_runtime': 10.9396, 'eval_samples_per_second': 91.32, 'eval_steps_per_second': 5.759, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.23it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.05it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 99.00it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.19it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.73it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.54it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.79it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.13it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.18it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.56it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.97it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.32it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.34it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.15it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.44it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.02it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.12it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.44it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.76it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.88it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.56it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.49it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.4066, 'grad_norm': 0.17094124853610992, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.844949722290039, 'eval_runtime': 10.9429, 'eval_samples_per_second': 91.292, 'eval_steps_per_second': 5.757, 'epoch': 0.08}
{'loss': 1.6269, 'grad_norm': 0.046006254851818085, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5932549238204956, 'eval_runtime': 10.9396, 'eval_samples_per_second': 91.32, 'eval_steps_per_second': 5.759, 'epoch': 0.12}
{'loss': 1.5675, 'grad_norm': 0.056958988308906555, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.494676113128662, 'eval_runtime': 10.9623, 'eval_samples_per_second': 91.13, 'eval_steps_per_second': 5.747, 'epoch': 0.16}
{'loss': 1.474, 'grad_norm': 0.036676209419965744, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4694212675094604, 'eval_runtime': 11.0573, 'eval_samples_per_second': 90.348, 'eval_steps_per_second': 5.698, 'epoch': 0.2}
{'loss': 1.47, 'grad_norm': 0.0427427664399147, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.454158067703247, 'eval_runtime': 11.0881, 'eval_samples_per_second': 90.097, 'eval_steps_per_second': 5.682, 'epoch': 0.24}
{'loss': 1.4524, 'grad_norm': 0.03788909688591957, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.443232536315918, 'eval_runtime': 11.098, 'eval_samples_per_second': 90.016, 'eval_steps_per_second': 5.677, 'epoch': 0.28}
{'loss': 1.4269, 'grad_norm': 0.04073180630803108, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4356071949005127, 'eval_runtime': 11.0684, 'eval_samples_per_second': 90.257, 'eval_steps_per_second': 5.692, 'epoch': 0.32}
{'loss': 1.4231, 'grad_norm': 0.04554025083780289, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.427787184715271, 'eval_runtime': 11.0689, 'eval_samples_per_second': 90.253, 'eval_steps_per_second': 5.692, 'epoch': 0.36}
{'loss': 1.4305, 'grad_norm': 0.04188188165426254, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4229921102523804, 'eval_runtime': 11.0472, 'eval_samples_per_second': 90.43, 'eval_steps_per_second': 5.703, 'epoch': 0.4}
{'loss': 1.4064, 'grad_norm': 0.04138372465968132, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4170422554016113, 'eval_runtime': 11.0783, 'eval_samples_per_second': 90.176, 'eval_steps_per_second': 5.687, 'epoch': 0.44}
{'loss': 1.4307, 'grad_norm': 0.03927045688033104, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4132081270217896, 'eval_runtime': 11.0986, 'eval_samples_per_second': 90.011, 'eval_steps_per_second': 5.676, 'epoch': 0.48}
{'loss': 1.4286, 'grad_norm': 0.04432979226112366, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4097398519515991, 'eval_runtime': 11.102, 'eval_samples_per_second': 89.984, 'eval_steps_per_second': 5.675, 'epoch': 0.52}
{'loss': 1.4119, 'grad_norm': 0.03892848640680313, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4043464660644531, 'eval_runtime': 11.0925, 'eval_samples_per_second': 90.061, 'eval_steps_per_second': 5.68, 'epoch': 0.56}
{'loss': 1.4293, 'grad_norm': 0.039141856133937836, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4019590616226196, 'eval_runtime': 11.0947, 'eval_samples_per_second': 90.043, 'eval_steps_per_second': 5.678, 'epoch': 0.6}
{'loss': 1.3352, 'grad_norm': 0.04151611775159836, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3975417613983154, 'eval_runtime': 11.0865, 'eval_samples_per_second': 90.11, 'eval_steps_per_second': 5.683, 'epoch': 0.64}
{'loss': 1.4115, 'grad_norm': 0.039221830666065216, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3949499130249023, 'eval_runtime': 11.0174, 'eval_samples_per_second': 90.674, 'eval_steps_per_second': 5.718, 'epoch': 0.68}
{'loss': 1.4075, 'grad_norm': 0.03910570591688156, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3914204835891724, 'eval_runtime': 11.0632, 'eval_samples_per_second': 90.299, 'eval_steps_per_second': 5.695, 'epoch': 0.72}
{'loss': 1.3976, 'grad_norm': 0.0440177358686924, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3893630504608154, 'eval_runtime': 11.0532, 'eval_samples_per_second': 90.381, 'eval_steps_per_second': 5.7, 'epoch': 0.76}
{'loss': 1.3422, 'grad_norm': 0.04496763274073601, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3869740962982178, 'eval_runtime': 11.0685, 'eval_samples_per_second': 90.256, 'eval_steps_per_second': 5.692, 'epoch': 0.8}
{'loss': 1.4258, 'grad_norm': 0.041689690202474594, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.384985327720642, 'eval_runtime': 11.1479, 'eval_samples_per_second': 89.613, 'eval_steps_per_second': 5.651, 'epoch': 0.84}
{'loss': 1.4303, 'grad_norm': 0.040702223777770996, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3834419250488281, 'eval_runtime': 11.1588, 'eval_samples_per_second': 89.526, 'eval_steps_per_second': 5.646, 'epoch': 0.88}
{'loss': 1.4271, 'grad_norm': 0.04670458287000656, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3824471235275269, 'eval_runtime': 11.1751, 'eval_samples_per_second': 89.395, 'eval_steps_per_second': 5.638, 'epoch': 0.92}
{'loss': 1.4213, 'grad_norm': 0.03862641751766205, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3815256357192993, 'eval_runtime': 11.1664, 'eval_samples_per_second': 89.465, 'eval_steps_per_second': 5.642, 'epoch': 0.96}
{'loss': 1.3733, 'grad_norm': 0.04305725917220116, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3811402320861816, 'eval_runtime': 11.1469, 'eval_samples_per_second': 89.621, 'eval_steps_per_second': 5.652, 'epoch': 1.0}
{'train_runtime': 571.1666, 'train_samples_per_second': 17.505, 'train_steps_per_second': 1.094, 'train_loss': 1.5798025451660156, 'epoch': 1.0}
train_results:  {'eval_loss': [3.154510259628296, 1.844949722290039, 1.5932549238204956, 1.494676113128662, 1.4694212675094604, 1.454158067703247, 1.443232536315918, 1.4356071949005127, 1.427787184715271, 1.4229921102523804, 1.4170422554016113, 1.4132081270217896, 1.4097398519515991, 1.4043464660644531, 1.4019590616226196, 1.3975417613983154, 1.3949499130249023, 1.3914204835891724, 1.3893630504608154, 1.3869740962982178, 1.384985327720642, 1.3834419250488281, 1.3824471235275269, 1.3815256357192993, 1.3811402320861816], 'performance': [0.51, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:47,  1.75it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 27.64it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 46.38it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:06, 57.82it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 66.67it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 73.87it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 81.40it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 90.51it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 92.56it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 94.60it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.36it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.33it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.06it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.64it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.55it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 117.36it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.69it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.88it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.31it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.49it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.32it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.83it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.23it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.51, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.422614574432373
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422, 1.422614574432373]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3487 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 1.009363552485195
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.9660781712434751
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.5880073093571704
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.9950970981417854
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.9746681252757909
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4606, dtype=torch.float64), tensor(0.1360, dtype=torch.float64), 0, tensor(0.1555, dtype=torch.float64), tensor(0.2480, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.03864402153526624, 6.139359264459934, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4606, dtype=torch.float64), tensor(0.1360, dtype=torch.float64), tensor(1.2365e-17, dtype=torch.float64), tensor(0.1555, dtype=torch.float64), tensor(0.2480, dtype=torch.float64), tensor(4.6133e-18, dtype=torch.float64), tensor(1.7085e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3864, dtype=torch.float64), tensor(0.1279, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.461
  sciq: 0.136
  triviaqa: 0
  truthfulqa_gen: 0.155
  wikitext: 0.248
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03864402153526624,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (6.139359264459934,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.03864402153526624
lora alpha:  6.139359264459934
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.27it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.06it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 99.03it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.42it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 111.03it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.91it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.12it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.44it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.43it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.77it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.21it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.67it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.76it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.58it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.88it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.39it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.41it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.54it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.83it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.98it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.74it/s]
Evaluation performance at step 25: 0.53
{'loss': 3.7065, 'grad_norm': 0.3218160569667816, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.53}
{'eval_loss': 2.726170301437378, 'eval_runtime': 10.9012, 'eval_samples_per_second': 91.641, 'eval_steps_per_second': 5.779, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.36it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.14it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.90it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.23it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.81it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.77it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.85it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.08it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.09it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.56it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.12it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.63it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.59it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.06it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.33it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 150.97it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.11it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.55it/s]Running loglikelihood requests:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 317/400 [00:02<00:00, 176.93it/s]Running loglikelihood requests:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 341/400 [00:02<00:00, 173.55it/s]Running loglikelihood requests:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/400 [00:02<00:00, 187.78it/s]Running loglikelihood requests:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 394/400 [00:02<00:00, 193.15it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.44it/s]
Evaluation performance at step 50: 0.42
{'loss': 2.2311, 'grad_norm': 0.1578010767698288, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.42}
{'eval_loss': 1.9342120885849, 'eval_runtime': 10.903, 'eval_samples_per_second': 91.626, 'eval_steps_per_second': 5.778, 'epoch': 0.08}
{'loss': 1.8741, 'grad_norm': 0.08417025208473206, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7910391092300415, 'eval_runtime': 10.9456, 'eval_samples_per_second': 91.269, 'eval_steps_per_second': 5.756, 'epoch': 0.12}
{'loss': 1.7343, 'grad_norm': 0.09485018998384476, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7284115552902222, 'eval_runtime': 10.9681, 'eval_samples_per_second': 91.082, 'eval_steps_per_second': 5.744, 'epoch': 0.16}
{'loss': 1.6987, 'grad_norm': 0.0762217789888382, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7085474729537964, 'eval_runtime': 11.0691, 'eval_samples_per_second': 90.252, 'eval_steps_per_second': 5.692, 'epoch': 0.2}
{'loss': 1.6917, 'grad_norm': 0.07596912235021591, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.694205403327942, 'eval_runtime': 11.1279, 'eval_samples_per_second': 89.775, 'eval_steps_per_second': 5.661, 'epoch': 0.24}
{'loss': 1.7062, 'grad_norm': 0.07203474640846252, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6810892820358276, 'eval_runtime': 11.0989, 'eval_samples_per_second': 90.009, 'eval_steps_per_second': 5.676, 'epoch': 0.28}
{'loss': 1.7124, 'grad_norm': 0.08403856307268143, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6699531078338623, 'eval_runtime': 11.0987, 'eval_samples_per_second': 90.01, 'eval_steps_per_second': 5.676, 'epoch': 0.32}
{'loss': 1.6889, 'grad_norm': 0.09819337725639343, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6603314876556396, 'eval_runtime': 11.1121, 'eval_samples_per_second': 89.902, 'eval_steps_per_second': 5.669, 'epoch': 0.36}
{'loss': 1.7001, 'grad_norm': 0.07353629171848297, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6537318229675293, 'eval_runtime': 11.0913, 'eval_samples_per_second': 90.071, 'eval_steps_per_second': 5.68, 'epoch': 0.4}
{'loss': 1.645, 'grad_norm': 0.07740452140569687, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6466560363769531, 'eval_runtime': 11.1, 'eval_samples_per_second': 90.0, 'eval_steps_per_second': 5.676, 'epoch': 0.44}
{'loss': 1.6776, 'grad_norm': 0.08779199421405792, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6409097909927368, 'eval_runtime': 11.0903, 'eval_samples_per_second': 90.079, 'eval_steps_per_second': 5.681, 'epoch': 0.48}
{'loss': 1.6543, 'grad_norm': 0.07566776126623154, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6351419687271118, 'eval_runtime': 11.1183, 'eval_samples_per_second': 89.852, 'eval_steps_per_second': 5.666, 'epoch': 0.52}
{'loss': 1.6387, 'grad_norm': 0.06916002929210663, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6315275430679321, 'eval_runtime': 11.1085, 'eval_samples_per_second': 89.931, 'eval_steps_per_second': 5.671, 'epoch': 0.56}
{'loss': 1.6287, 'grad_norm': 0.11346330493688583, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.625569462776184, 'eval_runtime': 11.0849, 'eval_samples_per_second': 90.123, 'eval_steps_per_second': 5.683, 'epoch': 0.6}
{'loss': 1.6721, 'grad_norm': 0.14585338532924652, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.620647668838501, 'eval_runtime': 11.0899, 'eval_samples_per_second': 90.082, 'eval_steps_per_second': 5.681, 'epoch': 0.64}
{'loss': 1.6627, 'grad_norm': 0.07586026191711426, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6170250177383423, 'eval_runtime': 11.0871, 'eval_samples_per_second': 90.105, 'eval_steps_per_second': 5.682, 'epoch': 0.68}
{'loss': 1.6103, 'grad_norm': 0.0827660858631134, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6117327213287354, 'eval_runtime': 11.1156, 'eval_samples_per_second': 89.874, 'eval_steps_per_second': 5.668, 'epoch': 0.72}
{'loss': 1.6218, 'grad_norm': 0.0891159176826477, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6071034669876099, 'eval_runtime': 11.1035, 'eval_samples_per_second': 89.972, 'eval_steps_per_second': 5.674, 'epoch': 0.76}
{'loss': 1.6472, 'grad_norm': 0.08841541409492493, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6021016836166382, 'eval_runtime': 11.1066, 'eval_samples_per_second': 89.947, 'eval_steps_per_second': 5.672, 'epoch': 0.8}
{'loss': 1.6019, 'grad_norm': 0.07912254333496094, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6000142097473145, 'eval_runtime': 11.0704, 'eval_samples_per_second': 90.241, 'eval_steps_per_second': 5.691, 'epoch': 0.84}
{'loss': 1.652, 'grad_norm': 0.1012386754155159, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5972944498062134, 'eval_runtime': 11.1116, 'eval_samples_per_second': 89.906, 'eval_steps_per_second': 5.67, 'epoch': 0.88}
{'loss': 1.5842, 'grad_norm': 0.09741419553756714, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.594091534614563, 'eval_runtime': 11.1015, 'eval_samples_per_second': 89.988, 'eval_steps_per_second': 5.675, 'epoch': 0.92}
{'loss': 1.5854, 'grad_norm': 0.09554153680801392, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5935622453689575, 'eval_runtime': 11.119, 'eval_samples_per_second': 89.846, 'eval_steps_per_second': 5.666, 'epoch': 0.96}
{'loss': 1.6713, 'grad_norm': 0.09616874903440475, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5928560495376587, 'eval_runtime': 11.1084, 'eval_samples_per_second': 89.932, 'eval_steps_per_second': 5.671, 'epoch': 1.0}
{'train_runtime': 570.6486, 'train_samples_per_second': 17.52, 'train_steps_per_second': 1.095, 'train_loss': 1.7718929504394532, 'epoch': 1.0}
train_results:  {'eval_loss': [2.726170301437378, 1.9342120885849, 1.7910391092300415, 1.7284115552902222, 1.7085474729537964, 1.694205403327942, 1.6810892820358276, 1.6699531078338623, 1.6603314876556396, 1.6537318229675293, 1.6466560363769531, 1.6409097909927368, 1.6351419687271118, 1.6315275430679321, 1.625569462776184, 1.620647668838501, 1.6170250177383423, 1.6117327213287354, 1.6071034669876099, 1.6021016836166382, 1.6000142097473145, 1.5972944498062134, 1.594091534614563, 1.5935622453689575, 1.5928560495376587], 'performance': [0.53, 0.42]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<03:38,  1.82it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:13, 28.61it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 47.36it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:05, 58.48it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 67.10it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 74.13it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 81.58it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 90.37it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 92.53it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 94.53it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 98.23it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 105.21it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 114.87it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 112.87it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 117.74it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 116.78it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.12it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.42it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.72it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.59it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 127.49it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 127.38it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 137.24it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 99.44it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.53, 0.42]
current iteration observed (possibly low-fid or predicted) performance:  1.399521827697754
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422, 1.422614574432373, 1.399521827697754]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3869 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 1.0754776909756392
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 1.2164964855237521
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 1.0737137194461206
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 1.1210983341530871
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 1.0460611236922905
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4437, dtype=torch.float64), 0, 0, tensor(0.0707, dtype=torch.float64), 0, 0, tensor(0.4857, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1.2949e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4437, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0707, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4857, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.444
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.071
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.486

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.10it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.80it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.78it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.69it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.35it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.21it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.30it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.59it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.64it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.93it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.04it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.48it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.70it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.62it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.00it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.59it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.66it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.17it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.48it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.53it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.02it/s]
Evaluation performance at step 25: 0.5
{'loss': 3.9443, 'grad_norm': 0.17097312211990356, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.0074751377105713, 'eval_runtime': 10.9515, 'eval_samples_per_second': 91.221, 'eval_steps_per_second': 5.753, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.10it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.65it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.48it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.72it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.39it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.25it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.45it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.44it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.39it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.73it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.08it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.39it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.43it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.38it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.84it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.49it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.52it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.92it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.44it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.60it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.36it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.95it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.3255, 'grad_norm': 0.1695210039615631, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.7988512516021729, 'eval_runtime': 10.9763, 'eval_samples_per_second': 91.014, 'eval_steps_per_second': 5.74, 'epoch': 0.08}
{'loss': 1.6659, 'grad_norm': 0.05117514729499817, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5536210536956787, 'eval_runtime': 10.9942, 'eval_samples_per_second': 90.866, 'eval_steps_per_second': 5.73, 'epoch': 0.12}
{'loss': 1.4279, 'grad_norm': 0.05382276326417923, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4554165601730347, 'eval_runtime': 10.9966, 'eval_samples_per_second': 90.846, 'eval_steps_per_second': 5.729, 'epoch': 0.16}
{'loss': 1.4517, 'grad_norm': 0.038327135145664215, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4297136068344116, 'eval_runtime': 10.9624, 'eval_samples_per_second': 91.13, 'eval_steps_per_second': 5.747, 'epoch': 0.2}
{'loss': 1.4035, 'grad_norm': 0.04074651747941971, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4150865077972412, 'eval_runtime': 10.9728, 'eval_samples_per_second': 91.043, 'eval_steps_per_second': 5.741, 'epoch': 0.24}
{'loss': 1.373, 'grad_norm': 0.032553356140851974, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4039242267608643, 'eval_runtime': 10.9745, 'eval_samples_per_second': 91.029, 'eval_steps_per_second': 5.741, 'epoch': 0.28}
{'loss': 1.4387, 'grad_norm': 0.03776692599058151, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3940504789352417, 'eval_runtime': 10.9691, 'eval_samples_per_second': 91.074, 'eval_steps_per_second': 5.743, 'epoch': 0.32}
{'loss': 1.374, 'grad_norm': 0.03543834760785103, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3854212760925293, 'eval_runtime': 10.9796, 'eval_samples_per_second': 90.987, 'eval_steps_per_second': 5.738, 'epoch': 0.36}
{'loss': 1.3955, 'grad_norm': 0.03777960687875748, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3763352632522583, 'eval_runtime': 11.0374, 'eval_samples_per_second': 90.511, 'eval_steps_per_second': 5.708, 'epoch': 0.4}
{'loss': 1.3771, 'grad_norm': 0.041429147124290466, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3712079524993896, 'eval_runtime': 11.1378, 'eval_samples_per_second': 89.695, 'eval_steps_per_second': 5.656, 'epoch': 0.44}
{'loss': 1.3817, 'grad_norm': 0.0429791696369648, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3655678033828735, 'eval_runtime': 11.1793, 'eval_samples_per_second': 89.361, 'eval_steps_per_second': 5.635, 'epoch': 0.48}
{'loss': 1.3709, 'grad_norm': 0.041774261742830276, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3579529523849487, 'eval_runtime': 11.1778, 'eval_samples_per_second': 89.373, 'eval_steps_per_second': 5.636, 'epoch': 0.52}
{'loss': 1.3233, 'grad_norm': 0.041293200105428696, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3526235818862915, 'eval_runtime': 11.185, 'eval_samples_per_second': 89.316, 'eval_steps_per_second': 5.633, 'epoch': 0.56}
{'loss': 1.3858, 'grad_norm': 0.041442640125751495, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3473252058029175, 'eval_runtime': 11.1739, 'eval_samples_per_second': 89.405, 'eval_steps_per_second': 5.638, 'epoch': 0.6}
{'loss': 1.3588, 'grad_norm': 0.046470705419778824, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3417490720748901, 'eval_runtime': 11.1701, 'eval_samples_per_second': 89.435, 'eval_steps_per_second': 5.64, 'epoch': 0.64}
{'loss': 1.3286, 'grad_norm': 0.04884837567806244, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3354811668395996, 'eval_runtime': 11.1847, 'eval_samples_per_second': 89.319, 'eval_steps_per_second': 5.633, 'epoch': 0.68}
{'loss': 1.314, 'grad_norm': 0.04635661467909813, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.329826831817627, 'eval_runtime': 11.1639, 'eval_samples_per_second': 89.485, 'eval_steps_per_second': 5.643, 'epoch': 0.72}
{'loss': 1.3613, 'grad_norm': 0.04408494383096695, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3243054151535034, 'eval_runtime': 11.1638, 'eval_samples_per_second': 89.486, 'eval_steps_per_second': 5.643, 'epoch': 0.76}
{'loss': 1.3791, 'grad_norm': 0.04544968903064728, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3191465139389038, 'eval_runtime': 11.1779, 'eval_samples_per_second': 89.373, 'eval_steps_per_second': 5.636, 'epoch': 0.8}
{'loss': 1.3171, 'grad_norm': 0.043830402195453644, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3155486583709717, 'eval_runtime': 11.1817, 'eval_samples_per_second': 89.343, 'eval_steps_per_second': 5.634, 'epoch': 0.84}
{'loss': 1.2922, 'grad_norm': 0.055417146533727646, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3115160465240479, 'eval_runtime': 11.1717, 'eval_samples_per_second': 89.422, 'eval_steps_per_second': 5.639, 'epoch': 0.88}
{'loss': 1.3118, 'grad_norm': 0.051728133112192154, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3082164525985718, 'eval_runtime': 11.1591, 'eval_samples_per_second': 89.523, 'eval_steps_per_second': 5.646, 'epoch': 0.92}
{'loss': 1.2783, 'grad_norm': 0.05341117084026337, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3068149089813232, 'eval_runtime': 11.162, 'eval_samples_per_second': 89.5, 'eval_steps_per_second': 5.644, 'epoch': 0.96}
{'loss': 1.3246, 'grad_norm': 0.0554547980427742, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3058011531829834, 'eval_runtime': 11.1537, 'eval_samples_per_second': 89.566, 'eval_steps_per_second': 5.648, 'epoch': 1.0}
{'train_runtime': 560.9045, 'train_samples_per_second': 17.825, 'train_steps_per_second': 1.114, 'train_loss': 1.5161870208740234, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0074751377105713, 1.7988512516021729, 1.5536210536956787, 1.4554165601730347, 1.4297136068344116, 1.4150865077972412, 1.4039242267608643, 1.3940504789352417, 1.3854212760925293, 1.3763352632522583, 1.3712079524993896, 1.3655678033828735, 1.3579529523849487, 1.3526235818862915, 1.3473252058029175, 1.3417490720748901, 1.3354811668395996, 1.329826831817627, 1.3243054151535034, 1.3191465139389038, 1.3155486583709717, 1.3115160465240479, 1.3082164525985718, 1.3068149089813232, 1.3058011531829834], 'performance': [0.5, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<04:47,  1.39it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:18, 20.34it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:09, 37.27it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:07, 49.27it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:05, 59.32it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 67.93it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 76.58it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 86.40it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:02, 89.69it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 92.47it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 96.75it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 104.20it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 114.16it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 112.22it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 117.30it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 116.40it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.18it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.55it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.78it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 127.71it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 127.73it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 128.14it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 137.30it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 92.89it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4231212139129639
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422, 1.422614574432373, 1.399521827697754, 1.4231212139129639]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8390 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 1.0078686823229235
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 1.0530854736684776
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.7087070875207374
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 1.0036590204832456
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 1.00395490578054
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4449, dtype=torch.float64), 0, 0, tensor(0.5551, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 3.9898639947466614e-18, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(1.1574e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4449, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5551, dtype=torch.float64), tensor(3.3085e-16, dtype=torch.float64), tensor(1.5134e-18, dtype=torch.float64), tensor(8.4780e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(3.9899e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.445
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.555
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.9898639947466614e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  3.9898639947466614e-18
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 53.86it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.59it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.49it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.98it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.06it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.00it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.23it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.56it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.46it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.04it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.70it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.11it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.30it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.20it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.54it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 151.04it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.05it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 167.47it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.84it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.94it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.64it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.22it/s]
Evaluation performance at step 25: 0.48
{'loss': 4.3054, 'grad_norm': 0.19964852929115295, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 3.311464786529541, 'eval_runtime': 10.9617, 'eval_samples_per_second': 91.136, 'eval_steps_per_second': 5.747, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.12it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.86it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.72it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.97it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.53it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.32it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.50it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.68it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.72it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.07it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.47it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.77it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.39it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.36it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.67it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.30it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.29it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.65it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.02it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.09it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 188.78it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.91it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.5248, 'grad_norm': 0.18338817358016968, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.9380234479904175, 'eval_runtime': 11.0062, 'eval_samples_per_second': 90.767, 'eval_steps_per_second': 5.724, 'epoch': 0.08}
{'loss': 1.7559, 'grad_norm': 0.07338733971118927, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6618918180465698, 'eval_runtime': 11.0367, 'eval_samples_per_second': 90.516, 'eval_steps_per_second': 5.708, 'epoch': 0.12}
{'loss': 1.5633, 'grad_norm': 0.05716240033507347, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5535615682601929, 'eval_runtime': 11.0331, 'eval_samples_per_second': 90.546, 'eval_steps_per_second': 5.71, 'epoch': 0.16}
{'loss': 1.4892, 'grad_norm': 0.04496804252266884, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.52273690700531, 'eval_runtime': 11.0543, 'eval_samples_per_second': 90.372, 'eval_steps_per_second': 5.699, 'epoch': 0.2}
{'loss': 1.5066, 'grad_norm': 0.043749596923589706, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5064462423324585, 'eval_runtime': 11.0796, 'eval_samples_per_second': 90.165, 'eval_steps_per_second': 5.686, 'epoch': 0.24}
{'loss': 1.4435, 'grad_norm': 0.04284320026636124, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4909993410110474, 'eval_runtime': 11.0695, 'eval_samples_per_second': 90.248, 'eval_steps_per_second': 5.691, 'epoch': 0.28}
{'loss': 1.4523, 'grad_norm': 0.03819141164422035, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.478508472442627, 'eval_runtime': 11.0325, 'eval_samples_per_second': 90.551, 'eval_steps_per_second': 5.71, 'epoch': 0.32}
{'loss': 1.495, 'grad_norm': 0.04669831320643425, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4712882041931152, 'eval_runtime': 11.0823, 'eval_samples_per_second': 90.144, 'eval_steps_per_second': 5.685, 'epoch': 0.36}
{'loss': 1.4396, 'grad_norm': 0.044601328670978546, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4557549953460693, 'eval_runtime': 11.0767, 'eval_samples_per_second': 90.189, 'eval_steps_per_second': 5.688, 'epoch': 0.4}
{'loss': 1.4762, 'grad_norm': 0.042703356593847275, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4426170587539673, 'eval_runtime': 11.0449, 'eval_samples_per_second': 90.449, 'eval_steps_per_second': 5.704, 'epoch': 0.44}
{'loss': 1.4508, 'grad_norm': 0.048358142375946045, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4327350854873657, 'eval_runtime': 11.0487, 'eval_samples_per_second': 90.418, 'eval_steps_per_second': 5.702, 'epoch': 0.48}
{'loss': 1.4636, 'grad_norm': 0.06292643398046494, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4241234064102173, 'eval_runtime': 11.068, 'eval_samples_per_second': 90.26, 'eval_steps_per_second': 5.692, 'epoch': 0.52}
{'loss': 1.4239, 'grad_norm': 0.0496908463537693, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.413534164428711, 'eval_runtime': 11.0908, 'eval_samples_per_second': 90.074, 'eval_steps_per_second': 5.68, 'epoch': 0.56}
{'loss': 1.3992, 'grad_norm': 0.05195062607526779, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4031696319580078, 'eval_runtime': 11.161, 'eval_samples_per_second': 89.508, 'eval_steps_per_second': 5.645, 'epoch': 0.6}
{'loss': 1.437, 'grad_norm': 0.05942177772521973, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3936020135879517, 'eval_runtime': 11.1669, 'eval_samples_per_second': 89.461, 'eval_steps_per_second': 5.642, 'epoch': 0.64}
{'loss': 1.3613, 'grad_norm': 0.07021774351596832, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3838509321212769, 'eval_runtime': 11.1729, 'eval_samples_per_second': 89.413, 'eval_steps_per_second': 5.639, 'epoch': 0.68}
{'loss': 1.4142, 'grad_norm': 0.0647033154964447, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3728208541870117, 'eval_runtime': 11.1605, 'eval_samples_per_second': 89.512, 'eval_steps_per_second': 5.645, 'epoch': 0.72}
{'loss': 1.366, 'grad_norm': 0.06832543760538101, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.363639235496521, 'eval_runtime': 11.1669, 'eval_samples_per_second': 89.461, 'eval_steps_per_second': 5.642, 'epoch': 0.76}
{'loss': 1.3739, 'grad_norm': 0.07996627688407898, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3557324409484863, 'eval_runtime': 11.1318, 'eval_samples_per_second': 89.743, 'eval_steps_per_second': 5.659, 'epoch': 0.8}
{'loss': 1.4092, 'grad_norm': 0.06407487392425537, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.347845196723938, 'eval_runtime': 11.2084, 'eval_samples_per_second': 89.13, 'eval_steps_per_second': 5.621, 'epoch': 0.84}
{'loss': 1.3918, 'grad_norm': 0.07127434015274048, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.341201663017273, 'eval_runtime': 11.1738, 'eval_samples_per_second': 89.405, 'eval_steps_per_second': 5.638, 'epoch': 0.88}
{'loss': 1.3388, 'grad_norm': 0.06600692123174667, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3367903232574463, 'eval_runtime': 11.1205, 'eval_samples_per_second': 89.834, 'eval_steps_per_second': 5.665, 'epoch': 0.92}
{'loss': 1.3869, 'grad_norm': 0.06117838993668556, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3333529233932495, 'eval_runtime': 11.0975, 'eval_samples_per_second': 90.02, 'eval_steps_per_second': 5.677, 'epoch': 0.96}
{'loss': 1.3743, 'grad_norm': 0.08922094106674194, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3322639465332031, 'eval_runtime': 11.1244, 'eval_samples_per_second': 89.803, 'eval_steps_per_second': 5.663, 'epoch': 1.0}
{'train_runtime': 565.5992, 'train_samples_per_second': 17.679, 'train_steps_per_second': 1.105, 'train_loss': 1.6017018249511719, 'epoch': 1.0}
train_results:  {'eval_loss': [3.311464786529541, 1.9380234479904175, 1.6618918180465698, 1.5535615682601929, 1.52273690700531, 1.5064462423324585, 1.4909993410110474, 1.478508472442627, 1.4712882041931152, 1.4557549953460693, 1.4426170587539673, 1.4327350854873657, 1.4241234064102173, 1.413534164428711, 1.4031696319580078, 1.3936020135879517, 1.3838509321212769, 1.3728208541870117, 1.363639235496521, 1.3557324409484863, 1.347845196723938, 1.341201663017273, 1.3367903232574463, 1.3333529233932495, 1.3322639465332031], 'performance': [0.48, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<05:11,  1.28it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:01<00:23, 16.62it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:01<00:11, 31.86it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:01<00:07, 43.93it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:06, 54.48it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 63.74it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:02<00:04, 72.99it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:02<00:03, 83.67it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:02<00:03, 87.31it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 90.77it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 95.53it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 103.12it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:03<00:01, 113.22it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:03<00:01, 112.30it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:03<00:01, 117.45it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:03<00:01, 116.64it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.21it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 118.22it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 120.66it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:04<00:00, 127.93it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:04<00:00, 127.88it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:04<00:00, 128.49it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:04<00:00, 138.26it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:04<00:00, 88.91it/s] 
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4270203113555908
current iteration best possible performance (full train run):  0.4305
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422, 1.422614574432373, 1.399521827697754, 1.4231212139129639, 1.4270203113555908]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5449 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.7022882511622307
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 1.005824630979564
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.9646911746566994
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 1.0773623736609894
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 1.2132523217702014
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4420, dtype=torch.float64), tensor(0.2067, dtype=torch.float64), 0, tensor(0.2118, dtype=torch.float64), tensor(0.1360, dtype=torch.float64), 0, 0, 32, 0, 1, 1, 1, 1, 128, 0.07849697983537485, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(9.4082e-17, dtype=torch.float64), tensor(0.0035, dtype=torch.float64), tensor(0.4420, dtype=torch.float64), tensor(0.2067, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2118, dtype=torch.float64), tensor(0.1360, dtype=torch.float64), tensor(3.5206e-17, dtype=torch.float64), tensor(7.3844e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.7850, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.442
  sciq: 0.207
  triviaqa: 0
  truthfulqa_gen: 0.212
  wikitext: 0.136
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07849697983537485,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.07849697983537485
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 247,463,936 || all params: 8,277,725,184 || trainable%: 2.9895
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9963
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  996
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.49it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.64it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 99.40it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.58it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 111.27it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 123.30it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.40it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.68it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.73it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 135.17it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.64it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 143.10it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 146.28it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 147.17it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 151.56it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 152.11it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 156.17it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 168.68it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 170.01it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 183.21it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 191.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 149.40it/s]
Evaluation performance at step 25: 0.5
{'loss': 4.1371, 'grad_norm': 0.1714860200881958, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 3.2604737281799316, 'eval_runtime': 10.8182, 'eval_samples_per_second': 92.067, 'eval_steps_per_second': 5.824, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.39it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 88.33it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 99.22it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 106.58it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 111.25it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 123.17it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 124.36it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 133.56it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 133.41it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.77it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 136.14it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 142.57it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 145.72it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 146.62it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 151.04it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:01<00:00, 151.53it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 155.56it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 168.12it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 169.49it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 182.66it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 190.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 149.01it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.5022, 'grad_norm': 0.1912260800600052, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.043710708618164, 'eval_runtime': 10.8515, 'eval_samples_per_second': 91.785, 'eval_steps_per_second': 5.806, 'epoch': 0.08}
{'loss': 1.9363, 'grad_norm': 0.06248004734516144, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.8023574352264404, 'eval_runtime': 10.9084, 'eval_samples_per_second': 91.306, 'eval_steps_per_second': 5.775, 'epoch': 0.12}
{'loss': 1.7291, 'grad_norm': 0.07420501857995987, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.7093777656555176, 'eval_runtime': 11.0022, 'eval_samples_per_second': 90.528, 'eval_steps_per_second': 5.726, 'epoch': 0.16}
{'loss': 1.6678, 'grad_norm': 0.05886884778738022, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.6740306615829468, 'eval_runtime': 11.0001, 'eval_samples_per_second': 90.545, 'eval_steps_per_second': 5.727, 'epoch': 0.2}
{'loss': 1.6293, 'grad_norm': 0.04541827738285065, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.6523268222808838, 'eval_runtime': 11.0128, 'eval_samples_per_second': 90.44, 'eval_steps_per_second': 5.721, 'epoch': 0.24}
{'loss': 1.603, 'grad_norm': 0.04332897067070007, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.6355503797531128, 'eval_runtime': 10.9739, 'eval_samples_per_second': 90.761, 'eval_steps_per_second': 5.741, 'epoch': 0.28}
{'loss': 1.6746, 'grad_norm': 0.04930312931537628, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.6234365701675415, 'eval_runtime': 10.9615, 'eval_samples_per_second': 90.864, 'eval_steps_per_second': 5.747, 'epoch': 0.32}
{'loss': 1.633, 'grad_norm': 0.05371063947677612, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.6121639013290405, 'eval_runtime': 10.9521, 'eval_samples_per_second': 90.942, 'eval_steps_per_second': 5.752, 'epoch': 0.36}
{'loss': 1.5532, 'grad_norm': 0.05290836840867996, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.6042031049728394, 'eval_runtime': 10.9424, 'eval_samples_per_second': 91.022, 'eval_steps_per_second': 5.757, 'epoch': 0.4}
{'loss': 1.5479, 'grad_norm': 0.05920166149735451, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.597193956375122, 'eval_runtime': 10.9927, 'eval_samples_per_second': 90.605, 'eval_steps_per_second': 5.731, 'epoch': 0.44}
{'loss': 1.6601, 'grad_norm': 0.04522547125816345, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.5919201374053955, 'eval_runtime': 10.9439, 'eval_samples_per_second': 91.01, 'eval_steps_per_second': 5.757, 'epoch': 0.48}
{'loss': 1.609, 'grad_norm': 0.05070073902606964, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.5868791341781616, 'eval_runtime': 10.8959, 'eval_samples_per_second': 91.41, 'eval_steps_per_second': 5.782, 'epoch': 0.52}
{'loss': 1.5654, 'grad_norm': 0.07361724227666855, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.583808422088623, 'eval_runtime': 10.9102, 'eval_samples_per_second': 91.29, 'eval_steps_per_second': 5.774, 'epoch': 0.56}
{'loss': 1.5893, 'grad_norm': 0.050005145370960236, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.577682614326477, 'eval_runtime': 10.9409, 'eval_samples_per_second': 91.034, 'eval_steps_per_second': 5.758, 'epoch': 0.6}
{'loss': 1.5919, 'grad_norm': 0.04564276710152626, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.574169397354126, 'eval_runtime': 10.961, 'eval_samples_per_second': 90.868, 'eval_steps_per_second': 5.748, 'epoch': 0.64}
{'loss': 1.5682, 'grad_norm': 0.04782679304480553, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.5706015825271606, 'eval_runtime': 10.8869, 'eval_samples_per_second': 91.486, 'eval_steps_per_second': 5.787, 'epoch': 0.68}
{'loss': 1.5985, 'grad_norm': 0.05106734856963158, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.5665390491485596, 'eval_runtime': 10.8982, 'eval_samples_per_second': 91.391, 'eval_steps_per_second': 5.781, 'epoch': 0.72}
{'loss': 1.5727, 'grad_norm': 0.04908395931124687, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.5646049976348877, 'eval_runtime': 10.9354, 'eval_samples_per_second': 91.08, 'eval_steps_per_second': 5.761, 'epoch': 0.76}
{'loss': 1.5952, 'grad_norm': 0.05317112058401108, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 1.5618617534637451, 'eval_runtime': 10.9649, 'eval_samples_per_second': 90.835, 'eval_steps_per_second': 5.746, 'epoch': 0.8}
{'loss': 1.5545, 'grad_norm': 0.0566619336605072, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 1.5590379238128662, 'eval_runtime': 10.9579, 'eval_samples_per_second': 90.893, 'eval_steps_per_second': 5.749, 'epoch': 0.84}
{'loss': 1.5368, 'grad_norm': 0.07043307274580002, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 1.5575380325317383, 'eval_runtime': 10.9643, 'eval_samples_per_second': 90.84, 'eval_steps_per_second': 5.746, 'epoch': 0.88}
{'loss': 1.5701, 'grad_norm': 0.056862685829401016, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 1.5556174516677856, 'eval_runtime': 10.9707, 'eval_samples_per_second': 90.787, 'eval_steps_per_second': 5.743, 'epoch': 0.92}
{'loss': 1.5717, 'grad_norm': 0.05536360293626785, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 1.554652214050293, 'eval_runtime': 11.0484, 'eval_samples_per_second': 90.149, 'eval_steps_per_second': 5.702, 'epoch': 0.96}
{'train_runtime': 554.8028, 'train_samples_per_second': 17.958, 'train_steps_per_second': 1.123, 'train_loss': 1.7509910865159135, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2604737281799316, 2.043710708618164, 1.8023574352264404, 1.7093777656555176, 1.6740306615829468, 1.6523268222808838, 1.6355503797531128, 1.6234365701675415, 1.6121639013290405, 1.6042031049728394, 1.597193956375122, 1.5919201374053955, 1.5868791341781616, 1.583808422088623, 1.577682614326477, 1.574169397354126, 1.5706015825271606, 1.5665390491485596, 1.5646049976348877, 1.5618617534637451, 1.5590379238128662, 1.5575380325317383, 1.5556174516677856, 1.554652214050293], 'performance': [0.5, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:26,  4.59it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:08, 47.34it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 64.95it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 72.20it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:00<00:04, 77.57it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 82.21it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 87.65it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 95.43it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 96.34it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 97.34it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 100.44it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.55it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.71it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.41it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.38it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 116.94it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 116.41it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 118.68it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.20it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.18it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.01it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.67it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 108.76it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.418076515197754
current iteration best possible performance (full train run):  0.48300000000000004
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422, 1.422614574432373, 1.399521827697754, 1.4231212139129639, 1.4270203113555908, 1.418076515197754]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1522 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 1.0095677112081967
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.5459845482928198
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.5586859501002945
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.5636658399029371
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.5952770425459539
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4506, dtype=torch.float64), 0, tensor(0.4531, dtype=torch.float64), tensor(0.0202, dtype=torch.float64), tensor(0.0762, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(2.0135e-16, dtype=torch.float64), tensor(1.3277e-17, dtype=torch.float64), tensor(0.4506, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4531, dtype=torch.float64), tensor(0.0202, dtype=torch.float64), tensor(0.0762, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.451
  sciq: 0
  triviaqa: 0.453
  truthfulqa_gen: 0.02
  wikitext: 0.076
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.15it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.94it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.69it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.95it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.48it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 122.36it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 123.56it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 132.85it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 132.82it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 134.14it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 135.64it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.99it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.91it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.71it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 150.00it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.53it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.56it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.98it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.37it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.52it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 148.17it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.2464, 'grad_norm': 0.1906386762857437, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.3007972240448, 'eval_runtime': 10.9529, 'eval_samples_per_second': 91.209, 'eval_steps_per_second': 5.752, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:07, 54.00it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:04, 87.67it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 98.47it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 105.78it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 110.25it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 121.92it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 122.42it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:02, 131.73it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 131.83it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 133.36it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 134.93it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 141.41it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 144.63it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 145.52it/s]Running loglikelihood requests:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 244/400 [00:01<00:01, 149.87it/s]Running loglikelihood requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/400 [00:02<00:00, 150.53it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:02<00:00, 154.60it/s]Running loglikelihood requests:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 297/400 [00:02<00:00, 166.86it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 322/400 [00:02<00:00, 168.23it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 351/400 [00:02<00:00, 181.40it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:02<00:00, 189.11it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 147.73it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.633, 'grad_norm': 0.13778860867023468, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 2.0523838996887207, 'eval_runtime': 10.9662, 'eval_samples_per_second': 91.098, 'eval_steps_per_second': 5.745, 'epoch': 0.08}
{'loss': 1.8897, 'grad_norm': 0.08148511499166489, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7991995811462402, 'eval_runtime': 10.9395, 'eval_samples_per_second': 91.321, 'eval_steps_per_second': 5.759, 'epoch': 0.12}
{'loss': 1.7162, 'grad_norm': 0.0618501752614975, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6932061910629272, 'eval_runtime': 10.9619, 'eval_samples_per_second': 91.134, 'eval_steps_per_second': 5.747, 'epoch': 0.16}
{'loss': 1.6727, 'grad_norm': 0.05284006893634796, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6657510995864868, 'eval_runtime': 11.0253, 'eval_samples_per_second': 90.61, 'eval_steps_per_second': 5.714, 'epoch': 0.2}
{'loss': 1.7145, 'grad_norm': 0.04386134818196297, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6465190649032593, 'eval_runtime': 11.0681, 'eval_samples_per_second': 90.259, 'eval_steps_per_second': 5.692, 'epoch': 0.24}
{'loss': 1.6551, 'grad_norm': 0.03867674991488457, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6348692178726196, 'eval_runtime': 11.063, 'eval_samples_per_second': 90.301, 'eval_steps_per_second': 5.695, 'epoch': 0.28}
{'loss': 1.6457, 'grad_norm': 0.038911886513233185, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6244709491729736, 'eval_runtime': 11.0554, 'eval_samples_per_second': 90.363, 'eval_steps_per_second': 5.699, 'epoch': 0.32}
{'loss': 1.6521, 'grad_norm': 0.04761346057057381, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6152143478393555, 'eval_runtime': 11.0407, 'eval_samples_per_second': 90.483, 'eval_steps_per_second': 5.706, 'epoch': 0.36}
{'loss': 1.6501, 'grad_norm': 0.04732271656394005, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6084866523742676, 'eval_runtime': 11.0405, 'eval_samples_per_second': 90.485, 'eval_steps_per_second': 5.706, 'epoch': 0.4}
{'loss': 1.6057, 'grad_norm': 0.04397690296173096, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.601462721824646, 'eval_runtime': 11.0384, 'eval_samples_per_second': 90.502, 'eval_steps_per_second': 5.707, 'epoch': 0.44}
{'loss': 1.6194, 'grad_norm': 0.04360280930995941, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5958563089370728, 'eval_runtime': 11.0524, 'eval_samples_per_second': 90.388, 'eval_steps_per_second': 5.7, 'epoch': 0.48}
{'loss': 1.6262, 'grad_norm': 0.039701927453279495, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.59164559841156, 'eval_runtime': 11.1054, 'eval_samples_per_second': 89.956, 'eval_steps_per_second': 5.673, 'epoch': 0.52}
{'loss': 1.652, 'grad_norm': 0.04280254617333412, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5868977308273315, 'eval_runtime': 11.1326, 'eval_samples_per_second': 89.737, 'eval_steps_per_second': 5.659, 'epoch': 0.56}
{'loss': 1.6332, 'grad_norm': 0.03946234658360481, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5825830698013306, 'eval_runtime': 11.1357, 'eval_samples_per_second': 89.711, 'eval_steps_per_second': 5.657, 'epoch': 0.6}
{'loss': 1.6101, 'grad_norm': 0.03904055804014206, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5813288688659668, 'eval_runtime': 11.1299, 'eval_samples_per_second': 89.758, 'eval_steps_per_second': 5.66, 'epoch': 0.64}
{'loss': 1.6135, 'grad_norm': 0.04485873878002167, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.577169418334961, 'eval_runtime': 11.0872, 'eval_samples_per_second': 90.104, 'eval_steps_per_second': 5.682, 'epoch': 0.68}
{'loss': 1.6202, 'grad_norm': 0.04671153798699379, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5736888647079468, 'eval_runtime': 11.1006, 'eval_samples_per_second': 89.995, 'eval_steps_per_second': 5.675, 'epoch': 0.72}
{'loss': 1.6236, 'grad_norm': 0.04215718433260918, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5716288089752197, 'eval_runtime': 11.098, 'eval_samples_per_second': 90.016, 'eval_steps_per_second': 5.677, 'epoch': 0.76}
{'loss': 1.5859, 'grad_norm': 0.05258466675877571, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5704044103622437, 'eval_runtime': 11.1213, 'eval_samples_per_second': 89.828, 'eval_steps_per_second': 5.665, 'epoch': 0.8}
{'loss': 1.5635, 'grad_norm': 0.04526308551430702, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.5675381422042847, 'eval_runtime': 11.1106, 'eval_samples_per_second': 89.914, 'eval_steps_per_second': 5.67, 'epoch': 0.84}
{'loss': 1.5834, 'grad_norm': 0.046737488359212875, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5657199621200562, 'eval_runtime': 11.0668, 'eval_samples_per_second': 90.27, 'eval_steps_per_second': 5.693, 'epoch': 0.88}
{'loss': 1.5925, 'grad_norm': 0.04517650604248047, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5644512176513672, 'eval_runtime': 11.044, 'eval_samples_per_second': 90.457, 'eval_steps_per_second': 5.704, 'epoch': 0.92}
{'loss': 1.6027, 'grad_norm': 0.04299933463335037, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5637143850326538, 'eval_runtime': 11.048, 'eval_samples_per_second': 90.424, 'eval_steps_per_second': 5.702, 'epoch': 0.96}
{'loss': 1.5954, 'grad_norm': 0.04557925462722778, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5632747411727905, 'eval_runtime': 11.038, 'eval_samples_per_second': 90.506, 'eval_steps_per_second': 5.708, 'epoch': 1.0}
{'train_runtime': 561.3476, 'train_samples_per_second': 17.809, 'train_steps_per_second': 1.113, 'train_loss': 1.7841135192871094, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3007972240448, 2.0523838996887207, 1.7991995811462402, 1.6932061910629272, 1.6657510995864868, 1.6465190649032593, 1.6348692178726196, 1.6244709491729736, 1.6152143478393555, 1.6084866523742676, 1.601462721824646, 1.5958563089370728, 1.59164559841156, 1.5868977308273315, 1.5825830698013306, 1.5813288688659668, 1.577169418334961, 1.5736888647079468, 1.5716288089752197, 1.5704044103622437, 1.5675381422042847, 1.5657199621200562, 1.5644512176513672, 1.5637143850326538, 1.5632747411727905], 'performance': [0.49, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<02:05,  3.18it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:11, 32.83it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:07, 51.89it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:05, 62.43it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 70.23it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:04, 76.51it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 83.43it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:03, 92.05it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 93.89it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:02<00:02, 95.62it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:02<00:02, 99.13it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:02, 106.00it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 115.68it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 113.58it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 118.37it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 117.45it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:03<00:01, 116.98it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:03<00:00, 119.16it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 121.48it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 128.41it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 128.37it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 128.89it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 138.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 103.60it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4269964694976807
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422, 1.422614574432373, 1.399521827697754, 1.4231212139129639, 1.4270203113555908, 1.418076515197754, 1.4269964694976807]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1644 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.9894604272680132
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.9890256452401015
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.5626929731701343
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.7485031997308369
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 1.1583246197427604
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1057, dtype=torch.float64), 0, tensor(0.4456, dtype=torch.float64), 0, tensor(0.1920, dtype=torch.float64), tensor(0.1557, dtype=torch.float64), tensor(0.1010, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 0, 1, 128, 0.01136464103066891, 1.4800000190734866, 0]
normalized proposed parameters for next round by BO: [tensor(0.1057, dtype=torch.float64), tensor(6.6026e-17, dtype=torch.float64), tensor(0.4456, dtype=torch.float64), tensor(1.7097e-17, dtype=torch.float64), tensor(0.1920, dtype=torch.float64), tensor(0.1557, dtype=torch.float64), tensor(0.1010, dtype=torch.float64), tensor(2.8872e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1136, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.106
  gsm8k: 0
  rowan_hellaswag: 0.446
  sciq: 0
  triviaqa: 0.192
  truthfulqa_gen: 0.156
  wikitext: 0.101
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.01136464103066891,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.01136464103066891
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'arc_challenge': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.39it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 94.47it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 106.20it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 114.00it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 119.00it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 131.17it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 132.56it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 142.12it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 142.49it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 144.30it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 146.05it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 153.04it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 156.36it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 157.38it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 162.33it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 166.57it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 177.97it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 187.52it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 192.67it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 204.00it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 159.82it/s]
Evaluation performance at step 25: 0.49
{'loss': 4.2394, 'grad_norm': 0.10449258238077164, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.49}
{'eval_loss': 3.5177431106567383, 'eval_runtime': 10.2416, 'eval_samples_per_second': 97.544, 'eval_steps_per_second': 6.151, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 3
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   2%|‚ñè         | 9/400 [00:00<00:06, 58.51it/s]Running loglikelihood requests:   6%|‚ñã         | 26/400 [00:00<00:03, 94.67it/s]Running loglikelihood requests:  10%|‚ñà         | 42/400 [00:00<00:03, 105.55it/s]Running loglikelihood requests:  14%|‚ñà‚ñç        | 58/400 [00:00<00:03, 113.69it/s]Running loglikelihood requests:  18%|‚ñà‚ñä        | 74/400 [00:00<00:02, 118.84it/s]Running loglikelihood requests:  23%|‚ñà‚ñà‚ñé       | 93/400 [00:00<00:02, 131.69it/s]Running loglikelihood requests:  27%|‚ñà‚ñà‚ñã       | 109/400 [00:00<00:02, 132.60it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/400 [00:01<00:01, 142.49it/s]Running loglikelihood requests:  36%|‚ñà‚ñà‚ñà‚ñå      | 144/400 [00:01<00:01, 142.79it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 160/400 [00:01<00:01, 144.49it/s]Running loglikelihood requests:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/400 [00:01<00:01, 146.15it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 194/400 [00:01<00:01, 153.11it/s]Running loglikelihood requests:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/400 [00:01<00:01, 156.45it/s]Running loglikelihood requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/400 [00:01<00:01, 157.38it/s]Running loglikelihood requests:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 252/400 [00:01<00:00, 162.15it/s]Running loglikelihood requests:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 277/400 [00:01<00:00, 166.40it/s]Running loglikelihood requests:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/400 [00:02<00:00, 177.69it/s]Running loglikelihood requests:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 333/400 [00:02<00:00, 187.33it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:02<00:00, 192.47it/s]Running loglikelihood requests:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 386/400 [00:02<00:00, 203.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:02<00:00, 159.80it/s]
Evaluation performance at step 50: 0.49
{'loss': 2.8108, 'grad_norm': 0.07746755331754684, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 2.2991158962249756, 'eval_runtime': 10.2318, 'eval_samples_per_second': 97.637, 'eval_steps_per_second': 6.157, 'epoch': 0.08}
{'loss': 2.0726, 'grad_norm': 0.06195911392569542, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9808534383773804, 'eval_runtime': 10.2825, 'eval_samples_per_second': 97.155, 'eval_steps_per_second': 6.127, 'epoch': 0.12}
{'loss': 1.903, 'grad_norm': 0.05712885782122612, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.869774580001831, 'eval_runtime': 10.3274, 'eval_samples_per_second': 96.733, 'eval_steps_per_second': 6.1, 'epoch': 0.16}
{'loss': 1.8244, 'grad_norm': 0.03637675568461418, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.826419711112976, 'eval_runtime': 10.3276, 'eval_samples_per_second': 96.731, 'eval_steps_per_second': 6.1, 'epoch': 0.2}
{'loss': 1.7921, 'grad_norm': 0.031697336584329605, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8024401664733887, 'eval_runtime': 10.3666, 'eval_samples_per_second': 96.368, 'eval_steps_per_second': 6.077, 'epoch': 0.24}
{'loss': 1.7568, 'grad_norm': 0.05229181796312332, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7807048559188843, 'eval_runtime': 10.3551, 'eval_samples_per_second': 96.474, 'eval_steps_per_second': 6.084, 'epoch': 0.28}
{'loss': 1.7829, 'grad_norm': 0.044554829597473145, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7648980617523193, 'eval_runtime': 10.3587, 'eval_samples_per_second': 96.441, 'eval_steps_per_second': 6.082, 'epoch': 0.32}
{'loss': 1.7073, 'grad_norm': 0.04039907082915306, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7463053464889526, 'eval_runtime': 10.3721, 'eval_samples_per_second': 96.316, 'eval_steps_per_second': 6.074, 'epoch': 0.36}
{'loss': 1.7319, 'grad_norm': 0.042772192507982254, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7248319387435913, 'eval_runtime': 10.3881, 'eval_samples_per_second': 96.168, 'eval_steps_per_second': 6.065, 'epoch': 0.4}
{'loss': 1.684, 'grad_norm': 0.05173584818840027, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6871095895767212, 'eval_runtime': 10.3843, 'eval_samples_per_second': 96.203, 'eval_steps_per_second': 6.067, 'epoch': 0.44}
{'loss': 1.636, 'grad_norm': 0.04517063498497009, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6601166725158691, 'eval_runtime': 10.3977, 'eval_samples_per_second': 96.079, 'eval_steps_per_second': 6.059, 'epoch': 0.48}
{'loss': 1.6466, 'grad_norm': 0.04179820790886879, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.644025206565857, 'eval_runtime': 10.3752, 'eval_samples_per_second': 96.287, 'eval_steps_per_second': 6.072, 'epoch': 0.52}
{'loss': 1.6101, 'grad_norm': 0.04179475083947182, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6288814544677734, 'eval_runtime': 10.3598, 'eval_samples_per_second': 96.431, 'eval_steps_per_second': 6.081, 'epoch': 0.56}
{'loss': 1.5717, 'grad_norm': 0.04564223438501358, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6111793518066406, 'eval_runtime': 10.3435, 'eval_samples_per_second': 96.582, 'eval_steps_per_second': 6.091, 'epoch': 0.6}
{'loss': 1.6155, 'grad_norm': 0.05260516703128815, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.602227807044983, 'eval_runtime': 10.327, 'eval_samples_per_second': 96.737, 'eval_steps_per_second': 6.101, 'epoch': 0.64}
{'loss': 1.6036, 'grad_norm': 0.04115914925932884, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5977718830108643, 'eval_runtime': 10.3318, 'eval_samples_per_second': 96.692, 'eval_steps_per_second': 6.098, 'epoch': 0.68}
{'loss': 1.5846, 'grad_norm': 0.04536479711532593, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5948340892791748, 'eval_runtime': 10.3345, 'eval_samples_per_second': 96.666, 'eval_steps_per_second': 6.096, 'epoch': 0.72}
{'loss': 1.5552, 'grad_norm': 0.04049177095293999, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.590621829032898, 'eval_runtime': 10.4205, 'eval_samples_per_second': 95.869, 'eval_steps_per_second': 6.046, 'epoch': 0.76}
{'loss': 1.568, 'grad_norm': 0.04271416366100311, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5883994102478027, 'eval_runtime': 10.4206, 'eval_samples_per_second': 95.868, 'eval_steps_per_second': 6.046, 'epoch': 0.8}
{'loss': 1.544, 'grad_norm': 0.03831174597144127, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.586002230644226, 'eval_runtime': 10.4187, 'eval_samples_per_second': 95.885, 'eval_steps_per_second': 6.047, 'epoch': 0.84}
{'loss': 1.5613, 'grad_norm': 0.06136536970734596, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5847231149673462, 'eval_runtime': 10.4296, 'eval_samples_per_second': 95.785, 'eval_steps_per_second': 6.041, 'epoch': 0.88}
{'loss': 1.5478, 'grad_norm': 0.04924989864230156, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5833038091659546, 'eval_runtime': 10.4285, 'eval_samples_per_second': 95.795, 'eval_steps_per_second': 6.041, 'epoch': 0.92}
{'loss': 1.5953, 'grad_norm': 0.0442335419356823, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5822358131408691, 'eval_runtime': 10.4335, 'eval_samples_per_second': 95.749, 'eval_steps_per_second': 6.038, 'epoch': 0.96}
{'loss': 1.6075, 'grad_norm': 0.05052480101585388, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.58182954788208, 'eval_runtime': 10.4316, 'eval_samples_per_second': 95.767, 'eval_steps_per_second': 6.039, 'epoch': 1.0}
{'train_runtime': 530.4028, 'train_samples_per_second': 18.85, 'train_steps_per_second': 1.178, 'train_loss': 1.8220920471191406, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5177431106567383, 2.2991158962249756, 1.9808534383773804, 1.869774580001831, 1.826419711112976, 1.8024401664733887, 1.7807048559188843, 1.7648980617523193, 1.7463053464889526, 1.7248319387435913, 1.6871095895767212, 1.6601166725158691, 1.644025206565857, 1.6288814544677734, 1.6111793518066406, 1.602227807044983, 1.5977718830108643, 1.5948340892791748, 1.590621829032898, 1.5883994102478027, 1.586002230644226, 1.5847231149673462, 1.5833038091659546, 1.5822358131408691, 1.58182954788208], 'performance': [0.49, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['arc_challenge']
Overwriting default num_fewshot of arc_challenge from None to 5
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:00<01:59,  3.35it/s]Running loglikelihood requests:   4%|‚ñç         | 17/400 [00:00<00:09, 42.24it/s]Running loglikelihood requests:   8%|‚ñä         | 34/400 [00:00<00:05, 62.43it/s]Running loglikelihood requests:  12%|‚ñà‚ñé        | 50/400 [00:00<00:04, 72.12it/s]Running loglikelihood requests:  16%|‚ñà‚ñã        | 66/400 [00:01<00:04, 79.21it/s]Running loglikelihood requests:  20%|‚ñà‚ñà        | 82/400 [00:01<00:03, 84.99it/s]Running loglikelihood requests:  25%|‚ñà‚ñà‚ñç       | 99/400 [00:01<00:03, 91.68it/s]Running loglikelihood requests:  30%|‚ñà‚ñà‚ñâ       | 118/400 [00:01<00:02, 100.23it/s]Running loglikelihood requests:  34%|‚ñà‚ñà‚ñà‚ñé      | 134/400 [00:01<00:02, 101.53it/s]Running loglikelihood requests:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/400 [00:01<00:02, 102.86it/s]Running loglikelihood requests:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/400 [00:01<00:02, 105.98it/s]Running loglikelihood requests:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/400 [00:02<00:01, 113.35it/s]Running loglikelihood requests:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/400 [00:02<00:01, 122.83it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 223/400 [00:02<00:01, 120.86it/s]Running loglikelihood requests:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 242/400 [00:02<00:01, 126.12it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 258/400 [00:02<00:01, 124.99it/s]Running loglikelihood requests:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 274/400 [00:02<00:01, 124.57it/s]Running loglikelihood requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/400 [00:02<00:00, 126.28it/s]Running loglikelihood requests:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 308/400 [00:03<00:00, 128.67it/s]Running loglikelihood requests:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327/400 [00:03<00:00, 136.21it/s]Running loglikelihood requests:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 343/400 [00:03<00:00, 135.73it/s]Running loglikelihood requests:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 359/400 [00:03<00:00, 135.85it/s]Running loglikelihood requests:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 378/400 [00:03<00:00, 146.30it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03<00:00, 112.72it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.49, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.422220230102539
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.5670000000000001
BO observations:  [1.0514811277389526, 1.0904102325439453, 1.4104036092758179, 0.4789681136608124, 1.2944432497024536, 0.609687864780426, 1.4146111011505127, 1.2918784618377686, 1.4244532585144043, 1.4243788719177246, 1.4275758266448975, 1.4248547554016113, 1.3060258626937866, 1.4257614612579346, 1.423555612564087, 1.4009408950805664, 1.4250919818878174, 1.412306308746338, 1.4260348081588745, 1.4243698120117188, 1.4268226623535156, 1.4269206523895264, 1.4264812469482422, 1.422614574432373, 1.399521827697754, 1.4231212139129639, 1.4270203113555908, 1.418076515197754, 1.4269964694976807, 1.422220230102539]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.3828 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.989269690981794
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.7617321427886109
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 1.0327878842150426
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.6144608854070753
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.5577414628237254
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4479, dtype=torch.float64), tensor(0.1826, dtype=torch.float64), 0, tensor(0.1964, dtype=torch.float64), tensor(0.1730, dtype=torch.float64), 0, 0, 32, 1, 0, 0, 1, 1, 128, 5.55254321661692e-18, 1.4800000190734903, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4479, dtype=torch.float64), tensor(0.1826, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1964, dtype=torch.float64), tensor(0.1730, dtype=torch.float64), tensor(2.8437e-18, dtype=torch.float64), tensor(7.2188e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.5525e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.462, 0.4935, 0.4935, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'arc_challenge', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_arc_challenge_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['arc_challenge'], 'weight': [1.0], 'random': [[0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001], [0.504, 0.504, 0.504, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5565000000000001, 0.5775000000000001, 0.5775000000000001, 0.5775000000000001, 0.5775000000000001, 0.5775000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001, 0.5880000000000001], [0.462, 0.4935, 0.4935, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001, 0.5670000000000001]], 'random_full_inputs': [[[0, 0, 0.3486220977433942, 0.22821320283357718, 0.1199558831110172, 0.16103105786743246, 0.09425278762743455, 0.04792497081714451, 0, 30, 1, 0, 1, 1, 1, 32, 0.0091781622496234, 16.47830600605779, 0], [0, 0, 0.4618733676528846, 0.3915978231920693, 0, 0.14652880915504615, 0, 0, 0, 32, 1, 1, 0, 1, 0, 55, 0.03663973508542471, 1.4800000190734863, 0], [0, 0, 0.6843770100585604, 0.31562298994143967, 0, 0, 0, 0, 0, 32, 1, 1, 0, 1, 0, 3, 0.0, 1.4800000190734868, 0], [0, 0, 0.43484031448052646, 0.5651596855194737, 0, 0, 0, 0, 0, 3, 1, 1, 0, 1, 0, 59, 0.010144099497573148, 1.4800000190734877, 0], [0, 0, 0.4999155000648991, 0, 0, 0.5000844999351012, 0, 0, 0, 32, 1, 1, 0, 1, 0, 128, 0.09999999999999985, 1.4800000190735152, 0], [0, 0, 0.4127745915642225, 0, 0, 0.5872254084357775, 0, 0, 0, 1, 1, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 0], [0, 0, 0.5331002374721174, 0, 0, 0.46689976252788296, 0, 0, 0, 1, 1, 1, 0, 1, 0, 128, 0.0999999999999997, 47.99999999999986, 0], [0, 0, 0.5264575028417653, 0, 0, 0.4735424971582178, 0, 0, 0, 1, 1, 1, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0.4843197773485421, 0, 0, 0.5156802226514563, 0, 0, 0, 32, 1, 1, 0, 1, 0, 2, 0.1, 1.480000019073501, 0], [0, 0, 0.49343186032068587, 0, 0, 0.35111029080953643, 0.1554578488697777, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 0], [0, 0, 0.49267273759612706, 0, 0, 0.3068817634294439, 0, 0.20044549897442918, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.480000019073487, 0], [0.5049027584729541, 0, 0.49509724152704604, 0, 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734866, 0], [0, 0, 0.5001665209487763, 0, 0, 0, 0, 0, 0.4998334790512238, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.48000001907349, 0], [0, 0, 0.4952393893261109, 0, 0.504760610673889, 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734885, 0], [0, 0, 0.5018668975622028, 0.1598584730561668, 0, 0, 0, 0.33827462938163044, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734894, 0], [0, 0, 0.4904845292290349, 0, 0, 0, 0, 0.5095154707709647, 0, 32, 1, 1, 0, 0, 0, 128, 0.09999999999999999, 1.4800000190734872, 1], [0, 0, 0.49286935212401006, 0.18744800435556616, 0.3196826435204222, 0, 0, 0, 0, 32, 0, 1, 0, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0.527135646266524, 0.05723684520751929, 0.3385749278251224, 0, 0, 0, 0.07705258070083452, 1, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734883, 0], [0, 0, 0.47092914587236767, 0, 0.023453626662746085, 0, 0, 0, 0.5056172274648862, 32, 0, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0.11564076347778159, 0.47968446134423687, 0, 0, 0.09197021672490255, 0, 0, 0.31266340677599486, 32, 0, 0, 1, 1, 1, 128, 0.09999999999999999, 1.4800000190734863, 0], [0, 0, 0.4688837485434263, 0.5311162514565739, 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734863, 0], [0, 0, 0.46550839436410574, 0, 0, 0.5344916056358944, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.480000019073489, 0], [0, 0, 0.4681349323632409, 0, 0, 0, 0, 0, 0.5318650676367591, 32, 0, 0, 1, 1, 1, 128, 0.1, 1.4800000190734868, 0], [0, 0, 0.47188830099050944, 0, 0, 0, 0.20663464562949596, 0, 0.32147705337999494, 32, 1, 0, 1, 1, 1, 128, 0.1, 1.4800000190734863, 0], [0, 0, 0.47132451529396424, 0, 0, 0, 0, 0.5286754847060358, 0, 32, 1, 0, 1, 1, 1, 128, 0.09999999999999998, 1.4800000190734866, 0], [0, 0, 0.4680618740806438, 0, 0.5319381259193559, 0, 0, 0, 0, 32, 0, 1, 1, 0, 1, 128, 0.09999999999999999, 1.4800000190734863, 1], [0, 0, 0.477852322465563, 0, 0.5221476775344366, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.3093506273941245e-18, 1.480000019073488, 0], [0, 0, 0.47559065858531524, 0, 0.42138517086033, 0, 0, 0, 0.10302417055435477, 32, 1, 0, 1, 1, 0, 128, 0.1, 1.4800000190734863, 0], [0.1958741234801421, 0, 0.47278761270234326, 0.08674563093518721, 0.03454918142743623, 0, 0.10409059791409579, 0.10595260352775443, 0, 32, 0, 0, 1, 1, 1, 128, 0.002910017978018559, 1.4800000190734872, 0], [0.052349532961900994, 0, 0.4771165181063832, 0.16860894290636697, 0.010648808616284314, 0.1755450918294197, 0, 0, 0.10611775235170665, 32, 1, 0, 1, 0, 1, 128, 0.07093225092969502, 1.4800000190734872, 0]], [[0, 0, 0.34098464827648917, 0.26713595741491325, 0.17925715657953661, 0.15078608105459843, 0.05887608885718402, 0, 0, 32, 1, 0, 1, 1, 1, 68, 0.0017964660814157977, 2.045682658071251, 0], [0, 0, 0.4375046148693675, 0.3052731235247575, 0.014554859212773425, 0.2426674023931017, 0, 0, 0, 32, 1, 1, 0, 1, 0, 77, 0.0, 1.4800000190734868, 0], [0, 0, 0.413328258084062, 0.14548497629955617, 0, 0, 0.4411867656163818, 0, 0, 32, 1, 1, 0, 1, 0, 128, 3.9681799512969464e-19, 1.4800000190734897, 0], [0, 0, 0.45091163098163295, 0, 0, 0, 0.549088369018367, 0, 0, 1, 0, 1, 0, 1, 0, 2, 0.1, 1.4800000190734877, 0], [0, 0, 0.5476401000491471, 0, 0, 0, 0.4523598999508529, 0, 0, 1, 0, 1, 0, 1, 0, 128, 5.551115123125794e-18, 1.4800000190734863, 0], [0, 0, 0.43527112984620336, 0, 0, 0, 0.5647288701537968, 0, 0, 32, 0, 1, 0, 1, 0, 2, 6.716536632422937e-18, 1.4800000190734863, 0], [0, 0, 0.4314375598026082, 0, 0.523656613868895, 0, 0.04490582632849653, 0, 0, 32, 0, 0, 0, 1, 0, 128, 8.673617379883919e-19, 1.4800000190734879, 0], [0.5784188499190057, 0, 0.4215811500809941, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 128, 2.460439600698641e-18, 1.4800000190734908, 0], [0, 0.5538583579908068, 0.44000616823176025, 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.4940117962016473e-18, 1.4800000190734943, 0], [0, 0, 0.43584255123722215, 0, 0, 0, 0, 0.5641574487627783, 0, 32, 1, 0, 1, 1, 1, 128, 7.342764528662472e-19, 1.480000019073497, 0], [0, 0, 0.4441079212877948, 0.5558920787122059, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734919, 0], [0, 0.10999918814205968, 0.45429657560538905, 0.18896655757502145, 0, 0, 0, 0.042791259550466004, 0.20394641912706354, 32, 1, 0, 1, 1, 1, 128, 4.163336342344338e-19, 1.4800000190734863, 0], [0, 0.023603636008104067, 0.4347847924483047, 0.07025472809756529, 0, 0, 0, 0, 0.47135684344602613, 1, 0, 0, 1, 1, 1, 128, 8.743006318923108e-19, 1.4800000190734868, 0], [0, 0.14160069667244762, 0.45944796270793253, 0.16579019129705488, 0, 0, 0.1319492959977342, 0.10121185332483064, 0, 32, 1, 0, 1, 1, 1, 128, 0.07257568108600239, 1.480000019073492, 0], [0, 0, 0.46723839410312756, 0, 0, 0, 0.5327616058968726, 0, 0, 32, 0, 0, 1, 1, 1, 128, 7.416883122540729e-18, 1.4800000190734863, 0], [0, 0, 0.4644030767193209, 0, 0, 0, 0, 0.5355969232806793, 0, 32, 0, 1, 0, 0, 1, 128, 2.2084803842197775e-18, 1.4800000190734863, 1], [0, 0, 0.4460327679166058, 0.37709839616615365, 0, 0, 0.17686883591724784, 0, 0, 32, 0, 1, 0, 0, 0, 128, 0.1, 1.480000019073491, 1], [0, 0.5458292367803487, 0.45417076321965133, 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 0, 128, 4.142954771576703e-19, 1.4800000190734863, 0], [0, 0, 0.45445680819137463, 0.5455431918086255, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.0624521344288573e-18, 1.480000019073488, 0], [0, 0.04929213152203195, 0.44510715292572456, 0.16906067832357244, 0, 0.15987081969764602, 0.17666921753102535, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0.4454281210620139, 0, 0, 0.5542541024028301, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734894, 0], [0, 0, 0.4489828200916777, 0, 0, 0.5510171799083226, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 6.294810148539685e-18, 1.4800000190734932, 0], [0.4329551577529243, 0, 0.45134525812063636, 0, 0, 0, 0.014977542634497468, 0, 0.10072204149194147, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.480000019073491, 0], [0, 0, 0.45030296008563436, 0, 0, 0, 0, 0, 0.5496970399143652, 32, 1, 0, 1, 0, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0.021212135952133594, 0.4653669556026478, 0.12904872560766606, 0, 0.18435728696962922, 0.20001489586792326, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 6.9875063456652065, 0], [0, 0, 0.45007374352057655, 0, 0, 0, 0.5499262564794232, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0.4473627470400826, 0, 0, 0.552637252959917, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.561251128379127e-18, 1.4800000190734863, 0], [0, 0.20606973388645172, 0.44579452709887785, 0, 0, 0.06699470191184584, 0.2811410371028249, 0, 0, 32, 1, 0, 1, 0, 1, 128, 0.01795969549658864, 1.4800000190734868, 0], [0, 0, 0.45917218099330426, 0, 0.5331002608868403, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0.45353569212483713, 0.5464643078751632, 0, 0, 0, 0, 0, 32, 1, 1, 1, 1, 1, 128, 4.789372940582841e-19, 1.4800000190734874, 0]], [[0, 0, 0.34098464827648917, 0.26713595741491325, 0.17925715657953661, 0.15078608105459843, 0.05887608885718402, 0, 0, 32, 1, 0, 1, 1, 1, 68, 0.0017964660814157977, 2.045682658071251, 0], [0, 0, 0.43733462455705185, 0.30402135628396254, 0.012140596468910244, 0.24650342269007533, 0, 0, 0, 32, 1, 1, 0, 1, 0, 77, 9.207185256757252e-05, 1.4800000190734863, 0], [0, 0, 0.413911251794464, 0.143910509952522, 0, 0, 0.44217823825301406, 0, 0, 32, 1, 1, 0, 1, 0, 128, 5.637851296924623e-19, 1.4800000190734868, 0], [0, 0, 0.44832228890193426, 0, 0, 0, 0.5516777110980658, 0, 0, 1, 0, 1, 0, 1, 0, 2, 0.09999999999999962, 1.4800000190734868, 0], [0, 0, 0.5480778558915459, 0, 0, 0, 0.45192214410845366, 0, 0, 1, 0, 1, 0, 1, 0, 128, 3.3492523426207303e-18, 1.4800000190734923, 0], [0, 0, 0.43682599360820273, 0, 0, 0, 0.5631740063917976, 0, 0, 32, 0, 1, 0, 1, 0, 2, 0.0, 1.4800000190734899, 0], [0, 0, 0.431934224345268, 0, 0.5274249083441375, 0, 0.040640867310594436, 0, 0, 32, 0, 0, 0, 1, 0, 128, 6.93889390390408e-19, 1.480000019073492, 0], [0.5782683315917938, 0, 0.42173166840820603, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 128, 4.917570893892482e-18, 1.4800000190734868, 0], [0, 0.5239717115460203, 0.4410074834970219, 0, 0, 0, 0.03502080495695779, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.7755575615628915e-18, 1.4800000190734908, 0], [0, 0, 0.43686429897426277, 0, 0, 0, 0, 0.5631357010257376, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0.44406934651646945, 0.5559306534835454, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.5862029558109827e-17, 1.4800000190735039, 0], [0, 0.11321895332942147, 0.45640429592660336, 0.19199090064463836, 0, 0, 0, 0.04761716323933368, 0.1907686868600031, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734939, 0], [0, 0.028896720426553175, 0.4354173422369321, 0.04869177371807869, 0, 0, 0, 0, 0.4869941636184361, 1, 0, 0, 1, 1, 1, 128, 1.873501354054952e-19, 1.4800000190734877, 0], [0, 0.0911233661069159, 0.4623044547185403, 0.15593447703931673, 0, 0, 0.21732682944439433, 0.07331087269083268, 0, 32, 1, 0, 1, 1, 1, 128, 0.06922330648106288, 1.4800000190734866, 0], [0, 0, 0.46723351566205595, 0, 0, 0, 0.5327664843379442, 0, 0, 32, 0, 0, 1, 1, 1, 128, 1.0376454246028031e-17, 1.4800000190734866, 0], [0, 0, 0.4662347942027373, 0, 0, 0, 0, 0.5337652057972638, 0, 32, 0, 1, 0, 0, 0, 128, 1.2059691520524628e-19, 1.480000019073487, 1], [0, 0, 0.454836763049271, 0.5451632369507292, 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 1.537975277757075e-19, 1.4800000190734885, 0], [0, 0.46060173103610275, 0.4422323291002511, 0.0971659398636465, 0, 0, 0, 0, 0, 32, 0, 0, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0.45063655533577684, 0, 0, 0, 0.5493634446642233, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734914, 0], [0, 0.5465713594422882, 0.4533950262105021, 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 3.212121977949959e-18, 1.4800000190734903, 0], [0, 0, 0.4432638784748155, 0.16608903830209096, 0, 0.16947939280782887, 0.21871038677104299, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.8201697238804387e-17, 1.4800000190734899, 0], [0, 0, 0.4449154453595102, 0, 0, 0.5487908411792617, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.480000019073491, 0], [0, 0, 0.44466000895322866, 0, 0, 0.5553399910467709, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.1, 1.4800000190734874, 0], [0.4487686853733306, 0, 0.4465329783989789, 0, 0, 0, 0, 0, 0.10469833622769074, 32, 1, 0, 1, 1, 1, 128, 0.04466557383162341, 1.480000019073489, 0], [0, 0, 0.46056635252135336, 0.13596580064471123, 0, 0.1554534628720752, 0.24801438396186046, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.03864402153526624, 6.139359264459934, 0], [0, 0, 0.4436705498675838, 0, 0, 0.0706645734292025, 0, 0, 0.4856648767032138, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0.4449359906066281, 0, 0, 0.5550640093933715, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 3.9898639947466614e-18, 1.4800000190734868, 0], [0, 0, 0.44198996603146085, 0.2066700901517996, 0, 0.2118455503419975, 0.13603500092921653, 0, 0, 32, 0, 1, 1, 1, 1, 128, 0.07849697983537485, 1.4800000190734863, 0], [0, 0, 0.45058357218994444, 0, 0.4530981534099009, 0.020163806155234304, 0.07615446824492027, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0.10567930385821481, 0, 0.44556724599195474, 0, 0.1920055902233668, 0.15570381092695915, 0.1010440489995046, 0, 0, 32, 1, 0, 1, 0, 1, 128, 0.01136464103066891, 1.4800000190734866, 0]]], 'random_full_train_performance': [0.462, 0.4935, 0.4935, 0.5670000000000001, 0.546, 0.504, 0.5145, 0.5670000000000001, 0.462, 0.48300000000000004, 0.4935, 0.5145, 0.5565000000000001, 0.5145, 0.399, 0.48300000000000004, 0.525, 0.462, 0.4515, 0.4515, 0.48300000000000004, 0.441, 0.462, 0.47250000000000003, 0.47250000000000003, 0.47250000000000003, 0.4305, 0.48300000000000004, 0.4515, 0.47250000000000003]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.055 MB of 0.081 MB uploadedwandb: \ 0.769 MB of 1.393 MB uploadedwandb: | 1.393 MB of 1.393 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            eval/runtime ‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñÇ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñÜ‚ñÅ‚ñÉ‚ñà‚ñá‚ñá‚ñÇ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÖ
wandb: eval/samples_per_second ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÇ‚ñÖ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:   eval/steps_per_second ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÇ‚ñÖ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÖ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ
wandb:         train/grad_norm ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:     train/learning_rate ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñà‚ñÉ‚ñá‚ñÇ‚ñà‚ñÉ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÇ‚ñÜ‚ñÅ
wandb:              train/loss ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                eval/loss 1.58183
wandb:             eval/runtime 10.4316
wandb:  eval/samples_per_second 95.767
wandb:    eval/steps_per_second 6.039
wandb:               total_flos 1.178150455738368e+17
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 0.05052
wandb:      train/learning_rate 0.0
wandb:               train/loss 1.6075
wandb:               train_loss 1.82209
wandb:            train_runtime 530.4028
wandb: train_samples_per_second 18.85
wandb:   train_steps_per_second 1.178
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/ofzqks7d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_054259-ofzqks7d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

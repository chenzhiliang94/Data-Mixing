2026-01-02 14:43:30.726399: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-02 14:43:30.755837: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-02 14:43:30.755898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-02 14:43:30.756639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-02 14:43:30.760969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-02 14:43:31.767139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'commonsense_qa', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_commonsense_qa_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['commonsense_qa']
evaluation tasks and weights:  {'commonsense_qa': (1.0, 'acc,none')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
Using the latest cached version of the dataset since cais/mmlu couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'all' at /home/alfred/Data-Mixing/datasets/cais___mmlu/all/0.0.0/c30699e8356da336a370243923dbaf21066bb9fe (last modified on Fri Jan  2 14:42:11 2026).
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/commonsense_qa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 17, 0, 1, 0, 1, 1, 74, 0.06944379333880846, 10, 0]
Checking history sample input_X_between_0_1:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 0.53125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.578125, 0.6944379333880846, 0.20833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 18, 0, 0, 1, 0, 0, 88, 0.07338847024538249, 48, 1]
Checking history sample input_X_between_0_1:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 0.5625, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6875, 0.7338847024538249, 1.0, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 27, 1, 1, 0, 1, 0, 35, 0.056205119903528195, 6, 0]
Checking history sample input_X_between_0_1:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 0.84375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2734375, 0.5620511990352819, 0.125, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 22, 0, 0, 0, 0, 1, 14, 0.05785794551060587, 9, 1]
Checking history sample input_X_between_0_1:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 0.6875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.109375, 0.5785794551060587, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 15, 1, 0, 0, 1, 1, 33, 0.046509797776235, 40, 0]
Checking history sample input_X_between_0_1:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 0.46875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2578125, 0.46509797776234996, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 11, 0, 0, 1, 1, 1, 70, 0.005359902393564798, 44, 1]
Checking history sample input_X_between_0_1:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 0.34375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.546875, 0.05359902393564797, 0.9166666666666666, 1.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 3, 1, 1, 0, 0, 1, 89, 0.004230716614147934, 21, 1]
Checking history sample input_X_between_0_1:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 0.09375, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6953125, 0.042307166141479335, 0.4375, 1.0]
Checking history sample performance at 625 steps:  0.79
Checking history sample input_X:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 11, 0, 0, 1, 0, 1, 128, 0.02126981773222546, 43, 0]
Checking history sample input_X_between_0_1:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 0.34375, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2126981773222546, 0.8958333333333334, 0.0]
Checking history sample performance at 625 steps:  0.77
Checking history sample input_X:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 4, 0, 1, 0, 0, 1, 51, 0.048394033913014715, 30, 0]
Checking history sample input_X_between_0_1:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3984375, 0.4839403391301471, 0.625, 0.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 25, 1, 0, 1, 0, 0, 103, 0.021006146654874183, 41, 0]
Checking history sample input_X_between_0_1:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 0.78125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8046875, 0.2100614665487418, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.71
Checking history sample input_X:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 30, 0, 0, 0, 1, 0, 30, 0.06473585772462145, 22, 0]
Checking history sample input_X_between_0_1:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.234375, 0.6473585772462145, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 10, 0, 0, 1, 0, 1, 33, 0.053839147652140054, 30, 1]
Checking history sample input_X_between_0_1:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 0.3125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2578125, 0.5383914765214005, 0.625, 1.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 27, 0, 1, 1, 1, 0, 91, 0.026257080995186557, 27, 0]
Checking history sample input_X_between_0_1:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 0.84375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7109375, 0.26257080995186555, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 28, 1, 1, 1, 0, 0, 23, 0.048743516472132736, 35, 1]
Checking history sample input_X_between_0_1:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 0.875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1796875, 0.48743516472132736, 0.7291666666666666, 1.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 14, 1, 0, 1, 0, 0, 32, 0.04806841934255027, 48, 0]
Checking history sample input_X_between_0_1:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 0.4375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.48068419342550267, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 19, 0, 0, 1, 1, 0, 22, 0.09470699670511286, 20, 1]
Checking history sample input_X_between_0_1:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 0.59375, 0.0, 0.0, 1.0, 1.0, 0.0, 0.171875, 0.9470699670511286, 0.4166666666666667, 1.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 2, 0, 1, 1, 1, 0, 72, 0.07202092774167915, 33, 1]
Checking history sample input_X_between_0_1:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5625, 0.7202092774167914, 0.6875, 1.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 15, 1, 0, 1, 0, 1, 69, 0.09577839904714924, 28, 0]
Checking history sample input_X_between_0_1:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 0.46875, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5390625, 0.9577839904714924, 0.5833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 30, 1, 1, 1, 0, 1, 19, 0.07443761643578949, 27, 0]
Checking history sample input_X_between_0_1:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 0.9375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1484375, 0.7443761643578948, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 12, 1, 0, 0, 1, 1, 19, 0.0774188690692264, 1, 1]
Checking history sample input_X_between_0_1:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1484375, 0.774188690692264, 0.020833333333333332, 1.0]
Checking history sample performance at 625 steps:  0.75
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5261 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6748884320259094, 0.6303252577781677, 0.45122551918029785, 0.5257068872451782, 0.8423717021942139, 0.5772936940193176, 0.1061011552810669, 0.2774418592453003, 0.7124675512313843, 0.16793550550937653, 0.7378118634223938, 0.8397462368011475, 0.02183586359024048, 0.635924220085144, 0.6683146357536316, 0.0333840437233448, 0.9766417145729065, 0.12407062202692032, 0.6649089455604553]  ‚Üí  acq = 0.814367276997614
X = [0.24807536602020264, 0.2534061670303345, 0.7061936855316162, 0.15592330694198608, 0.13806980848312378, 0.7860366702079773, 0.9366238117218018, 0.015771985054016113, 0.4134841561317444, 0.34608742594718933, 0.46828967332839966, 0.7713990807533264, 0.6382732391357422, 0.1674214005470276, 0.1275690197944641, 0.18772318959236145, 0.2236396074295044, 0.8496488332748413, 0.3363347053527832]  ‚Üí  acq = 0.814367276997614
X = [0.013236939907073975, 0.7887355089187622, 0.09093403816223145, 0.6727840304374695, 0.9500905275344849, 0.6937209367752075, 0.9432356357574463, 0.6116105318069458, 0.518531858921051, 0.10232050716876984, 0.5960594415664673, 0.4218653440475464, 0.9870723485946655, 0.06797009706497192, 0.13094043731689453, 0.6193310618400574, 0.3055227994918823, 0.8952270746231079, 0.6170487999916077]  ‚Üí  acq = 0.814367276997614
X = [0.6351932287216187, 0.18204593658447266, 0.6492195129394531, 0.7795006632804871, 0.04995673894882202, 0.277421772480011, 0.6261661648750305, 0.6927430033683777, 0.0744139552116394, 0.12399573624134064, 0.18647819757461548, 0.324509859085083, 0.7441951632499695, 0.31088322401046753, 0.8332905769348145, 0.686494767665863, 0.5450034141540527, 0.619135856628418, 0.13159984350204468]  ‚Üí  acq = 0.814367276997614
X = [0.7972372174263, 0.5773959755897522, 0.26049280166625977, 0.682296633720398, 0.540012538433075, 0.42391252517700195, 0.38969844579696655, 0.7221259474754333, 0.48066776990890503, 0.046559032052755356, 0.5994484424591064, 0.01847696304321289, 0.62251216173172, 0.7287628650665283, 0.059460580348968506, 0.07708705216646194, 0.8080663084983826, 0.12595248222351074, 0.915192186832428]  ‚Üí  acq = 0.814367276997614
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.0623, dtype=torch.float64), tensor(0.0835, dtype=torch.float64), 0, tensor(0.4623, dtype=torch.float64), 0, tensor(0.3539, dtype=torch.float64), 0, tensor(0.0380, dtype=torch.float64), 0, 22, 1, 1, 0, 0, 1, 114, 0.03870725114564932, 11.316541150265632, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.0623, dtype=torch.float64), tensor(0.0835, dtype=torch.float64), tensor(2.4605e-20, dtype=torch.float64), tensor(0.4623, dtype=torch.float64), tensor(1.8115e-19, dtype=torch.float64), tensor(0.3539, dtype=torch.float64), tensor(1.7368e-19, dtype=torch.float64), tensor(0.0380, dtype=torch.float64), tensor(9.1812e-20, dtype=torch.float64), tensor(0.6838, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8908, dtype=torch.float64), tensor(0.3871, dtype=torch.float64), tensor(0.2358, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.062
  gsm8k: 0.084
  rowan_hellaswag: 0
  sciq: 0.462
  triviaqa: 0
  truthfulqa_gen: 0.354
  wikitext: 0
  mmlu: 0.038
  arc_challenge: 0

LoRA Parameters:
  lora_r: (114,)
  lora_dropout: (0.03870725114564932,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (11.316541150265632,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  114
lora dropout:  0.03870725114564932
lora alpha:  11.316541150265632
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 79,613,952 || all params: 8,109,875,200 || trainable%: 0.9817
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260102_144621-77z8f64m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/77z8f64m
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.37it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 444.01it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 517.00it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 557.92it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 587.74it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 610.52it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 599.00it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.1312, 'grad_norm': 0.3872820734977722, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.3285465240478516, 'eval_runtime': 7.0166, 'eval_samples_per_second': 142.377, 'eval_steps_per_second': 8.979, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 285.44it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 474.81it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 553.90it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 600.83it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 633.89it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 658.20it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 645.51it/s]
Evaluation performance at step 50: 0.74
{'loss': 1.7029, 'grad_norm': 0.13860049843788147, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.3288304805755615, 'eval_runtime': 7.0269, 'eval_samples_per_second': 142.169, 'eval_steps_per_second': 8.966, 'epoch': 0.08}
{'loss': 1.2779, 'grad_norm': 0.13916148245334625, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1565097570419312, 'eval_runtime': 7.0279, 'eval_samples_per_second': 142.148, 'eval_steps_per_second': 8.964, 'epoch': 0.12}
{'loss': 1.1398, 'grad_norm': 0.1462218165397644, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0903621912002563, 'eval_runtime': 7.038, 'eval_samples_per_second': 141.943, 'eval_steps_per_second': 8.951, 'epoch': 0.16}
{'loss': 1.0572, 'grad_norm': 0.1252259612083435, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0009821653366089, 'eval_runtime': 7.0555, 'eval_samples_per_second': 141.593, 'eval_steps_per_second': 8.929, 'epoch': 0.2}
{'loss': 0.9457, 'grad_norm': 0.12178300321102142, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9072353839874268, 'eval_runtime': 7.0735, 'eval_samples_per_second': 141.231, 'eval_steps_per_second': 8.906, 'epoch': 0.24}
{'loss': 0.9063, 'grad_norm': 0.13149532675743103, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.83590167760849, 'eval_runtime': 7.0875, 'eval_samples_per_second': 140.953, 'eval_steps_per_second': 8.889, 'epoch': 0.28}
{'loss': 0.8568, 'grad_norm': 0.143156498670578, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8178641200065613, 'eval_runtime': 7.0768, 'eval_samples_per_second': 141.165, 'eval_steps_per_second': 8.902, 'epoch': 0.32}
{'loss': 0.8528, 'grad_norm': 0.15475384891033173, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.80272376537323, 'eval_runtime': 7.1023, 'eval_samples_per_second': 140.659, 'eval_steps_per_second': 8.87, 'epoch': 0.36}
{'loss': 0.8553, 'grad_norm': 0.1691211611032486, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7915769219398499, 'eval_runtime': 7.1012, 'eval_samples_per_second': 140.68, 'eval_steps_per_second': 8.872, 'epoch': 0.4}
{'loss': 0.8402, 'grad_norm': 0.13899102807044983, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7800403237342834, 'eval_runtime': 7.1008, 'eval_samples_per_second': 140.689, 'eval_steps_per_second': 8.872, 'epoch': 0.44}
{'loss': 0.848, 'grad_norm': 0.15729506313800812, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7716401219367981, 'eval_runtime': 7.0972, 'eval_samples_per_second': 140.76, 'eval_steps_per_second': 8.877, 'epoch': 0.48}
{'loss': 0.8339, 'grad_norm': 0.16201049089431763, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7592332363128662, 'eval_runtime': 7.0978, 'eval_samples_per_second': 140.747, 'eval_steps_per_second': 8.876, 'epoch': 0.52}
{'loss': 0.7693, 'grad_norm': 0.16485829651355743, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7505601644515991, 'eval_runtime': 7.0958, 'eval_samples_per_second': 140.788, 'eval_steps_per_second': 8.879, 'epoch': 0.56}
{'loss': 0.7794, 'grad_norm': 0.1702706515789032, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7381266951560974, 'eval_runtime': 7.1023, 'eval_samples_per_second': 140.659, 'eval_steps_per_second': 8.87, 'epoch': 0.6}
{'loss': 0.7789, 'grad_norm': 0.1763152778148651, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.727491021156311, 'eval_runtime': 7.12, 'eval_samples_per_second': 140.309, 'eval_steps_per_second': 8.848, 'epoch': 0.64}
{'loss': 0.7722, 'grad_norm': 0.16868487000465393, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7170819640159607, 'eval_runtime': 7.1191, 'eval_samples_per_second': 140.327, 'eval_steps_per_second': 8.849, 'epoch': 0.68}
{'loss': 0.7817, 'grad_norm': 0.21166156232357025, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7099266648292542, 'eval_runtime': 7.1281, 'eval_samples_per_second': 140.149, 'eval_steps_per_second': 8.838, 'epoch': 0.72}
{'loss': 0.7686, 'grad_norm': 0.1489066630601883, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7006226778030396, 'eval_runtime': 7.1227, 'eval_samples_per_second': 140.256, 'eval_steps_per_second': 8.845, 'epoch': 0.76}
{'loss': 0.7444, 'grad_norm': 0.18989568948745728, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6932944059371948, 'eval_runtime': 7.1126, 'eval_samples_per_second': 140.456, 'eval_steps_per_second': 8.858, 'epoch': 0.8}
{'loss': 0.7476, 'grad_norm': 0.23808294534683228, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6876923441886902, 'eval_runtime': 7.1019, 'eval_samples_per_second': 140.667, 'eval_steps_per_second': 8.871, 'epoch': 0.84}
{'loss': 0.758, 'grad_norm': 0.19933611154556274, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6826050877571106, 'eval_runtime': 7.1111, 'eval_samples_per_second': 140.486, 'eval_steps_per_second': 8.859, 'epoch': 0.88}
{'loss': 0.7504, 'grad_norm': 0.22943982481956482, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6775089502334595, 'eval_runtime': 7.094, 'eval_samples_per_second': 140.822, 'eval_steps_per_second': 8.881, 'epoch': 0.92}
{'loss': 0.7081, 'grad_norm': 0.17909669876098633, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.676604688167572, 'eval_runtime': 7.0876, 'eval_samples_per_second': 140.95, 'eval_steps_per_second': 8.889, 'epoch': 0.96}
{'loss': 0.724, 'grad_norm': 0.17917507886886597, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6741360425949097, 'eval_runtime': 7.1084, 'eval_samples_per_second': 140.539, 'eval_steps_per_second': 8.863, 'epoch': 1.0}
{'train_runtime': 362.7864, 'train_samples_per_second': 27.559, 'train_steps_per_second': 1.723, 'train_loss': 1.0132306396484374, 'epoch': 1.0}
train_results:  {'eval_loss': [2.3285465240478516, 1.3288304805755615, 1.1565097570419312, 1.0903621912002563, 1.0009821653366089, 0.9072353839874268, 0.83590167760849, 0.8178641200065613, 0.80272376537323, 0.7915769219398499, 0.7800403237342834, 0.7716401219367981, 0.7592332363128662, 0.7505601644515991, 0.7381266951560974, 0.727491021156311, 0.7170819640159607, 0.7099266648292542, 0.7006226778030396, 0.6932944059371948, 0.6876923441886902, 0.6826050877571106, 0.6775089502334595, 0.676604688167572, 0.6741360425949097], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:11,  1.98it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 126.30it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 218.12it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 288.96it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 342.84it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 388.97it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 330.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8265793919563293
current iteration best possible performance (full train run):  0.777
max performance so far:  0.777
BO observations:  [0.8265793919563293]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2406 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 0.8323279720535237
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.8323279720535237
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 0.8323279720535237
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 0.8323279720535237
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 0.8323279720535237
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7660, dtype=torch.float64), 0, tensor(0.2324, dtype=torch.float64), 0, 0, 0, 15, 1, 1, 0, 1, 1, 128, 0.011031599296155284, 9.01917275306552, 0]
normalized proposed parameters for next round by BO: [tensor(6.1311e-18, dtype=torch.float64), tensor(0.0016, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7660, dtype=torch.float64), tensor(1.5774e-18, dtype=torch.float64), tensor(0.2324, dtype=torch.float64), tensor(1.5774e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4639, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1103, dtype=torch.float64), tensor(0.1879, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.766
  triviaqa: 0
  truthfulqa_gen: 0.232
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.011031599296155284,)
  num_layers_to_apply: (15,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (9.01917275306552,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  15
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.011031599296155284
lora alpha:  9.01917275306552
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,337,920 || all params: 8,126,599,168 || trainable%: 1.1855
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9983
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  998
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:22,  6.05it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:03, 132.24it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:01, 305.77it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 418.01it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 502.15it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 560.31it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 604.92it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 498.86it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.7103, 'grad_norm': 0.7386803030967712, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.464799642562866, 'eval_runtime': 3.2043, 'eval_samples_per_second': 311.455, 'eval_steps_per_second': 19.661, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 289.30it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 480.39it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 559.89it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 608.17it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 643.37it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 665.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 653.76it/s]
Evaluation performance at step 50: 0.74
{'loss': 1.5201, 'grad_norm': 0.3537900447845459, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.045738935470581, 'eval_runtime': 3.1851, 'eval_samples_per_second': 313.338, 'eval_steps_per_second': 19.78, 'epoch': 0.08}
{'loss': 0.9325, 'grad_norm': 0.16004733741283417, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.8761544227600098, 'eval_runtime': 3.182, 'eval_samples_per_second': 313.642, 'eval_steps_per_second': 19.799, 'epoch': 0.12}
{'loss': 0.844, 'grad_norm': 0.10904591530561447, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.842637300491333, 'eval_runtime': 3.1948, 'eval_samples_per_second': 312.38, 'eval_steps_per_second': 19.719, 'epoch': 0.16}
{'loss': 0.8174, 'grad_norm': 0.11123571544885635, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.8279417753219604, 'eval_runtime': 3.1903, 'eval_samples_per_second': 312.823, 'eval_steps_per_second': 19.747, 'epoch': 0.2}
{'loss': 0.8546, 'grad_norm': 0.13110846281051636, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.8145880103111267, 'eval_runtime': 3.2022, 'eval_samples_per_second': 311.66, 'eval_steps_per_second': 19.674, 'epoch': 0.24}
{'loss': 0.8234, 'grad_norm': 0.12160800397396088, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.7987940311431885, 'eval_runtime': 3.2033, 'eval_samples_per_second': 311.555, 'eval_steps_per_second': 19.667, 'epoch': 0.28}
{'loss': 0.7806, 'grad_norm': 0.11943922191858292, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.787247359752655, 'eval_runtime': 3.2035, 'eval_samples_per_second': 311.537, 'eval_steps_per_second': 19.666, 'epoch': 0.32}
{'loss': 0.7981, 'grad_norm': 0.12610267102718353, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.7784621715545654, 'eval_runtime': 3.1947, 'eval_samples_per_second': 312.392, 'eval_steps_per_second': 19.72, 'epoch': 0.36}
{'loss': 0.8039, 'grad_norm': 0.12670893967151642, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.7657607197761536, 'eval_runtime': 3.183, 'eval_samples_per_second': 313.538, 'eval_steps_per_second': 19.792, 'epoch': 0.4}
{'loss': 0.806, 'grad_norm': 0.11500953137874603, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.7567970752716064, 'eval_runtime': 3.1899, 'eval_samples_per_second': 312.867, 'eval_steps_per_second': 19.75, 'epoch': 0.44}
{'loss': 0.7521, 'grad_norm': 0.13489127159118652, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.7474068999290466, 'eval_runtime': 3.1821, 'eval_samples_per_second': 313.626, 'eval_steps_per_second': 19.798, 'epoch': 0.48}
{'loss': 0.7822, 'grad_norm': 0.13849782943725586, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.740888237953186, 'eval_runtime': 3.181, 'eval_samples_per_second': 313.734, 'eval_steps_per_second': 19.805, 'epoch': 0.52}
{'loss': 0.7853, 'grad_norm': 0.131755992770195, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.7332680225372314, 'eval_runtime': 3.1914, 'eval_samples_per_second': 312.713, 'eval_steps_per_second': 19.74, 'epoch': 0.56}
{'loss': 0.7642, 'grad_norm': 0.12280946224927902, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.7247854471206665, 'eval_runtime': 3.1887, 'eval_samples_per_second': 312.985, 'eval_steps_per_second': 19.758, 'epoch': 0.6}
{'loss': 0.7624, 'grad_norm': 0.14029237627983093, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.7151197195053101, 'eval_runtime': 3.1851, 'eval_samples_per_second': 313.33, 'eval_steps_per_second': 19.779, 'epoch': 0.64}
{'loss': 0.7362, 'grad_norm': 0.13122941553592682, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.7104138135910034, 'eval_runtime': 3.2028, 'eval_samples_per_second': 311.603, 'eval_steps_per_second': 19.67, 'epoch': 0.68}
{'loss': 0.7382, 'grad_norm': 0.14504991471767426, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.7015267014503479, 'eval_runtime': 3.1928, 'eval_samples_per_second': 312.581, 'eval_steps_per_second': 19.732, 'epoch': 0.72}
{'loss': 0.7696, 'grad_norm': 0.15570537745952606, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.6958128809928894, 'eval_runtime': 3.2032, 'eval_samples_per_second': 311.567, 'eval_steps_per_second': 19.668, 'epoch': 0.76}
{'loss': 0.7511, 'grad_norm': 0.1515025794506073, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.6911900043487549, 'eval_runtime': 3.1961, 'eval_samples_per_second': 312.255, 'eval_steps_per_second': 19.711, 'epoch': 0.8}
{'loss': 0.7542, 'grad_norm': 0.147617906332016, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.6849779486656189, 'eval_runtime': 3.1924, 'eval_samples_per_second': 312.622, 'eval_steps_per_second': 19.735, 'epoch': 0.84}
{'loss': 0.7563, 'grad_norm': 0.14996324479579926, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.6809453964233398, 'eval_runtime': 3.2048, 'eval_samples_per_second': 311.413, 'eval_steps_per_second': 19.658, 'epoch': 0.88}
{'loss': 0.7219, 'grad_norm': 0.15338046848773956, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.676866352558136, 'eval_runtime': 3.1977, 'eval_samples_per_second': 312.099, 'eval_steps_per_second': 19.702, 'epoch': 0.92}
{'loss': 0.7409, 'grad_norm': 0.14938050508499146, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.6747958660125732, 'eval_runtime': 3.1884, 'eval_samples_per_second': 313.008, 'eval_steps_per_second': 19.759, 'epoch': 0.96}
{'train_runtime': 168.9362, 'train_samples_per_second': 59.093, 'train_steps_per_second': 3.694, 'train_loss': 0.9693315671040461, 'epoch': 1.0}
train_results:  {'eval_loss': [2.464799642562866, 1.045738935470581, 0.8761544227600098, 0.842637300491333, 0.8279417753219604, 0.8145880103111267, 0.7987940311431885, 0.787247359752655, 0.7784621715545654, 0.7657607197761536, 0.7567970752716064, 0.7474068999290466, 0.740888237953186, 0.7332680225372314, 0.7247854471206665, 0.7151197195053101, 0.7104138135910034, 0.7015267014503479, 0.6958128809928894, 0.6911900043487549, 0.6849779486656189, 0.6809453964233398, 0.676866352558136, 0.6747958660125732], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:37,  1.48it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 123.47it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 218.42it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 290.94it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 346.37it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 393.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 320.75it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8341447114944458
current iteration best possible performance (full train run):  0.8295000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2257 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 0.8289811621437316
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.8199912615185765
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 0.8293068762726432
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.8295531232195992
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.8295393574263035
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.8536, dtype=torch.float64), 0, tensor(0.1464, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 1, 0, 1, 122, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(9.6366e-18, dtype=torch.float64), tensor(2.2825e-17, dtype=torch.float64), tensor(9.6833e-17, dtype=torch.float64), tensor(0.8536, dtype=torch.float64), tensor(4.6633e-17, dtype=torch.float64), tensor(0.1464, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9538, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.854
  triviaqa: 0
  truthfulqa_gen: 0.146
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (122,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  122
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 143,917,056 || all params: 8,174,178,304 || trainable%: 1.7606
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:01<12:10,  1.46s/it]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:06, 68.93it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 143.49it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 222.00it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 299.69it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:02<00:00, 374.75it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 226.88it/s]
Evaluation performance at step 25: 0.73
{'loss': 5.2745, 'grad_norm': 0.2293841689825058, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.6497507095336914, 'eval_runtime': 3.4206, 'eval_samples_per_second': 292.052, 'eval_steps_per_second': 18.418, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 268.17it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 446.50it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 519.78it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 564.45it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 595.78it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 619.44it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 607.19it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.321, 'grad_norm': 0.08955411612987518, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.5408930778503418, 'eval_runtime': 3.3823, 'eval_samples_per_second': 295.364, 'eval_steps_per_second': 18.627, 'epoch': 0.08}
{'loss': 1.4153, 'grad_norm': 0.15288279950618744, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.306716799736023, 'eval_runtime': 3.3823, 'eval_samples_per_second': 295.363, 'eval_steps_per_second': 18.626, 'epoch': 0.12}
{'loss': 1.219, 'grad_norm': 0.056621719151735306, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1829653978347778, 'eval_runtime': 3.3844, 'eval_samples_per_second': 295.179, 'eval_steps_per_second': 18.615, 'epoch': 0.16}
{'loss': 1.1623, 'grad_norm': 0.047273773699998856, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1370450258255005, 'eval_runtime': 3.3877, 'eval_samples_per_second': 294.893, 'eval_steps_per_second': 18.597, 'epoch': 0.2}
{'loss': 1.1154, 'grad_norm': 0.05124770849943161, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1021685600280762, 'eval_runtime': 3.3792, 'eval_samples_per_second': 295.632, 'eval_steps_per_second': 18.643, 'epoch': 0.24}
{'loss': 1.0858, 'grad_norm': 0.06237431988120079, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0627800226211548, 'eval_runtime': 3.3842, 'eval_samples_per_second': 295.199, 'eval_steps_per_second': 18.616, 'epoch': 0.28}
{'loss': 1.0299, 'grad_norm': 0.07117859274148941, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0062419176101685, 'eval_runtime': 3.3843, 'eval_samples_per_second': 295.191, 'eval_steps_per_second': 18.616, 'epoch': 0.32}
{'loss': 0.9342, 'grad_norm': 0.0904376357793808, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8751617670059204, 'eval_runtime': 3.3881, 'eval_samples_per_second': 294.858, 'eval_steps_per_second': 18.595, 'epoch': 0.36}
{'loss': 0.8622, 'grad_norm': 0.04657461866736412, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8229559063911438, 'eval_runtime': 3.3929, 'eval_samples_per_second': 294.438, 'eval_steps_per_second': 18.568, 'epoch': 0.4}
{'loss': 0.829, 'grad_norm': 0.05836661159992218, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7615885138511658, 'eval_runtime': 3.388, 'eval_samples_per_second': 294.866, 'eval_steps_per_second': 18.595, 'epoch': 0.44}
{'loss': 0.7383, 'grad_norm': 0.042490337044000626, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7392615675926208, 'eval_runtime': 3.3966, 'eval_samples_per_second': 294.12, 'eval_steps_per_second': 18.548, 'epoch': 0.48}
{'loss': 0.7515, 'grad_norm': 0.04066184163093567, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7338031530380249, 'eval_runtime': 3.3918, 'eval_samples_per_second': 294.535, 'eval_steps_per_second': 18.574, 'epoch': 0.52}
{'loss': 0.7335, 'grad_norm': 0.03976241871714592, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7294146418571472, 'eval_runtime': 3.3979, 'eval_samples_per_second': 294.005, 'eval_steps_per_second': 18.541, 'epoch': 0.56}
{'loss': 0.732, 'grad_norm': 0.056959979236125946, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.723794162273407, 'eval_runtime': 3.3948, 'eval_samples_per_second': 294.273, 'eval_steps_per_second': 18.558, 'epoch': 0.6}
{'loss': 0.7454, 'grad_norm': 0.045671429485082626, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7211626768112183, 'eval_runtime': 3.4026, 'eval_samples_per_second': 293.597, 'eval_steps_per_second': 18.515, 'epoch': 0.64}
{'loss': 0.7387, 'grad_norm': 0.04163413494825363, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7169002294540405, 'eval_runtime': 3.3991, 'eval_samples_per_second': 293.902, 'eval_steps_per_second': 18.534, 'epoch': 0.68}
{'loss': 0.7218, 'grad_norm': 0.04833114892244339, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7121343016624451, 'eval_runtime': 3.3947, 'eval_samples_per_second': 294.286, 'eval_steps_per_second': 18.559, 'epoch': 0.72}
{'loss': 0.7203, 'grad_norm': 0.04547138139605522, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7104836702346802, 'eval_runtime': 3.4126, 'eval_samples_per_second': 292.737, 'eval_steps_per_second': 18.461, 'epoch': 0.76}
{'loss': 0.7289, 'grad_norm': 0.047891587018966675, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.707556426525116, 'eval_runtime': 3.3934, 'eval_samples_per_second': 294.399, 'eval_steps_per_second': 18.566, 'epoch': 0.8}
{'loss': 0.7308, 'grad_norm': 0.05004219338297844, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7051361799240112, 'eval_runtime': 3.3958, 'eval_samples_per_second': 294.188, 'eval_steps_per_second': 18.552, 'epoch': 0.84}
{'loss': 0.725, 'grad_norm': 0.05613168329000473, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7030848264694214, 'eval_runtime': 3.3911, 'eval_samples_per_second': 294.599, 'eval_steps_per_second': 18.578, 'epoch': 0.88}
{'loss': 0.7283, 'grad_norm': 0.048213548958301544, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7017849087715149, 'eval_runtime': 3.3956, 'eval_samples_per_second': 294.201, 'eval_steps_per_second': 18.553, 'epoch': 0.92}
{'loss': 0.7398, 'grad_norm': 0.0559997595846653, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.700549304485321, 'eval_runtime': 3.3975, 'eval_samples_per_second': 294.042, 'eval_steps_per_second': 18.543, 'epoch': 0.96}
{'loss': 0.703, 'grad_norm': 0.04950139671564102, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6999695301055908, 'eval_runtime': 3.4052, 'eval_samples_per_second': 293.373, 'eval_steps_per_second': 18.501, 'epoch': 1.0}
{'train_runtime': 196.4754, 'train_samples_per_second': 50.892, 'train_steps_per_second': 3.181, 'train_loss': 1.0994350189208983, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6497507095336914, 1.5408930778503418, 1.306716799736023, 1.1829653978347778, 1.1370450258255005, 1.1021685600280762, 1.0627800226211548, 1.0062419176101685, 0.8751617670059204, 0.8229559063911438, 0.7615885138511658, 0.7392615675926208, 0.7338031530380249, 0.7294146418571472, 0.723794162273407, 0.7211626768112183, 0.7169002294540405, 0.7121343016624451, 0.7104836702346802, 0.707556426525116, 0.7051361799240112, 0.7030848264694214, 0.7017849087715149, 0.700549304485321, 0.6999695301055908], 'performance': [0.73, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:10,  2.00it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 148.88it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 243.70it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 308.90it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 355.86it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 394.70it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 348.02it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8439578413963318
current iteration best possible performance (full train run):  0.8400000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0384 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.819736521035695
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 0.8285908267495985
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.8264613851842949
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 0.8295723012545582
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 0.8287109187278229
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7575, dtype=torch.float64), 0, tensor(0.2425, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(3.7359e-18, dtype=torch.float64), tensor(2.5401e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7575, dtype=torch.float64), tensor(5.4117e-17, dtype=torch.float64), tensor(0.2425, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.3451e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.757
  triviaqa: 0
  truthfulqa_gen: 0.243
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:19,  2.50it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:05, 80.82it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:01, 225.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 340.30it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 432.49it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:01<00:00, 506.44it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:01<00:00, 564.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 399.93it/s]
Evaluation performance at step 25: 0.74
{'loss': 5.2639, 'grad_norm': 0.49795183539390564, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.6960959434509277, 'eval_runtime': 3.3116, 'eval_samples_per_second': 301.666, 'eval_steps_per_second': 19.024, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 286.80it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 477.45it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 557.86it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 605.91it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 639.81it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 665.56it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 651.67it/s]
Evaluation performance at step 50: 0.72
{'loss': 2.2916, 'grad_norm': 0.4059467613697052, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 1.3401225805282593, 'eval_runtime': 3.2873, 'eval_samples_per_second': 303.896, 'eval_steps_per_second': 19.165, 'epoch': 0.08}
{'loss': 1.1376, 'grad_norm': 0.11420971155166626, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9874582886695862, 'eval_runtime': 3.2696, 'eval_samples_per_second': 305.539, 'eval_steps_per_second': 19.268, 'epoch': 0.12}
{'loss': 0.9076, 'grad_norm': 0.10526949912309647, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8275490999221802, 'eval_runtime': 3.3229, 'eval_samples_per_second': 300.639, 'eval_steps_per_second': 18.959, 'epoch': 0.16}
{'loss': 0.8128, 'grad_norm': 0.05080809444189072, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8046634197235107, 'eval_runtime': 3.2945, 'eval_samples_per_second': 303.232, 'eval_steps_per_second': 19.123, 'epoch': 0.2}
{'loss': 0.7954, 'grad_norm': 0.08112016320228577, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7955793738365173, 'eval_runtime': 3.3247, 'eval_samples_per_second': 300.478, 'eval_steps_per_second': 18.949, 'epoch': 0.24}
{'loss': 0.7795, 'grad_norm': 0.06056411936879158, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7843117713928223, 'eval_runtime': 3.3038, 'eval_samples_per_second': 302.378, 'eval_steps_per_second': 19.069, 'epoch': 0.28}
{'loss': 0.7863, 'grad_norm': 0.056215669959783554, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7800094485282898, 'eval_runtime': 3.3166, 'eval_samples_per_second': 301.215, 'eval_steps_per_second': 18.996, 'epoch': 0.32}
{'loss': 0.7784, 'grad_norm': 0.07285400480031967, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7751760482788086, 'eval_runtime': 3.3454, 'eval_samples_per_second': 298.617, 'eval_steps_per_second': 18.832, 'epoch': 0.36}
{'loss': 0.7531, 'grad_norm': 0.06397223472595215, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7693039178848267, 'eval_runtime': 3.3477, 'eval_samples_per_second': 298.41, 'eval_steps_per_second': 18.819, 'epoch': 0.4}
{'loss': 0.7648, 'grad_norm': 0.06963400542736053, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7666648030281067, 'eval_runtime': 3.3222, 'eval_samples_per_second': 300.704, 'eval_steps_per_second': 18.963, 'epoch': 0.44}
{'loss': 0.776, 'grad_norm': 0.05605214461684227, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7629327774047852, 'eval_runtime': 3.3131, 'eval_samples_per_second': 301.531, 'eval_steps_per_second': 19.015, 'epoch': 0.48}
{'loss': 0.7556, 'grad_norm': 0.0633816346526146, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7590894103050232, 'eval_runtime': 3.3064, 'eval_samples_per_second': 302.145, 'eval_steps_per_second': 19.054, 'epoch': 0.52}
{'loss': 0.7513, 'grad_norm': 0.08580559492111206, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7573265433311462, 'eval_runtime': 3.3096, 'eval_samples_per_second': 301.852, 'eval_steps_per_second': 19.036, 'epoch': 0.56}
{'loss': 0.7531, 'grad_norm': 0.0826840028166771, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7554267048835754, 'eval_runtime': 3.3087, 'eval_samples_per_second': 301.934, 'eval_steps_per_second': 19.041, 'epoch': 0.6}
{'loss': 0.7599, 'grad_norm': 0.06644251942634583, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7524802684783936, 'eval_runtime': 3.3091, 'eval_samples_per_second': 301.898, 'eval_steps_per_second': 19.039, 'epoch': 0.64}
{'loss': 0.739, 'grad_norm': 0.06269355118274689, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7502222061157227, 'eval_runtime': 3.3043, 'eval_samples_per_second': 302.331, 'eval_steps_per_second': 19.066, 'epoch': 0.68}
{'loss': 0.7499, 'grad_norm': 0.06580473482608795, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7483300566673279, 'eval_runtime': 3.3146, 'eval_samples_per_second': 301.39, 'eval_steps_per_second': 19.007, 'epoch': 0.72}
{'loss': 0.7554, 'grad_norm': 0.06477692723274231, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7465418577194214, 'eval_runtime': 3.3073, 'eval_samples_per_second': 302.058, 'eval_steps_per_second': 19.049, 'epoch': 0.76}
{'loss': 0.7509, 'grad_norm': 0.06893197447061539, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7449078559875488, 'eval_runtime': 3.3091, 'eval_samples_per_second': 301.899, 'eval_steps_per_second': 19.039, 'epoch': 0.8}
{'loss': 0.7445, 'grad_norm': 0.0737156867980957, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7433287501335144, 'eval_runtime': 3.3158, 'eval_samples_per_second': 301.283, 'eval_steps_per_second': 19.0, 'epoch': 0.84}
{'loss': 0.7415, 'grad_norm': 0.06034180894494057, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7420217990875244, 'eval_runtime': 3.3095, 'eval_samples_per_second': 301.856, 'eval_steps_per_second': 19.036, 'epoch': 0.88}
{'loss': 0.7316, 'grad_norm': 0.06935527920722961, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7413054704666138, 'eval_runtime': 3.311, 'eval_samples_per_second': 301.717, 'eval_steps_per_second': 19.027, 'epoch': 0.92}
{'loss': 0.72, 'grad_norm': 0.07069703936576843, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7408683896064758, 'eval_runtime': 3.3188, 'eval_samples_per_second': 301.011, 'eval_steps_per_second': 18.983, 'epoch': 0.96}
{'loss': 0.7381, 'grad_norm': 0.07294575870037079, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7408292293548584, 'eval_runtime': 3.3113, 'eval_samples_per_second': 301.693, 'eval_steps_per_second': 19.026, 'epoch': 1.0}
{'train_runtime': 193.6038, 'train_samples_per_second': 51.647, 'train_steps_per_second': 3.228, 'train_loss': 1.0215185363769532, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6960959434509277, 1.3401225805282593, 0.9874582886695862, 0.8275490999221802, 0.8046634197235107, 0.7955793738365173, 0.7843117713928223, 0.7800094485282898, 0.7751760482788086, 0.7693039178848267, 0.7666648030281067, 0.7629327774047852, 0.7590894103050232, 0.7573265433311462, 0.7554267048835754, 0.7524802684783936, 0.7502222061157227, 0.7483300566673279, 0.7465418577194214, 0.7449078559875488, 0.7433287501335144, 0.7420217990875244, 0.7413054704666138, 0.7408683896064758, 0.7408292293548584], 'performance': [0.74, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<06:26,  1.29it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 95.04it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 177.66it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 249.81it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 309.65it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 361.82it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 279.65it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  0.8477370142936707
current iteration best possible performance (full train run):  0.756
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0262 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 0.8277957453005079
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.8235171511623183
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 0.8286066571385957
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.8267418912070903
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.8179975959318376
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7404, dtype=torch.float64), 0, 0, tensor(0.2596, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 0, 0, 113, 0.0, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9975e-17, dtype=torch.float64), tensor(0.7404, dtype=torch.float64), tensor(3.6919e-17, dtype=torch.float64), tensor(3.6797e-17, dtype=torch.float64), tensor(0.2596, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8805, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.74
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.26
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (113,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  113
lora dropout:  0.0
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 114,786,304 || all params: 8,145,047,552 || trainable%: 1.4093
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.49it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 429.30it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 500.49it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 544.25it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 574.45it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 594.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 583.89it/s]
Evaluation performance at step 25: 0.73
{'loss': 4.8402, 'grad_norm': 0.16879001259803772, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.6523094177246094, 'eval_runtime': 7.0149, 'eval_samples_per_second': 142.411, 'eval_steps_per_second': 8.981, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 256.01it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 428.41it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 499.47it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 542.28it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 573.54it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 597.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 584.78it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.6056, 'grad_norm': 0.08144154399633408, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.9340176582336426, 'eval_runtime': 7.0433, 'eval_samples_per_second': 141.836, 'eval_steps_per_second': 8.945, 'epoch': 0.08}
{'loss': 1.8391, 'grad_norm': 0.07800757139921188, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6407310962677002, 'eval_runtime': 7.027, 'eval_samples_per_second': 142.165, 'eval_steps_per_second': 8.965, 'epoch': 0.12}
{'loss': 1.5787, 'grad_norm': 0.0681450143456459, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5326851606369019, 'eval_runtime': 7.0224, 'eval_samples_per_second': 142.258, 'eval_steps_per_second': 8.971, 'epoch': 0.16}
{'loss': 1.5869, 'grad_norm': 0.0529450997710228, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.486856460571289, 'eval_runtime': 7.0093, 'eval_samples_per_second': 142.525, 'eval_steps_per_second': 8.988, 'epoch': 0.2}
{'loss': 1.5013, 'grad_norm': 0.056858211755752563, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4492990970611572, 'eval_runtime': 7.0269, 'eval_samples_per_second': 142.169, 'eval_steps_per_second': 8.966, 'epoch': 0.24}
{'loss': 1.4893, 'grad_norm': 0.06322284042835236, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.424405813217163, 'eval_runtime': 7.0123, 'eval_samples_per_second': 142.465, 'eval_steps_per_second': 8.984, 'epoch': 0.28}
{'loss': 1.3981, 'grad_norm': 0.08178895711898804, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4039517641067505, 'eval_runtime': 7.0146, 'eval_samples_per_second': 142.417, 'eval_steps_per_second': 8.981, 'epoch': 0.32}
{'loss': 1.4459, 'grad_norm': 0.06503143906593323, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3843073844909668, 'eval_runtime': 7.0354, 'eval_samples_per_second': 141.996, 'eval_steps_per_second': 8.955, 'epoch': 0.36}
{'loss': 1.4041, 'grad_norm': 0.0608619824051857, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3660459518432617, 'eval_runtime': 7.0226, 'eval_samples_per_second': 142.255, 'eval_steps_per_second': 8.971, 'epoch': 0.4}
{'loss': 1.3904, 'grad_norm': 0.07609374076128006, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3471580743789673, 'eval_runtime': 7.0232, 'eval_samples_per_second': 142.244, 'eval_steps_per_second': 8.97, 'epoch': 0.44}
{'loss': 1.4067, 'grad_norm': 0.08019851893186569, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.321066975593567, 'eval_runtime': 7.0222, 'eval_samples_per_second': 142.263, 'eval_steps_per_second': 8.972, 'epoch': 0.48}
{'loss': 1.235, 'grad_norm': 0.09924746304750443, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2764111757278442, 'eval_runtime': 7.0123, 'eval_samples_per_second': 142.464, 'eval_steps_per_second': 8.984, 'epoch': 0.52}
{'loss': 1.2296, 'grad_norm': 0.07743048667907715, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.247225046157837, 'eval_runtime': 7.0191, 'eval_samples_per_second': 142.326, 'eval_steps_per_second': 8.976, 'epoch': 0.56}
{'loss': 1.3012, 'grad_norm': 0.0730842798948288, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2320997714996338, 'eval_runtime': 7.0185, 'eval_samples_per_second': 142.339, 'eval_steps_per_second': 8.976, 'epoch': 0.6}
{'loss': 1.2074, 'grad_norm': 0.07424266636371613, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2201848030090332, 'eval_runtime': 7.0218, 'eval_samples_per_second': 142.271, 'eval_steps_per_second': 8.972, 'epoch': 0.64}
{'loss': 1.2327, 'grad_norm': 0.07572585344314575, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2096476554870605, 'eval_runtime': 7.0746, 'eval_samples_per_second': 141.209, 'eval_steps_per_second': 8.905, 'epoch': 0.68}
{'loss': 1.2111, 'grad_norm': 0.07865051925182343, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1977760791778564, 'eval_runtime': 7.0648, 'eval_samples_per_second': 141.406, 'eval_steps_per_second': 8.917, 'epoch': 0.72}
{'loss': 1.249, 'grad_norm': 0.08325830101966858, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1865290403366089, 'eval_runtime': 7.0662, 'eval_samples_per_second': 141.378, 'eval_steps_per_second': 8.916, 'epoch': 0.76}
{'loss': 1.151, 'grad_norm': 0.08431237190961838, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.174755334854126, 'eval_runtime': 7.0584, 'eval_samples_per_second': 141.534, 'eval_steps_per_second': 8.926, 'epoch': 0.8}
{'loss': 1.0945, 'grad_norm': 0.10410736501216888, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1633108854293823, 'eval_runtime': 7.0292, 'eval_samples_per_second': 142.122, 'eval_steps_per_second': 8.963, 'epoch': 0.84}
{'loss': 1.1597, 'grad_norm': 0.09264211356639862, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.153023600578308, 'eval_runtime': 7.0309, 'eval_samples_per_second': 142.086, 'eval_steps_per_second': 8.96, 'epoch': 0.88}
{'loss': 1.1732, 'grad_norm': 0.07989010214805603, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1450430154800415, 'eval_runtime': 7.0365, 'eval_samples_per_second': 141.974, 'eval_steps_per_second': 8.953, 'epoch': 0.92}
{'loss': 1.0803, 'grad_norm': 0.07387176156044006, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1396740674972534, 'eval_runtime': 7.0272, 'eval_samples_per_second': 142.161, 'eval_steps_per_second': 8.965, 'epoch': 0.96}
{'loss': 1.1301, 'grad_norm': 0.08644644170999527, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1381542682647705, 'eval_runtime': 7.0257, 'eval_samples_per_second': 142.193, 'eval_steps_per_second': 8.967, 'epoch': 1.0}
{'train_runtime': 370.4799, 'train_samples_per_second': 26.989, 'train_steps_per_second': 1.687, 'train_loss': 1.51764228515625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6523094177246094, 1.9340176582336426, 1.6407310962677002, 1.5326851606369019, 1.486856460571289, 1.4492990970611572, 1.424405813217163, 1.4039517641067505, 1.3843073844909668, 1.3660459518432617, 1.3471580743789673, 1.321066975593567, 1.2764111757278442, 1.247225046157837, 1.2320997714996338, 1.2201848030090332, 1.2096476554870605, 1.1977760791778564, 1.1865290403366089, 1.174755334854126, 1.1633108854293823, 1.153023600578308, 1.1450430154800415, 1.1396740674972534, 1.1381542682647705], 'performance': [0.73, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:01<09:00,  1.08s/it]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 84.64it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 159.98it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 225.55it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 281.36it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 329.66it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 244.64it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8383235931396484
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.0559 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 0.8246892104187569
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.823327661892429
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.8237724229581709
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.8095475839619185
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 0.8205147554895074
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 1, 1, 0, 0, 109, 0.1, 36.48962017173048, 1]
normalized proposed parameters for next round by BO: [tensor(1.0273e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2171e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(8.7061e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8517, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7602, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 1.0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (109,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (36.48962017173048,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  109
lora dropout:  0.1
lora alpha:  36.48962017173048
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 110,723,072 || all params: 8,140,984,320 || trainable%: 1.3601
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 259.65it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 432.53it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 502.59it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 541.81it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 570.34it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 591.78it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 581.94it/s]
Evaluation performance at step 25: 0.73
{'loss': 3.3982, 'grad_norm': 0.8621888160705566, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 1.2993462085723877, 'eval_runtime': 3.5436, 'eval_samples_per_second': 282.2, 'eval_steps_per_second': 17.779, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 260.26it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 431.46it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 501.70it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 520.84it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 552.07it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 581.43it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 571.78it/s]
Evaluation performance at step 50: 0.74
{'loss': 1.1267, 'grad_norm': 0.3505643904209137, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 0.9954332709312439, 'eval_runtime': 3.5243, 'eval_samples_per_second': 283.744, 'eval_steps_per_second': 17.876, 'epoch': 0.08}
{'loss': 0.9089, 'grad_norm': 0.3293847143650055, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.835796058177948, 'eval_runtime': 3.5179, 'eval_samples_per_second': 284.257, 'eval_steps_per_second': 17.908, 'epoch': 0.12}
{'loss': 0.789, 'grad_norm': 0.3566122353076935, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7173677682876587, 'eval_runtime': 3.5315, 'eval_samples_per_second': 283.166, 'eval_steps_per_second': 17.839, 'epoch': 0.16}
{'loss': 0.7076, 'grad_norm': 0.3188546299934387, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6890558004379272, 'eval_runtime': 3.5345, 'eval_samples_per_second': 282.926, 'eval_steps_per_second': 17.824, 'epoch': 0.2}
{'loss': 0.7241, 'grad_norm': 0.30396562814712524, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6716347932815552, 'eval_runtime': 3.5302, 'eval_samples_per_second': 283.272, 'eval_steps_per_second': 17.846, 'epoch': 0.24}
{'loss': 0.672, 'grad_norm': 0.30004069209098816, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6603000164031982, 'eval_runtime': 3.5165, 'eval_samples_per_second': 284.375, 'eval_steps_per_second': 17.916, 'epoch': 0.28}
{'loss': 0.6914, 'grad_norm': 0.28879314661026, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6476176977157593, 'eval_runtime': 3.554, 'eval_samples_per_second': 281.375, 'eval_steps_per_second': 17.727, 'epoch': 0.32}
{'loss': 0.6799, 'grad_norm': 0.2636886537075043, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.639316976070404, 'eval_runtime': 3.5535, 'eval_samples_per_second': 281.411, 'eval_steps_per_second': 17.729, 'epoch': 0.36}
{'loss': 0.6743, 'grad_norm': 0.2721498906612396, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6248884201049805, 'eval_runtime': 3.5461, 'eval_samples_per_second': 282.001, 'eval_steps_per_second': 17.766, 'epoch': 0.4}
{'loss': 0.6814, 'grad_norm': 0.2789112627506256, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6112617254257202, 'eval_runtime': 3.5684, 'eval_samples_per_second': 280.235, 'eval_steps_per_second': 17.655, 'epoch': 0.44}
{'loss': 0.6887, 'grad_norm': 0.3304162323474884, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6002563834190369, 'eval_runtime': 3.5357, 'eval_samples_per_second': 282.828, 'eval_steps_per_second': 17.818, 'epoch': 0.48}
{'loss': 0.6525, 'grad_norm': 0.30438703298568726, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5906553268432617, 'eval_runtime': 3.5503, 'eval_samples_per_second': 281.663, 'eval_steps_per_second': 17.745, 'epoch': 0.52}
{'loss': 0.6596, 'grad_norm': 0.36619871854782104, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.573843777179718, 'eval_runtime': 3.5519, 'eval_samples_per_second': 281.542, 'eval_steps_per_second': 17.737, 'epoch': 0.56}
{'loss': 0.6571, 'grad_norm': 0.3402092158794403, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5678048133850098, 'eval_runtime': 3.5538, 'eval_samples_per_second': 281.385, 'eval_steps_per_second': 17.727, 'epoch': 0.6}
{'loss': 0.6365, 'grad_norm': 0.27658766508102417, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5569854974746704, 'eval_runtime': 3.5308, 'eval_samples_per_second': 283.219, 'eval_steps_per_second': 17.843, 'epoch': 0.64}
{'loss': 0.6359, 'grad_norm': 0.3165041506290436, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5446287989616394, 'eval_runtime': 3.5069, 'eval_samples_per_second': 285.15, 'eval_steps_per_second': 17.964, 'epoch': 0.68}
{'loss': 0.6302, 'grad_norm': 0.3287094235420227, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5327437520027161, 'eval_runtime': 3.5241, 'eval_samples_per_second': 283.757, 'eval_steps_per_second': 17.877, 'epoch': 0.72}
{'loss': 0.608, 'grad_norm': 0.35969409346580505, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.523476779460907, 'eval_runtime': 3.5114, 'eval_samples_per_second': 284.783, 'eval_steps_per_second': 17.941, 'epoch': 0.76}
{'loss': 0.6447, 'grad_norm': 0.34258508682250977, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5158368945121765, 'eval_runtime': 3.4974, 'eval_samples_per_second': 285.926, 'eval_steps_per_second': 18.013, 'epoch': 0.8}
{'loss': 0.5906, 'grad_norm': 0.2926737368106842, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5029744505882263, 'eval_runtime': 3.495, 'eval_samples_per_second': 286.123, 'eval_steps_per_second': 18.026, 'epoch': 0.84}
{'loss': 0.6137, 'grad_norm': 0.3192264139652252, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.49872705340385437, 'eval_runtime': 3.5102, 'eval_samples_per_second': 284.881, 'eval_steps_per_second': 17.947, 'epoch': 0.88}
{'loss': 0.6267, 'grad_norm': 0.4046924412250519, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.49385955929756165, 'eval_runtime': 3.4958, 'eval_samples_per_second': 286.055, 'eval_steps_per_second': 18.021, 'epoch': 0.92}
{'loss': 0.5985, 'grad_norm': 0.3016766905784607, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.48913127183914185, 'eval_runtime': 3.4993, 'eval_samples_per_second': 285.773, 'eval_steps_per_second': 18.004, 'epoch': 0.96}
{'loss': 0.605, 'grad_norm': 0.2938283681869507, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.48745033144950867, 'eval_runtime': 3.5066, 'eval_samples_per_second': 285.174, 'eval_steps_per_second': 17.966, 'epoch': 1.0}
{'train_runtime': 207.5214, 'train_samples_per_second': 48.188, 'train_steps_per_second': 3.012, 'train_loss': 0.7960436538696289, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2993462085723877, 0.9954332709312439, 0.835796058177948, 0.7173677682876587, 0.6890558004379272, 0.6716347932815552, 0.6603000164031982, 0.6476176977157593, 0.639316976070404, 0.6248884201049805, 0.6112617254257202, 0.6002563834190369, 0.5906553268432617, 0.573843777179718, 0.5678048133850098, 0.5569854974746704, 0.5446287989616394, 0.5327437520027161, 0.523476779460907, 0.5158368945121765, 0.5029744505882263, 0.49872705340385437, 0.49385955929756165, 0.48913127183914185, 0.48745033144950867], 'performance': [0.73, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:50,  4.51it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 233.11it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 319.60it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 366.89it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 397.60it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 423.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 426.23it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8264036178588867
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5345 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 0.8178956264823077
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.813091373746502
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 0.8203411950686407
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.8171755162071749
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 0.8102197612853917
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4001, dtype=torch.float64), tensor(0.5999, dtype=torch.float64), 0, 0, 0, 0, 0, 27, 1, 1, 1, 0, 0, 125, 0.1, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4001, dtype=torch.float64), tensor(0.5999, dtype=torch.float64), tensor(2.6866e-17, dtype=torch.float64), tensor(8.9969e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5564e-18, dtype=torch.float64), tensor(0.8428, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9769, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.4
  sciq: 0.6
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (125,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  125
lora dropout:  0.1
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 107,136,000 || all params: 8,137,397,248 || trainable%: 1.3166
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 266.41it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 443.46it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 516.94it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 562.13it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 590.82it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 615.88it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 603.80it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.4829, 'grad_norm': 0.24151672422885895, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.7175238132476807, 'eval_runtime': 10.1985, 'eval_samples_per_second': 97.956, 'eval_steps_per_second': 6.177, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 267.81it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 445.50it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 519.49it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 564.25it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 594.43it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 617.60it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 606.09it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.91, 'grad_norm': 0.12434671819210052, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 2.3011093139648438, 'eval_runtime': 10.1979, 'eval_samples_per_second': 97.961, 'eval_steps_per_second': 6.178, 'epoch': 0.08}
{'loss': 2.0375, 'grad_norm': 0.05301583930850029, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9398590326309204, 'eval_runtime': 10.1883, 'eval_samples_per_second': 98.054, 'eval_steps_per_second': 6.184, 'epoch': 0.12}
{'loss': 1.8501, 'grad_norm': 0.05940541997551918, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.789247751235962, 'eval_runtime': 10.2127, 'eval_samples_per_second': 97.819, 'eval_steps_per_second': 6.169, 'epoch': 0.16}
{'loss': 1.7293, 'grad_norm': 0.0464826337993145, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7282629013061523, 'eval_runtime': 10.221, 'eval_samples_per_second': 97.74, 'eval_steps_per_second': 6.164, 'epoch': 0.2}
{'loss': 1.6867, 'grad_norm': 0.05880509316921234, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.700834035873413, 'eval_runtime': 10.2443, 'eval_samples_per_second': 97.517, 'eval_steps_per_second': 6.15, 'epoch': 0.24}
{'loss': 1.6183, 'grad_norm': 0.0593765527009964, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6861085891723633, 'eval_runtime': 10.2443, 'eval_samples_per_second': 97.517, 'eval_steps_per_second': 6.15, 'epoch': 0.28}
{'loss': 1.6587, 'grad_norm': 0.05115330219268799, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6747299432754517, 'eval_runtime': 10.2488, 'eval_samples_per_second': 97.475, 'eval_steps_per_second': 6.147, 'epoch': 0.32}
{'loss': 1.6574, 'grad_norm': 0.04849032312631607, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.666778564453125, 'eval_runtime': 10.2377, 'eval_samples_per_second': 97.58, 'eval_steps_per_second': 6.154, 'epoch': 0.36}
{'loss': 1.6132, 'grad_norm': 0.05413072183728218, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.65470552444458, 'eval_runtime': 10.2374, 'eval_samples_per_second': 97.583, 'eval_steps_per_second': 6.154, 'epoch': 0.4}
{'loss': 1.6763, 'grad_norm': 0.04563925415277481, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6463921070098877, 'eval_runtime': 10.3071, 'eval_samples_per_second': 96.923, 'eval_steps_per_second': 6.112, 'epoch': 0.44}
{'loss': 1.6301, 'grad_norm': 0.053506068885326385, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.639260172843933, 'eval_runtime': 10.3849, 'eval_samples_per_second': 96.197, 'eval_steps_per_second': 6.067, 'epoch': 0.48}
{'loss': 1.6472, 'grad_norm': 0.051749635487794876, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6340231895446777, 'eval_runtime': 10.3801, 'eval_samples_per_second': 96.241, 'eval_steps_per_second': 6.069, 'epoch': 0.52}
{'loss': 1.6489, 'grad_norm': 0.0659906268119812, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6270345449447632, 'eval_runtime': 10.3761, 'eval_samples_per_second': 96.279, 'eval_steps_per_second': 6.072, 'epoch': 0.56}
{'loss': 1.6204, 'grad_norm': 0.05109525844454765, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6163438558578491, 'eval_runtime': 10.3889, 'eval_samples_per_second': 96.16, 'eval_steps_per_second': 6.064, 'epoch': 0.6}
{'loss': 1.612, 'grad_norm': 0.05378415435552597, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6063209772109985, 'eval_runtime': 10.3434, 'eval_samples_per_second': 96.583, 'eval_steps_per_second': 6.091, 'epoch': 0.64}
{'loss': 1.6042, 'grad_norm': 0.057490602135658264, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5945298671722412, 'eval_runtime': 10.3086, 'eval_samples_per_second': 96.909, 'eval_steps_per_second': 6.111, 'epoch': 0.68}
{'loss': 1.5719, 'grad_norm': 0.06336449086666107, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5811561346054077, 'eval_runtime': 10.3212, 'eval_samples_per_second': 96.791, 'eval_steps_per_second': 6.104, 'epoch': 0.72}
{'loss': 1.5595, 'grad_norm': 0.05338633432984352, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5680145025253296, 'eval_runtime': 10.3046, 'eval_samples_per_second': 96.947, 'eval_steps_per_second': 6.114, 'epoch': 0.76}
{'loss': 1.5375, 'grad_norm': 0.06878598034381866, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5561871528625488, 'eval_runtime': 10.3114, 'eval_samples_per_second': 96.883, 'eval_steps_per_second': 6.11, 'epoch': 0.8}
{'loss': 1.5918, 'grad_norm': 0.05650894716382027, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.549529790878296, 'eval_runtime': 10.3066, 'eval_samples_per_second': 96.928, 'eval_steps_per_second': 6.113, 'epoch': 0.84}
{'loss': 1.5531, 'grad_norm': 0.05294525995850563, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5462919473648071, 'eval_runtime': 10.3296, 'eval_samples_per_second': 96.712, 'eval_steps_per_second': 6.099, 'epoch': 0.88}
{'loss': 1.5386, 'grad_norm': 0.05644626170396805, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.5445988178253174, 'eval_runtime': 10.3607, 'eval_samples_per_second': 96.422, 'eval_steps_per_second': 6.081, 'epoch': 0.92}
{'loss': 1.5354, 'grad_norm': 0.05166438966989517, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5432742834091187, 'eval_runtime': 10.3597, 'eval_samples_per_second': 96.432, 'eval_steps_per_second': 6.081, 'epoch': 0.96}
{'loss': 1.5941, 'grad_norm': 0.05407750979065895, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5426298379898071, 'eval_runtime': 10.333, 'eval_samples_per_second': 96.681, 'eval_steps_per_second': 6.097, 'epoch': 1.0}
{'train_runtime': 526.4696, 'train_samples_per_second': 18.993, 'train_steps_per_second': 1.187, 'train_loss': 1.8066024658203126, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7175238132476807, 2.3011093139648438, 1.9398590326309204, 1.789247751235962, 1.7282629013061523, 1.700834035873413, 1.6861085891723633, 1.6747299432754517, 1.666778564453125, 1.65470552444458, 1.6463921070098877, 1.639260172843933, 1.6340231895446777, 1.6270345449447632, 1.6163438558578491, 1.6063209772109985, 1.5945298671722412, 1.5811561346054077, 1.5680145025253296, 1.5561871528625488, 1.549529790878296, 1.5462919473648071, 1.5445988178253174, 1.5432742834091187, 1.5426298379898071], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:54,  2.13it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 155.65it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 251.23it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 315.78it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 361.06it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 398.03it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 355.63it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8436137437820435
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8360 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 0.8181402955847044
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.8166072037972566
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 0.8180846521614542
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.8110407042987252
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.809261406143531
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2193, dtype=torch.float64), 0, 0, tensor(0.7807, dtype=torch.float64), 0, 0, 0, 0, 0, 26, 1, 0, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.2193, dtype=torch.float64), tensor(3.0765e-17, dtype=torch.float64), tensor(4.4499e-17, dtype=torch.float64), tensor(0.7807, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.6833e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8054, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.219
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.781
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 88,604,672 || all params: 8,118,865,920 || trainable%: 1.0913
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 294.51it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 490.10it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 570.57it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 620.64it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 655.36it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 682.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 668.03it/s]
Evaluation performance at step 25: 0.74
{'loss': 5.1466, 'grad_norm': 0.2302442491054535, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 4.055542469024658, 'eval_runtime': 4.5405, 'eval_samples_per_second': 220.019, 'eval_steps_per_second': 13.875, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 284.49it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 468.53it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.94it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 588.36it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 623.21it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 650.97it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 634.76it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.7142, 'grad_norm': 0.20544211566448212, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8749901056289673, 'eval_runtime': 4.5078, 'eval_samples_per_second': 221.616, 'eval_steps_per_second': 13.976, 'epoch': 0.08}
{'loss': 1.5895, 'grad_norm': 0.05269191041588783, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4597045183181763, 'eval_runtime': 4.553, 'eval_samples_per_second': 219.414, 'eval_steps_per_second': 13.837, 'epoch': 0.12}
{'loss': 1.4442, 'grad_norm': 0.04504717141389847, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4101506471633911, 'eval_runtime': 4.5927, 'eval_samples_per_second': 217.521, 'eval_steps_per_second': 13.718, 'epoch': 0.16}
{'loss': 1.399, 'grad_norm': 0.05125981569290161, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3622034788131714, 'eval_runtime': 4.5171, 'eval_samples_per_second': 221.16, 'eval_steps_per_second': 13.947, 'epoch': 0.2}
{'loss': 1.3248, 'grad_norm': 0.07376335561275482, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2921607494354248, 'eval_runtime': 4.5933, 'eval_samples_per_second': 217.489, 'eval_steps_per_second': 13.716, 'epoch': 0.24}
{'loss': 1.2434, 'grad_norm': 0.061253007501363754, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2101572751998901, 'eval_runtime': 4.5954, 'eval_samples_per_second': 217.392, 'eval_steps_per_second': 13.709, 'epoch': 0.28}
{'loss': 1.194, 'grad_norm': 0.04325781390070915, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1758513450622559, 'eval_runtime': 4.5753, 'eval_samples_per_second': 218.346, 'eval_steps_per_second': 13.77, 'epoch': 0.32}
{'loss': 1.1713, 'grad_norm': 0.04404135048389435, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1580177545547485, 'eval_runtime': 4.5388, 'eval_samples_per_second': 220.101, 'eval_steps_per_second': 13.88, 'epoch': 0.36}
{'loss': 1.1385, 'grad_norm': 0.03607909753918648, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.142372965812683, 'eval_runtime': 4.5339, 'eval_samples_per_second': 220.342, 'eval_steps_per_second': 13.895, 'epoch': 0.4}
{'loss': 1.1346, 'grad_norm': 0.038393784314394, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1326417922973633, 'eval_runtime': 4.549, 'eval_samples_per_second': 219.609, 'eval_steps_per_second': 13.849, 'epoch': 0.44}
{'loss': 1.1254, 'grad_norm': 0.05290038511157036, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1246047019958496, 'eval_runtime': 4.5222, 'eval_samples_per_second': 220.912, 'eval_steps_per_second': 13.931, 'epoch': 0.48}
{'loss': 1.1234, 'grad_norm': 0.04161834716796875, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1175438165664673, 'eval_runtime': 4.5259, 'eval_samples_per_second': 220.73, 'eval_steps_per_second': 13.92, 'epoch': 0.52}
{'loss': 1.098, 'grad_norm': 0.0398188941180706, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.109315276145935, 'eval_runtime': 4.4634, 'eval_samples_per_second': 223.818, 'eval_steps_per_second': 14.115, 'epoch': 0.56}
{'loss': 1.1035, 'grad_norm': 0.04291243106126785, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1033257246017456, 'eval_runtime': 4.4808, 'eval_samples_per_second': 222.949, 'eval_steps_per_second': 14.06, 'epoch': 0.6}
{'loss': 1.0949, 'grad_norm': 0.03903118893504143, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0971503257751465, 'eval_runtime': 4.4498, 'eval_samples_per_second': 224.505, 'eval_steps_per_second': 14.158, 'epoch': 0.64}
{'loss': 1.0998, 'grad_norm': 0.04143405333161354, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0931233167648315, 'eval_runtime': 4.4526, 'eval_samples_per_second': 224.364, 'eval_steps_per_second': 14.149, 'epoch': 0.68}
{'loss': 1.0901, 'grad_norm': 0.040449365973472595, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.087843418121338, 'eval_runtime': 4.4454, 'eval_samples_per_second': 224.728, 'eval_steps_per_second': 14.172, 'epoch': 0.72}
{'loss': 1.0866, 'grad_norm': 0.047013722360134125, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0843416452407837, 'eval_runtime': 4.4564, 'eval_samples_per_second': 224.171, 'eval_steps_per_second': 14.137, 'epoch': 0.76}
{'loss': 1.0925, 'grad_norm': 0.04729687049984932, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.08006751537323, 'eval_runtime': 4.4432, 'eval_samples_per_second': 224.839, 'eval_steps_per_second': 14.179, 'epoch': 0.8}
{'loss': 1.0862, 'grad_norm': 0.04722234606742859, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0767351388931274, 'eval_runtime': 4.4537, 'eval_samples_per_second': 224.309, 'eval_steps_per_second': 14.146, 'epoch': 0.84}
{'loss': 1.0492, 'grad_norm': 0.03935785964131355, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.075203537940979, 'eval_runtime': 4.4519, 'eval_samples_per_second': 224.399, 'eval_steps_per_second': 14.151, 'epoch': 0.88}
{'loss': 1.0761, 'grad_norm': 0.04280852526426315, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.072679877281189, 'eval_runtime': 4.4634, 'eval_samples_per_second': 223.823, 'eval_steps_per_second': 14.115, 'epoch': 0.92}
{'loss': 1.053, 'grad_norm': 0.04220929741859436, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.07157301902771, 'eval_runtime': 4.4508, 'eval_samples_per_second': 224.452, 'eval_steps_per_second': 14.155, 'epoch': 0.96}
{'loss': 1.0739, 'grad_norm': 0.04464572295546532, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0710965394973755, 'eval_runtime': 4.459, 'eval_samples_per_second': 224.043, 'eval_steps_per_second': 14.129, 'epoch': 1.0}
{'train_runtime': 247.656, 'train_samples_per_second': 40.375, 'train_steps_per_second': 2.524, 'train_loss': 1.3901202239990234, 'epoch': 1.0}
train_results:  {'eval_loss': [4.055542469024658, 1.8749901056289673, 1.4597045183181763, 1.4101506471633911, 1.3622034788131714, 1.2921607494354248, 1.2101572751998901, 1.1758513450622559, 1.1580177545547485, 1.142372965812683, 1.1326417922973633, 1.1246047019958496, 1.1175438165664673, 1.109315276145935, 1.1033257246017456, 1.0971503257751465, 1.0931233167648315, 1.087843418121338, 1.0843416452407837, 1.08006751537323, 1.0767351388931274, 1.075203537940979, 1.072679877281189, 1.07157301902771, 1.0710965394973755], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:31,  1.50it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 125.05it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 219.26it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 294.07it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 351.62it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 399.39it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 325.06it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8453781604766846
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2745 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.7990534473080769
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.8113989429117876
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 0.8175758346096886
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 0.8171537239751706
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 0.8175782462686877
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 19, 1, 1, 1, 0, 0, 108, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(2.3271e-17, dtype=torch.float64), tensor(1.1766e-17, dtype=torch.float64), tensor(2.0206e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2641e-17, dtype=torch.float64), tensor(1.3916e-17, dtype=torch.float64), tensor(0.5860, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8452, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 1.0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (108,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  108
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 65,138,688 || all params: 8,095,399,936 || trainable%: 0.8046
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 284.31it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 470.84it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 548.74it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 595.88it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 628.60it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 651.30it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 639.14it/s]
Evaluation performance at step 25: 0.73
{'loss': 5.362, 'grad_norm': 0.41075435280799866, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 4.007773399353027, 'eval_runtime': 4.9121, 'eval_samples_per_second': 203.58, 'eval_steps_per_second': 12.826, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 284.28it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 470.77it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 549.71it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 595.73it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 627.39it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 648.20it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 637.89it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.6516, 'grad_norm': 0.10889769345521927, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.7275071144104004, 'eval_runtime': 3.2607, 'eval_samples_per_second': 306.68, 'eval_steps_per_second': 19.321, 'epoch': 0.08}
{'loss': 1.5352, 'grad_norm': 0.05832178518176079, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4426976442337036, 'eval_runtime': 3.2797, 'eval_samples_per_second': 304.907, 'eval_steps_per_second': 19.209, 'epoch': 0.12}
{'loss': 1.4152, 'grad_norm': 0.061205603182315826, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3870964050292969, 'eval_runtime': 3.2807, 'eval_samples_per_second': 304.813, 'eval_steps_per_second': 19.203, 'epoch': 0.16}
{'loss': 1.3385, 'grad_norm': 0.08885598182678223, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2684433460235596, 'eval_runtime': 3.2888, 'eval_samples_per_second': 304.061, 'eval_steps_per_second': 19.156, 'epoch': 0.2}
{'loss': 1.2286, 'grad_norm': 0.06006815284490585, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1605886220932007, 'eval_runtime': 3.2443, 'eval_samples_per_second': 308.237, 'eval_steps_per_second': 19.419, 'epoch': 0.24}
{'loss': 1.1183, 'grad_norm': 0.06175564229488373, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1307564973831177, 'eval_runtime': 3.2017, 'eval_samples_per_second': 312.337, 'eval_steps_per_second': 19.677, 'epoch': 0.28}
{'loss': 1.1227, 'grad_norm': 0.054634660482406616, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1142815351486206, 'eval_runtime': 3.1515, 'eval_samples_per_second': 317.314, 'eval_steps_per_second': 19.991, 'epoch': 0.32}
{'loss': 1.1072, 'grad_norm': 0.05795716494321823, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1034821271896362, 'eval_runtime': 3.1517, 'eval_samples_per_second': 317.287, 'eval_steps_per_second': 19.989, 'epoch': 0.36}
{'loss': 1.1011, 'grad_norm': 0.06867291778326035, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.094543695449829, 'eval_runtime': 3.1501, 'eval_samples_per_second': 317.452, 'eval_steps_per_second': 20.0, 'epoch': 0.4}
{'loss': 1.1042, 'grad_norm': 0.0625852420926094, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0873950719833374, 'eval_runtime': 3.1404, 'eval_samples_per_second': 318.43, 'eval_steps_per_second': 20.061, 'epoch': 0.44}
{'loss': 1.1098, 'grad_norm': 0.06352686136960983, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0792657136917114, 'eval_runtime': 3.1399, 'eval_samples_per_second': 318.484, 'eval_steps_per_second': 20.065, 'epoch': 0.48}
{'loss': 1.0827, 'grad_norm': 0.06278210133314133, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0722196102142334, 'eval_runtime': 3.1486, 'eval_samples_per_second': 317.606, 'eval_steps_per_second': 20.009, 'epoch': 0.52}
{'loss': 1.086, 'grad_norm': 0.07151630520820618, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0651767253875732, 'eval_runtime': 3.1427, 'eval_samples_per_second': 318.195, 'eval_steps_per_second': 20.046, 'epoch': 0.56}
{'loss': 1.0785, 'grad_norm': 0.06847984343767166, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.060083270072937, 'eval_runtime': 3.1375, 'eval_samples_per_second': 318.721, 'eval_steps_per_second': 20.079, 'epoch': 0.6}
{'loss': 1.0575, 'grad_norm': 0.05901167541742325, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.049765706062317, 'eval_runtime': 3.1472, 'eval_samples_per_second': 317.743, 'eval_steps_per_second': 20.018, 'epoch': 0.64}
{'loss': 1.0548, 'grad_norm': 0.06742473691701889, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.040112018585205, 'eval_runtime': 3.1516, 'eval_samples_per_second': 317.295, 'eval_steps_per_second': 19.99, 'epoch': 0.68}
{'loss': 1.0428, 'grad_norm': 0.07221746444702148, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0279030799865723, 'eval_runtime': 3.1527, 'eval_samples_per_second': 317.185, 'eval_steps_per_second': 19.983, 'epoch': 0.72}
{'loss': 1.0206, 'grad_norm': 0.09521643072366714, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0101115703582764, 'eval_runtime': 3.166, 'eval_samples_per_second': 315.852, 'eval_steps_per_second': 19.899, 'epoch': 0.76}
{'loss': 1.0351, 'grad_norm': 0.09148209542036057, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9957224130630493, 'eval_runtime': 3.1505, 'eval_samples_per_second': 317.414, 'eval_steps_per_second': 19.997, 'epoch': 0.8}
{'loss': 0.9855, 'grad_norm': 0.08636657893657684, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9789240956306458, 'eval_runtime': 3.1755, 'eval_samples_per_second': 314.913, 'eval_steps_per_second': 19.84, 'epoch': 0.84}
{'loss': 0.9794, 'grad_norm': 0.08708593994379044, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9674584269523621, 'eval_runtime': 3.1974, 'eval_samples_per_second': 312.75, 'eval_steps_per_second': 19.703, 'epoch': 0.88}
{'loss': 0.9905, 'grad_norm': 0.09800270944833755, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9577783346176147, 'eval_runtime': 3.1827, 'eval_samples_per_second': 314.194, 'eval_steps_per_second': 19.794, 'epoch': 0.92}
{'loss': 0.9591, 'grad_norm': 0.10751227289438248, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9527769684791565, 'eval_runtime': 3.1767, 'eval_samples_per_second': 314.795, 'eval_steps_per_second': 19.832, 'epoch': 0.96}
{'loss': 0.9699, 'grad_norm': 0.08324166387319565, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.952825129032135, 'eval_runtime': 3.1677, 'eval_samples_per_second': 315.682, 'eval_steps_per_second': 19.888, 'epoch': 1.0}
{'train_runtime': 189.352, 'train_samples_per_second': 52.812, 'train_steps_per_second': 3.301, 'train_loss': 1.3414687530517577, 'epoch': 1.0}
train_results:  {'eval_loss': [4.007773399353027, 1.7275071144104004, 1.4426976442337036, 1.3870964050292969, 1.2684433460235596, 1.1605886220932007, 1.1307564973831177, 1.1142815351486206, 1.1034821271896362, 1.094543695449829, 1.0873950719833374, 1.0792657136917114, 1.0722196102142334, 1.0651767253875732, 1.060083270072937, 1.049765706062317, 1.040112018585205, 1.0279030799865723, 1.0101115703582764, 0.9957224130630493, 0.9789240956306458, 0.9674584269523621, 0.9577783346176147, 0.9527769684791565, 0.952825129032135], 'performance': [0.73, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:41,  4.93it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 196.34it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 298.74it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 361.04it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 403.16it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 437.43it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 425.27it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8287791013717651
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2346 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.8072226186096482
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.8041549259036678
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 0.8113938672869344
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7651033369793221
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.802926481366557
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.6989, dtype=torch.float64), 0, 0, 0, tensor(0.3011, dtype=torch.float64), 0, 32, 1, 0, 1, 0, 0, 128, 0.04977896194447279, 1.4800000190734879, 1]
normalized proposed parameters for next round by BO: [tensor(1.8389e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6989, dtype=torch.float64), tensor(1.8253e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.1131e-18, dtype=torch.float64), tensor(0.3011, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4978, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.699
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.301
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04977896194447279,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734879,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.04977896194447279
lora alpha:  1.4800000190734879
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 288.10it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 480.96it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 560.51it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 609.42it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 643.02it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 667.71it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 654.75it/s]
Evaluation performance at step 25: 0.73
{'loss': 4.4938, 'grad_norm': 0.1783250868320465, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.536524772644043, 'eval_runtime': 7.9092, 'eval_samples_per_second': 126.309, 'eval_steps_per_second': 7.965, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 289.55it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 481.67it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 560.57it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 609.37it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 642.76it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 668.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 655.57it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.514, 'grad_norm': 0.05563582852482796, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.7814559936523438, 'eval_runtime': 7.9327, 'eval_samples_per_second': 125.934, 'eval_steps_per_second': 7.942, 'epoch': 0.08}
{'loss': 1.6089, 'grad_norm': 0.09468357264995575, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5117324590682983, 'eval_runtime': 8.012, 'eval_samples_per_second': 124.688, 'eval_steps_per_second': 7.863, 'epoch': 0.12}
{'loss': 1.405, 'grad_norm': 0.07295152544975281, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3866591453552246, 'eval_runtime': 8.0158, 'eval_samples_per_second': 124.629, 'eval_steps_per_second': 7.859, 'epoch': 0.16}
{'loss': 1.3666, 'grad_norm': 0.05224228277802467, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.339607834815979, 'eval_runtime': 7.9911, 'eval_samples_per_second': 125.013, 'eval_steps_per_second': 7.884, 'epoch': 0.2}
{'loss': 1.3254, 'grad_norm': 0.042697153985500336, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.311273217201233, 'eval_runtime': 7.9838, 'eval_samples_per_second': 125.128, 'eval_steps_per_second': 7.891, 'epoch': 0.24}
{'loss': 1.2904, 'grad_norm': 0.048542849719524384, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.287065863609314, 'eval_runtime': 7.9637, 'eval_samples_per_second': 125.445, 'eval_steps_per_second': 7.911, 'epoch': 0.28}
{'loss': 1.2452, 'grad_norm': 0.04676160588860512, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2649375200271606, 'eval_runtime': 7.9595, 'eval_samples_per_second': 125.51, 'eval_steps_per_second': 7.915, 'epoch': 0.32}
{'loss': 1.278, 'grad_norm': 0.05401802435517311, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2447885274887085, 'eval_runtime': 7.9695, 'eval_samples_per_second': 125.352, 'eval_steps_per_second': 7.905, 'epoch': 0.36}
{'loss': 1.244, 'grad_norm': 0.043062590062618256, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2282748222351074, 'eval_runtime': 7.9753, 'eval_samples_per_second': 125.261, 'eval_steps_per_second': 7.899, 'epoch': 0.4}
{'loss': 1.2608, 'grad_norm': 0.04236828163266182, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2117246389389038, 'eval_runtime': 7.9963, 'eval_samples_per_second': 124.933, 'eval_steps_per_second': 7.879, 'epoch': 0.44}
{'loss': 1.22, 'grad_norm': 0.04997558519244194, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1924102306365967, 'eval_runtime': 8.0062, 'eval_samples_per_second': 124.779, 'eval_steps_per_second': 7.869, 'epoch': 0.48}
{'loss': 1.1694, 'grad_norm': 0.06394925713539124, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1665606498718262, 'eval_runtime': 7.9835, 'eval_samples_per_second': 125.133, 'eval_steps_per_second': 7.891, 'epoch': 0.52}
{'loss': 1.1213, 'grad_norm': 0.07862965762615204, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1280490159988403, 'eval_runtime': 7.9883, 'eval_samples_per_second': 125.058, 'eval_steps_per_second': 7.887, 'epoch': 0.56}
{'loss': 1.1368, 'grad_norm': 0.054207395762205124, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0784974098205566, 'eval_runtime': 7.9779, 'eval_samples_per_second': 125.221, 'eval_steps_per_second': 7.897, 'epoch': 0.6}
{'loss': 1.0427, 'grad_norm': 0.046003539115190506, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0624064207077026, 'eval_runtime': 7.9725, 'eval_samples_per_second': 125.306, 'eval_steps_per_second': 7.902, 'epoch': 0.64}
{'loss': 1.068, 'grad_norm': 0.048538871109485626, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0503963232040405, 'eval_runtime': 7.9685, 'eval_samples_per_second': 125.369, 'eval_steps_per_second': 7.906, 'epoch': 0.68}
{'loss': 1.0284, 'grad_norm': 0.04868998005986214, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.038191318511963, 'eval_runtime': 7.9752, 'eval_samples_per_second': 125.264, 'eval_steps_per_second': 7.9, 'epoch': 0.72}
{'loss': 1.059, 'grad_norm': 0.043203096836805344, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0250647068023682, 'eval_runtime': 7.9694, 'eval_samples_per_second': 125.354, 'eval_steps_per_second': 7.905, 'epoch': 0.76}
{'loss': 1.027, 'grad_norm': 0.05148543044924736, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0139786005020142, 'eval_runtime': 7.9818, 'eval_samples_per_second': 125.159, 'eval_steps_per_second': 7.893, 'epoch': 0.8}
{'loss': 1.0079, 'grad_norm': 0.06016113609075546, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0026088953018188, 'eval_runtime': 7.9703, 'eval_samples_per_second': 125.341, 'eval_steps_per_second': 7.904, 'epoch': 0.84}
{'loss': 0.9979, 'grad_norm': 0.05658157914876938, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9933871030807495, 'eval_runtime': 7.9787, 'eval_samples_per_second': 125.208, 'eval_steps_per_second': 7.896, 'epoch': 0.88}
{'loss': 0.9888, 'grad_norm': 0.047540124505758286, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9863302111625671, 'eval_runtime': 7.9755, 'eval_samples_per_second': 125.259, 'eval_steps_per_second': 7.899, 'epoch': 0.92}
{'loss': 1.0431, 'grad_norm': 0.06654054671525955, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9823814630508423, 'eval_runtime': 7.9887, 'eval_samples_per_second': 125.052, 'eval_steps_per_second': 7.886, 'epoch': 0.96}
{'loss': 1.0633, 'grad_norm': 0.04680579900741577, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9813027381896973, 'eval_runtime': 7.9786, 'eval_samples_per_second': 125.211, 'eval_steps_per_second': 7.896, 'epoch': 1.0}
{'train_runtime': 410.5437, 'train_samples_per_second': 24.356, 'train_steps_per_second': 1.522, 'train_loss': 1.360221859741211, 'epoch': 1.0}
train_results:  {'eval_loss': [3.536524772644043, 1.7814559936523438, 1.5117324590682983, 1.3866591453552246, 1.339607834815979, 1.311273217201233, 1.287065863609314, 1.2649375200271606, 1.2447885274887085, 1.2282748222351074, 1.2117246389389038, 1.1924102306365967, 1.1665606498718262, 1.1280490159988403, 1.0784974098205566, 1.0624064207077026, 1.0503963232040405, 1.038191318511963, 1.0250647068023682, 1.0139786005020142, 1.0026088953018188, 0.9933871030807495, 0.9863302111625671, 0.9823814630508423, 0.9813027381896973], 'performance': [0.73, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:40,  4.95it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 256.45it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 349.74it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 401.82it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 433.36it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 460.46it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 464.90it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8480426669120789
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7401 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.8044281032787718
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.8000560252262038
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.8083767748042389
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.795582167300069
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.7985382693293908
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7161, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2839, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.027575143222777057, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(5.3505e-17, dtype=torch.float64), tensor(1.8439e-17, dtype=torch.float64), tensor(1.7589e-17, dtype=torch.float64), tensor(0.7161, dtype=torch.float64), tensor(7.1633e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.7520e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2839, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2758, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.716
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.284

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.027575143222777057,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.027575143222777057
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 287.70it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 481.23it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 561.62it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 610.24it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 644.63it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 670.01it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 656.22it/s]
Evaluation performance at step 25: 0.73
{'loss': 4.7425, 'grad_norm': 0.1887666881084442, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.6720664501190186, 'eval_runtime': 6.279, 'eval_samples_per_second': 159.103, 'eval_steps_per_second': 10.034, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 285.44it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 477.33it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 556.39it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 603.75it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 638.84it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 664.51it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 650.45it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.4491, 'grad_norm': 0.09206322580575943, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.641098141670227, 'eval_runtime': 6.0445, 'eval_samples_per_second': 165.275, 'eval_steps_per_second': 10.423, 'epoch': 0.08}
{'loss': 1.3901, 'grad_norm': 0.1379852145910263, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2982401847839355, 'eval_runtime': 6.0787, 'eval_samples_per_second': 164.345, 'eval_steps_per_second': 10.364, 'epoch': 0.12}
{'loss': 1.2169, 'grad_norm': 0.0765698254108429, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1673084497451782, 'eval_runtime': 6.082, 'eval_samples_per_second': 164.254, 'eval_steps_per_second': 10.358, 'epoch': 0.16}
{'loss': 1.1388, 'grad_norm': 0.05043807998299599, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.122759461402893, 'eval_runtime': 6.0871, 'eval_samples_per_second': 164.117, 'eval_steps_per_second': 10.35, 'epoch': 0.2}
{'loss': 1.1189, 'grad_norm': 0.04511983320116997, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0995324850082397, 'eval_runtime': 6.0718, 'eval_samples_per_second': 164.53, 'eval_steps_per_second': 10.376, 'epoch': 0.24}
{'loss': 1.0793, 'grad_norm': 0.039574362337589264, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0814299583435059, 'eval_runtime': 6.0809, 'eval_samples_per_second': 164.284, 'eval_steps_per_second': 10.36, 'epoch': 0.28}
{'loss': 1.0759, 'grad_norm': 0.04652512073516846, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0624743700027466, 'eval_runtime': 6.0625, 'eval_samples_per_second': 164.784, 'eval_steps_per_second': 10.392, 'epoch': 0.32}
{'loss': 1.0558, 'grad_norm': 0.04497374966740608, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0477272272109985, 'eval_runtime': 6.0626, 'eval_samples_per_second': 164.781, 'eval_steps_per_second': 10.392, 'epoch': 0.36}
{'loss': 1.0422, 'grad_norm': 0.04744282737374306, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0315641164779663, 'eval_runtime': 6.0887, 'eval_samples_per_second': 164.075, 'eval_steps_per_second': 10.347, 'epoch': 0.4}
{'loss': 1.0307, 'grad_norm': 0.04584207013249397, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0146994590759277, 'eval_runtime': 6.0961, 'eval_samples_per_second': 163.875, 'eval_steps_per_second': 10.334, 'epoch': 0.44}
{'loss': 0.9969, 'grad_norm': 0.046885669231414795, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.994085431098938, 'eval_runtime': 6.0818, 'eval_samples_per_second': 164.262, 'eval_steps_per_second': 10.359, 'epoch': 0.48}
{'loss': 0.9881, 'grad_norm': 0.06399128586053848, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9674915671348572, 'eval_runtime': 6.094, 'eval_samples_per_second': 163.931, 'eval_steps_per_second': 10.338, 'epoch': 0.52}
{'loss': 0.9498, 'grad_norm': 0.07974052429199219, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9259770512580872, 'eval_runtime': 6.0971, 'eval_samples_per_second': 163.849, 'eval_steps_per_second': 10.333, 'epoch': 0.56}
{'loss': 0.9229, 'grad_norm': 0.05441208556294441, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8728718161582947, 'eval_runtime': 6.0861, 'eval_samples_per_second': 164.145, 'eval_steps_per_second': 10.351, 'epoch': 0.6}
{'loss': 0.8743, 'grad_norm': 0.04904421046376228, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.858212947845459, 'eval_runtime': 6.0865, 'eval_samples_per_second': 164.135, 'eval_steps_per_second': 10.351, 'epoch': 0.64}
{'loss': 0.8668, 'grad_norm': 0.0440971702337265, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8459333181381226, 'eval_runtime': 6.0874, 'eval_samples_per_second': 164.11, 'eval_steps_per_second': 10.349, 'epoch': 0.68}
{'loss': 0.8342, 'grad_norm': 0.0543261393904686, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8323289155960083, 'eval_runtime': 6.0679, 'eval_samples_per_second': 164.637, 'eval_steps_per_second': 10.383, 'epoch': 0.72}
{'loss': 0.8346, 'grad_norm': 0.04944509640336037, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8201923966407776, 'eval_runtime': 6.0525, 'eval_samples_per_second': 165.056, 'eval_steps_per_second': 10.409, 'epoch': 0.76}
{'loss': 0.8022, 'grad_norm': 0.053310077637434006, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8070014715194702, 'eval_runtime': 6.0511, 'eval_samples_per_second': 165.093, 'eval_steps_per_second': 10.411, 'epoch': 0.8}
{'loss': 0.8173, 'grad_norm': 0.051855579018592834, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7947807312011719, 'eval_runtime': 6.0377, 'eval_samples_per_second': 165.461, 'eval_steps_per_second': 10.434, 'epoch': 0.84}
{'loss': 0.8233, 'grad_norm': 0.05662290379405022, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.78445965051651, 'eval_runtime': 6.059, 'eval_samples_per_second': 164.88, 'eval_steps_per_second': 10.398, 'epoch': 0.88}
{'loss': 0.7924, 'grad_norm': 0.05779280140995979, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7767689824104309, 'eval_runtime': 6.0507, 'eval_samples_per_second': 165.104, 'eval_steps_per_second': 10.412, 'epoch': 0.92}
{'loss': 0.7597, 'grad_norm': 0.0689728856086731, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.773171603679657, 'eval_runtime': 6.0491, 'eval_samples_per_second': 165.147, 'eval_steps_per_second': 10.415, 'epoch': 0.96}
{'loss': 0.7778, 'grad_norm': 0.054770831018686295, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7718241810798645, 'eval_runtime': 6.0345, 'eval_samples_per_second': 165.547, 'eval_steps_per_second': 10.44, 'epoch': 1.0}
{'train_runtime': 328.6324, 'train_samples_per_second': 30.426, 'train_steps_per_second': 1.902, 'train_loss': 1.1752319549560546, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6720664501190186, 1.641098141670227, 1.2982401847839355, 1.1673084497451782, 1.122759461402893, 1.0995324850082397, 1.0814299583435059, 1.0624743700027466, 1.0477272272109985, 1.0315641164779663, 1.0146994590759277, 0.994085431098938, 0.9674915671348572, 0.9259770512580872, 0.8728718161582947, 0.858212947845459, 0.8459333181381226, 0.8323289155960083, 0.8201923966407776, 0.8070014715194702, 0.7947807312011719, 0.78445965051651, 0.7767689824104309, 0.773171603679657, 0.7718241810798645], 'performance': [0.73, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:56,  1.40it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 94.15it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 176.85it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 249.20it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 310.13it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 362.91it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 282.22it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8485226631164551
current iteration best possible performance (full train run):  0.8295000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7848 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.8081812268753361
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.8078133331370224
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.7907476586405748
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.8095276757085014
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.8095328296942065
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.5029, dtype=torch.float64), 0, 0, 0, 0, tensor(0.4971, dtype=torch.float64), 32, 0, 0, 1, 1, 0, 128, 0.1, 1.480000019073488, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(4.2515e-17, dtype=torch.float64), tensor(3.7763e-17, dtype=torch.float64), tensor(0.5029, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5070e-16, dtype=torch.float64), tensor(4.4043e-17, dtype=torch.float64), tensor(0.4971, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.503
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.497

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.63it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 460.42it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 537.29it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 584.86it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 618.81it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 644.22it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 630.01it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.2599, 'grad_norm': 0.3581993877887726, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.97940731048584, 'eval_runtime': 6.397, 'eval_samples_per_second': 156.167, 'eval_steps_per_second': 9.848, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 276.11it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 460.47it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 537.72it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 584.34it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 617.41it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 643.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 628.51it/s]
Evaluation performance at step 50: 0.76
{'loss': 1.9102, 'grad_norm': 0.25210028886795044, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 1.1932446956634521, 'eval_runtime': 6.3918, 'eval_samples_per_second': 156.295, 'eval_steps_per_second': 9.856, 'epoch': 0.08}
{'loss': 1.0607, 'grad_norm': 0.07592795044183731, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9661571979522705, 'eval_runtime': 6.4101, 'eval_samples_per_second': 155.847, 'eval_steps_per_second': 9.828, 'epoch': 0.12}
{'loss': 0.8935, 'grad_norm': 0.06867561489343643, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8534297943115234, 'eval_runtime': 6.4029, 'eval_samples_per_second': 156.024, 'eval_steps_per_second': 9.839, 'epoch': 0.16}
{'loss': 0.829, 'grad_norm': 0.04672206938266754, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8356364369392395, 'eval_runtime': 6.4292, 'eval_samples_per_second': 155.385, 'eval_steps_per_second': 9.799, 'epoch': 0.2}
{'loss': 0.8086, 'grad_norm': 0.039675600826740265, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8241040706634521, 'eval_runtime': 6.4488, 'eval_samples_per_second': 154.912, 'eval_steps_per_second': 9.769, 'epoch': 0.24}
{'loss': 0.8112, 'grad_norm': 0.041567303240299225, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8163294196128845, 'eval_runtime': 6.454, 'eval_samples_per_second': 154.787, 'eval_steps_per_second': 9.761, 'epoch': 0.28}
{'loss': 0.8114, 'grad_norm': 0.04647902771830559, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8093376755714417, 'eval_runtime': 6.4462, 'eval_samples_per_second': 154.975, 'eval_steps_per_second': 9.773, 'epoch': 0.32}
{'loss': 0.7899, 'grad_norm': 0.04238908737897873, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8031287789344788, 'eval_runtime': 6.4512, 'eval_samples_per_second': 154.855, 'eval_steps_per_second': 9.766, 'epoch': 0.36}
{'loss': 0.7769, 'grad_norm': 0.04123818501830101, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7953312397003174, 'eval_runtime': 6.4844, 'eval_samples_per_second': 154.061, 'eval_steps_per_second': 9.716, 'epoch': 0.4}
{'loss': 0.7941, 'grad_norm': 0.0483519583940506, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7870928645133972, 'eval_runtime': 6.4994, 'eval_samples_per_second': 153.706, 'eval_steps_per_second': 9.693, 'epoch': 0.44}
{'loss': 0.7724, 'grad_norm': 0.04523438587784767, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.777837336063385, 'eval_runtime': 6.5006, 'eval_samples_per_second': 153.678, 'eval_steps_per_second': 9.691, 'epoch': 0.48}
{'loss': 0.7706, 'grad_norm': 0.04913666471838951, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7714678049087524, 'eval_runtime': 6.5152, 'eval_samples_per_second': 153.333, 'eval_steps_per_second': 9.67, 'epoch': 0.52}
{'loss': 0.7771, 'grad_norm': 0.05040944367647171, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7650034427642822, 'eval_runtime': 6.5023, 'eval_samples_per_second': 153.637, 'eval_steps_per_second': 9.689, 'epoch': 0.56}
{'loss': 0.7832, 'grad_norm': 0.05450664088129997, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7575464844703674, 'eval_runtime': 6.4854, 'eval_samples_per_second': 154.037, 'eval_steps_per_second': 9.714, 'epoch': 0.6}
{'loss': 0.7599, 'grad_norm': 0.05874931812286377, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.752370297908783, 'eval_runtime': 6.4843, 'eval_samples_per_second': 154.064, 'eval_steps_per_second': 9.716, 'epoch': 0.64}
{'loss': 0.765, 'grad_norm': 0.05953463166952133, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7482370138168335, 'eval_runtime': 6.5061, 'eval_samples_per_second': 153.549, 'eval_steps_per_second': 9.683, 'epoch': 0.68}
{'loss': 0.7457, 'grad_norm': 0.06023184955120087, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7420877814292908, 'eval_runtime': 6.5005, 'eval_samples_per_second': 153.68, 'eval_steps_per_second': 9.692, 'epoch': 0.72}
{'loss': 0.7398, 'grad_norm': 0.058674611151218414, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7361211180686951, 'eval_runtime': 6.5052, 'eval_samples_per_second': 153.569, 'eval_steps_per_second': 9.685, 'epoch': 0.76}
{'loss': 0.7314, 'grad_norm': 0.05961999669671059, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7316099405288696, 'eval_runtime': 6.514, 'eval_samples_per_second': 153.361, 'eval_steps_per_second': 9.671, 'epoch': 0.8}
{'loss': 0.7465, 'grad_norm': 0.06282138079404831, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7268628478050232, 'eval_runtime': 6.4752, 'eval_samples_per_second': 154.282, 'eval_steps_per_second': 9.729, 'epoch': 0.84}
{'loss': 0.7384, 'grad_norm': 0.06849035620689392, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7230693697929382, 'eval_runtime': 6.4674, 'eval_samples_per_second': 154.468, 'eval_steps_per_second': 9.741, 'epoch': 0.88}
{'loss': 0.742, 'grad_norm': 0.06852445751428604, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7193073034286499, 'eval_runtime': 6.4486, 'eval_samples_per_second': 154.917, 'eval_steps_per_second': 9.77, 'epoch': 0.92}
{'loss': 0.726, 'grad_norm': 0.0770883709192276, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7174466252326965, 'eval_runtime': 6.4645, 'eval_samples_per_second': 154.537, 'eval_steps_per_second': 9.746, 'epoch': 0.96}
{'loss': 0.7376, 'grad_norm': 0.06514802575111389, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7167118787765503, 'eval_runtime': 6.4436, 'eval_samples_per_second': 155.036, 'eval_steps_per_second': 9.777, 'epoch': 1.0}
{'train_runtime': 349.7531, 'train_samples_per_second': 28.589, 'train_steps_per_second': 1.787, 'train_loss': 0.9712483001708985, 'epoch': 1.0}
train_results:  {'eval_loss': [2.97940731048584, 1.1932446956634521, 0.9661571979522705, 0.8534297943115234, 0.8356364369392395, 0.8241040706634521, 0.8163294196128845, 0.8093376755714417, 0.8031287789344788, 0.7953312397003174, 0.7870928645133972, 0.777837336063385, 0.7714678049087524, 0.7650034427642822, 0.7575464844703674, 0.752370297908783, 0.7482370138168335, 0.7420877814292908, 0.7361211180686951, 0.7316099405288696, 0.7268628478050232, 0.7230693697929382, 0.7193073034286499, 0.7174466252326965, 0.7167118787765503], 'performance': [0.74, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:38,  2.28it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 164.52it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 263.38it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 329.63it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 375.04it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 412.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 371.80it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.8481138348579407
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6681 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.8130142924854025
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.8130244990355606
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.8130250158713394
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.7919347821553612
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.813023830436951
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1798, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7458, dtype=torch.float64), 0, tensor(0.0744, dtype=torch.float64), 0, 32, 1, 0, 1, 0, 0, 128, 0.0004933716822149742, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.1798, dtype=torch.float64), tensor(2.4145e-17, dtype=torch.float64), tensor(4.6426e-18, dtype=torch.float64), tensor(1.3294e-17, dtype=torch.float64), tensor(5.8533e-17, dtype=torch.float64), tensor(0.7458, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0744, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0049, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.18
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.746
  wikitext: 0
  mmlu: 0.074
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0004933716822149742,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.0004933716822149742
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 290.08it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 482.19it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 559.86it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 605.95it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 640.02it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 664.85it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.65it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.8336, 'grad_norm': 0.22558598220348358, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.7496094703674316, 'eval_runtime': 5.3984, 'eval_samples_per_second': 185.054, 'eval_steps_per_second': 11.67, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 288.72it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.01it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 557.77it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 605.36it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 638.08it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 664.07it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 650.60it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5961, 'grad_norm': 0.09328458458185196, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8305703401565552, 'eval_runtime': 5.4439, 'eval_samples_per_second': 183.507, 'eval_steps_per_second': 11.573, 'epoch': 0.08}
{'loss': 1.577, 'grad_norm': 0.12567728757858276, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3878310918807983, 'eval_runtime': 5.4317, 'eval_samples_per_second': 183.92, 'eval_steps_per_second': 11.599, 'epoch': 0.12}
{'loss': 1.3118, 'grad_norm': 0.07094410061836243, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2366644144058228, 'eval_runtime': 5.4304, 'eval_samples_per_second': 183.963, 'eval_steps_per_second': 11.601, 'epoch': 0.16}
{'loss': 1.2173, 'grad_norm': 0.05493275821208954, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.182979702949524, 'eval_runtime': 5.4246, 'eval_samples_per_second': 184.16, 'eval_steps_per_second': 11.614, 'epoch': 0.2}
{'loss': 1.2085, 'grad_norm': 0.050791215151548386, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1514167785644531, 'eval_runtime': 5.4162, 'eval_samples_per_second': 184.448, 'eval_steps_per_second': 11.632, 'epoch': 0.24}
{'loss': 1.1408, 'grad_norm': 0.051069021224975586, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1246585845947266, 'eval_runtime': 5.4311, 'eval_samples_per_second': 183.941, 'eval_steps_per_second': 11.6, 'epoch': 0.28}
{'loss': 1.1293, 'grad_norm': 0.058548733592033386, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0989032983779907, 'eval_runtime': 5.4311, 'eval_samples_per_second': 183.939, 'eval_steps_per_second': 11.6, 'epoch': 0.32}
{'loss': 1.1208, 'grad_norm': 0.06160704419016838, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0708577632904053, 'eval_runtime': 5.442, 'eval_samples_per_second': 183.571, 'eval_steps_per_second': 11.577, 'epoch': 0.36}
{'loss': 1.0657, 'grad_norm': 0.061650458723306656, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0451021194458008, 'eval_runtime': 5.4343, 'eval_samples_per_second': 183.831, 'eval_steps_per_second': 11.593, 'epoch': 0.4}
{'loss': 1.0569, 'grad_norm': 0.06059744209051132, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0179648399353027, 'eval_runtime': 5.4333, 'eval_samples_per_second': 183.865, 'eval_steps_per_second': 11.595, 'epoch': 0.44}
{'loss': 1.0341, 'grad_norm': 0.06693872064352036, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9887328743934631, 'eval_runtime': 5.429, 'eval_samples_per_second': 184.012, 'eval_steps_per_second': 11.604, 'epoch': 0.48}
{'loss': 1.0566, 'grad_norm': 0.09376227110624313, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9550094604492188, 'eval_runtime': 5.4292, 'eval_samples_per_second': 184.003, 'eval_steps_per_second': 11.604, 'epoch': 0.52}
{'loss': 0.9558, 'grad_norm': 0.09482463449239731, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9130392670631409, 'eval_runtime': 5.432, 'eval_samples_per_second': 183.909, 'eval_steps_per_second': 11.598, 'epoch': 0.56}
{'loss': 0.9441, 'grad_norm': 0.09971778094768524, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8537269830703735, 'eval_runtime': 5.4328, 'eval_samples_per_second': 183.882, 'eval_steps_per_second': 11.596, 'epoch': 0.6}
{'loss': 0.8503, 'grad_norm': 0.08582153171300888, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8001102805137634, 'eval_runtime': 5.4262, 'eval_samples_per_second': 184.107, 'eval_steps_per_second': 11.61, 'epoch': 0.64}
{'loss': 0.8245, 'grad_norm': 0.09469503164291382, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7718092203140259, 'eval_runtime': 5.432, 'eval_samples_per_second': 183.908, 'eval_steps_per_second': 11.598, 'epoch': 0.68}
{'loss': 0.8139, 'grad_norm': 0.1067553460597992, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7481631636619568, 'eval_runtime': 5.4293, 'eval_samples_per_second': 184.002, 'eval_steps_per_second': 11.604, 'epoch': 0.72}
{'loss': 0.7936, 'grad_norm': 0.09828973561525345, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7241592407226562, 'eval_runtime': 5.4305, 'eval_samples_per_second': 183.96, 'eval_steps_per_second': 11.601, 'epoch': 0.76}
{'loss': 0.7381, 'grad_norm': 0.15106119215488434, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7028014063835144, 'eval_runtime': 5.4359, 'eval_samples_per_second': 183.779, 'eval_steps_per_second': 11.59, 'epoch': 0.8}
{'loss': 0.7751, 'grad_norm': 0.10162492096424103, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6821125745773315, 'eval_runtime': 5.4379, 'eval_samples_per_second': 183.712, 'eval_steps_per_second': 11.585, 'epoch': 0.84}
{'loss': 0.719, 'grad_norm': 0.1106431782245636, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6667526364326477, 'eval_runtime': 5.4368, 'eval_samples_per_second': 183.748, 'eval_steps_per_second': 11.588, 'epoch': 0.88}
{'loss': 0.7115, 'grad_norm': 0.11389847099781036, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6521424055099487, 'eval_runtime': 5.437, 'eval_samples_per_second': 183.742, 'eval_steps_per_second': 11.587, 'epoch': 0.92}
{'loss': 0.6687, 'grad_norm': 0.1455707997083664, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6436077952384949, 'eval_runtime': 5.4438, 'eval_samples_per_second': 183.511, 'eval_steps_per_second': 11.573, 'epoch': 0.96}
{'loss': 0.6739, 'grad_norm': 0.15435540676116943, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6410192251205444, 'eval_runtime': 5.4573, 'eval_samples_per_second': 183.057, 'eval_steps_per_second': 11.544, 'epoch': 1.0}
{'train_runtime': 301.1501, 'train_samples_per_second': 33.203, 'train_steps_per_second': 2.075, 'train_loss': 1.192676156616211, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7496094703674316, 1.8305703401565552, 1.3878310918807983, 1.2366644144058228, 1.182979702949524, 1.1514167785644531, 1.1246585845947266, 1.0989032983779907, 1.0708577632904053, 1.0451021194458008, 1.0179648399353027, 0.9887328743934631, 0.9550094604492188, 0.9130392670631409, 0.8537269830703735, 0.8001102805137634, 0.7718092203140259, 0.7481631636619568, 0.7241592407226562, 0.7028014063835144, 0.6821125745773315, 0.6667526364326477, 0.6521424055099487, 0.6436077952384949, 0.6410192251205444], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:12,  1.98it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 144.57it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 243.94it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 315.83it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 367.19it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 409.53it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 354.10it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8481206297874451
current iteration best possible performance (full train run):  0.8400000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2372 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.8091314507852727
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.8115638249651533
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.7813468975103928
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.8103419841231038
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8121893723898874
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.0567, dtype=torch.float64), tensor(0.3209, dtype=torch.float64), tensor(0.6224, dtype=torch.float64), 0, 32, 0, 0, 1, 1, 0, 128, 0.1, 1.480000019073489, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.9263e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.0035e-18, dtype=torch.float64), tensor(6.8255e-17, dtype=torch.float64), tensor(0.0567, dtype=torch.float64), tensor(0.3209, dtype=torch.float64), tensor(0.6224, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.057
  wikitext: 0.321
  mmlu: 0.622
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.480000019073489,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.480000019073489
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 271.77it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 452.41it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 527.89it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 573.63it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 600.89it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 627.77it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 614.99it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.4902, 'grad_norm': 0.19152027368545532, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.729097843170166, 'eval_runtime': 9.4273, 'eval_samples_per_second': 105.969, 'eval_steps_per_second': 6.683, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 272.60it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 451.10it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 527.43it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 573.42it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 604.05it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 627.51it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 614.96it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.1941, 'grad_norm': 0.1844802349805832, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.850600004196167, 'eval_runtime': 9.443, 'eval_samples_per_second': 105.792, 'eval_steps_per_second': 6.672, 'epoch': 0.08}
{'loss': 1.7798, 'grad_norm': 0.10317745059728622, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.650832176208496, 'eval_runtime': 9.4894, 'eval_samples_per_second': 105.275, 'eval_steps_per_second': 6.639, 'epoch': 0.12}
{'loss': 1.6234, 'grad_norm': 0.07768324017524719, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5598829984664917, 'eval_runtime': 9.551, 'eval_samples_per_second': 104.597, 'eval_steps_per_second': 6.596, 'epoch': 0.16}
{'loss': 1.5103, 'grad_norm': 0.046267736703157425, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4838374853134155, 'eval_runtime': 9.5555, 'eval_samples_per_second': 104.547, 'eval_steps_per_second': 6.593, 'epoch': 0.2}
{'loss': 1.531, 'grad_norm': 0.05103640630841255, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4571754932403564, 'eval_runtime': 9.5739, 'eval_samples_per_second': 104.346, 'eval_steps_per_second': 6.58, 'epoch': 0.24}
{'loss': 1.4193, 'grad_norm': 0.06884386390447617, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4401700496673584, 'eval_runtime': 9.5585, 'eval_samples_per_second': 104.515, 'eval_steps_per_second': 6.591, 'epoch': 0.28}
{'loss': 1.5192, 'grad_norm': 0.04845714941620827, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4289954900741577, 'eval_runtime': 9.5519, 'eval_samples_per_second': 104.586, 'eval_steps_per_second': 6.596, 'epoch': 0.32}
{'loss': 1.4439, 'grad_norm': 0.054695188999176025, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.419294834136963, 'eval_runtime': 9.5297, 'eval_samples_per_second': 104.831, 'eval_steps_per_second': 6.611, 'epoch': 0.36}
{'loss': 1.4176, 'grad_norm': 0.07815831154584885, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.411299705505371, 'eval_runtime': 9.5524, 'eval_samples_per_second': 104.581, 'eval_steps_per_second': 6.595, 'epoch': 0.4}
{'loss': 1.3987, 'grad_norm': 0.0468432679772377, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4041837453842163, 'eval_runtime': 9.5543, 'eval_samples_per_second': 104.56, 'eval_steps_per_second': 6.594, 'epoch': 0.44}
{'loss': 1.4226, 'grad_norm': 0.049987565726041794, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3989934921264648, 'eval_runtime': 9.583, 'eval_samples_per_second': 104.247, 'eval_steps_per_second': 6.574, 'epoch': 0.48}
{'loss': 1.3735, 'grad_norm': 0.06512052565813065, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3928396701812744, 'eval_runtime': 9.557, 'eval_samples_per_second': 104.531, 'eval_steps_per_second': 6.592, 'epoch': 0.52}
{'loss': 1.3548, 'grad_norm': 0.05114025995135307, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3898893594741821, 'eval_runtime': 9.5369, 'eval_samples_per_second': 104.751, 'eval_steps_per_second': 6.606, 'epoch': 0.56}
{'loss': 1.4185, 'grad_norm': 0.049285050481557846, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.384963870048523, 'eval_runtime': 9.5497, 'eval_samples_per_second': 104.61, 'eval_steps_per_second': 6.597, 'epoch': 0.6}
{'loss': 1.3635, 'grad_norm': 0.049541279673576355, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3813064098358154, 'eval_runtime': 9.5416, 'eval_samples_per_second': 104.699, 'eval_steps_per_second': 6.603, 'epoch': 0.64}
{'loss': 1.3881, 'grad_norm': 0.057618413120508194, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3782349824905396, 'eval_runtime': 9.5466, 'eval_samples_per_second': 104.644, 'eval_steps_per_second': 6.599, 'epoch': 0.68}
{'loss': 1.4129, 'grad_norm': 0.05760159343481064, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3751105070114136, 'eval_runtime': 9.5465, 'eval_samples_per_second': 104.645, 'eval_steps_per_second': 6.599, 'epoch': 0.72}
{'loss': 1.3823, 'grad_norm': 0.05505668744444847, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3724193572998047, 'eval_runtime': 9.5416, 'eval_samples_per_second': 104.699, 'eval_steps_per_second': 6.603, 'epoch': 0.76}
{'loss': 1.3427, 'grad_norm': 0.058277588337659836, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3699419498443604, 'eval_runtime': 9.5558, 'eval_samples_per_second': 104.544, 'eval_steps_per_second': 6.593, 'epoch': 0.8}
{'loss': 1.3715, 'grad_norm': 0.05735653266310692, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3681374788284302, 'eval_runtime': 9.4439, 'eval_samples_per_second': 105.782, 'eval_steps_per_second': 6.671, 'epoch': 0.84}
{'loss': 1.4026, 'grad_norm': 0.053257446736097336, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.366104245185852, 'eval_runtime': 9.4237, 'eval_samples_per_second': 106.009, 'eval_steps_per_second': 6.685, 'epoch': 0.88}
{'loss': 1.378, 'grad_norm': 0.06353583931922913, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.36470365524292, 'eval_runtime': 9.4126, 'eval_samples_per_second': 106.134, 'eval_steps_per_second': 6.693, 'epoch': 0.92}
{'loss': 1.3943, 'grad_norm': 0.0570690743625164, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3639684915542603, 'eval_runtime': 9.4151, 'eval_samples_per_second': 106.106, 'eval_steps_per_second': 6.691, 'epoch': 0.96}
{'loss': 1.4658, 'grad_norm': 0.06305229663848877, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3635591268539429, 'eval_runtime': 9.4064, 'eval_samples_per_second': 106.204, 'eval_steps_per_second': 6.698, 'epoch': 1.0}
{'train_runtime': 505.2054, 'train_samples_per_second': 19.79, 'train_steps_per_second': 1.237, 'train_loss': 1.5519438720703125, 'epoch': 1.0}
train_results:  {'eval_loss': [2.729097843170166, 1.850600004196167, 1.650832176208496, 1.5598829984664917, 1.4838374853134155, 1.4571754932403564, 1.4401700496673584, 1.4289954900741577, 1.419294834136963, 1.411299705505371, 1.4041837453842163, 1.3989934921264648, 1.3928396701812744, 1.3898893594741821, 1.384963870048523, 1.3813064098358154, 1.3782349824905396, 1.3751105070114136, 1.3724193572998047, 1.3699419498443604, 1.3681374788284302, 1.366104245185852, 1.36470365524292, 1.3639684915542603, 1.3635591268539429], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:45,  4.74it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 245.37it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 336.21it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 386.41it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 416.43it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 442.46it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 447.01it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8470820188522339
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4952 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7748416914200252
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.8105177931138675
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.8106134029558234
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.767423344208593
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8115463802886766
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1703, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.8297, dtype=torch.float64), 0, 32, 0, 1, 1, 1, 0, 128, 1.2446640940133589e-17, 1.4800000190734874, 1]
normalized proposed parameters for next round by BO: [tensor(0.1703, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3496e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.7928e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4874e-17, dtype=torch.float64), tensor(0.8297, dtype=torch.float64), tensor(3.8858e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.2447e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.17
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.83
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.2446640940133589e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734874,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  1.2446640940133589e-17
lora alpha:  1.4800000190734874
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 266.73it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 446.26it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 520.84it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 559.59it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 591.60it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 616.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 604.77it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.5612, 'grad_norm': 0.24953818321228027, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.5384411811828613, 'eval_runtime': 9.8087, 'eval_samples_per_second': 101.848, 'eval_steps_per_second': 6.423, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 266.48it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 444.18it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 518.22it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 564.62it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 596.60it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 619.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 606.50it/s]
Evaluation performance at step 50: 0.75
{'loss': 1.9613, 'grad_norm': 0.15207676589488983, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.538154125213623, 'eval_runtime': 9.8875, 'eval_samples_per_second': 101.037, 'eval_steps_per_second': 6.372, 'epoch': 0.08}
{'loss': 1.3836, 'grad_norm': 0.07639037817716599, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3382339477539062, 'eval_runtime': 9.8549, 'eval_samples_per_second': 101.371, 'eval_steps_per_second': 6.393, 'epoch': 0.12}
{'loss': 1.3244, 'grad_norm': 0.06268095225095749, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2354918718338013, 'eval_runtime': 9.8933, 'eval_samples_per_second': 100.977, 'eval_steps_per_second': 6.368, 'epoch': 0.16}
{'loss': 1.2176, 'grad_norm': 0.042777784168720245, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2094582319259644, 'eval_runtime': 9.9266, 'eval_samples_per_second': 100.638, 'eval_steps_per_second': 6.347, 'epoch': 0.2}
{'loss': 1.2477, 'grad_norm': 0.0520111545920372, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.197197437286377, 'eval_runtime': 9.9311, 'eval_samples_per_second': 100.593, 'eval_steps_per_second': 6.344, 'epoch': 0.24}
{'loss': 1.2382, 'grad_norm': 0.04280863329768181, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1893396377563477, 'eval_runtime': 9.9445, 'eval_samples_per_second': 100.458, 'eval_steps_per_second': 6.335, 'epoch': 0.28}
{'loss': 1.197, 'grad_norm': 0.04430019110441208, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1819825172424316, 'eval_runtime': 9.9191, 'eval_samples_per_second': 100.715, 'eval_steps_per_second': 6.351, 'epoch': 0.32}
{'loss': 1.2451, 'grad_norm': 0.0455811470746994, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1760293245315552, 'eval_runtime': 9.9447, 'eval_samples_per_second': 100.456, 'eval_steps_per_second': 6.335, 'epoch': 0.36}
{'loss': 1.252, 'grad_norm': 0.04425063356757164, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.168990969657898, 'eval_runtime': 9.9729, 'eval_samples_per_second': 100.171, 'eval_steps_per_second': 6.317, 'epoch': 0.4}
{'loss': 1.1765, 'grad_norm': 0.04726738855242729, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.164018988609314, 'eval_runtime': 9.9614, 'eval_samples_per_second': 100.287, 'eval_steps_per_second': 6.324, 'epoch': 0.44}
{'loss': 1.2097, 'grad_norm': 0.04987524822354317, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.160810947418213, 'eval_runtime': 9.9331, 'eval_samples_per_second': 100.573, 'eval_steps_per_second': 6.342, 'epoch': 0.48}
{'loss': 1.1604, 'grad_norm': 0.05345327407121658, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1552329063415527, 'eval_runtime': 9.9486, 'eval_samples_per_second': 100.416, 'eval_steps_per_second': 6.333, 'epoch': 0.52}
{'loss': 1.1546, 'grad_norm': 0.04357169196009636, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.150810956954956, 'eval_runtime': 9.9564, 'eval_samples_per_second': 100.338, 'eval_steps_per_second': 6.328, 'epoch': 0.56}
{'loss': 1.1739, 'grad_norm': 0.04750404506921768, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1477913856506348, 'eval_runtime': 10.0222, 'eval_samples_per_second': 99.679, 'eval_steps_per_second': 6.286, 'epoch': 0.6}
{'loss': 1.1767, 'grad_norm': 0.04740149527788162, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1433961391448975, 'eval_runtime': 10.0288, 'eval_samples_per_second': 99.613, 'eval_steps_per_second': 6.282, 'epoch': 0.64}
{'loss': 1.1821, 'grad_norm': 0.05092369765043259, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1390924453735352, 'eval_runtime': 10.0032, 'eval_samples_per_second': 99.868, 'eval_steps_per_second': 6.298, 'epoch': 0.68}
{'loss': 1.2241, 'grad_norm': 0.04515865445137024, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1362924575805664, 'eval_runtime': 10.0588, 'eval_samples_per_second': 99.316, 'eval_steps_per_second': 6.263, 'epoch': 0.72}
{'loss': 1.1779, 'grad_norm': 0.05074296146631241, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.133581280708313, 'eval_runtime': 10.0559, 'eval_samples_per_second': 99.345, 'eval_steps_per_second': 6.265, 'epoch': 0.76}
{'loss': 1.2187, 'grad_norm': 0.05323352664709091, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1302555799484253, 'eval_runtime': 10.0571, 'eval_samples_per_second': 99.333, 'eval_steps_per_second': 6.264, 'epoch': 0.8}
{'loss': 1.1646, 'grad_norm': 0.054301127791404724, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1273607015609741, 'eval_runtime': 10.0438, 'eval_samples_per_second': 99.465, 'eval_steps_per_second': 6.273, 'epoch': 0.84}
{'loss': 1.1846, 'grad_norm': 0.05404575541615486, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1259431838989258, 'eval_runtime': 10.0796, 'eval_samples_per_second': 99.111, 'eval_steps_per_second': 6.25, 'epoch': 0.88}
{'loss': 1.1077, 'grad_norm': 0.08624456077814102, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1241486072540283, 'eval_runtime': 10.0577, 'eval_samples_per_second': 99.327, 'eval_steps_per_second': 6.264, 'epoch': 0.92}
{'loss': 1.1891, 'grad_norm': 0.048328664153814316, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.123359203338623, 'eval_runtime': 10.0522, 'eval_samples_per_second': 99.382, 'eval_steps_per_second': 6.267, 'epoch': 0.96}
{'loss': 1.1768, 'grad_norm': 0.05399538576602936, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.123165488243103, 'eval_runtime': 10.0483, 'eval_samples_per_second': 99.42, 'eval_steps_per_second': 6.27, 'epoch': 1.0}
{'train_runtime': 511.1285, 'train_samples_per_second': 19.563, 'train_steps_per_second': 1.223, 'train_loss': 1.3322149322509766, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5384411811828613, 1.538154125213623, 1.3382339477539062, 1.2354918718338013, 1.2094582319259644, 1.197197437286377, 1.1893396377563477, 1.1819825172424316, 1.1760293245315552, 1.168990969657898, 1.164018988609314, 1.160810947418213, 1.1552329063415527, 1.150810956954956, 1.1477913856506348, 1.1433961391448975, 1.1390924453735352, 1.1362924575805664, 1.133581280708313, 1.1302555799484253, 1.1273607015609741, 1.1259431838989258, 1.1241486072540283, 1.123359203338623, 1.123165488243103], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:48,  4.60it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 208.56it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 302.88it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 356.49it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 391.49it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 421.01it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 416.17it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8467063903808594
current iteration best possible performance (full train run):  0.8295000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9879 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.8066175217860077
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.7506409752655107
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.8101885360487742
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.8095096860284832
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.7823838605150244
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1371, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2518, dtype=torch.float64), 0, tensor(0.6111, dtype=torch.float64), 32, 0, 1, 1, 0, 0, 128, 0.012103003850376465, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.1371, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.3641e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2518, dtype=torch.float64), tensor(2.9028e-18, dtype=torch.float64), tensor(0.6111, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1210, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.137
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.252
  mmlu: 0
  arc_challenge: 0.611

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.012103003850376465,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.012103003850376465
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 291.11it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 481.41it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 560.91it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 609.17it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 633.38it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 661.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 650.24it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.7768, 'grad_norm': 0.20179162919521332, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.0521326065063477, 'eval_runtime': 7.5596, 'eval_samples_per_second': 132.151, 'eval_steps_per_second': 8.334, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 287.76it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 481.34it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 556.98it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 606.91it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 640.66it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 667.02it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 653.13it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.1859, 'grad_norm': 0.06729426234960556, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.6347041130065918, 'eval_runtime': 7.5362, 'eval_samples_per_second': 132.56, 'eval_steps_per_second': 8.36, 'epoch': 0.08}
{'loss': 1.4577, 'grad_norm': 0.07545153051614761, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4343618154525757, 'eval_runtime': 7.59, 'eval_samples_per_second': 131.621, 'eval_steps_per_second': 8.3, 'epoch': 0.12}
{'loss': 1.2871, 'grad_norm': 0.04150136187672615, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3553749322891235, 'eval_runtime': 7.5315, 'eval_samples_per_second': 132.643, 'eval_steps_per_second': 8.365, 'epoch': 0.16}
{'loss': 1.3103, 'grad_norm': 0.03594208508729935, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.322701096534729, 'eval_runtime': 7.5495, 'eval_samples_per_second': 132.326, 'eval_steps_per_second': 8.345, 'epoch': 0.2}
{'loss': 1.2606, 'grad_norm': 0.04201563447713852, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2970800399780273, 'eval_runtime': 7.5646, 'eval_samples_per_second': 132.063, 'eval_steps_per_second': 8.328, 'epoch': 0.24}
{'loss': 1.216, 'grad_norm': 0.04660188406705856, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2728538513183594, 'eval_runtime': 7.5994, 'eval_samples_per_second': 131.458, 'eval_steps_per_second': 8.29, 'epoch': 0.28}
{'loss': 1.2144, 'grad_norm': 0.05498924106359482, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2520805597305298, 'eval_runtime': 7.5885, 'eval_samples_per_second': 131.647, 'eval_steps_per_second': 8.302, 'epoch': 0.32}
{'loss': 1.1812, 'grad_norm': 0.0684129074215889, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2335922718048096, 'eval_runtime': 7.6274, 'eval_samples_per_second': 130.975, 'eval_steps_per_second': 8.26, 'epoch': 0.36}
{'loss': 1.1847, 'grad_norm': 0.05549873784184456, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2160590887069702, 'eval_runtime': 7.6476, 'eval_samples_per_second': 130.629, 'eval_steps_per_second': 8.238, 'epoch': 0.4}
{'loss': 1.2169, 'grad_norm': 0.06787033379077911, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2001312971115112, 'eval_runtime': 7.6412, 'eval_samples_per_second': 130.738, 'eval_steps_per_second': 8.245, 'epoch': 0.44}
{'loss': 1.1025, 'grad_norm': 0.06746924668550491, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1811628341674805, 'eval_runtime': 7.6451, 'eval_samples_per_second': 130.672, 'eval_steps_per_second': 8.241, 'epoch': 0.48}
{'loss': 1.1745, 'grad_norm': 0.056449417024850845, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1652355194091797, 'eval_runtime': 7.6109, 'eval_samples_per_second': 131.259, 'eval_steps_per_second': 8.278, 'epoch': 0.52}
{'loss': 1.1585, 'grad_norm': 0.06435243785381317, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1464828252792358, 'eval_runtime': 7.5951, 'eval_samples_per_second': 131.531, 'eval_steps_per_second': 8.295, 'epoch': 0.56}
{'loss': 1.1235, 'grad_norm': 0.06432308256626129, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1247116327285767, 'eval_runtime': 7.6057, 'eval_samples_per_second': 131.348, 'eval_steps_per_second': 8.283, 'epoch': 0.6}
{'loss': 1.0875, 'grad_norm': 0.06895830482244492, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0995265245437622, 'eval_runtime': 7.6175, 'eval_samples_per_second': 131.146, 'eval_steps_per_second': 8.27, 'epoch': 0.64}
{'loss': 1.0631, 'grad_norm': 0.0759957879781723, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0834541320800781, 'eval_runtime': 7.7015, 'eval_samples_per_second': 129.715, 'eval_steps_per_second': 8.18, 'epoch': 0.68}
{'loss': 1.0768, 'grad_norm': 0.10635507851839066, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0711579322814941, 'eval_runtime': 7.7175, 'eval_samples_per_second': 129.446, 'eval_steps_per_second': 8.163, 'epoch': 0.72}
{'loss': 1.0698, 'grad_norm': 0.07311288267374039, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0612629652023315, 'eval_runtime': 7.6934, 'eval_samples_per_second': 129.852, 'eval_steps_per_second': 8.189, 'epoch': 0.76}
{'loss': 1.0095, 'grad_norm': 0.08863276988267899, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0492010116577148, 'eval_runtime': 7.6873, 'eval_samples_per_second': 129.955, 'eval_steps_per_second': 8.195, 'epoch': 0.8}
{'loss': 1.0076, 'grad_norm': 0.08932951092720032, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0410417318344116, 'eval_runtime': 7.6639, 'eval_samples_per_second': 130.352, 'eval_steps_per_second': 8.22, 'epoch': 0.84}
{'loss': 1.0627, 'grad_norm': 0.09012704342603683, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0323233604431152, 'eval_runtime': 7.6634, 'eval_samples_per_second': 130.359, 'eval_steps_per_second': 8.221, 'epoch': 0.88}
{'loss': 0.9964, 'grad_norm': 0.11105791479349136, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.027899980545044, 'eval_runtime': 7.6246, 'eval_samples_per_second': 131.023, 'eval_steps_per_second': 8.263, 'epoch': 0.92}
{'loss': 1.0462, 'grad_norm': 0.0793897733092308, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0246211290359497, 'eval_runtime': 7.6142, 'eval_samples_per_second': 131.202, 'eval_steps_per_second': 8.274, 'epoch': 0.96}
{'loss': 1.0534, 'grad_norm': 0.10418489575386047, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.023013710975647, 'eval_runtime': 7.5635, 'eval_samples_per_second': 132.082, 'eval_steps_per_second': 8.33, 'epoch': 1.0}
{'train_runtime': 398.7526, 'train_samples_per_second': 25.076, 'train_steps_per_second': 1.567, 'train_loss': 1.2929374389648438, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0521326065063477, 1.6347041130065918, 1.4343618154525757, 1.3553749322891235, 1.322701096534729, 1.2970800399780273, 1.2728538513183594, 1.2520805597305298, 1.2335922718048096, 1.2160590887069702, 1.2001312971115112, 1.1811628341674805, 1.1652355194091797, 1.1464828252792358, 1.1247116327285767, 1.0995265245437622, 1.0834541320800781, 1.0711579322814941, 1.0612629652023315, 1.0492010116577148, 1.0410417318344116, 1.0323233604431152, 1.027899980545044, 1.0246211290359497, 1.023013710975647], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:05,  3.99it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 229.77it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 331.18it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 388.45it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 426.02it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 458.56it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 448.36it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8479698896408081
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9505 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.8101807437076006
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.810355550500258
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.811373532553659
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.8072200494748846
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.8103806573344258
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1481, dtype=torch.float64), 0, tensor(0.6313, dtype=torch.float64), tensor(0.0655, dtype=torch.float64), 0, 0, tensor(0.1551, dtype=torch.float64), 0, 0, 32, 0, 0, 1, 1, 0, 128, 0.010825838706351955, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.1481, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6313, dtype=torch.float64), tensor(0.0655, dtype=torch.float64), tensor(1.8434e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1551, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1083, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.148
  gsm8k: 0
  rowan_hellaswag: 0.631
  sciq: 0.065
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.155
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.010825838706351955,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.010825838706351955
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.44it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 461.86it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 536.09it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 583.71it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 616.98it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 642.34it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 629.11it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.0751, 'grad_norm': 0.23016680777072906, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.313166618347168, 'eval_runtime': 10.0513, 'eval_samples_per_second': 99.39, 'eval_steps_per_second': 6.268, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 276.09it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 460.51it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 537.45it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 584.35it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 616.43it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 641.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 628.60it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.6651, 'grad_norm': 0.19924521446228027, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 2.1973671913146973, 'eval_runtime': 10.0422, 'eval_samples_per_second': 99.48, 'eval_steps_per_second': 6.274, 'epoch': 0.08}
{'loss': 1.9984, 'grad_norm': 0.04552820324897766, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8979886770248413, 'eval_runtime': 10.0894, 'eval_samples_per_second': 99.015, 'eval_steps_per_second': 6.244, 'epoch': 0.12}
{'loss': 1.8961, 'grad_norm': 0.058434344828128815, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8180495500564575, 'eval_runtime': 10.118, 'eval_samples_per_second': 98.735, 'eval_steps_per_second': 6.227, 'epoch': 0.16}
{'loss': 1.7951, 'grad_norm': 0.04525882005691528, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.770185947418213, 'eval_runtime': 10.1274, 'eval_samples_per_second': 98.644, 'eval_steps_per_second': 6.221, 'epoch': 0.2}
{'loss': 1.7409, 'grad_norm': 0.04399285838007927, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7511229515075684, 'eval_runtime': 10.1366, 'eval_samples_per_second': 98.553, 'eval_steps_per_second': 6.215, 'epoch': 0.24}
{'loss': 1.7033, 'grad_norm': 0.04265834018588066, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7358072996139526, 'eval_runtime': 10.1876, 'eval_samples_per_second': 98.06, 'eval_steps_per_second': 6.184, 'epoch': 0.28}
{'loss': 1.7349, 'grad_norm': 0.04275864362716675, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.72669517993927, 'eval_runtime': 10.1832, 'eval_samples_per_second': 98.102, 'eval_steps_per_second': 6.187, 'epoch': 0.32}
{'loss': 1.7141, 'grad_norm': 0.0444888137280941, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7164641618728638, 'eval_runtime': 10.2174, 'eval_samples_per_second': 97.774, 'eval_steps_per_second': 6.166, 'epoch': 0.36}
{'loss': 1.7062, 'grad_norm': 0.036105863749980927, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7098509073257446, 'eval_runtime': 10.2855, 'eval_samples_per_second': 97.127, 'eval_steps_per_second': 6.125, 'epoch': 0.4}
{'loss': 1.7091, 'grad_norm': 0.04955248534679413, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7035653591156006, 'eval_runtime': 10.2825, 'eval_samples_per_second': 97.155, 'eval_steps_per_second': 6.127, 'epoch': 0.44}
{'loss': 1.6442, 'grad_norm': 0.04201396927237511, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6992075443267822, 'eval_runtime': 10.3081, 'eval_samples_per_second': 96.914, 'eval_steps_per_second': 6.112, 'epoch': 0.48}
{'loss': 1.6767, 'grad_norm': 0.0430588573217392, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6948720216751099, 'eval_runtime': 10.3158, 'eval_samples_per_second': 96.842, 'eval_steps_per_second': 6.107, 'epoch': 0.52}
{'loss': 1.6811, 'grad_norm': 0.04180999472737312, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.6914840936660767, 'eval_runtime': 10.2886, 'eval_samples_per_second': 97.098, 'eval_steps_per_second': 6.123, 'epoch': 0.56}
{'loss': 1.6454, 'grad_norm': 0.04625197872519493, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6885271072387695, 'eval_runtime': 10.3296, 'eval_samples_per_second': 96.712, 'eval_steps_per_second': 6.099, 'epoch': 0.6}
{'loss': 1.7029, 'grad_norm': 0.05093640089035034, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6856268644332886, 'eval_runtime': 10.2862, 'eval_samples_per_second': 97.12, 'eval_steps_per_second': 6.125, 'epoch': 0.64}
{'loss': 1.6632, 'grad_norm': 0.048889078199863434, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.682883620262146, 'eval_runtime': 10.3105, 'eval_samples_per_second': 96.892, 'eval_steps_per_second': 6.11, 'epoch': 0.68}
{'loss': 1.6674, 'grad_norm': 0.04858599230647087, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6795790195465088, 'eval_runtime': 10.2861, 'eval_samples_per_second': 97.122, 'eval_steps_per_second': 6.125, 'epoch': 0.72}
{'loss': 1.6686, 'grad_norm': 0.05009905993938446, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6769425868988037, 'eval_runtime': 10.3042, 'eval_samples_per_second': 96.951, 'eval_steps_per_second': 6.114, 'epoch': 0.76}
{'loss': 1.6449, 'grad_norm': 0.05415564030408859, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6754578351974487, 'eval_runtime': 10.2997, 'eval_samples_per_second': 96.993, 'eval_steps_per_second': 6.117, 'epoch': 0.8}
{'loss': 1.7138, 'grad_norm': 0.043122850358486176, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.674263834953308, 'eval_runtime': 10.3329, 'eval_samples_per_second': 96.681, 'eval_steps_per_second': 6.097, 'epoch': 0.84}
{'loss': 1.7039, 'grad_norm': 0.05070896074175835, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6723467111587524, 'eval_runtime': 10.3108, 'eval_samples_per_second': 96.888, 'eval_steps_per_second': 6.11, 'epoch': 0.88}
{'loss': 1.6715, 'grad_norm': 0.050579722970724106, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6713093519210815, 'eval_runtime': 10.3127, 'eval_samples_per_second': 96.871, 'eval_steps_per_second': 6.109, 'epoch': 0.92}
{'loss': 1.6472, 'grad_norm': 0.044196538627147675, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.670654296875, 'eval_runtime': 10.3261, 'eval_samples_per_second': 96.746, 'eval_steps_per_second': 6.101, 'epoch': 0.96}
{'loss': 1.6737, 'grad_norm': 0.05152500793337822, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.670467734336853, 'eval_runtime': 10.3004, 'eval_samples_per_second': 96.986, 'eval_steps_per_second': 6.116, 'epoch': 1.0}
{'train_runtime': 524.3405, 'train_samples_per_second': 19.068, 'train_steps_per_second': 1.192, 'train_loss': 1.8457093811035157, 'epoch': 1.0}
train_results:  {'eval_loss': [3.313166618347168, 2.1973671913146973, 1.8979886770248413, 1.8180495500564575, 1.770185947418213, 1.7511229515075684, 1.7358072996139526, 1.72669517993927, 1.7164641618728638, 1.7098509073257446, 1.7035653591156006, 1.6992075443267822, 1.6948720216751099, 1.6914840936660767, 1.6885271072387695, 1.6856268644332886, 1.682883620262146, 1.6795790195465088, 1.6769425868988037, 1.6754578351974487, 1.674263834953308, 1.6723467111587524, 1.6713093519210815, 1.670654296875, 1.670467734336853], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:32,  1.50it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 116.36it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 205.78it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 275.56it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 327.14it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 372.17it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 305.63it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8470630049705505
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0506 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.8020953430700817
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.7982879858910535
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.8062789265393199
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.8108744970341657
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.794134439351379
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2916, dtype=torch.float64), 0, 0, tensor(0.1784, dtype=torch.float64), 0, tensor(0.0773, dtype=torch.float64), tensor(0.4228, dtype=torch.float64), 0, tensor(0.0298, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 128, 0.09400558727506707, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.2916, dtype=torch.float64), tensor(4.0863e-17, dtype=torch.float64), tensor(1.6653e-16, dtype=torch.float64), tensor(0.1784, dtype=torch.float64), tensor(4.0312e-17, dtype=torch.float64), tensor(0.0773, dtype=torch.float64), tensor(0.4228, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0298, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9401, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.292
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.178
  triviaqa: 0
  truthfulqa_gen: 0.077
  wikitext: 0.423
  mmlu: 0
  arc_challenge: 0.03

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09400558727506707,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.09400558727506707
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 263.68it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 438.69it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 510.99it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 554.38it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 586.62it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 609.47it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 597.06it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.2063, 'grad_norm': 0.23405790328979492, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.3057785034179688, 'eval_runtime': 8.5762, 'eval_samples_per_second': 116.485, 'eval_steps_per_second': 7.346, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 265.21it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 442.70it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 515.36it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 559.73it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 591.38it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 613.22it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 601.44it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.4361, 'grad_norm': 0.2432195544242859, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8677830696105957, 'eval_runtime': 8.5763, 'eval_samples_per_second': 116.484, 'eval_steps_per_second': 7.346, 'epoch': 0.08}
{'loss': 1.6938, 'grad_norm': 0.08432496339082718, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6208564043045044, 'eval_runtime': 8.5505, 'eval_samples_per_second': 116.836, 'eval_steps_per_second': 7.368, 'epoch': 0.12}
{'loss': 1.5811, 'grad_norm': 0.07818331569433212, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.509339690208435, 'eval_runtime': 8.5627, 'eval_samples_per_second': 116.669, 'eval_steps_per_second': 7.357, 'epoch': 0.16}
{'loss': 1.5008, 'grad_norm': 0.0901612713932991, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4301486015319824, 'eval_runtime': 8.5666, 'eval_samples_per_second': 116.616, 'eval_steps_per_second': 7.354, 'epoch': 0.2}
{'loss': 1.3892, 'grad_norm': 0.07059076428413391, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.400214433670044, 'eval_runtime': 8.5749, 'eval_samples_per_second': 116.502, 'eval_steps_per_second': 7.347, 'epoch': 0.24}
{'loss': 1.4195, 'grad_norm': 0.08376709371805191, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3848694562911987, 'eval_runtime': 8.5742, 'eval_samples_per_second': 116.512, 'eval_steps_per_second': 7.348, 'epoch': 0.28}
{'loss': 1.3733, 'grad_norm': 0.06070227548480034, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3770012855529785, 'eval_runtime': 8.5812, 'eval_samples_per_second': 116.418, 'eval_steps_per_second': 7.342, 'epoch': 0.32}
{'loss': 1.3312, 'grad_norm': 0.07491908222436905, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3671009540557861, 'eval_runtime': 8.5812, 'eval_samples_per_second': 116.418, 'eval_steps_per_second': 7.342, 'epoch': 0.36}
{'loss': 1.418, 'grad_norm': 0.0599314384162426, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3602226972579956, 'eval_runtime': 8.5898, 'eval_samples_per_second': 116.301, 'eval_steps_per_second': 7.334, 'epoch': 0.4}
{'loss': 1.2943, 'grad_norm': 0.05933941900730133, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3530471324920654, 'eval_runtime': 8.5913, 'eval_samples_per_second': 116.28, 'eval_steps_per_second': 7.333, 'epoch': 0.44}
{'loss': 1.3525, 'grad_norm': 0.06715286523103714, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.347232460975647, 'eval_runtime': 8.5952, 'eval_samples_per_second': 116.227, 'eval_steps_per_second': 7.33, 'epoch': 0.48}
{'loss': 1.3042, 'grad_norm': 0.0705520436167717, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3410956859588623, 'eval_runtime': 8.5919, 'eval_samples_per_second': 116.273, 'eval_steps_per_second': 7.333, 'epoch': 0.52}
{'loss': 1.4131, 'grad_norm': 0.05912289023399353, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3351813554763794, 'eval_runtime': 8.5938, 'eval_samples_per_second': 116.246, 'eval_steps_per_second': 7.331, 'epoch': 0.56}
{'loss': 1.3428, 'grad_norm': 0.08020751923322678, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3311834335327148, 'eval_runtime': 8.5913, 'eval_samples_per_second': 116.281, 'eval_steps_per_second': 7.333, 'epoch': 0.6}
{'loss': 1.2176, 'grad_norm': 0.07700840383768082, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.328956127166748, 'eval_runtime': 8.5913, 'eval_samples_per_second': 116.28, 'eval_steps_per_second': 7.333, 'epoch': 0.64}
{'loss': 1.3961, 'grad_norm': 0.07314543426036835, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3252241611480713, 'eval_runtime': 8.5888, 'eval_samples_per_second': 116.314, 'eval_steps_per_second': 7.335, 'epoch': 0.68}
{'loss': 1.3382, 'grad_norm': 0.05547325313091278, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3211195468902588, 'eval_runtime': 8.5789, 'eval_samples_per_second': 116.449, 'eval_steps_per_second': 7.344, 'epoch': 0.72}
{'loss': 1.3113, 'grad_norm': 0.048298873007297516, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3196555376052856, 'eval_runtime': 8.5818, 'eval_samples_per_second': 116.409, 'eval_steps_per_second': 7.341, 'epoch': 0.76}
{'loss': 1.3244, 'grad_norm': 0.07445929199457169, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3170417547225952, 'eval_runtime': 8.5813, 'eval_samples_per_second': 116.416, 'eval_steps_per_second': 7.342, 'epoch': 0.8}
{'loss': 1.2758, 'grad_norm': 0.06733749806880951, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3139541149139404, 'eval_runtime': 8.5803, 'eval_samples_per_second': 116.43, 'eval_steps_per_second': 7.342, 'epoch': 0.84}
{'loss': 1.3324, 'grad_norm': 0.07591229677200317, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.312142252922058, 'eval_runtime': 8.6082, 'eval_samples_per_second': 116.052, 'eval_steps_per_second': 7.319, 'epoch': 0.88}
{'loss': 1.2918, 'grad_norm': 0.06524256616830826, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.311239242553711, 'eval_runtime': 8.6137, 'eval_samples_per_second': 115.977, 'eval_steps_per_second': 7.314, 'epoch': 0.92}
{'loss': 1.3509, 'grad_norm': 0.05845813825726509, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3098739385604858, 'eval_runtime': 8.6677, 'eval_samples_per_second': 115.256, 'eval_steps_per_second': 7.268, 'epoch': 0.96}
{'loss': 1.3845, 'grad_norm': 0.07100358605384827, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3095570802688599, 'eval_runtime': 8.6353, 'eval_samples_per_second': 115.688, 'eval_steps_per_second': 7.296, 'epoch': 1.0}
{'train_runtime': 443.5703, 'train_samples_per_second': 22.542, 'train_steps_per_second': 1.409, 'train_loss': 1.5311636993408204, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3057785034179688, 1.8677830696105957, 1.6208564043045044, 1.509339690208435, 1.4301486015319824, 1.400214433670044, 1.3848694562911987, 1.3770012855529785, 1.3671009540557861, 1.3602226972579956, 1.3530471324920654, 1.347232460975647, 1.3410956859588623, 1.3351813554763794, 1.3311834335327148, 1.328956127166748, 1.3252241611480713, 1.3211195468902588, 1.3196555376052856, 1.3170417547225952, 1.3139541149139404, 1.312142252922058, 1.311239242553711, 1.3098739385604858, 1.3095570802688599], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<06:35,  1.26it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 90.40it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 168.79it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 236.41it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 292.70it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 340.83it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 265.42it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8477179408073425
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0400 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.807725685809682
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.8091615974144277
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.7920313514851891
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.8100527253031354
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.8099047527767106
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1464, dtype=torch.float64), 0, tensor(0.3841, dtype=torch.float64), tensor(0.4695, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 0, 128, 2.573256931286264e-17, 1.4800000190734925, 1]
normalized proposed parameters for next round by BO: [tensor(1.9091e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1464, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3841, dtype=torch.float64), tensor(0.4695, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.5733e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.146
  triviaqa: 0
  truthfulqa_gen: 0.384
  wikitext: 0.47
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.573256931286264e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734925,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  2.573256931286264e-17
lora alpha:  1.4800000190734925
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 262.46it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 436.37it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 509.81it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 553.04it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 583.93it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 607.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 594.86it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.0754, 'grad_norm': 0.3644847273826599, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.3207695484161377, 'eval_runtime': 8.5721, 'eval_samples_per_second': 116.541, 'eval_steps_per_second': 7.349, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 263.68it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 440.90it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 512.54it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 556.79it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 589.10it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 613.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 600.02it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5063, 'grad_norm': 0.26879915595054626, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.9312005043029785, 'eval_runtime': 8.5626, 'eval_samples_per_second': 116.67, 'eval_steps_per_second': 7.358, 'epoch': 0.08}
{'loss': 1.7761, 'grad_norm': 0.19626331329345703, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6968541145324707, 'eval_runtime': 8.6014, 'eval_samples_per_second': 116.144, 'eval_steps_per_second': 7.324, 'epoch': 0.12}
{'loss': 1.7503, 'grad_norm': 0.07369188219308853, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5580049753189087, 'eval_runtime': 8.5917, 'eval_samples_per_second': 116.276, 'eval_steps_per_second': 7.333, 'epoch': 0.16}
{'loss': 1.5531, 'grad_norm': 0.09527850896120071, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4726555347442627, 'eval_runtime': 8.6108, 'eval_samples_per_second': 116.017, 'eval_steps_per_second': 7.316, 'epoch': 0.2}
{'loss': 1.4794, 'grad_norm': 0.08992574363946915, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4435406923294067, 'eval_runtime': 8.6015, 'eval_samples_per_second': 116.142, 'eval_steps_per_second': 7.324, 'epoch': 0.24}
{'loss': 1.499, 'grad_norm': 0.1053367406129837, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4238560199737549, 'eval_runtime': 8.5915, 'eval_samples_per_second': 116.278, 'eval_steps_per_second': 7.333, 'epoch': 0.28}
{'loss': 1.4984, 'grad_norm': 0.09149292856454849, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4069212675094604, 'eval_runtime': 8.5632, 'eval_samples_per_second': 116.662, 'eval_steps_per_second': 7.357, 'epoch': 0.32}
{'loss': 1.5041, 'grad_norm': 0.08175788819789886, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.393054723739624, 'eval_runtime': 8.577, 'eval_samples_per_second': 116.474, 'eval_steps_per_second': 7.345, 'epoch': 0.36}
{'loss': 1.4296, 'grad_norm': 0.06440149247646332, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3782429695129395, 'eval_runtime': 8.6578, 'eval_samples_per_second': 115.388, 'eval_steps_per_second': 7.277, 'epoch': 0.4}
{'loss': 1.3896, 'grad_norm': 0.08361698687076569, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3653709888458252, 'eval_runtime': 8.6042, 'eval_samples_per_second': 116.106, 'eval_steps_per_second': 7.322, 'epoch': 0.44}
{'loss': 1.3302, 'grad_norm': 0.1371246576309204, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3548612594604492, 'eval_runtime': 8.594, 'eval_samples_per_second': 116.244, 'eval_steps_per_second': 7.331, 'epoch': 0.48}
{'loss': 1.4365, 'grad_norm': 0.08212573081254959, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3446025848388672, 'eval_runtime': 8.5874, 'eval_samples_per_second': 116.334, 'eval_steps_per_second': 7.336, 'epoch': 0.52}
{'loss': 1.4126, 'grad_norm': 0.16599635779857635, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3348959684371948, 'eval_runtime': 8.5459, 'eval_samples_per_second': 116.898, 'eval_steps_per_second': 7.372, 'epoch': 0.56}
{'loss': 1.31, 'grad_norm': 0.1082739308476448, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3235639333724976, 'eval_runtime': 8.5435, 'eval_samples_per_second': 116.931, 'eval_steps_per_second': 7.374, 'epoch': 0.6}
{'loss': 1.2425, 'grad_norm': 0.16045084595680237, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3111599683761597, 'eval_runtime': 8.5459, 'eval_samples_per_second': 116.898, 'eval_steps_per_second': 7.372, 'epoch': 0.64}
{'loss': 1.4674, 'grad_norm': 0.10238111019134521, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3019278049468994, 'eval_runtime': 8.544, 'eval_samples_per_second': 116.925, 'eval_steps_per_second': 7.374, 'epoch': 0.68}
{'loss': 1.3975, 'grad_norm': 0.10761063545942307, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2916275262832642, 'eval_runtime': 8.5387, 'eval_samples_per_second': 116.997, 'eval_steps_per_second': 7.378, 'epoch': 0.72}
{'loss': 1.2534, 'grad_norm': 0.09300677478313446, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2836709022521973, 'eval_runtime': 8.5425, 'eval_samples_per_second': 116.944, 'eval_steps_per_second': 7.375, 'epoch': 0.76}
{'loss': 1.3495, 'grad_norm': 0.10224281996488571, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.277087926864624, 'eval_runtime': 8.5437, 'eval_samples_per_second': 116.928, 'eval_steps_per_second': 7.374, 'epoch': 0.8}
{'loss': 1.3906, 'grad_norm': 0.11432729661464691, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.269305944442749, 'eval_runtime': 8.5509, 'eval_samples_per_second': 116.83, 'eval_steps_per_second': 7.368, 'epoch': 0.84}
{'loss': 1.3186, 'grad_norm': 0.09514828026294708, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2637194395065308, 'eval_runtime': 8.5315, 'eval_samples_per_second': 117.096, 'eval_steps_per_second': 7.384, 'epoch': 0.88}
{'loss': 1.3474, 'grad_norm': 0.10533686727285385, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2579833269119263, 'eval_runtime': 8.5287, 'eval_samples_per_second': 117.134, 'eval_steps_per_second': 7.387, 'epoch': 0.92}
{'loss': 1.2494, 'grad_norm': 0.12125583738088608, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2552151679992676, 'eval_runtime': 8.5209, 'eval_samples_per_second': 117.241, 'eval_steps_per_second': 7.394, 'epoch': 0.96}
{'loss': 1.3258, 'grad_norm': 0.11563168466091156, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2540245056152344, 'eval_runtime': 8.5277, 'eval_samples_per_second': 117.148, 'eval_steps_per_second': 7.388, 'epoch': 1.0}
{'train_runtime': 445.8419, 'train_samples_per_second': 22.425, 'train_steps_per_second': 1.402, 'train_loss': 1.5717227111816405, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3207695484161377, 1.9312005043029785, 1.6968541145324707, 1.5580049753189087, 1.4726555347442627, 1.4435406923294067, 1.4238560199737549, 1.4069212675094604, 1.393054723739624, 1.3782429695129395, 1.3653709888458252, 1.3548612594604492, 1.3446025848388672, 1.3348959684371948, 1.3235639333724976, 1.3111599683761597, 1.3019278049468994, 1.2916275262832642, 1.2836709022521973, 1.277087926864624, 1.269305944442749, 1.2637194395065308, 1.2579833269119263, 1.2552151679992676, 1.2540245056152344], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:37,  1.80it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 129.56it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 219.87it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 287.83it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 338.21it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 379.61it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 324.49it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8481637239456177
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1799 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.7990447746574348
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.8093148891776225
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.7588352047715482
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.8084206794012663
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.7985944853262535
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.5782, dtype=torch.float64), 0, 0, tensor(0.4218, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 128, 0.09386065074231442, 1.4800000190734872, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0457e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2978e-17, dtype=torch.float64), tensor(0.5782, dtype=torch.float64), tensor(1.7369e-16, dtype=torch.float64), tensor(7.9866e-17, dtype=torch.float64), tensor(0.4218, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9386, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.578
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.422

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09386065074231442,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.09386065074231442
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 266.92it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 443.89it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 516.92it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 560.85it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 590.48it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 613.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 602.37it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.2137, 'grad_norm': 0.3272608518600464, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.0007483959198, 'eval_runtime': 7.6486, 'eval_samples_per_second': 130.613, 'eval_steps_per_second': 8.237, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 263.69it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 438.25it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 510.68it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 555.74it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 588.13it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 612.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 598.82it/s]
Evaluation performance at step 50: 0.75
{'loss': 1.9575, 'grad_norm': 0.28139811754226685, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.2178807258605957, 'eval_runtime': 6.7968, 'eval_samples_per_second': 146.981, 'eval_steps_per_second': 9.269, 'epoch': 0.08}
{'loss': 1.0659, 'grad_norm': 0.09014599770307541, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.966328501701355, 'eval_runtime': 6.8226, 'eval_samples_per_second': 146.425, 'eval_steps_per_second': 9.234, 'epoch': 0.12}
{'loss': 0.9075, 'grad_norm': 0.07568420469760895, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8440097570419312, 'eval_runtime': 6.7734, 'eval_samples_per_second': 147.489, 'eval_steps_per_second': 9.301, 'epoch': 0.16}
{'loss': 0.8459, 'grad_norm': 0.044728901237249374, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8207298517227173, 'eval_runtime': 6.8389, 'eval_samples_per_second': 146.076, 'eval_steps_per_second': 9.212, 'epoch': 0.2}
{'loss': 0.8131, 'grad_norm': 0.045266829431056976, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8052452802658081, 'eval_runtime': 6.8554, 'eval_samples_per_second': 145.725, 'eval_steps_per_second': 9.19, 'epoch': 0.24}
{'loss': 0.8099, 'grad_norm': 0.042747966945171356, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7924151420593262, 'eval_runtime': 6.8312, 'eval_samples_per_second': 146.242, 'eval_steps_per_second': 9.222, 'epoch': 0.28}
{'loss': 0.7995, 'grad_norm': 0.04304677993059158, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.777379035949707, 'eval_runtime': 6.8283, 'eval_samples_per_second': 146.303, 'eval_steps_per_second': 9.226, 'epoch': 0.32}
{'loss': 0.7815, 'grad_norm': 0.06057269871234894, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7649945020675659, 'eval_runtime': 6.837, 'eval_samples_per_second': 146.116, 'eval_steps_per_second': 9.215, 'epoch': 0.36}
{'loss': 0.7551, 'grad_norm': 0.05225759372115135, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7516026496887207, 'eval_runtime': 6.8343, 'eval_samples_per_second': 146.173, 'eval_steps_per_second': 9.218, 'epoch': 0.4}
{'loss': 0.7258, 'grad_norm': 0.05433117598295212, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7386382818222046, 'eval_runtime': 6.8483, 'eval_samples_per_second': 145.876, 'eval_steps_per_second': 9.199, 'epoch': 0.44}
{'loss': 0.7349, 'grad_norm': 0.06142426282167435, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7215943932533264, 'eval_runtime': 6.8418, 'eval_samples_per_second': 146.014, 'eval_steps_per_second': 9.208, 'epoch': 0.48}
{'loss': 0.7269, 'grad_norm': 0.06404519081115723, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7094467282295227, 'eval_runtime': 6.8289, 'eval_samples_per_second': 146.29, 'eval_steps_per_second': 9.225, 'epoch': 0.52}
{'loss': 0.6785, 'grad_norm': 0.08081319183111191, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6926801204681396, 'eval_runtime': 6.8357, 'eval_samples_per_second': 146.144, 'eval_steps_per_second': 9.216, 'epoch': 0.56}
{'loss': 0.7263, 'grad_norm': 0.06987669318914413, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.678752601146698, 'eval_runtime': 6.8461, 'eval_samples_per_second': 145.923, 'eval_steps_per_second': 9.202, 'epoch': 0.6}
{'loss': 0.6833, 'grad_norm': 0.09616079926490784, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6643931269645691, 'eval_runtime': 6.8377, 'eval_samples_per_second': 146.102, 'eval_steps_per_second': 9.214, 'epoch': 0.64}
{'loss': 0.685, 'grad_norm': 0.07974521070718765, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6518210172653198, 'eval_runtime': 6.8274, 'eval_samples_per_second': 146.321, 'eval_steps_per_second': 9.227, 'epoch': 0.68}
{'loss': 0.6633, 'grad_norm': 0.09928584098815918, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.636090874671936, 'eval_runtime': 6.8435, 'eval_samples_per_second': 145.977, 'eval_steps_per_second': 9.206, 'epoch': 0.72}
{'loss': 0.6852, 'grad_norm': 0.08996225893497467, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6213194727897644, 'eval_runtime': 6.8223, 'eval_samples_per_second': 146.432, 'eval_steps_per_second': 9.234, 'epoch': 0.76}
{'loss': 0.6526, 'grad_norm': 0.0893189087510109, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6074321269989014, 'eval_runtime': 6.8013, 'eval_samples_per_second': 146.885, 'eval_steps_per_second': 9.263, 'epoch': 0.8}
{'loss': 0.6245, 'grad_norm': 0.10879795998334885, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5959055423736572, 'eval_runtime': 6.8043, 'eval_samples_per_second': 146.819, 'eval_steps_per_second': 9.259, 'epoch': 0.84}
{'loss': 0.6239, 'grad_norm': 0.11733179539442062, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5838915109634399, 'eval_runtime': 6.8038, 'eval_samples_per_second': 146.829, 'eval_steps_per_second': 9.259, 'epoch': 0.88}
{'loss': 0.6236, 'grad_norm': 0.11323949694633484, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5766602158546448, 'eval_runtime': 6.8286, 'eval_samples_per_second': 146.297, 'eval_steps_per_second': 9.226, 'epoch': 0.92}
{'loss': 0.5958, 'grad_norm': 0.14536313712596893, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5725666284561157, 'eval_runtime': 6.7951, 'eval_samples_per_second': 147.017, 'eval_steps_per_second': 9.271, 'epoch': 0.96}
{'loss': 0.6014, 'grad_norm': 0.14315763115882874, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.570612907409668, 'eval_runtime': 6.8063, 'eval_samples_per_second': 146.777, 'eval_steps_per_second': 9.256, 'epoch': 1.0}
{'train_runtime': 362.2363, 'train_samples_per_second': 27.604, 'train_steps_per_second': 1.725, 'train_loss': 0.9192238037109375, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0007483959198, 1.2178807258605957, 0.966328501701355, 0.8440097570419312, 0.8207298517227173, 0.8052452802658081, 0.7924151420593262, 0.777379035949707, 0.7649945020675659, 0.7516026496887207, 0.7386382818222046, 0.7215943932533264, 0.7094467282295227, 0.6926801204681396, 0.678752601146698, 0.6643931269645691, 0.6518210172653198, 0.636090874671936, 0.6213194727897644, 0.6074321269989014, 0.5959055423736572, 0.5838915109634399, 0.5766602158546448, 0.5725666284561157, 0.570612907409668], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:48,  4.60it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 237.53it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 326.41it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 374.19it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 403.86it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 429.13it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 432.92it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8481503129005432
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6759 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.7561380955177178
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.8061358218739579
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.7913319742313292
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.7999419191512359
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.7855333909704589
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1128, dtype=torch.float64), 0, 0, tensor(0.3589, dtype=torch.float64), 0, tensor(0.1680, dtype=torch.float64), tensor(0.1979, dtype=torch.float64), 0, tensor(0.1625, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 128, 0.03989517198000765, 8.254002950226319, 1]
normalized proposed parameters for next round by BO: [tensor(0.1128, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3589, dtype=torch.float64), tensor(1.8297e-17, dtype=torch.float64), tensor(0.1680, dtype=torch.float64), tensor(0.1979, dtype=torch.float64), tensor(4.7573e-18, dtype=torch.float64), tensor(0.1625, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3990, dtype=torch.float64), tensor(0.1720, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.113
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.359
  triviaqa: 0
  truthfulqa_gen: 0.168
  wikitext: 0.198
  mmlu: 0
  arc_challenge: 0.162

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03989517198000765,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (8.254002950226319,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.03989517198000765
lora alpha:  8.254002950226319
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 265.70it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 443.69it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 517.37it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 562.29it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 593.71it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 616.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 604.61it/s]
Evaluation performance at step 25: 0.73
{'loss': 3.7974, 'grad_norm': 0.6860058903694153, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 2.1000168323516846, 'eval_runtime': 7.3449, 'eval_samples_per_second': 136.012, 'eval_steps_per_second': 8.577, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 266.12it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 443.05it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 515.63it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 559.82it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 588.72it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 610.13it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 600.06it/s]
Evaluation performance at step 50: 0.75
{'loss': 1.6008, 'grad_norm': 0.4320428967475891, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.2897475957870483, 'eval_runtime': 7.3675, 'eval_samples_per_second': 135.595, 'eval_steps_per_second': 8.551, 'epoch': 0.08}
{'loss': 1.1881, 'grad_norm': 0.16068066656589508, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1293689012527466, 'eval_runtime': 7.4096, 'eval_samples_per_second': 134.825, 'eval_steps_per_second': 8.502, 'epoch': 0.12}
{'loss': 1.0672, 'grad_norm': 0.12084020674228668, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0798556804656982, 'eval_runtime': 7.4212, 'eval_samples_per_second': 134.614, 'eval_steps_per_second': 8.489, 'epoch': 0.16}
{'loss': 1.1413, 'grad_norm': 0.17288459837436676, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.053196907043457, 'eval_runtime': 7.4513, 'eval_samples_per_second': 134.071, 'eval_steps_per_second': 8.455, 'epoch': 0.2}
{'loss': 1.0923, 'grad_norm': 0.10980445891618729, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.030968427658081, 'eval_runtime': 7.4186, 'eval_samples_per_second': 134.662, 'eval_steps_per_second': 8.492, 'epoch': 0.24}
{'loss': 1.0112, 'grad_norm': 0.14361457526683807, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0228767395019531, 'eval_runtime': 7.4413, 'eval_samples_per_second': 134.252, 'eval_steps_per_second': 8.466, 'epoch': 0.28}
{'loss': 1.0821, 'grad_norm': 0.11970587819814682, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0088163614273071, 'eval_runtime': 7.4239, 'eval_samples_per_second': 134.566, 'eval_steps_per_second': 8.486, 'epoch': 0.32}
{'loss': 1.0602, 'grad_norm': 0.12668929994106293, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.998927116394043, 'eval_runtime': 7.4152, 'eval_samples_per_second': 134.724, 'eval_steps_per_second': 8.496, 'epoch': 0.36}
{'loss': 1.0302, 'grad_norm': 0.11181534081697464, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9931209683418274, 'eval_runtime': 7.3669, 'eval_samples_per_second': 135.607, 'eval_steps_per_second': 8.552, 'epoch': 0.4}
{'loss': 1.0783, 'grad_norm': 0.1267475038766861, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9853323101997375, 'eval_runtime': 7.3637, 'eval_samples_per_second': 135.666, 'eval_steps_per_second': 8.555, 'epoch': 0.44}
{'loss': 0.9839, 'grad_norm': 0.12906168401241302, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9763052463531494, 'eval_runtime': 7.3703, 'eval_samples_per_second': 135.544, 'eval_steps_per_second': 8.548, 'epoch': 0.48}
{'loss': 0.9813, 'grad_norm': 0.15312620997428894, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9697794914245605, 'eval_runtime': 7.3727, 'eval_samples_per_second': 135.5, 'eval_steps_per_second': 8.545, 'epoch': 0.52}
{'loss': 1.0605, 'grad_norm': 0.11184453964233398, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9641470313072205, 'eval_runtime': 7.372, 'eval_samples_per_second': 135.514, 'eval_steps_per_second': 8.546, 'epoch': 0.56}
{'loss': 1.0852, 'grad_norm': 0.13603335618972778, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9564931392669678, 'eval_runtime': 7.3659, 'eval_samples_per_second': 135.625, 'eval_steps_per_second': 8.553, 'epoch': 0.6}
{'loss': 1.0849, 'grad_norm': 0.16049449145793915, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9516042470932007, 'eval_runtime': 7.3604, 'eval_samples_per_second': 135.726, 'eval_steps_per_second': 8.559, 'epoch': 0.64}
{'loss': 1.045, 'grad_norm': 0.1217103824019432, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9457923769950867, 'eval_runtime': 7.3831, 'eval_samples_per_second': 135.308, 'eval_steps_per_second': 8.533, 'epoch': 0.68}
{'loss': 1.0372, 'grad_norm': 0.1387578248977661, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9401386380195618, 'eval_runtime': 7.3913, 'eval_samples_per_second': 135.16, 'eval_steps_per_second': 8.524, 'epoch': 0.72}
{'loss': 0.9779, 'grad_norm': 0.1370692104101181, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9337086081504822, 'eval_runtime': 7.4169, 'eval_samples_per_second': 134.692, 'eval_steps_per_second': 8.494, 'epoch': 0.76}
{'loss': 1.042, 'grad_norm': 0.15745587646961212, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9276561737060547, 'eval_runtime': 7.3979, 'eval_samples_per_second': 135.038, 'eval_steps_per_second': 8.516, 'epoch': 0.8}
{'loss': 0.9831, 'grad_norm': 0.1594475507736206, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9226093888282776, 'eval_runtime': 7.3915, 'eval_samples_per_second': 135.155, 'eval_steps_per_second': 8.523, 'epoch': 0.84}
{'loss': 1.0322, 'grad_norm': 0.14556653797626495, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9209188222885132, 'eval_runtime': 7.3773, 'eval_samples_per_second': 135.415, 'eval_steps_per_second': 8.54, 'epoch': 0.88}
{'loss': 0.9518, 'grad_norm': 0.12872782349586487, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9179078340530396, 'eval_runtime': 7.3757, 'eval_samples_per_second': 135.444, 'eval_steps_per_second': 8.542, 'epoch': 0.92}
{'loss': 1.0307, 'grad_norm': 0.1459672898054123, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9162508845329285, 'eval_runtime': 7.3794, 'eval_samples_per_second': 135.377, 'eval_steps_per_second': 8.537, 'epoch': 0.96}
{'loss': 1.0004, 'grad_norm': 0.13674145936965942, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9157724380493164, 'eval_runtime': 7.3727, 'eval_samples_per_second': 135.5, 'eval_steps_per_second': 8.545, 'epoch': 1.0}
{'train_runtime': 399.1368, 'train_samples_per_second': 25.047, 'train_steps_per_second': 1.566, 'train_loss': 1.1778079406738282, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1000168323516846, 1.2897475957870483, 1.1293689012527466, 1.0798556804656982, 1.053196907043457, 1.030968427658081, 1.0228767395019531, 1.0088163614273071, 0.998927116394043, 0.9931209683418274, 0.9853323101997375, 0.9763052463531494, 0.9697794914245605, 0.9641470313072205, 0.9564931392669678, 0.9516042470932007, 0.9457923769950867, 0.9401386380195618, 0.9337086081504822, 0.9276561737060547, 0.9226093888282776, 0.9209188222885132, 0.9179078340530396, 0.9162508845329285, 0.9157724380493164], 'performance': [0.73, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:42,  2.24it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 160.66it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 255.21it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 319.46it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 364.22it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 399.87it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 360.96it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.843762218952179
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.2876 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.8188718984266805
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.8082231874793823
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.8218742215554798
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.7848336863297468
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.7187414877985362
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.1899, dtype=torch.float64), tensor(0.1769, dtype=torch.float64), 0, tensor(0.6331, dtype=torch.float64), 32, 1, 0, 0, 0, 0, 128, 0.004870508447921457, 1.4800000190734905, 1]
normalized proposed parameters for next round by BO: [tensor(0.0001, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.2806e-16, dtype=torch.float64), tensor(3.2604e-17, dtype=torch.float64), tensor(0.1899, dtype=torch.float64), tensor(0.1769, dtype=torch.float64), tensor(9.8585e-16, dtype=torch.float64), tensor(0.6331, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0487, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.19
  wikitext: 0.177
  mmlu: 0
  arc_challenge: 0.633

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.004870508447921457,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (1.4800000190734905,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.004870508447921457
lora alpha:  1.4800000190734905
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 33,554,432 || all params: 8,063,815,680 || trainable%: 0.4161
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 303.95it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 504.09it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 588.20it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 633.89it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 667.44it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 693.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 681.33it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.0787, 'grad_norm': 0.082021065056324, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.978538990020752, 'eval_runtime': 6.8135, 'eval_samples_per_second': 146.621, 'eval_steps_per_second': 9.246, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 306.91it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 509.42it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 592.02it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 644.77it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 682.17it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 710.01it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 695.06it/s]
Evaluation performance at step 50: 0.75
{'loss': 3.3802, 'grad_norm': 0.12899012863636017, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.7354226112365723, 'eval_runtime': 6.806, 'eval_samples_per_second': 146.783, 'eval_steps_per_second': 9.257, 'epoch': 0.08}
{'loss': 2.3386, 'grad_norm': 0.057015325874090195, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9580801725387573, 'eval_runtime': 6.8106, 'eval_samples_per_second': 146.683, 'eval_steps_per_second': 9.25, 'epoch': 0.12}
{'loss': 1.7446, 'grad_norm': 0.049609504640102386, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6113663911819458, 'eval_runtime': 6.8118, 'eval_samples_per_second': 146.657, 'eval_steps_per_second': 9.249, 'epoch': 0.16}
{'loss': 1.5228, 'grad_norm': 0.047654956579208374, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5054720640182495, 'eval_runtime': 6.8309, 'eval_samples_per_second': 146.247, 'eval_steps_per_second': 9.223, 'epoch': 0.2}
{'loss': 1.4601, 'grad_norm': 0.03616371750831604, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.476002812385559, 'eval_runtime': 6.8428, 'eval_samples_per_second': 145.994, 'eval_steps_per_second': 9.207, 'epoch': 0.24}
{'loss': 1.579, 'grad_norm': 0.027948766946792603, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4605578184127808, 'eval_runtime': 6.8482, 'eval_samples_per_second': 145.877, 'eval_steps_per_second': 9.199, 'epoch': 0.28}
{'loss': 1.503, 'grad_norm': 0.027499282732605934, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4466521739959717, 'eval_runtime': 6.8462, 'eval_samples_per_second': 145.919, 'eval_steps_per_second': 9.202, 'epoch': 0.32}
{'loss': 1.4374, 'grad_norm': 0.027624404057860374, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4356979131698608, 'eval_runtime': 6.8467, 'eval_samples_per_second': 145.909, 'eval_steps_per_second': 9.201, 'epoch': 0.36}
{'loss': 1.4166, 'grad_norm': 0.03157396987080574, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4270551204681396, 'eval_runtime': 6.8663, 'eval_samples_per_second': 145.493, 'eval_steps_per_second': 9.175, 'epoch': 0.4}
{'loss': 1.4323, 'grad_norm': 0.022321755066514015, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4189521074295044, 'eval_runtime': 6.8642, 'eval_samples_per_second': 145.539, 'eval_steps_per_second': 9.178, 'epoch': 0.44}
{'loss': 1.3902, 'grad_norm': 0.02490210346877575, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4122951030731201, 'eval_runtime': 6.8681, 'eval_samples_per_second': 145.456, 'eval_steps_per_second': 9.173, 'epoch': 0.48}
{'loss': 1.4469, 'grad_norm': 0.027359433472156525, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.40553879737854, 'eval_runtime': 6.8694, 'eval_samples_per_second': 145.427, 'eval_steps_per_second': 9.171, 'epoch': 0.52}
{'loss': 1.385, 'grad_norm': 0.03413766250014305, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4005851745605469, 'eval_runtime': 6.864, 'eval_samples_per_second': 145.543, 'eval_steps_per_second': 9.178, 'epoch': 0.56}
{'loss': 1.4215, 'grad_norm': 0.02857009693980217, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3960462808609009, 'eval_runtime': 6.8598, 'eval_samples_per_second': 145.632, 'eval_steps_per_second': 9.184, 'epoch': 0.6}
{'loss': 1.3968, 'grad_norm': 0.03301122784614563, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.395012378692627, 'eval_runtime': 6.8738, 'eval_samples_per_second': 145.334, 'eval_steps_per_second': 9.165, 'epoch': 0.64}
{'loss': 1.4243, 'grad_norm': 0.02951371669769287, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3894363641738892, 'eval_runtime': 6.8714, 'eval_samples_per_second': 145.385, 'eval_steps_per_second': 9.168, 'epoch': 0.68}
{'loss': 1.4182, 'grad_norm': 0.029110150411725044, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3868488073349, 'eval_runtime': 6.8839, 'eval_samples_per_second': 145.121, 'eval_steps_per_second': 9.152, 'epoch': 0.72}
{'loss': 1.4056, 'grad_norm': 0.03467297554016113, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3841586112976074, 'eval_runtime': 6.8598, 'eval_samples_per_second': 145.632, 'eval_steps_per_second': 9.184, 'epoch': 0.76}
{'loss': 1.3739, 'grad_norm': 0.04114354029297829, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3827530145645142, 'eval_runtime': 6.8527, 'eval_samples_per_second': 145.782, 'eval_steps_per_second': 9.193, 'epoch': 0.8}
{'loss': 1.4049, 'grad_norm': 0.023302556946873665, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3810250759124756, 'eval_runtime': 6.8572, 'eval_samples_per_second': 145.687, 'eval_steps_per_second': 9.187, 'epoch': 0.84}
{'loss': 1.3639, 'grad_norm': 0.03385065495967865, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3798439502716064, 'eval_runtime': 6.8568, 'eval_samples_per_second': 145.694, 'eval_steps_per_second': 9.188, 'epoch': 0.88}
{'loss': 1.4091, 'grad_norm': 0.02983509935438633, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3790082931518555, 'eval_runtime': 6.8456, 'eval_samples_per_second': 145.934, 'eval_steps_per_second': 9.203, 'epoch': 0.92}
{'loss': 1.3794, 'grad_norm': 0.036420099437236786, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3783798217773438, 'eval_runtime': 6.8588, 'eval_samples_per_second': 145.652, 'eval_steps_per_second': 9.185, 'epoch': 0.96}
{'loss': 1.354, 'grad_norm': 0.03867442533373833, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3782360553741455, 'eval_runtime': 6.8476, 'eval_samples_per_second': 145.89, 'eval_steps_per_second': 9.2, 'epoch': 1.0}
{'train_runtime': 361.2019, 'train_samples_per_second': 27.677, 'train_steps_per_second': 1.73, 'train_loss': 1.6586700439453126, 'epoch': 1.0}
train_results:  {'eval_loss': [3.978538990020752, 2.7354226112365723, 1.9580801725387573, 1.6113663911819458, 1.5054720640182495, 1.476002812385559, 1.4605578184127808, 1.4466521739959717, 1.4356979131698608, 1.4270551204681396, 1.4189521074295044, 1.4122951030731201, 1.40553879737854, 1.4005851745605469, 1.3960462808609009, 1.395012378692627, 1.3894363641738892, 1.3868488073349, 1.3841586112976074, 1.3827530145645142, 1.3810250759124756, 1.3798439502716064, 1.3790082931518555, 1.3783798217773438, 1.3782360553741455], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:18,  1.93it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 137.11it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 237.87it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 315.97it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 375.72it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 423.98it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 356.37it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8476830720901489
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1373 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.7940465026195351
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.8170194030528591
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.8168767169130305
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.8170166413499436
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.8015638670045567
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3904, dtype=torch.float64), 0, 0, tensor(0.0467, dtype=torch.float64), 0, tensor(0.2076, dtype=torch.float64), 0, 0, tensor(0.3553, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.07503071888068505, 1.480000019073487, 0]
normalized proposed parameters for next round by BO: [tensor(0.3904, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2218e-17, dtype=torch.float64), tensor(0.0467, dtype=torch.float64), tensor(1.2332e-16, dtype=torch.float64), tensor(0.2076, dtype=torch.float64), tensor(2.4625e-17, dtype=torch.float64), tensor(1.2490e-16, dtype=torch.float64), tensor(0.3553, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7503, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.39
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.047
  triviaqa: 0
  truthfulqa_gen: 0.208
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.355

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07503071888068505,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.07503071888068505
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 247.73it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 411.04it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 481.55it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 524.64it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 554.53it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 576.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 564.17it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.2849, 'grad_norm': 0.2791828215122223, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 2.9462571144104004, 'eval_runtime': 7.1283, 'eval_samples_per_second': 140.146, 'eval_steps_per_second': 8.838, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 249.56it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 416.40it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 484.89it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 526.24it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 554.97it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 577.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 566.14it/s]
Evaluation performance at step 50: 0.74
{'loss': 1.8335, 'grad_norm': 0.22777795791625977, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.2371203899383545, 'eval_runtime': 6.9712, 'eval_samples_per_second': 143.304, 'eval_steps_per_second': 9.037, 'epoch': 0.08}
{'loss': 1.1038, 'grad_norm': 0.0824836865067482, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0309364795684814, 'eval_runtime': 6.9799, 'eval_samples_per_second': 143.124, 'eval_steps_per_second': 9.026, 'epoch': 0.12}
{'loss': 0.9591, 'grad_norm': 0.04158763214945793, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9221943020820618, 'eval_runtime': 6.9977, 'eval_samples_per_second': 142.762, 'eval_steps_per_second': 9.003, 'epoch': 0.16}
{'loss': 0.8841, 'grad_norm': 0.03616547957062721, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9023349285125732, 'eval_runtime': 6.9925, 'eval_samples_per_second': 142.868, 'eval_steps_per_second': 9.01, 'epoch': 0.2}
{'loss': 0.9034, 'grad_norm': 0.039725128561258316, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8892982006072998, 'eval_runtime': 6.9754, 'eval_samples_per_second': 143.218, 'eval_steps_per_second': 9.032, 'epoch': 0.24}
{'loss': 0.8634, 'grad_norm': 0.0485781766474247, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.877481997013092, 'eval_runtime': 6.9739, 'eval_samples_per_second': 143.249, 'eval_steps_per_second': 9.034, 'epoch': 0.28}
{'loss': 0.8635, 'grad_norm': 0.04062343388795853, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.868099570274353, 'eval_runtime': 6.9596, 'eval_samples_per_second': 143.544, 'eval_steps_per_second': 9.052, 'epoch': 0.32}
{'loss': 0.869, 'grad_norm': 0.03555212914943695, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8580438494682312, 'eval_runtime': 6.9681, 'eval_samples_per_second': 143.367, 'eval_steps_per_second': 9.041, 'epoch': 0.36}
{'loss': 0.854, 'grad_norm': 0.0444171205163002, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8504460453987122, 'eval_runtime': 6.961, 'eval_samples_per_second': 143.515, 'eval_steps_per_second': 9.05, 'epoch': 0.4}
{'loss': 0.8506, 'grad_norm': 0.042806655168533325, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8420647382736206, 'eval_runtime': 6.9694, 'eval_samples_per_second': 143.341, 'eval_steps_per_second': 9.04, 'epoch': 0.44}
{'loss': 0.8585, 'grad_norm': 0.04325096309185028, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8355295658111572, 'eval_runtime': 6.9643, 'eval_samples_per_second': 143.445, 'eval_steps_per_second': 9.046, 'epoch': 0.48}
{'loss': 0.8581, 'grad_norm': 0.04987284541130066, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8303751945495605, 'eval_runtime': 6.9644, 'eval_samples_per_second': 143.444, 'eval_steps_per_second': 9.046, 'epoch': 0.52}
{'loss': 0.8343, 'grad_norm': 0.04952557757496834, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8213984370231628, 'eval_runtime': 6.9608, 'eval_samples_per_second': 143.517, 'eval_steps_per_second': 9.051, 'epoch': 0.56}
{'loss': 0.8308, 'grad_norm': 0.04653448238968849, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8153557777404785, 'eval_runtime': 6.9554, 'eval_samples_per_second': 143.63, 'eval_steps_per_second': 9.058, 'epoch': 0.6}
{'loss': 0.8176, 'grad_norm': 0.05098460987210274, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8106418251991272, 'eval_runtime': 6.9601, 'eval_samples_per_second': 143.533, 'eval_steps_per_second': 9.052, 'epoch': 0.64}
{'loss': 0.8273, 'grad_norm': 0.055708542466163635, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8034889101982117, 'eval_runtime': 7.0001, 'eval_samples_per_second': 142.713, 'eval_steps_per_second': 9.0, 'epoch': 0.68}
{'loss': 0.7968, 'grad_norm': 0.04940144345164299, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7981958985328674, 'eval_runtime': 6.9805, 'eval_samples_per_second': 143.113, 'eval_steps_per_second': 9.025, 'epoch': 0.72}
{'loss': 0.7981, 'grad_norm': 0.051243457943201065, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7924766540527344, 'eval_runtime': 6.9857, 'eval_samples_per_second': 143.007, 'eval_steps_per_second': 9.018, 'epoch': 0.76}
{'loss': 0.8177, 'grad_norm': 0.05463758111000061, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7877079844474792, 'eval_runtime': 6.9749, 'eval_samples_per_second': 143.229, 'eval_steps_per_second': 9.032, 'epoch': 0.8}
{'loss': 0.7935, 'grad_norm': 0.056818973273038864, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7827903628349304, 'eval_runtime': 6.9758, 'eval_samples_per_second': 143.21, 'eval_steps_per_second': 9.031, 'epoch': 0.84}
{'loss': 0.7763, 'grad_norm': 0.061074741184711456, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7787293195724487, 'eval_runtime': 6.9781, 'eval_samples_per_second': 143.161, 'eval_steps_per_second': 9.028, 'epoch': 0.88}
{'loss': 0.7946, 'grad_norm': 0.05747753754258156, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7753704786300659, 'eval_runtime': 6.9777, 'eval_samples_per_second': 143.17, 'eval_steps_per_second': 9.029, 'epoch': 0.92}
{'loss': 0.7968, 'grad_norm': 0.06535417586565018, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7731533050537109, 'eval_runtime': 6.9826, 'eval_samples_per_second': 143.07, 'eval_steps_per_second': 9.022, 'epoch': 0.96}
{'loss': 0.7925, 'grad_norm': 0.06949088722467422, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7725867033004761, 'eval_runtime': 6.9881, 'eval_samples_per_second': 142.956, 'eval_steps_per_second': 9.015, 'epoch': 1.0}
{'train_runtime': 371.3039, 'train_samples_per_second': 26.927, 'train_steps_per_second': 1.683, 'train_loss': 1.0264817932128907, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9462571144104004, 1.2371203899383545, 1.0309364795684814, 0.9221943020820618, 0.9023349285125732, 0.8892982006072998, 0.877481997013092, 0.868099570274353, 0.8580438494682312, 0.8504460453987122, 0.8420647382736206, 0.8355295658111572, 0.8303751945495605, 0.8213984370231628, 0.8153557777404785, 0.8106418251991272, 0.8034889101982117, 0.7981958985328674, 0.7924766540527344, 0.7877079844474792, 0.7827903628349304, 0.7787293195724487, 0.7753704786300659, 0.7731533050537109, 0.7725867033004761], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:19,  1.56it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 87.85it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 163.32it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 227.87it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 280.50it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 326.06it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 262.59it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8470314741134644
current iteration best possible performance (full train run):  0.8400000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7252 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.8155834065420361
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.7650899519883709
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.819319033048079
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.7924639042500858
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.8182466671302189
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3211, dtype=torch.float64), 0, tensor(0.1854, dtype=torch.float64), tensor(0.4444, dtype=torch.float64), 0, tensor(0.0492, dtype=torch.float64), 32, 0, 1, 1, 0, 0, 128, 4.7342270802550844e-18, 1.4800000190734897, 0]
normalized proposed parameters for next round by BO: [tensor(2.0900e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.5001e-17, dtype=torch.float64), tensor(0.3211, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1854, dtype=torch.float64), tensor(0.4444, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0492, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(4.7342e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.321
  triviaqa: 0
  truthfulqa_gen: 0.185
  wikitext: 0.444
  mmlu: 0
  arc_challenge: 0.049

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.7342270802550844e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734897,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  128
lora dropout:  4.7342270802550844e-18
lora alpha:  1.4800000190734897
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 290.38it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 484.52it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 562.55it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 611.35it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 646.27it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 672.71it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 658.73it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.3156, 'grad_norm': 0.2607535719871521, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.503196954727173, 'eval_runtime': 7.1787, 'eval_samples_per_second': 139.161, 'eval_steps_per_second': 8.776, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 288.31it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 482.19it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 563.09it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 611.37it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 645.48it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 670.99it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 657.51it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.6504, 'grad_norm': 0.10862789303064346, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.1824417114257812, 'eval_runtime': 7.2134, 'eval_samples_per_second': 138.493, 'eval_steps_per_second': 8.734, 'epoch': 0.08}
{'loss': 2.0419, 'grad_norm': 0.10061656683683395, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8525285720825195, 'eval_runtime': 7.2567, 'eval_samples_per_second': 137.666, 'eval_steps_per_second': 8.682, 'epoch': 0.12}
{'loss': 1.8292, 'grad_norm': 0.06493847072124481, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.727697491645813, 'eval_runtime': 7.2407, 'eval_samples_per_second': 137.969, 'eval_steps_per_second': 8.701, 'epoch': 0.16}
{'loss': 1.7081, 'grad_norm': 0.054468784481287, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6677895784378052, 'eval_runtime': 7.2488, 'eval_samples_per_second': 137.815, 'eval_steps_per_second': 8.691, 'epoch': 0.2}
{'loss': 1.702, 'grad_norm': 0.0607525110244751, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.638314127922058, 'eval_runtime': 7.2543, 'eval_samples_per_second': 137.711, 'eval_steps_per_second': 8.684, 'epoch': 0.24}
{'loss': 1.7318, 'grad_norm': 0.07058898359537125, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6149441003799438, 'eval_runtime': 7.2381, 'eval_samples_per_second': 138.019, 'eval_steps_per_second': 8.704, 'epoch': 0.28}
{'loss': 1.6067, 'grad_norm': 0.07956676185131073, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.599880576133728, 'eval_runtime': 7.2204, 'eval_samples_per_second': 138.358, 'eval_steps_per_second': 8.725, 'epoch': 0.32}
{'loss': 1.6652, 'grad_norm': 0.06634470075368881, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5845520496368408, 'eval_runtime': 7.2341, 'eval_samples_per_second': 138.097, 'eval_steps_per_second': 8.709, 'epoch': 0.36}
{'loss': 1.5664, 'grad_norm': 0.0651305690407753, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5721060037612915, 'eval_runtime': 7.2171, 'eval_samples_per_second': 138.421, 'eval_steps_per_second': 8.729, 'epoch': 0.4}
{'loss': 1.6063, 'grad_norm': 0.06945131719112396, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5598291158676147, 'eval_runtime': 7.2173, 'eval_samples_per_second': 138.418, 'eval_steps_per_second': 8.729, 'epoch': 0.44}
{'loss': 1.6131, 'grad_norm': 0.08554922044277191, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.546817421913147, 'eval_runtime': 7.2184, 'eval_samples_per_second': 138.396, 'eval_steps_per_second': 8.728, 'epoch': 0.48}
{'loss': 1.6233, 'grad_norm': 0.07574602961540222, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5300861597061157, 'eval_runtime': 7.2207, 'eval_samples_per_second': 138.353, 'eval_steps_per_second': 8.725, 'epoch': 0.52}
{'loss': 1.6227, 'grad_norm': 0.08534076064825058, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5082910060882568, 'eval_runtime': 7.2252, 'eval_samples_per_second': 138.266, 'eval_steps_per_second': 8.719, 'epoch': 0.56}
{'loss': 1.5474, 'grad_norm': 0.1045386791229248, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4632831811904907, 'eval_runtime': 7.2225, 'eval_samples_per_second': 138.317, 'eval_steps_per_second': 8.723, 'epoch': 0.6}
{'loss': 1.4641, 'grad_norm': 0.07814483344554901, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4504411220550537, 'eval_runtime': 7.229, 'eval_samples_per_second': 138.193, 'eval_steps_per_second': 8.715, 'epoch': 0.64}
{'loss': 1.5276, 'grad_norm': 0.07286535948514938, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4381908178329468, 'eval_runtime': 7.2228, 'eval_samples_per_second': 138.312, 'eval_steps_per_second': 8.722, 'epoch': 0.68}
{'loss': 1.5522, 'grad_norm': 0.07520835846662521, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4242676496505737, 'eval_runtime': 7.2277, 'eval_samples_per_second': 138.217, 'eval_steps_per_second': 8.716, 'epoch': 0.72}
{'loss': 1.4008, 'grad_norm': 0.0788666307926178, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4125211238861084, 'eval_runtime': 7.2254, 'eval_samples_per_second': 138.263, 'eval_steps_per_second': 8.719, 'epoch': 0.76}
{'loss': 1.45, 'grad_norm': 0.07034186273813248, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.396982192993164, 'eval_runtime': 7.2132, 'eval_samples_per_second': 138.497, 'eval_steps_per_second': 8.734, 'epoch': 0.8}
{'loss': 1.4026, 'grad_norm': 0.07213619351387024, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3844352960586548, 'eval_runtime': 7.2208, 'eval_samples_per_second': 138.35, 'eval_steps_per_second': 8.725, 'epoch': 0.84}
{'loss': 1.4382, 'grad_norm': 0.07276149839162827, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3734004497528076, 'eval_runtime': 7.207, 'eval_samples_per_second': 138.615, 'eval_steps_per_second': 8.741, 'epoch': 0.88}
{'loss': 1.4484, 'grad_norm': 0.08661297708749771, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.366129994392395, 'eval_runtime': 7.2059, 'eval_samples_per_second': 138.637, 'eval_steps_per_second': 8.743, 'epoch': 0.92}
{'loss': 1.4122, 'grad_norm': 0.07630649209022522, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3624471426010132, 'eval_runtime': 7.201, 'eval_samples_per_second': 138.731, 'eval_steps_per_second': 8.749, 'epoch': 0.96}
{'loss': 1.3651, 'grad_norm': 0.07433395832777023, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.360313892364502, 'eval_runtime': 7.2083, 'eval_samples_per_second': 138.591, 'eval_steps_per_second': 8.74, 'epoch': 1.0}
{'train_runtime': 387.0515, 'train_samples_per_second': 25.831, 'train_steps_per_second': 1.615, 'train_loss': 1.7316495849609375, 'epoch': 1.0}
train_results:  {'eval_loss': [3.503196954727173, 2.1824417114257812, 1.8525285720825195, 1.727697491645813, 1.6677895784378052, 1.638314127922058, 1.6149441003799438, 1.599880576133728, 1.5845520496368408, 1.5721060037612915, 1.5598291158676147, 1.546817421913147, 1.5300861597061157, 1.5082910060882568, 1.4632831811904907, 1.4504411220550537, 1.4381908178329468, 1.4242676496505737, 1.4125211238861084, 1.396982192993164, 1.3844352960586548, 1.3734004497528076, 1.366129994392395, 1.3624471426010132, 1.360313892364502], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:51,  2.16it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 161.08it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 261.09it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 332.40it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 382.59it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 425.51it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 374.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8481844067573547
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644, 0.8481844067573547]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1715 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.8133164291667744
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.7958741444019782
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.8175604277817368
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.8174599167739514
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.8163731922892388
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.0880, dtype=torch.float64), 0, tensor(0.6475, dtype=torch.float64), tensor(0.1990, dtype=torch.float64), 0, tensor(0.0655, dtype=torch.float64), 32, 0, 1, 1, 0, 0, 128, 0.044193144309328365, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(6.4538e-17, dtype=torch.float64), tensor(5.7548e-17, dtype=torch.float64), tensor(1.5738e-17, dtype=torch.float64), tensor(0.0880, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6475, dtype=torch.float64), tensor(0.1990, dtype=torch.float64), tensor(6.5573e-16, dtype=torch.float64), tensor(0.0655, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4419, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.088
  triviaqa: 0
  truthfulqa_gen: 0.648
  wikitext: 0.199
  mmlu: 0
  arc_challenge: 0.066

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.044193144309328365,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.044193144309328365
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 289.31it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 482.91it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 561.35it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 610.94it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 645.86it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 671.80it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 657.76it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.7529, 'grad_norm': 0.19043739140033722, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.5729711055755615, 'eval_runtime': 6.2578, 'eval_samples_per_second': 159.64, 'eval_steps_per_second': 10.067, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 289.99it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 482.06it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 557.09it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 605.58it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 641.06it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 667.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 653.66it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.543, 'grad_norm': 0.10336842387914658, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.8936636447906494, 'eval_runtime': 6.2471, 'eval_samples_per_second': 159.915, 'eval_steps_per_second': 10.085, 'epoch': 0.08}
{'loss': 1.7059, 'grad_norm': 0.09682410210371017, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5231786966323853, 'eval_runtime': 6.2686, 'eval_samples_per_second': 159.366, 'eval_steps_per_second': 10.05, 'epoch': 0.12}
{'loss': 1.5151, 'grad_norm': 0.05503654479980469, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4060879945755005, 'eval_runtime': 6.2718, 'eval_samples_per_second': 159.283, 'eval_steps_per_second': 10.045, 'epoch': 0.16}
{'loss': 1.4316, 'grad_norm': 0.05810220539569855, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3605700731277466, 'eval_runtime': 6.2832, 'eval_samples_per_second': 158.996, 'eval_steps_per_second': 10.027, 'epoch': 0.2}
{'loss': 1.3865, 'grad_norm': 0.05513734370470047, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3235706090927124, 'eval_runtime': 6.2918, 'eval_samples_per_second': 158.779, 'eval_steps_per_second': 10.013, 'epoch': 0.24}
{'loss': 1.3906, 'grad_norm': 0.06753013283014297, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.288330078125, 'eval_runtime': 6.3042, 'eval_samples_per_second': 158.465, 'eval_steps_per_second': 9.993, 'epoch': 0.28}
{'loss': 1.2756, 'grad_norm': 0.056441206485033035, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2598670721054077, 'eval_runtime': 6.3162, 'eval_samples_per_second': 158.163, 'eval_steps_per_second': 9.974, 'epoch': 0.32}
{'loss': 1.283, 'grad_norm': 0.08534590154886246, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.226852536201477, 'eval_runtime': 6.3073, 'eval_samples_per_second': 158.387, 'eval_steps_per_second': 9.988, 'epoch': 0.36}
{'loss': 1.2971, 'grad_norm': 0.06669743359088898, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1934950351715088, 'eval_runtime': 6.3094, 'eval_samples_per_second': 158.335, 'eval_steps_per_second': 9.985, 'epoch': 0.4}
{'loss': 1.2039, 'grad_norm': 0.08109037578105927, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.160490870475769, 'eval_runtime': 6.3118, 'eval_samples_per_second': 158.275, 'eval_steps_per_second': 9.981, 'epoch': 0.44}
{'loss': 1.2197, 'grad_norm': 0.08090144395828247, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1257948875427246, 'eval_runtime': 6.3388, 'eval_samples_per_second': 157.6, 'eval_steps_per_second': 9.939, 'epoch': 0.48}
{'loss': 1.1175, 'grad_norm': 0.11188654601573944, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0775649547576904, 'eval_runtime': 6.3409, 'eval_samples_per_second': 157.55, 'eval_steps_per_second': 9.936, 'epoch': 0.52}
{'loss': 1.0254, 'grad_norm': 0.09996318817138672, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.028172254562378, 'eval_runtime': 6.3264, 'eval_samples_per_second': 157.909, 'eval_steps_per_second': 9.958, 'epoch': 0.56}
{'loss': 1.1082, 'grad_norm': 0.10014793276786804, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9973347783088684, 'eval_runtime': 6.3219, 'eval_samples_per_second': 158.023, 'eval_steps_per_second': 9.965, 'epoch': 0.6}
{'loss': 1.0674, 'grad_norm': 0.11496409773826599, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9739041328430176, 'eval_runtime': 6.3428, 'eval_samples_per_second': 157.501, 'eval_steps_per_second': 9.933, 'epoch': 0.64}
{'loss': 1.0397, 'grad_norm': 0.0972556322813034, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.953082799911499, 'eval_runtime': 6.3106, 'eval_samples_per_second': 158.304, 'eval_steps_per_second': 9.983, 'epoch': 0.68}
{'loss': 0.9441, 'grad_norm': 0.11176510155200958, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9308722615242004, 'eval_runtime': 6.3037, 'eval_samples_per_second': 158.479, 'eval_steps_per_second': 9.994, 'epoch': 0.72}
{'loss': 1.0259, 'grad_norm': 0.10500889271497726, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9068973064422607, 'eval_runtime': 6.3183, 'eval_samples_per_second': 158.112, 'eval_steps_per_second': 9.971, 'epoch': 0.76}
{'loss': 0.9134, 'grad_norm': 0.13963627815246582, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8849475979804993, 'eval_runtime': 6.3294, 'eval_samples_per_second': 157.836, 'eval_steps_per_second': 9.954, 'epoch': 0.8}
{'loss': 1.0232, 'grad_norm': 0.11736713349819183, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.867426872253418, 'eval_runtime': 6.3165, 'eval_samples_per_second': 158.157, 'eval_steps_per_second': 9.974, 'epoch': 0.84}
{'loss': 0.9322, 'grad_norm': 0.13580013811588287, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8497464656829834, 'eval_runtime': 6.3086, 'eval_samples_per_second': 158.355, 'eval_steps_per_second': 9.986, 'epoch': 0.88}
{'loss': 0.9174, 'grad_norm': 0.1801866739988327, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.835993766784668, 'eval_runtime': 6.3104, 'eval_samples_per_second': 158.31, 'eval_steps_per_second': 9.983, 'epoch': 0.92}
{'loss': 0.9331, 'grad_norm': 0.13615015149116516, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8276329636573792, 'eval_runtime': 6.3211, 'eval_samples_per_second': 158.043, 'eval_steps_per_second': 9.967, 'epoch': 0.96}
{'loss': 0.9327, 'grad_norm': 0.11902036517858505, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8244594931602478, 'eval_runtime': 6.3164, 'eval_samples_per_second': 158.16, 'eval_steps_per_second': 9.974, 'epoch': 1.0}
{'train_runtime': 336.717, 'train_samples_per_second': 29.696, 'train_steps_per_second': 1.856, 'train_loss': 1.3594071685791016, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5729711055755615, 1.8936636447906494, 1.5231786966323853, 1.4060879945755005, 1.3605700731277466, 1.3235706090927124, 1.288330078125, 1.2598670721054077, 1.226852536201477, 1.1934950351715088, 1.160490870475769, 1.1257948875427246, 1.0775649547576904, 1.028172254562378, 0.9973347783088684, 0.9739041328430176, 0.953082799911499, 0.9308722615242004, 0.9068973064422607, 0.8849475979804993, 0.867426872253418, 0.8497464656829834, 0.835993766784668, 0.8276329636573792, 0.8244594931602478], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:01<09:38,  1.16s/it]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:05, 78.85it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 154.17it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 224.25it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 286.58it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:02<00:00, 342.36it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 242.05it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8483726382255554
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644, 0.8481844067573547, 0.8483726382255554]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6160 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.8014425579882021
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.8173670992733657
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.7848042357825273
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.8173987827330768
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.8106129538307978
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0655, dtype=torch.float64), 0, 0, tensor(0.2185, dtype=torch.float64), 0, tensor(0.1800, dtype=torch.float64), tensor(0.4320, dtype=torch.float64), 0, tensor(0.1041, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.006788488747274855, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0655, dtype=torch.float64), tensor(1.4237e-16, dtype=torch.float64), tensor(3.3136e-17, dtype=torch.float64), tensor(0.2185, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1800, dtype=torch.float64), tensor(0.4320, dtype=torch.float64), tensor(8.3267e-16, dtype=torch.float64), tensor(0.1041, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0679, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.065
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.218
  triviaqa: 0
  truthfulqa_gen: 0.18
  wikitext: 0.432
  mmlu: 0
  arc_challenge: 0.104

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.006788488747274855,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.006788488747274855
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 243.27it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 406.60it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 473.45it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 514.67it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 542.96it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 564.18it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 552.92it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.1107, 'grad_norm': 0.189555823802948, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.119939088821411, 'eval_runtime': 9.4131, 'eval_samples_per_second': 106.129, 'eval_steps_per_second': 6.693, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 243.12it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 404.59it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 471.22it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 512.35it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 539.22it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 560.76it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 549.98it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.3509, 'grad_norm': 0.32254931330680847, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8525468111038208, 'eval_runtime': 9.3912, 'eval_samples_per_second': 106.376, 'eval_steps_per_second': 6.708, 'epoch': 0.08}
{'loss': 1.7008, 'grad_norm': 0.15350674092769623, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.624847173690796, 'eval_runtime': 9.4145, 'eval_samples_per_second': 106.113, 'eval_steps_per_second': 6.692, 'epoch': 0.12}
{'loss': 1.6365, 'grad_norm': 0.05635137856006622, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4954378604888916, 'eval_runtime': 9.4024, 'eval_samples_per_second': 106.25, 'eval_steps_per_second': 6.7, 'epoch': 0.16}
{'loss': 1.5004, 'grad_norm': 0.0809355080127716, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4128623008728027, 'eval_runtime': 9.4062, 'eval_samples_per_second': 106.206, 'eval_steps_per_second': 6.698, 'epoch': 0.2}
{'loss': 1.3848, 'grad_norm': 0.06450667977333069, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3918278217315674, 'eval_runtime': 9.4136, 'eval_samples_per_second': 106.123, 'eval_steps_per_second': 6.692, 'epoch': 0.24}
{'loss': 1.4001, 'grad_norm': 0.08079531043767929, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3743358850479126, 'eval_runtime': 9.4081, 'eval_samples_per_second': 106.185, 'eval_steps_per_second': 6.696, 'epoch': 0.28}
{'loss': 1.3903, 'grad_norm': 0.0766671746969223, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3629957437515259, 'eval_runtime': 9.4076, 'eval_samples_per_second': 106.191, 'eval_steps_per_second': 6.697, 'epoch': 0.32}
{'loss': 1.4159, 'grad_norm': 0.06684532016515732, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3509788513183594, 'eval_runtime': 9.4056, 'eval_samples_per_second': 106.213, 'eval_steps_per_second': 6.698, 'epoch': 0.36}
{'loss': 1.2821, 'grad_norm': 0.07667254656553268, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3425298929214478, 'eval_runtime': 9.4092, 'eval_samples_per_second': 106.172, 'eval_steps_per_second': 6.696, 'epoch': 0.4}
{'loss': 1.3836, 'grad_norm': 0.08540672063827515, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3351908922195435, 'eval_runtime': 9.4041, 'eval_samples_per_second': 106.23, 'eval_steps_per_second': 6.699, 'epoch': 0.44}
{'loss': 1.3206, 'grad_norm': 0.08501414209604263, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.327466607093811, 'eval_runtime': 9.3987, 'eval_samples_per_second': 106.291, 'eval_steps_per_second': 6.703, 'epoch': 0.48}
{'loss': 1.2984, 'grad_norm': 0.11050792783498764, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3213884830474854, 'eval_runtime': 9.3937, 'eval_samples_per_second': 106.347, 'eval_steps_per_second': 6.707, 'epoch': 0.52}
{'loss': 1.3419, 'grad_norm': 0.08794557303190231, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3119701147079468, 'eval_runtime': 9.3986, 'eval_samples_per_second': 106.293, 'eval_steps_per_second': 6.703, 'epoch': 0.56}
{'loss': 1.3548, 'grad_norm': 0.0801328644156456, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3070812225341797, 'eval_runtime': 9.3931, 'eval_samples_per_second': 106.355, 'eval_steps_per_second': 6.707, 'epoch': 0.6}
{'loss': 1.3057, 'grad_norm': 0.08963719755411148, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3020507097244263, 'eval_runtime': 9.3955, 'eval_samples_per_second': 106.328, 'eval_steps_per_second': 6.705, 'epoch': 0.64}
{'loss': 1.3579, 'grad_norm': 0.08291018009185791, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2949304580688477, 'eval_runtime': 9.4214, 'eval_samples_per_second': 106.035, 'eval_steps_per_second': 6.687, 'epoch': 0.68}
{'loss': 1.2941, 'grad_norm': 0.12474456429481506, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2906280755996704, 'eval_runtime': 9.4021, 'eval_samples_per_second': 106.252, 'eval_steps_per_second': 6.701, 'epoch': 0.72}
{'loss': 1.3056, 'grad_norm': 0.1071433573961258, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2873508930206299, 'eval_runtime': 9.4038, 'eval_samples_per_second': 106.234, 'eval_steps_per_second': 6.699, 'epoch': 0.76}
{'loss': 1.4088, 'grad_norm': 0.10087858885526657, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2839736938476562, 'eval_runtime': 9.4035, 'eval_samples_per_second': 106.237, 'eval_steps_per_second': 6.7, 'epoch': 0.8}
{'loss': 1.4102, 'grad_norm': 0.08727811276912689, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2802433967590332, 'eval_runtime': 9.4054, 'eval_samples_per_second': 106.215, 'eval_steps_per_second': 6.698, 'epoch': 0.84}
{'loss': 1.3195, 'grad_norm': 0.08057846873998642, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2778828144073486, 'eval_runtime': 9.3979, 'eval_samples_per_second': 106.3, 'eval_steps_per_second': 6.704, 'epoch': 0.88}
{'loss': 1.3201, 'grad_norm': 0.079507015645504, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.27609384059906, 'eval_runtime': 9.4712, 'eval_samples_per_second': 105.478, 'eval_steps_per_second': 6.652, 'epoch': 0.92}
{'loss': 1.3078, 'grad_norm': 0.085054412484169, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2751281261444092, 'eval_runtime': 9.4532, 'eval_samples_per_second': 105.679, 'eval_steps_per_second': 6.664, 'epoch': 0.96}
{'loss': 1.2614, 'grad_norm': 0.08334384113550186, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2747447490692139, 'eval_runtime': 9.4354, 'eval_samples_per_second': 105.878, 'eval_steps_per_second': 6.677, 'epoch': 1.0}
{'train_runtime': 489.6376, 'train_samples_per_second': 20.417, 'train_steps_per_second': 1.276, 'train_loss': 1.5265204223632813, 'epoch': 1.0}
train_results:  {'eval_loss': [3.119939088821411, 1.8525468111038208, 1.624847173690796, 1.4954378604888916, 1.4128623008728027, 1.3918278217315674, 1.3743358850479126, 1.3629957437515259, 1.3509788513183594, 1.3425298929214478, 1.3351908922195435, 1.327466607093811, 1.3213884830474854, 1.3119701147079468, 1.3070812225341797, 1.3020507097244263, 1.2949304580688477, 1.2906280755996704, 1.2873508930206299, 1.2839736938476562, 1.2802433967590332, 1.2778828144073486, 1.27609384059906, 1.2751281261444092, 1.2747447490692139], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:04,  1.64it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:04, 100.50it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 179.64it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 243.29it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 292.75it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 334.76it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 278.15it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8476762771606445
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644, 0.8481844067573547, 0.8483726382255554, 0.8476762771606445]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3797 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.8151152175959924
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.8185628003608136
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.7564434723434142
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.8065029943035135
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.7788607730475656
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3437, dtype=torch.float64), 0, tensor(0.4890, dtype=torch.float64), 0, 0, tensor(0.1673, dtype=torch.float64), 32, 0, 0, 1, 1, 0, 128, 0.00029088842590773437, 1.4800000190734885, 1]
normalized proposed parameters for next round by BO: [tensor(3.4939e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9327e-17, dtype=torch.float64), tensor(0.3437, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4890, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1673, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0029, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.344
  triviaqa: 0
  truthfulqa_gen: 0.489
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.167

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.00029088842590773437,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.00029088842590773437
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.52it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 462.29it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 538.67it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 584.82it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 617.82it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 642.98it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 630.08it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.7365, 'grad_norm': 0.3688379228115082, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2949094772338867, 'eval_runtime': 5.7904, 'eval_samples_per_second': 172.528, 'eval_steps_per_second': 10.88, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.42it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 460.73it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 537.19it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 583.33it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 615.98it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 641.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 628.44it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.0942, 'grad_norm': 0.3120472729206085, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.3142075538635254, 'eval_runtime': 5.5098, 'eval_samples_per_second': 181.313, 'eval_steps_per_second': 11.434, 'epoch': 0.08}
{'loss': 1.1115, 'grad_norm': 0.11445126682519913, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9943093657493591, 'eval_runtime': 5.5321, 'eval_samples_per_second': 180.582, 'eval_steps_per_second': 11.388, 'epoch': 0.12}
{'loss': 0.945, 'grad_norm': 0.05714285373687744, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8552459478378296, 'eval_runtime': 5.5386, 'eval_samples_per_second': 180.37, 'eval_steps_per_second': 11.375, 'epoch': 0.16}
{'loss': 0.8513, 'grad_norm': 0.05563904717564583, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8319838643074036, 'eval_runtime': 5.536, 'eval_samples_per_second': 180.454, 'eval_steps_per_second': 11.38, 'epoch': 0.2}
{'loss': 0.7994, 'grad_norm': 0.051635030657052994, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8186935782432556, 'eval_runtime': 5.5383, 'eval_samples_per_second': 180.379, 'eval_steps_per_second': 11.375, 'epoch': 0.24}
{'loss': 0.7951, 'grad_norm': 0.052141133695840836, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8073413968086243, 'eval_runtime': 5.5421, 'eval_samples_per_second': 180.258, 'eval_steps_per_second': 11.368, 'epoch': 0.28}
{'loss': 0.7822, 'grad_norm': 0.06284655630588531, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7937372326850891, 'eval_runtime': 5.5473, 'eval_samples_per_second': 180.087, 'eval_steps_per_second': 11.357, 'epoch': 0.32}
{'loss': 0.7805, 'grad_norm': 0.05497618392109871, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7852451801300049, 'eval_runtime': 5.5461, 'eval_samples_per_second': 180.126, 'eval_steps_per_second': 11.359, 'epoch': 0.36}
{'loss': 0.7681, 'grad_norm': 0.05854364112019539, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7747927308082581, 'eval_runtime': 5.5564, 'eval_samples_per_second': 179.792, 'eval_steps_per_second': 11.338, 'epoch': 0.4}
{'loss': 0.7909, 'grad_norm': 0.05352237448096275, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7639136910438538, 'eval_runtime': 5.5575, 'eval_samples_per_second': 179.756, 'eval_steps_per_second': 11.336, 'epoch': 0.44}
{'loss': 0.7594, 'grad_norm': 0.0637441873550415, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7544416189193726, 'eval_runtime': 5.5631, 'eval_samples_per_second': 179.576, 'eval_steps_per_second': 11.325, 'epoch': 0.48}
{'loss': 0.7548, 'grad_norm': 0.061850499361753464, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7449767589569092, 'eval_runtime': 5.5516, 'eval_samples_per_second': 179.947, 'eval_steps_per_second': 11.348, 'epoch': 0.52}
{'loss': 0.7633, 'grad_norm': 0.06593376398086548, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7367317080497742, 'eval_runtime': 5.5809, 'eval_samples_per_second': 179.004, 'eval_steps_per_second': 11.289, 'epoch': 0.56}
{'loss': 0.7588, 'grad_norm': 0.05990976840257645, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7271265983581543, 'eval_runtime': 5.6064, 'eval_samples_per_second': 178.188, 'eval_steps_per_second': 11.237, 'epoch': 0.6}
{'loss': 0.7596, 'grad_norm': 0.05774727836251259, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7195662260055542, 'eval_runtime': 5.6005, 'eval_samples_per_second': 178.378, 'eval_steps_per_second': 11.249, 'epoch': 0.64}
{'loss': 0.7126, 'grad_norm': 0.07208012044429779, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7103649377822876, 'eval_runtime': 5.6128, 'eval_samples_per_second': 177.985, 'eval_steps_per_second': 11.224, 'epoch': 0.68}
{'loss': 0.7387, 'grad_norm': 0.07892655581235886, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7037767767906189, 'eval_runtime': 5.5946, 'eval_samples_per_second': 178.565, 'eval_steps_per_second': 11.261, 'epoch': 0.72}
{'loss': 0.7008, 'grad_norm': 0.06658424437046051, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6969898343086243, 'eval_runtime': 5.585, 'eval_samples_per_second': 178.87, 'eval_steps_per_second': 11.28, 'epoch': 0.76}
{'loss': 0.7056, 'grad_norm': 0.08589201420545578, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6896060705184937, 'eval_runtime': 5.5894, 'eval_samples_per_second': 178.732, 'eval_steps_per_second': 11.271, 'epoch': 0.8}
{'loss': 0.7264, 'grad_norm': 0.08020637184381485, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6840043067932129, 'eval_runtime': 5.5915, 'eval_samples_per_second': 178.663, 'eval_steps_per_second': 11.267, 'epoch': 0.84}
{'loss': 0.6987, 'grad_norm': 0.07343275845050812, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6786249279975891, 'eval_runtime': 5.5818, 'eval_samples_per_second': 178.974, 'eval_steps_per_second': 11.287, 'epoch': 0.88}
{'loss': 0.6945, 'grad_norm': 0.08490528911352158, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6741423010826111, 'eval_runtime': 5.5914, 'eval_samples_per_second': 178.667, 'eval_steps_per_second': 11.267, 'epoch': 0.92}
{'loss': 0.703, 'grad_norm': 0.08354933559894562, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6717244386672974, 'eval_runtime': 5.5854, 'eval_samples_per_second': 178.859, 'eval_steps_per_second': 11.279, 'epoch': 0.96}
{'loss': 0.7079, 'grad_norm': 0.07698988169431686, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6711016297340393, 'eval_runtime': 5.6285, 'eval_samples_per_second': 177.489, 'eval_steps_per_second': 11.193, 'epoch': 1.0}
{'train_runtime': 302.3435, 'train_samples_per_second': 33.068, 'train_steps_per_second': 2.067, 'train_loss': 0.9855444519042968, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2949094772338867, 1.3142075538635254, 0.9943093657493591, 0.8552459478378296, 0.8319838643074036, 0.8186935782432556, 0.8073413968086243, 0.7937372326850891, 0.7852451801300049, 0.7747927308082581, 0.7639136910438538, 0.7544416189193726, 0.7449767589569092, 0.7367317080497742, 0.7271265983581543, 0.7195662260055542, 0.7103649377822876, 0.7037767767906189, 0.6969898343086243, 0.6896060705184937, 0.6840043067932129, 0.6786249279975891, 0.6741423010826111, 0.6717244386672974, 0.6711016297340393], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:25,  2.43it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 168.40it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 264.19it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 329.30it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 373.98it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 410.44it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 373.91it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8482329249382019
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644, 0.8481844067573547, 0.8483726382255554, 0.8476762771606445, 0.8482329249382019]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2401 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.8171010491083552
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.7677789228027057
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.7688688049407678
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.8156601812082382
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.8221604475326244
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3639, dtype=torch.float64), 0, tensor(0.6183, dtype=torch.float64), 0, 0, tensor(0.0179, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.052100361592288064, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(1.2698e-15, dtype=torch.float64), tensor(1.6520e-16, dtype=torch.float64), tensor(5.6522e-17, dtype=torch.float64), tensor(0.3639, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6183, dtype=torch.float64), tensor(1.4180e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0179, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.5210, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.364
  triviaqa: 0
  truthfulqa_gen: 0.618
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.018

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.052100361592288064,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.052100361592288064
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 286.29it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 473.54it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 551.20it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 602.17it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 637.07it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 664.88it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 649.53it/s]
Evaluation performance at step 25: 0.76
{'loss': 5.2056, 'grad_norm': 0.2528141736984253, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.84789776802063, 'eval_runtime': 3.7976, 'eval_samples_per_second': 263.063, 'eval_steps_per_second': 16.59, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 288.61it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.50it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 606.39it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 640.01it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 666.48it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.96it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5054, 'grad_norm': 0.08674482256174088, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.726560354232788, 'eval_runtime': 3.7498, 'eval_samples_per_second': 266.415, 'eval_steps_per_second': 16.801, 'epoch': 0.08}
{'loss': 1.5184, 'grad_norm': 0.1657022088766098, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.403993010520935, 'eval_runtime': 3.7438, 'eval_samples_per_second': 266.838, 'eval_steps_per_second': 16.828, 'epoch': 0.12}
{'loss': 1.3001, 'grad_norm': 0.0897318571805954, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2441983222961426, 'eval_runtime': 3.7513, 'eval_samples_per_second': 266.306, 'eval_steps_per_second': 16.794, 'epoch': 0.16}
{'loss': 1.2213, 'grad_norm': 0.05247336998581886, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1851959228515625, 'eval_runtime': 3.7662, 'eval_samples_per_second': 265.257, 'eval_steps_per_second': 16.728, 'epoch': 0.2}
{'loss': 1.1924, 'grad_norm': 0.047391701489686966, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1515350341796875, 'eval_runtime': 3.7527, 'eval_samples_per_second': 266.21, 'eval_steps_per_second': 16.788, 'epoch': 0.24}
{'loss': 1.1188, 'grad_norm': 0.05176611617207527, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.121382236480713, 'eval_runtime': 3.7625, 'eval_samples_per_second': 265.518, 'eval_steps_per_second': 16.744, 'epoch': 0.28}
{'loss': 1.0832, 'grad_norm': 0.05210569128394127, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.093140959739685, 'eval_runtime': 3.7507, 'eval_samples_per_second': 266.351, 'eval_steps_per_second': 16.797, 'epoch': 0.32}
{'loss': 1.0711, 'grad_norm': 0.05938005447387695, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0693200826644897, 'eval_runtime': 3.7561, 'eval_samples_per_second': 265.97, 'eval_steps_per_second': 16.773, 'epoch': 0.36}
{'loss': 1.0402, 'grad_norm': 0.06621002405881882, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0461126565933228, 'eval_runtime': 3.7554, 'eval_samples_per_second': 266.019, 'eval_steps_per_second': 16.776, 'epoch': 0.4}
{'loss': 1.0189, 'grad_norm': 0.06547562777996063, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0128587484359741, 'eval_runtime': 3.7553, 'eval_samples_per_second': 266.024, 'eval_steps_per_second': 16.776, 'epoch': 0.44}
{'loss': 0.9895, 'grad_norm': 0.07004714012145996, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9806345701217651, 'eval_runtime': 3.756, 'eval_samples_per_second': 265.974, 'eval_steps_per_second': 16.773, 'epoch': 0.48}
{'loss': 0.9666, 'grad_norm': 0.08785673975944519, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.936594545841217, 'eval_runtime': 3.7699, 'eval_samples_per_second': 264.996, 'eval_steps_per_second': 16.711, 'epoch': 0.52}
{'loss': 0.9165, 'grad_norm': 0.11418843269348145, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8716992139816284, 'eval_runtime': 3.7763, 'eval_samples_per_second': 264.545, 'eval_steps_per_second': 16.683, 'epoch': 0.56}
{'loss': 0.8514, 'grad_norm': 0.07738161832094193, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8118830919265747, 'eval_runtime': 3.7747, 'eval_samples_per_second': 264.657, 'eval_steps_per_second': 16.69, 'epoch': 0.6}
{'loss': 0.8079, 'grad_norm': 0.07719431817531586, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7880002856254578, 'eval_runtime': 3.7626, 'eval_samples_per_second': 265.509, 'eval_steps_per_second': 16.744, 'epoch': 0.64}
{'loss': 0.7782, 'grad_norm': 0.08797383308410645, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7613046169281006, 'eval_runtime': 3.7659, 'eval_samples_per_second': 265.275, 'eval_steps_per_second': 16.729, 'epoch': 0.68}
{'loss': 0.7773, 'grad_norm': 0.10752078145742416, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7403976321220398, 'eval_runtime': 3.7743, 'eval_samples_per_second': 264.683, 'eval_steps_per_second': 16.692, 'epoch': 0.72}
{'loss': 0.7257, 'grad_norm': 0.093901127576828, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7159184217453003, 'eval_runtime': 3.7676, 'eval_samples_per_second': 265.159, 'eval_steps_per_second': 16.722, 'epoch': 0.76}
{'loss': 0.7175, 'grad_norm': 0.1003866195678711, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.693092405796051, 'eval_runtime': 3.7718, 'eval_samples_per_second': 264.859, 'eval_steps_per_second': 16.703, 'epoch': 0.8}
{'loss': 0.6999, 'grad_norm': 0.12226961553096771, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6709986329078674, 'eval_runtime': 3.7659, 'eval_samples_per_second': 265.278, 'eval_steps_per_second': 16.729, 'epoch': 0.84}
{'loss': 0.6885, 'grad_norm': 0.10883572697639465, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6535733938217163, 'eval_runtime': 3.7616, 'eval_samples_per_second': 265.581, 'eval_steps_per_second': 16.748, 'epoch': 0.88}
{'loss': 0.6723, 'grad_norm': 0.10830594599246979, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6406435370445251, 'eval_runtime': 3.7617, 'eval_samples_per_second': 265.571, 'eval_steps_per_second': 16.748, 'epoch': 0.92}
{'loss': 0.6564, 'grad_norm': 0.11469992250204086, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6323577165603638, 'eval_runtime': 3.7579, 'eval_samples_per_second': 265.841, 'eval_steps_per_second': 16.765, 'epoch': 0.96}
{'loss': 0.6622, 'grad_norm': 0.11445288360118866, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6308866739273071, 'eval_runtime': 3.7522, 'eval_samples_per_second': 266.245, 'eval_steps_per_second': 16.79, 'epoch': 1.0}
{'train_runtime': 220.5733, 'train_samples_per_second': 45.327, 'train_steps_per_second': 2.834, 'train_loss': 1.1674083587646484, 'epoch': 1.0}
train_results:  {'eval_loss': [3.84789776802063, 1.726560354232788, 1.403993010520935, 1.2441983222961426, 1.1851959228515625, 1.1515350341796875, 1.121382236480713, 1.093140959739685, 1.0693200826644897, 1.0461126565933228, 1.0128587484359741, 0.9806345701217651, 0.936594545841217, 0.8716992139816284, 0.8118830919265747, 0.7880002856254578, 0.7613046169281006, 0.7403976321220398, 0.7159184217453003, 0.693092405796051, 0.6709986329078674, 0.6535733938217163, 0.6406435370445251, 0.6323577165603638, 0.6308866739273071], 'performance': [0.76, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:29,  2.38it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 171.06it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 271.29it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 340.58it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 389.44it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 428.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 385.60it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8485330939292908
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644, 0.8481844067573547, 0.8483726382255554, 0.8476762771606445, 0.8482329249382019, 0.8485330939292908]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5789 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.8173355518586097
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.8110852599146615
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.7823091519678073
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.8166852213574208
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.8171209450190591
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2160, dtype=torch.float64), 0, tensor(0.1821, dtype=torch.float64), 0, 0, tensor(0.6019, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 0, 0, 52, 0.028166568155987, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2160, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1821, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.4244e-18, dtype=torch.float64), tensor(0.6019, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4089, dtype=torch.float64), tensor(0.2817, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.216
  rowan_hellaswag: 0
  sciq: 0.182
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.602
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (52,)
  lora_dropout: (0.028166568155987,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  52
lora dropout:  0.028166568155987
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 44,302,336 || all params: 8,074,563,584 || trainable%: 0.5487
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 284.06it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 472.87it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 550.11it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 592.34it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 626.42it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 653.85it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 640.93it/s]
Evaluation performance at step 25: 0.74
{'loss': 2.6677, 'grad_norm': 0.7385491728782654, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 1.9931682348251343, 'eval_runtime': 9.0115, 'eval_samples_per_second': 110.858, 'eval_steps_per_second': 6.991, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 284.19it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 473.45it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 550.73it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 598.78it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 632.11it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 657.94it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 644.69it/s]
Evaluation performance at step 50: 0.73
{'loss': 1.7906, 'grad_norm': 0.4892106056213379, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.5968722105026245, 'eval_runtime': 8.978, 'eval_samples_per_second': 111.272, 'eval_steps_per_second': 7.017, 'epoch': 0.08}
{'loss': 1.5194, 'grad_norm': 0.44626981019973755, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5439878702163696, 'eval_runtime': 8.9906, 'eval_samples_per_second': 111.116, 'eval_steps_per_second': 7.007, 'epoch': 0.12}
{'loss': 1.6675, 'grad_norm': 0.42196476459503174, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4915262460708618, 'eval_runtime': 9.0245, 'eval_samples_per_second': 110.698, 'eval_steps_per_second': 6.981, 'epoch': 0.16}
{'loss': 1.4866, 'grad_norm': 0.3841341435909271, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.434022307395935, 'eval_runtime': 9.0432, 'eval_samples_per_second': 110.47, 'eval_steps_per_second': 6.967, 'epoch': 0.2}
{'loss': 1.4503, 'grad_norm': 0.3737657368183136, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.389809489250183, 'eval_runtime': 9.0835, 'eval_samples_per_second': 109.98, 'eval_steps_per_second': 6.936, 'epoch': 0.24}
{'loss': 1.377, 'grad_norm': 0.32962751388549805, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3617157936096191, 'eval_runtime': 9.1102, 'eval_samples_per_second': 109.658, 'eval_steps_per_second': 6.915, 'epoch': 0.28}
{'loss': 1.3661, 'grad_norm': 0.3480244278907776, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3450896739959717, 'eval_runtime': 9.0786, 'eval_samples_per_second': 110.039, 'eval_steps_per_second': 6.939, 'epoch': 0.32}
{'loss': 1.4188, 'grad_norm': 0.39946267008781433, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3326464891433716, 'eval_runtime': 9.0907, 'eval_samples_per_second': 109.892, 'eval_steps_per_second': 6.93, 'epoch': 0.36}
{'loss': 1.349, 'grad_norm': 0.3312578797340393, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.32444167137146, 'eval_runtime': 9.0865, 'eval_samples_per_second': 109.943, 'eval_steps_per_second': 6.933, 'epoch': 0.4}
{'loss': 1.4251, 'grad_norm': 0.6095607280731201, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3158448934555054, 'eval_runtime': 9.0733, 'eval_samples_per_second': 110.103, 'eval_steps_per_second': 6.943, 'epoch': 0.44}
{'loss': 1.4309, 'grad_norm': 0.44342267513275146, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3083674907684326, 'eval_runtime': 9.0606, 'eval_samples_per_second': 110.258, 'eval_steps_per_second': 6.953, 'epoch': 0.48}
{'loss': 1.4129, 'grad_norm': 0.39300408959388733, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2988430261611938, 'eval_runtime': 9.0311, 'eval_samples_per_second': 110.618, 'eval_steps_per_second': 6.976, 'epoch': 0.52}
{'loss': 1.4401, 'grad_norm': 0.3455347716808319, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.286581039428711, 'eval_runtime': 9.0185, 'eval_samples_per_second': 110.772, 'eval_steps_per_second': 6.986, 'epoch': 0.56}
{'loss': 1.4107, 'grad_norm': 0.3952547013759613, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.27243173122406, 'eval_runtime': 9.0117, 'eval_samples_per_second': 110.855, 'eval_steps_per_second': 6.991, 'epoch': 0.6}
{'loss': 1.418, 'grad_norm': 0.39463165402412415, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2617703676223755, 'eval_runtime': 9.0184, 'eval_samples_per_second': 110.774, 'eval_steps_per_second': 6.986, 'epoch': 0.64}
{'loss': 1.413, 'grad_norm': 0.4350030720233917, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2584474086761475, 'eval_runtime': 9.0134, 'eval_samples_per_second': 110.835, 'eval_steps_per_second': 6.99, 'epoch': 0.68}
{'loss': 1.3548, 'grad_norm': 0.4501529335975647, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2498321533203125, 'eval_runtime': 9.0153, 'eval_samples_per_second': 110.811, 'eval_steps_per_second': 6.988, 'epoch': 0.72}
{'loss': 1.3653, 'grad_norm': 0.3407188355922699, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2409000396728516, 'eval_runtime': 9.0185, 'eval_samples_per_second': 110.772, 'eval_steps_per_second': 6.986, 'epoch': 0.76}
{'loss': 1.3873, 'grad_norm': 0.3857255280017853, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2365177869796753, 'eval_runtime': 9.026, 'eval_samples_per_second': 110.68, 'eval_steps_per_second': 6.98, 'epoch': 0.8}
{'loss': 1.3888, 'grad_norm': 0.5360511541366577, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.23209810256958, 'eval_runtime': 9.0205, 'eval_samples_per_second': 110.748, 'eval_steps_per_second': 6.984, 'epoch': 0.84}
{'loss': 1.4086, 'grad_norm': 0.5380847454071045, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2277333736419678, 'eval_runtime': 9.0296, 'eval_samples_per_second': 110.637, 'eval_steps_per_second': 6.977, 'epoch': 0.88}
{'loss': 1.3714, 'grad_norm': 0.3262811601161957, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2233449220657349, 'eval_runtime': 9.0294, 'eval_samples_per_second': 110.638, 'eval_steps_per_second': 6.977, 'epoch': 0.92}
{'loss': 1.3637, 'grad_norm': 0.32162636518478394, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2206096649169922, 'eval_runtime': 9.0301, 'eval_samples_per_second': 110.63, 'eval_steps_per_second': 6.977, 'epoch': 0.96}
{'loss': 1.3521, 'grad_norm': 0.42957016825675964, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.220224380493164, 'eval_runtime': 9.0263, 'eval_samples_per_second': 110.676, 'eval_steps_per_second': 6.98, 'epoch': 1.0}
{'train_runtime': 465.6601, 'train_samples_per_second': 21.471, 'train_steps_per_second': 1.342, 'train_loss': 1.4814203063964844, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9931682348251343, 1.5968722105026245, 1.5439878702163696, 1.4915262460708618, 1.434022307395935, 1.389809489250183, 1.3617157936096191, 1.3450896739959717, 1.3326464891433716, 1.32444167137146, 1.3158448934555054, 1.3083674907684326, 1.2988430261611938, 1.286581039428711, 1.27243173122406, 1.2617703676223755, 1.2584474086761475, 1.2498321533203125, 1.2409000396728516, 1.2365177869796753, 1.23209810256958, 1.2277333736419678, 1.2233449220657349, 1.2206096649169922, 1.220224380493164], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:42,  4.88it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 250.58it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 345.35it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 396.16it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 428.66it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 455.33it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 459.49it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.7955310940742493
current iteration best possible performance (full train run):  0.8295000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644, 0.8481844067573547, 0.8483726382255554, 0.8476762771606445, 0.8482329249382019, 0.8485330939292908, 0.7955310940742493]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9416 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.776160716821466
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.8102394029323786
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.8079237160539364
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.7813705294318797
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.7871830179373775
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3589, dtype=torch.float64), tensor(0.2007, dtype=torch.float64), 0, tensor(0.2675, dtype=torch.float64), tensor(0.0309, dtype=torch.float64), tensor(0.0585, dtype=torch.float64), tensor(0.0345, dtype=torch.float64), tensor(0.0477, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 2, 0.0032466031964377786, 38.44121406207496, 1]
normalized proposed parameters for next round by BO: [tensor(1.3890e-17, dtype=torch.float64), tensor(0.3589, dtype=torch.float64), tensor(0.2007, dtype=torch.float64), tensor(0.0014, dtype=torch.float64), tensor(0.2675, dtype=torch.float64), tensor(0.0309, dtype=torch.float64), tensor(0.0585, dtype=torch.float64), tensor(0.0345, dtype=torch.float64), tensor(0.0477, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0325, dtype=torch.float64), tensor(0.8009, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.359
  rowan_hellaswag: 0.201
  sciq: 0
  triviaqa: 0.267
  truthfulqa_gen: 0.031
  wikitext: 0.058
  mmlu: 0.034
  arc_challenge: 0.048

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0032466031964377786,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (38.44121406207496,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.0032466031964377786
lora alpha:  38.44121406207496
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,703,936 || all params: 8,031,965,184 || trainable%: 0.0212
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9982
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  998
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 284.61it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 472.29it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 550.68it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 598.77it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 632.67it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 656.97it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 644.03it/s]
Evaluation performance at step 25: 0.75
{'loss': 2.7031, 'grad_norm': 3.247201919555664, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 1.901867389678955, 'eval_runtime': 9.58, 'eval_samples_per_second': 104.176, 'eval_steps_per_second': 6.576, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 283.99it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 473.48it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 550.94it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 595.69it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 629.19it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 654.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 642.47it/s]
Evaluation performance at step 50: 0.76
{'loss': 1.5511, 'grad_norm': 1.683612585067749, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 1.4536787271499634, 'eval_runtime': 9.5699, 'eval_samples_per_second': 104.285, 'eval_steps_per_second': 6.583, 'epoch': 0.08}
{'loss': 1.3993, 'grad_norm': 1.1243562698364258, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.3242141008377075, 'eval_runtime': 9.637, 'eval_samples_per_second': 103.56, 'eval_steps_per_second': 6.537, 'epoch': 0.12}
{'loss': 1.273, 'grad_norm': 0.9093985557556152, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.2784874439239502, 'eval_runtime': 9.6386, 'eval_samples_per_second': 103.542, 'eval_steps_per_second': 6.536, 'epoch': 0.16}
{'loss': 1.2429, 'grad_norm': 1.1218090057373047, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.2287317514419556, 'eval_runtime': 9.6512, 'eval_samples_per_second': 103.407, 'eval_steps_per_second': 6.528, 'epoch': 0.2}
{'loss': 1.2591, 'grad_norm': 1.0230814218521118, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.2115615606307983, 'eval_runtime': 9.649, 'eval_samples_per_second': 103.431, 'eval_steps_per_second': 6.529, 'epoch': 0.24}
{'loss': 1.2256, 'grad_norm': 0.9728517532348633, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.204674482345581, 'eval_runtime': 9.7091, 'eval_samples_per_second': 102.79, 'eval_steps_per_second': 6.489, 'epoch': 0.28}
{'loss': 1.1967, 'grad_norm': 1.0629605054855347, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.1939600706100464, 'eval_runtime': 9.6886, 'eval_samples_per_second': 103.008, 'eval_steps_per_second': 6.502, 'epoch': 0.32}
{'loss': 1.192, 'grad_norm': 0.8803400993347168, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.18659508228302, 'eval_runtime': 9.6838, 'eval_samples_per_second': 103.059, 'eval_steps_per_second': 6.506, 'epoch': 0.36}
{'loss': 1.1867, 'grad_norm': 0.8686275482177734, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.1804289817810059, 'eval_runtime': 9.6822, 'eval_samples_per_second': 103.076, 'eval_steps_per_second': 6.507, 'epoch': 0.4}
{'loss': 1.2583, 'grad_norm': 1.0123697519302368, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.174115777015686, 'eval_runtime': 9.6684, 'eval_samples_per_second': 103.223, 'eval_steps_per_second': 6.516, 'epoch': 0.44}
{'loss': 1.2019, 'grad_norm': 0.9390669465065002, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.1667698621749878, 'eval_runtime': 9.6719, 'eval_samples_per_second': 103.186, 'eval_steps_per_second': 6.514, 'epoch': 0.48}
{'loss': 1.2038, 'grad_norm': 0.9361051917076111, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.1592304706573486, 'eval_runtime': 9.6654, 'eval_samples_per_second': 103.255, 'eval_steps_per_second': 6.518, 'epoch': 0.52}
{'loss': 1.159, 'grad_norm': 0.9033564925193787, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.152898907661438, 'eval_runtime': 9.6539, 'eval_samples_per_second': 103.378, 'eval_steps_per_second': 6.526, 'epoch': 0.56}
{'loss': 1.1287, 'grad_norm': 0.8829830288887024, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 1.1520800590515137, 'eval_runtime': 9.7028, 'eval_samples_per_second': 102.857, 'eval_steps_per_second': 6.493, 'epoch': 0.6}
{'loss': 1.1447, 'grad_norm': 1.027870535850525, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 1.1471391916275024, 'eval_runtime': 9.7303, 'eval_samples_per_second': 102.566, 'eval_steps_per_second': 6.475, 'epoch': 0.64}
{'loss': 1.2226, 'grad_norm': 0.728061318397522, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 1.1425470113754272, 'eval_runtime': 9.6546, 'eval_samples_per_second': 103.37, 'eval_steps_per_second': 6.525, 'epoch': 0.68}
{'loss': 1.1895, 'grad_norm': 0.945435643196106, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 1.1361546516418457, 'eval_runtime': 9.6732, 'eval_samples_per_second': 103.171, 'eval_steps_per_second': 6.513, 'epoch': 0.72}
{'loss': 1.2405, 'grad_norm': 2.032731771469116, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 1.1329426765441895, 'eval_runtime': 9.6743, 'eval_samples_per_second': 103.16, 'eval_steps_per_second': 6.512, 'epoch': 0.76}
{'loss': 1.1977, 'grad_norm': 0.9432256817817688, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 1.1283223628997803, 'eval_runtime': 9.6712, 'eval_samples_per_second': 103.193, 'eval_steps_per_second': 6.514, 'epoch': 0.8}
{'loss': 1.18, 'grad_norm': 0.949260950088501, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 1.1260782480239868, 'eval_runtime': 9.6757, 'eval_samples_per_second': 103.145, 'eval_steps_per_second': 6.511, 'epoch': 0.84}
{'loss': 1.135, 'grad_norm': 0.9113110303878784, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 1.1234896183013916, 'eval_runtime': 9.6651, 'eval_samples_per_second': 103.258, 'eval_steps_per_second': 6.518, 'epoch': 0.88}
{'loss': 1.2088, 'grad_norm': 0.9223832488059998, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 1.1211321353912354, 'eval_runtime': 9.6729, 'eval_samples_per_second': 103.175, 'eval_steps_per_second': 6.513, 'epoch': 0.92}
{'loss': 1.1616, 'grad_norm': 1.2006328105926514, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 1.1199971437454224, 'eval_runtime': 9.6741, 'eval_samples_per_second': 103.162, 'eval_steps_per_second': 6.512, 'epoch': 0.96}
{'train_runtime': 478.3328, 'train_samples_per_second': 20.868, 'train_steps_per_second': 1.305, 'train_loss': 1.2814228809796846, 'epoch': 1.0}
train_results:  {'eval_loss': [1.901867389678955, 1.4536787271499634, 1.3242141008377075, 1.2784874439239502, 1.2287317514419556, 1.2115615606307983, 1.204674482345581, 1.1939600706100464, 1.18659508228302, 1.1804289817810059, 1.174115777015686, 1.1667698621749878, 1.1592304706573486, 1.152898907661438, 1.1520800590515137, 1.1471391916275024, 1.1425470113754272, 1.1361546516418457, 1.1329426765441895, 1.1283223628997803, 1.1260782480239868, 1.1234896183013916, 1.1211321353912354, 1.1199971437454224], 'performance': [0.75, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:34,  1.49it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 119.65it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 211.15it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 283.91it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 340.69it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 387.60it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 315.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.8285784721374512
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.8265793919563293, 0.8341447114944458, 0.8439578413963318, 0.8477370142936707, 0.8383235931396484, 0.8264036178588867, 0.8436137437820435, 0.8453781604766846, 0.8287791013717651, 0.8480426669120789, 0.8485226631164551, 0.8481138348579407, 0.8481206297874451, 0.8470820188522339, 0.8467063903808594, 0.8479698896408081, 0.8470630049705505, 0.8477179408073425, 0.8481637239456177, 0.8481503129005432, 0.843762218952179, 0.8476830720901489, 0.8470314741134644, 0.8481844067573547, 0.8483726382255554, 0.8476762771606445, 0.8482329249382019, 0.8485330939292908, 0.7955310940742493, 0.8285784721374512]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1702 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.8206799019695367
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.8056140692771316
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.8206923958549818
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.8185484346700728
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.76816604925368
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0307, dtype=torch.float64), tensor(0.0237, dtype=torch.float64), tensor(0.3039, dtype=torch.float64), 0, tensor(0.2025, dtype=torch.float64), tensor(0.1740, dtype=torch.float64), 0, tensor(0.2652, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.053789032087391976, 1.480000019073489, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0307, dtype=torch.float64), tensor(0.0237, dtype=torch.float64), tensor(0.3039, dtype=torch.float64), tensor(3.6038e-17, dtype=torch.float64), tensor(0.2025, dtype=torch.float64), tensor(0.1740, dtype=torch.float64), tensor(3.6776e-16, dtype=torch.float64), tensor(0.2652, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5379, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.777, 0.8295000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/commonsense_qa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 17, 0, 1, 0, 1, 1, 74, 0.06944379333880846, 10, 0]
Checking history sample input_X_between_0_1:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 0.53125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.578125, 0.6944379333880846, 0.20833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 18, 0, 0, 1, 0, 0, 88, 0.07338847024538249, 48, 1]
Checking history sample input_X_between_0_1:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 0.5625, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6875, 0.7338847024538249, 1.0, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 27, 1, 1, 0, 1, 0, 35, 0.056205119903528195, 6, 0]
Checking history sample input_X_between_0_1:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 0.84375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2734375, 0.5620511990352819, 0.125, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 22, 0, 0, 0, 0, 1, 14, 0.05785794551060587, 9, 1]
Checking history sample input_X_between_0_1:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 0.6875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.109375, 0.5785794551060587, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 15, 1, 0, 0, 1, 1, 33, 0.046509797776235, 40, 0]
Checking history sample input_X_between_0_1:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 0.46875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2578125, 0.46509797776234996, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 11, 0, 0, 1, 1, 1, 70, 0.005359902393564798, 44, 1]
Checking history sample input_X_between_0_1:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 0.34375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.546875, 0.05359902393564797, 0.9166666666666666, 1.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 3, 1, 1, 0, 0, 1, 89, 0.004230716614147934, 21, 1]
Checking history sample input_X_between_0_1:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 0.09375, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6953125, 0.042307166141479335, 0.4375, 1.0]
Checking history sample performance at 625 steps:  0.79
Checking history sample input_X:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 11, 0, 0, 1, 0, 1, 128, 0.02126981773222546, 43, 0]
Checking history sample input_X_between_0_1:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 0.34375, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2126981773222546, 0.8958333333333334, 0.0]
Checking history sample performance at 625 steps:  0.77
Checking history sample input_X:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 4, 0, 1, 0, 0, 1, 51, 0.048394033913014715, 30, 0]
Checking history sample input_X_between_0_1:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3984375, 0.4839403391301471, 0.625, 0.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 25, 1, 0, 1, 0, 0, 103, 0.021006146654874183, 41, 0]
Checking history sample input_X_between_0_1:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 0.78125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8046875, 0.2100614665487418, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.71
Checking history sample input_X:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 30, 0, 0, 0, 1, 0, 30, 0.06473585772462145, 22, 0]
Checking history sample input_X_between_0_1:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.234375, 0.6473585772462145, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 10, 0, 0, 1, 0, 1, 33, 0.053839147652140054, 30, 1]
Checking history sample input_X_between_0_1:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 0.3125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2578125, 0.5383914765214005, 0.625, 1.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 27, 0, 1, 1, 1, 0, 91, 0.026257080995186557, 27, 0]
Checking history sample input_X_between_0_1:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 0.84375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7109375, 0.26257080995186555, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 28, 1, 1, 1, 0, 0, 23, 0.048743516472132736, 35, 1]
Checking history sample input_X_between_0_1:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 0.875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1796875, 0.48743516472132736, 0.7291666666666666, 1.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 14, 1, 0, 1, 0, 0, 32, 0.04806841934255027, 48, 0]
Checking history sample input_X_between_0_1:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 0.4375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.48068419342550267, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 19, 0, 0, 1, 1, 0, 22, 0.09470699670511286, 20, 1]
Checking history sample input_X_between_0_1:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 0.59375, 0.0, 0.0, 1.0, 1.0, 0.0, 0.171875, 0.9470699670511286, 0.4166666666666667, 1.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 2, 0, 1, 1, 1, 0, 72, 0.07202092774167915, 33, 1]
Checking history sample input_X_between_0_1:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5625, 0.7202092774167914, 0.6875, 1.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 15, 1, 0, 1, 0, 1, 69, 0.09577839904714924, 28, 0]
Checking history sample input_X_between_0_1:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 0.46875, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5390625, 0.9577839904714924, 0.5833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 30, 1, 1, 1, 0, 1, 19, 0.07443761643578949, 27, 0]
Checking history sample input_X_between_0_1:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 0.9375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1484375, 0.7443761643578948, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 12, 1, 0, 0, 1, 1, 19, 0.0774188690692264, 1, 1]
Checking history sample input_X_between_0_1:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1484375, 0.774188690692264, 0.020833333333333332, 1.0]
Checking history sample performance at 625 steps:  0.75
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9067 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.814367276997614
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.814367276997614
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.814367276997614
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.814367276997614
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.814367276997614
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.0823, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), 0, tensor(0.4440, dtype=torch.float64), 0, tensor(0.3798, dtype=torch.float64), 0, tensor(0.0276, dtype=torch.float64), 0, 9, 1, 1, 0, 0, 1, 116, 0.0206892298889877, 9.7280745592472, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.0823, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), tensor(2.0330e-19, dtype=torch.float64), tensor(0.4440, dtype=torch.float64), tensor(1.6197e-19, dtype=torch.float64), tensor(0.3798, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0276, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2816, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9026, dtype=torch.float64), tensor(0.2069, dtype=torch.float64), tensor(0.2027, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.082
  gsm8k: 0.066
  rowan_hellaswag: 0
  sciq: 0.444
  triviaqa: 0
  truthfulqa_gen: 0.38
  wikitext: 0
  mmlu: 0.028
  arc_challenge: 0

LoRA Parameters:
  lora_r: (116,)
  lora_dropout: (0.0206892298889877,)
  num_layers_to_apply: (9,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (9.7280745592472,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  9
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  116
lora dropout:  0.0206892298889877
lora alpha:  9.7280745592472
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 33,140,736 || all params: 8,063,401,984 || trainable%: 0.4110
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 304.67it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 507.86it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 592.29it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 643.17it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 679.03it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 705.77it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 691.75it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.6481, 'grad_norm': 0.33892059326171875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.3590199947357178, 'eval_runtime': 6.1317, 'eval_samples_per_second': 162.923, 'eval_steps_per_second': 10.274, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 305.85it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 508.25it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 591.60it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 644.23it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 679.88it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 706.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 692.75it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.3548, 'grad_norm': 0.22219279408454895, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.7121394872665405, 'eval_runtime': 6.13, 'eval_samples_per_second': 162.97, 'eval_steps_per_second': 10.277, 'epoch': 0.08}
{'loss': 1.4712, 'grad_norm': 0.09396325051784515, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3730683326721191, 'eval_runtime': 6.1764, 'eval_samples_per_second': 161.744, 'eval_steps_per_second': 10.2, 'epoch': 0.12}
{'loss': 1.3002, 'grad_norm': 0.09235754609107971, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2778364419937134, 'eval_runtime': 6.187, 'eval_samples_per_second': 161.469, 'eval_steps_per_second': 10.183, 'epoch': 0.16}
{'loss': 1.2405, 'grad_norm': 0.13471642136573792, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1945925951004028, 'eval_runtime': 6.205, 'eval_samples_per_second': 160.999, 'eval_steps_per_second': 10.153, 'epoch': 0.2}
{'loss': 1.1381, 'grad_norm': 0.1363275796175003, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0559486150741577, 'eval_runtime': 6.2153, 'eval_samples_per_second': 160.733, 'eval_steps_per_second': 10.136, 'epoch': 0.24}
{'loss': 1.0188, 'grad_norm': 0.1125931516289711, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9783317446708679, 'eval_runtime': 6.2189, 'eval_samples_per_second': 160.64, 'eval_steps_per_second': 10.13, 'epoch': 0.28}
{'loss': 0.9686, 'grad_norm': 0.12232112884521484, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9289408326148987, 'eval_runtime': 6.2267, 'eval_samples_per_second': 160.437, 'eval_steps_per_second': 10.118, 'epoch': 0.32}
{'loss': 0.948, 'grad_norm': 0.13559490442276, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9159112572669983, 'eval_runtime': 6.2174, 'eval_samples_per_second': 160.677, 'eval_steps_per_second': 10.133, 'epoch': 0.36}
{'loss': 0.9175, 'grad_norm': 0.1466410905122757, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9051685929298401, 'eval_runtime': 6.2006, 'eval_samples_per_second': 161.114, 'eval_steps_per_second': 10.16, 'epoch': 0.4}
{'loss': 0.9437, 'grad_norm': 0.13132497668266296, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8985099792480469, 'eval_runtime': 6.2012, 'eval_samples_per_second': 161.097, 'eval_steps_per_second': 10.159, 'epoch': 0.44}
{'loss': 0.9221, 'grad_norm': 0.10755804181098938, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8898362517356873, 'eval_runtime': 6.1879, 'eval_samples_per_second': 161.443, 'eval_steps_per_second': 10.181, 'epoch': 0.48}
{'loss': 0.8985, 'grad_norm': 0.1116122454404831, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.883732259273529, 'eval_runtime': 6.1881, 'eval_samples_per_second': 161.439, 'eval_steps_per_second': 10.181, 'epoch': 0.52}
{'loss': 0.9002, 'grad_norm': 0.12516072392463684, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8751035928726196, 'eval_runtime': 6.1904, 'eval_samples_per_second': 161.38, 'eval_steps_per_second': 10.177, 'epoch': 0.56}
{'loss': 0.8854, 'grad_norm': 0.12770409882068634, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8693928122520447, 'eval_runtime': 6.1829, 'eval_samples_per_second': 161.575, 'eval_steps_per_second': 10.189, 'epoch': 0.6}
{'loss': 0.9202, 'grad_norm': 0.14782603085041046, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8621892333030701, 'eval_runtime': 6.193, 'eval_samples_per_second': 161.312, 'eval_steps_per_second': 10.173, 'epoch': 0.64}
{'loss': 0.8748, 'grad_norm': 0.11889833956956863, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8562817573547363, 'eval_runtime': 6.1898, 'eval_samples_per_second': 161.394, 'eval_steps_per_second': 10.178, 'epoch': 0.68}
{'loss': 0.8693, 'grad_norm': 0.15992304682731628, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8494096398353577, 'eval_runtime': 6.1979, 'eval_samples_per_second': 161.184, 'eval_steps_per_second': 10.165, 'epoch': 0.72}
{'loss': 0.8818, 'grad_norm': 0.1172400414943695, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8441206216812134, 'eval_runtime': 6.1843, 'eval_samples_per_second': 161.539, 'eval_steps_per_second': 10.187, 'epoch': 0.76}
{'loss': 0.8597, 'grad_norm': 0.12379532307386398, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8398963809013367, 'eval_runtime': 6.1834, 'eval_samples_per_second': 161.562, 'eval_steps_per_second': 10.189, 'epoch': 0.8}
{'loss': 0.8809, 'grad_norm': 0.13774575293064117, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8361207842826843, 'eval_runtime': 6.1826, 'eval_samples_per_second': 161.583, 'eval_steps_per_second': 10.19, 'epoch': 0.84}
{'loss': 0.8608, 'grad_norm': 0.13771486282348633, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8318250179290771, 'eval_runtime': 6.1844, 'eval_samples_per_second': 161.536, 'eval_steps_per_second': 10.187, 'epoch': 0.88}
{'loss': 0.8807, 'grad_norm': 0.14406198263168335, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8295512795448303, 'eval_runtime': 6.1908, 'eval_samples_per_second': 161.369, 'eval_steps_per_second': 10.176, 'epoch': 0.92}
{'loss': 0.8497, 'grad_norm': 0.13644754886627197, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.828357458114624, 'eval_runtime': 6.1918, 'eval_samples_per_second': 161.343, 'eval_steps_per_second': 10.175, 'epoch': 0.96}
{'loss': 0.8543, 'grad_norm': 0.1407676637172699, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8277343511581421, 'eval_runtime': 6.1831, 'eval_samples_per_second': 161.569, 'eval_steps_per_second': 10.189, 'epoch': 1.0}
{'train_runtime': 281.8117, 'train_samples_per_second': 35.478, 'train_steps_per_second': 2.218, 'train_loss': 1.1715179016113282, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3590199947357178, 1.7121394872665405, 1.3730683326721191, 1.2778364419937134, 1.1945925951004028, 1.0559486150741577, 0.9783317446708679, 0.9289408326148987, 0.9159112572669983, 0.9051685929298401, 0.8985099792480469, 0.8898362517356873, 0.883732259273529, 0.8751035928726196, 0.8693928122520447, 0.8621892333030701, 0.8562817573547363, 0.8494096398353577, 0.8441206216812134, 0.8398963809013367, 0.8361207842826843, 0.8318250179290771, 0.8295512795448303, 0.828357458114624, 0.8277343511581421], 'performance': [0.75, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:18,  1.57it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 109.87it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 201.06it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 277.60it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 339.50it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 392.91it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 313.13it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8227295875549316
current iteration best possible performance (full train run):  0.798
max performance so far:  0.798
BO observations:  [0.8227295875549316]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5065 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 0.8277545125953799
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.8277545125953799
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 0.8277545125953799
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 0.8277545125953799
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 0.8277545125953799
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0433, dtype=torch.float64), 0, tensor(0.7711, dtype=torch.float64), 0, tensor(0.1856, dtype=torch.float64), 0, 0, 0, 5, 1, 1, 0, 1, 1, 128, 0.005304672690025761, 7.2908735337414114, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0433, dtype=torch.float64), tensor(3.5321e-19, dtype=torch.float64), tensor(0.7711, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1856, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1717, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(0.1519, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.043
  rowan_hellaswag: 0
  sciq: 0.771
  triviaqa: 0
  truthfulqa_gen: 0.186
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.005304672690025761,)
  num_layers_to_apply: (5,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (7.2908735337414114,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  5
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.005304672690025761
lora alpha:  7.2908735337414114
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 32,112,640 || all params: 8,062,373,888 || trainable%: 0.3983
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 308.32it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 513.78it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 597.95it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 647.65it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 686.74it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 715.28it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 700.15it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.7687, 'grad_norm': 1.7476047277450562, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.513857841491699, 'eval_runtime': 7.1065, 'eval_samples_per_second': 140.575, 'eval_steps_per_second': 8.865, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 310.20it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 514.31it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 599.35it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 652.73it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 688.60it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 715.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 701.55it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.4415, 'grad_norm': 0.8485679626464844, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.4084516763687134, 'eval_runtime': 4.97, 'eval_samples_per_second': 201.007, 'eval_steps_per_second': 12.676, 'epoch': 0.08}
{'loss': 1.227, 'grad_norm': 0.39006707072257996, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0867420434951782, 'eval_runtime': 4.9588, 'eval_samples_per_second': 201.459, 'eval_steps_per_second': 12.705, 'epoch': 0.12}
{'loss': 1.0202, 'grad_norm': 0.14826712012290955, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.960155189037323, 'eval_runtime': 4.9641, 'eval_samples_per_second': 201.244, 'eval_steps_per_second': 12.691, 'epoch': 0.16}
{'loss': 0.925, 'grad_norm': 0.22224625945091248, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8562643527984619, 'eval_runtime': 4.9495, 'eval_samples_per_second': 201.839, 'eval_steps_per_second': 12.729, 'epoch': 0.2}
{'loss': 0.8492, 'grad_norm': 0.1936788260936737, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8212717175483704, 'eval_runtime': 4.9596, 'eval_samples_per_second': 201.428, 'eval_steps_per_second': 12.703, 'epoch': 0.24}
{'loss': 0.8182, 'grad_norm': 0.13194133341312408, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8103556632995605, 'eval_runtime': 4.9898, 'eval_samples_per_second': 200.208, 'eval_steps_per_second': 12.626, 'epoch': 0.28}
{'loss': 0.8178, 'grad_norm': 0.15634037554264069, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8020678162574768, 'eval_runtime': 5.025, 'eval_samples_per_second': 198.807, 'eval_steps_per_second': 12.537, 'epoch': 0.32}
{'loss': 0.8355, 'grad_norm': 0.1936568021774292, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7930492162704468, 'eval_runtime': 5.0154, 'eval_samples_per_second': 199.188, 'eval_steps_per_second': 12.561, 'epoch': 0.36}
{'loss': 0.8054, 'grad_norm': 0.1643674522638321, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7916091084480286, 'eval_runtime': 5.027, 'eval_samples_per_second': 198.729, 'eval_steps_per_second': 12.532, 'epoch': 0.4}
{'loss': 0.7804, 'grad_norm': 0.17576538026332855, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7838573455810547, 'eval_runtime': 5.0037, 'eval_samples_per_second': 199.651, 'eval_steps_per_second': 12.591, 'epoch': 0.44}
{'loss': 0.7996, 'grad_norm': 0.20088699460029602, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7846128940582275, 'eval_runtime': 5.0065, 'eval_samples_per_second': 199.54, 'eval_steps_per_second': 12.584, 'epoch': 0.48}
{'loss': 0.7874, 'grad_norm': 0.20128804445266724, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7778050303459167, 'eval_runtime': 5.016, 'eval_samples_per_second': 199.162, 'eval_steps_per_second': 12.56, 'epoch': 0.52}
{'loss': 0.7748, 'grad_norm': 0.17089815437793732, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7756055593490601, 'eval_runtime': 5.018, 'eval_samples_per_second': 199.084, 'eval_steps_per_second': 12.555, 'epoch': 0.56}
{'loss': 0.8152, 'grad_norm': 0.1675460934638977, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7723864316940308, 'eval_runtime': 4.9886, 'eval_samples_per_second': 200.258, 'eval_steps_per_second': 12.629, 'epoch': 0.6}
{'loss': 0.7629, 'grad_norm': 0.16978208720684052, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7718847393989563, 'eval_runtime': 5.013, 'eval_samples_per_second': 199.283, 'eval_steps_per_second': 12.567, 'epoch': 0.64}
{'loss': 0.8054, 'grad_norm': 0.1762351542711258, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7699127197265625, 'eval_runtime': 5.0139, 'eval_samples_per_second': 199.248, 'eval_steps_per_second': 12.565, 'epoch': 0.68}
{'loss': 0.8107, 'grad_norm': 0.16732275485992432, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7674012184143066, 'eval_runtime': 5.0197, 'eval_samples_per_second': 199.017, 'eval_steps_per_second': 12.551, 'epoch': 0.72}
{'loss': 0.7918, 'grad_norm': 0.14976759254932404, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7618750333786011, 'eval_runtime': 5.016, 'eval_samples_per_second': 199.163, 'eval_steps_per_second': 12.56, 'epoch': 0.76}
{'loss': 0.7966, 'grad_norm': 0.16730044782161713, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7621634006500244, 'eval_runtime': 5.0169, 'eval_samples_per_second': 199.129, 'eval_steps_per_second': 12.558, 'epoch': 0.8}
{'loss': 0.7705, 'grad_norm': 0.1452626883983612, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7605478763580322, 'eval_runtime': 5.0209, 'eval_samples_per_second': 198.967, 'eval_steps_per_second': 12.547, 'epoch': 0.84}
{'loss': 0.7883, 'grad_norm': 0.1718461513519287, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7585065364837646, 'eval_runtime': 5.0145, 'eval_samples_per_second': 199.221, 'eval_steps_per_second': 12.564, 'epoch': 0.88}
{'loss': 0.8023, 'grad_norm': 0.15912620723247528, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.758285641670227, 'eval_runtime': 5.0186, 'eval_samples_per_second': 199.059, 'eval_steps_per_second': 12.553, 'epoch': 0.92}
{'loss': 0.7791, 'grad_norm': 0.12711377441883087, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.757424533367157, 'eval_runtime': 5.0276, 'eval_samples_per_second': 198.702, 'eval_steps_per_second': 12.531, 'epoch': 0.96}
{'loss': 0.7936, 'grad_norm': 0.19815094769001007, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7568358182907104, 'eval_runtime': 5.0146, 'eval_samples_per_second': 199.22, 'eval_steps_per_second': 12.563, 'epoch': 1.0}
{'train_runtime': 282.5532, 'train_samples_per_second': 35.384, 'train_steps_per_second': 2.212, 'train_loss': 1.0546829833984375, 'epoch': 1.0}
train_results:  {'eval_loss': [3.513857841491699, 1.4084516763687134, 1.0867420434951782, 0.960155189037323, 0.8562643527984619, 0.8212717175483704, 0.8103556632995605, 0.8020678162574768, 0.7930492162704468, 0.7916091084480286, 0.7838573455810547, 0.7846128940582275, 0.7778050303459167, 0.7756055593490601, 0.7723864316940308, 0.7718847393989563, 0.7699127197265625, 0.7674012184143066, 0.7618750333786011, 0.7621634006500244, 0.7605478763580322, 0.7585065364837646, 0.758285641670227, 0.757424533367157, 0.7568358182907104], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:57,  1.68it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 93.13it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 176.98it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 252.49it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 317.18it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 373.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 292.23it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8317444920539856
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.798
BO observations:  [0.8227295875549316, 0.8317444920539856]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4182 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 0.826877539797609
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.8187174915739537
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 0.8275396189170193
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.8277309011429739
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.8277135919342009
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7871, dtype=torch.float64), 0, tensor(0.2129, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 0, 1, 128, 0.1, 1.4800000190734877, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.7702e-17, dtype=torch.float64), tensor(0.7871, dtype=torch.float64), tensor(1.4794e-17, dtype=torch.float64), tensor(0.2129, dtype=torch.float64), tensor(3.9965e-18, dtype=torch.float64), tensor(1.3119e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.787
  triviaqa: 0
  truthfulqa_gen: 0.213
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734877,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734877
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:43,  4.82it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:02, 165.24it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:01, 352.25it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 458.53it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 529.35it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 579.96it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 616.87it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 523.60it/s]
Evaluation performance at step 25: 0.73
{'loss': 5.3171, 'grad_norm': 0.3568316698074341, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.969177007675171, 'eval_runtime': 3.356, 'eval_samples_per_second': 297.676, 'eval_steps_per_second': 18.772, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.22it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 466.59it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 542.55it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 589.92it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 622.67it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 646.48it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 633.72it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5867, 'grad_norm': 0.10569792240858078, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.6480118036270142, 'eval_runtime': 3.3643, 'eval_samples_per_second': 296.939, 'eval_steps_per_second': 18.726, 'epoch': 0.08}
{'loss': 1.4942, 'grad_norm': 0.06477443873882294, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3401551246643066, 'eval_runtime': 3.3528, 'eval_samples_per_second': 297.959, 'eval_steps_per_second': 18.79, 'epoch': 0.12}
{'loss': 1.2401, 'grad_norm': 0.05938796326518059, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.173822283744812, 'eval_runtime': 3.3505, 'eval_samples_per_second': 298.168, 'eval_steps_per_second': 18.803, 'epoch': 0.16}
{'loss': 1.1871, 'grad_norm': 0.04691845551133156, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1279597282409668, 'eval_runtime': 3.3572, 'eval_samples_per_second': 297.566, 'eval_steps_per_second': 18.765, 'epoch': 0.2}
{'loss': 1.1358, 'grad_norm': 0.05113343521952629, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.092542052268982, 'eval_runtime': 3.349, 'eval_samples_per_second': 298.294, 'eval_steps_per_second': 18.811, 'epoch': 0.24}
{'loss': 1.1142, 'grad_norm': 0.07108084857463837, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0582475662231445, 'eval_runtime': 3.4753, 'eval_samples_per_second': 287.457, 'eval_steps_per_second': 18.128, 'epoch': 0.28}
{'loss': 1.0534, 'grad_norm': 0.055076371878385544, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0126980543136597, 'eval_runtime': 3.3511, 'eval_samples_per_second': 298.112, 'eval_steps_per_second': 18.8, 'epoch': 0.32}
{'loss': 0.998, 'grad_norm': 0.07474619150161743, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9147688150405884, 'eval_runtime': 3.3451, 'eval_samples_per_second': 298.649, 'eval_steps_per_second': 18.834, 'epoch': 0.36}
{'loss': 0.8511, 'grad_norm': 0.06575020402669907, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8383927345275879, 'eval_runtime': 3.3506, 'eval_samples_per_second': 298.159, 'eval_steps_per_second': 18.803, 'epoch': 0.4}
{'loss': 0.832, 'grad_norm': 0.06032339483499527, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7778952717781067, 'eval_runtime': 3.3854, 'eval_samples_per_second': 295.091, 'eval_steps_per_second': 18.609, 'epoch': 0.44}
{'loss': 0.7831, 'grad_norm': 0.05977708101272583, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7433173656463623, 'eval_runtime': 3.3985, 'eval_samples_per_second': 293.95, 'eval_steps_per_second': 18.537, 'epoch': 0.48}
{'loss': 0.7459, 'grad_norm': 0.06598704308271408, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7366624474525452, 'eval_runtime': 3.3788, 'eval_samples_per_second': 295.665, 'eval_steps_per_second': 18.646, 'epoch': 0.52}
{'loss': 0.7687, 'grad_norm': 0.06153825297951698, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7339743375778198, 'eval_runtime': 3.3926, 'eval_samples_per_second': 294.468, 'eval_steps_per_second': 18.57, 'epoch': 0.56}
{'loss': 0.7413, 'grad_norm': 0.05808653682470322, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7284714579582214, 'eval_runtime': 3.383, 'eval_samples_per_second': 295.304, 'eval_steps_per_second': 18.623, 'epoch': 0.6}
{'loss': 0.7249, 'grad_norm': 0.05486917495727539, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7259817123413086, 'eval_runtime': 3.3936, 'eval_samples_per_second': 294.377, 'eval_steps_per_second': 18.564, 'epoch': 0.64}
{'loss': 0.7376, 'grad_norm': 0.05704321339726448, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.723667562007904, 'eval_runtime': 3.3701, 'eval_samples_per_second': 296.434, 'eval_steps_per_second': 18.694, 'epoch': 0.68}
{'loss': 0.7844, 'grad_norm': 0.060819726437330246, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7189975380897522, 'eval_runtime': 3.3631, 'eval_samples_per_second': 297.048, 'eval_steps_per_second': 18.733, 'epoch': 0.72}
{'loss': 0.7308, 'grad_norm': 0.05883513391017914, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7168571352958679, 'eval_runtime': 3.3661, 'eval_samples_per_second': 296.783, 'eval_steps_per_second': 18.716, 'epoch': 0.76}
{'loss': 0.7406, 'grad_norm': 0.06030638888478279, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7137632966041565, 'eval_runtime': 3.3688, 'eval_samples_per_second': 296.548, 'eval_steps_per_second': 18.701, 'epoch': 0.8}
{'loss': 0.7555, 'grad_norm': 0.0590522438287735, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7125611901283264, 'eval_runtime': 3.3674, 'eval_samples_per_second': 296.665, 'eval_steps_per_second': 18.709, 'epoch': 0.84}
{'loss': 0.7266, 'grad_norm': 0.06206018105149269, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.710013747215271, 'eval_runtime': 3.3758, 'eval_samples_per_second': 295.93, 'eval_steps_per_second': 18.662, 'epoch': 0.88}
{'loss': 0.7496, 'grad_norm': 0.0597856342792511, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7091060876846313, 'eval_runtime': 3.3579, 'eval_samples_per_second': 297.512, 'eval_steps_per_second': 18.762, 'epoch': 0.92}
{'loss': 0.7188, 'grad_norm': 0.0700053945183754, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7081610560417175, 'eval_runtime': 3.3619, 'eval_samples_per_second': 297.156, 'eval_steps_per_second': 18.74, 'epoch': 0.96}
{'loss': 0.7325, 'grad_norm': 0.07654111832380295, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7078790068626404, 'eval_runtime': 3.3681, 'eval_samples_per_second': 296.605, 'eval_steps_per_second': 18.705, 'epoch': 1.0}
{'train_runtime': 198.3331, 'train_samples_per_second': 50.415, 'train_steps_per_second': 3.151, 'train_loss': 1.1299951263427734, 'epoch': 1.0}
train_results:  {'eval_loss': [3.969177007675171, 1.6480118036270142, 1.3401551246643066, 1.173822283744812, 1.1279597282409668, 1.092542052268982, 1.0582475662231445, 1.0126980543136597, 0.9147688150405884, 0.8383927345275879, 0.7778952717781067, 0.7433173656463623, 0.7366624474525452, 0.7339743375778198, 0.7284714579582214, 0.7259817123413086, 0.723667562007904, 0.7189975380897522, 0.7168571352958679, 0.7137632966041565, 0.7125611901283264, 0.710013747215271, 0.7091060876846313, 0.7081610560417175, 0.7078790068626404], 'performance': [0.73, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:01<09:06,  1.10s/it]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:05, 79.63it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 152.80it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 220.64it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 280.43it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:02<00:00, 333.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 240.89it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.847478449344635
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.798
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7898 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.8233631747257121
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 0.8326485798685686
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.82951506126329
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 0.8337469924392424
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 0.8326774927690997
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.8194, dtype=torch.float64), 0, tensor(0.1806, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734928, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.4489e-17, dtype=torch.float64), tensor(3.3557e-17, dtype=torch.float64), tensor(0.8194, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1806, dtype=torch.float64), tensor(2.7603e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.6511e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.819
  triviaqa: 0
  truthfulqa_gen: 0.181
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734928,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734928
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 285.41it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 476.80it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 557.45it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 606.45it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 640.80it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 665.60it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 651.74it/s]
Evaluation performance at step 25: 0.75
{'loss': 5.241, 'grad_norm': 0.5714493989944458, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.7261838912963867, 'eval_runtime': 5.4808, 'eval_samples_per_second': 182.273, 'eval_steps_per_second': 11.495, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 284.77it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 475.96it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 556.91it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 604.10it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 638.69it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 664.53it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 650.51it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.2986, 'grad_norm': 0.4445953965187073, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.3260890245437622, 'eval_runtime': 3.2893, 'eval_samples_per_second': 303.713, 'eval_steps_per_second': 19.153, 'epoch': 0.08}
{'loss': 1.1216, 'grad_norm': 0.13818947970867157, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.979518473148346, 'eval_runtime': 3.2725, 'eval_samples_per_second': 305.276, 'eval_steps_per_second': 19.252, 'epoch': 0.12}
{'loss': 0.9078, 'grad_norm': 0.12411540746688843, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8205586075782776, 'eval_runtime': 3.2778, 'eval_samples_per_second': 304.782, 'eval_steps_per_second': 19.22, 'epoch': 0.16}
{'loss': 0.8038, 'grad_norm': 0.05471048876643181, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7992255091667175, 'eval_runtime': 3.2793, 'eval_samples_per_second': 304.638, 'eval_steps_per_second': 19.211, 'epoch': 0.2}
{'loss': 0.7966, 'grad_norm': 0.04955581948161125, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7871376276016235, 'eval_runtime': 3.278, 'eval_samples_per_second': 304.758, 'eval_steps_per_second': 19.219, 'epoch': 0.24}
{'loss': 0.7803, 'grad_norm': 0.05751020461320877, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7803224921226501, 'eval_runtime': 3.2883, 'eval_samples_per_second': 303.804, 'eval_steps_per_second': 19.159, 'epoch': 0.28}
{'loss': 0.7769, 'grad_norm': 0.059371720999479294, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7751225233078003, 'eval_runtime': 3.2824, 'eval_samples_per_second': 304.355, 'eval_steps_per_second': 19.194, 'epoch': 0.32}
{'loss': 0.7884, 'grad_norm': 0.06703899800777435, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7688303589820862, 'eval_runtime': 3.2878, 'eval_samples_per_second': 303.853, 'eval_steps_per_second': 19.162, 'epoch': 0.36}
{'loss': 0.7643, 'grad_norm': 0.06144440919160843, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7652249932289124, 'eval_runtime': 3.2881, 'eval_samples_per_second': 303.826, 'eval_steps_per_second': 19.16, 'epoch': 0.4}
{'loss': 0.7658, 'grad_norm': 0.05996812880039215, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7617462277412415, 'eval_runtime': 3.2894, 'eval_samples_per_second': 303.703, 'eval_steps_per_second': 19.152, 'epoch': 0.44}
{'loss': 0.7268, 'grad_norm': 0.0666428804397583, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7586528062820435, 'eval_runtime': 3.2849, 'eval_samples_per_second': 304.118, 'eval_steps_per_second': 19.179, 'epoch': 0.48}
{'loss': 0.7303, 'grad_norm': 0.07566697150468826, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7571893930435181, 'eval_runtime': 3.2906, 'eval_samples_per_second': 303.591, 'eval_steps_per_second': 19.145, 'epoch': 0.52}
{'loss': 0.754, 'grad_norm': 0.0639030709862709, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7532060146331787, 'eval_runtime': 3.2899, 'eval_samples_per_second': 303.661, 'eval_steps_per_second': 19.15, 'epoch': 0.56}
{'loss': 0.7404, 'grad_norm': 0.0650685727596283, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7517789006233215, 'eval_runtime': 3.2974, 'eval_samples_per_second': 302.965, 'eval_steps_per_second': 19.106, 'epoch': 0.6}
{'loss': 0.7351, 'grad_norm': 0.06833595037460327, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7495706081390381, 'eval_runtime': 3.292, 'eval_samples_per_second': 303.465, 'eval_steps_per_second': 19.137, 'epoch': 0.64}
{'loss': 0.7504, 'grad_norm': 0.08067478984594345, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.747342050075531, 'eval_runtime': 3.2908, 'eval_samples_per_second': 303.57, 'eval_steps_per_second': 19.144, 'epoch': 0.68}
{'loss': 0.7428, 'grad_norm': 0.05977266654372215, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7457664012908936, 'eval_runtime': 3.2975, 'eval_samples_per_second': 302.954, 'eval_steps_per_second': 19.105, 'epoch': 0.72}
{'loss': 0.7458, 'grad_norm': 0.06550787389278412, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7442352175712585, 'eval_runtime': 3.293, 'eval_samples_per_second': 303.368, 'eval_steps_per_second': 19.131, 'epoch': 0.76}
{'loss': 0.7454, 'grad_norm': 0.08112359791994095, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7421965599060059, 'eval_runtime': 3.3003, 'eval_samples_per_second': 302.696, 'eval_steps_per_second': 19.089, 'epoch': 0.8}
{'loss': 0.7524, 'grad_norm': 0.10035958141088486, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7412291765213013, 'eval_runtime': 3.2957, 'eval_samples_per_second': 303.122, 'eval_steps_per_second': 19.116, 'epoch': 0.84}
{'loss': 0.7325, 'grad_norm': 0.07265782356262207, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7404940128326416, 'eval_runtime': 3.2923, 'eval_samples_per_second': 303.434, 'eval_steps_per_second': 19.135, 'epoch': 0.88}
{'loss': 0.7557, 'grad_norm': 0.059822432696819305, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7397797107696533, 'eval_runtime': 3.2895, 'eval_samples_per_second': 303.691, 'eval_steps_per_second': 19.152, 'epoch': 0.92}
{'loss': 0.7383, 'grad_norm': 0.07261650264263153, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7391588091850281, 'eval_runtime': 3.2811, 'eval_samples_per_second': 304.473, 'eval_steps_per_second': 19.201, 'epoch': 0.96}
{'loss': 0.769, 'grad_norm': 0.07453720271587372, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7390224933624268, 'eval_runtime': 3.282, 'eval_samples_per_second': 304.385, 'eval_steps_per_second': 19.195, 'epoch': 1.0}
{'train_runtime': 195.4344, 'train_samples_per_second': 51.163, 'train_steps_per_second': 3.198, 'train_loss': 1.0185505828857422, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7261838912963867, 1.3260890245437622, 0.979518473148346, 0.8205586075782776, 0.7992255091667175, 0.7871376276016235, 0.7803224921226501, 0.7751225233078003, 0.7688303589820862, 0.7652249932289124, 0.7617462277412415, 0.7586528062820435, 0.7571893930435181, 0.7532060146331787, 0.7517789006233215, 0.7495706081390381, 0.747342050075531, 0.7457664012908936, 0.7442352175712585, 0.7421965599060059, 0.7412291765213013, 0.7404940128326416, 0.7397797107696533, 0.7391588091850281, 0.7390224933624268], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:58,  2.80it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 142.61it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 240.58it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 312.00it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 364.40it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 407.71it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 362.96it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8477747440338135
current iteration best possible performance (full train run):  0.756
max performance so far:  0.798
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0097 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 0.8462950379047354
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.8463108798597184
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 0.8468747229310137
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.8476496031964091
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.8344394009715967
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.5574, dtype=torch.float64), 0, 0, tensor(0.4426, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.480000019073488, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.3144e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5574, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5285e-17, dtype=torch.float64), tensor(0.4426, dtype=torch.float64), tensor(8.0574e-17, dtype=torch.float64), tensor(1.3209e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.557
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.443
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.57it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.58it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.51it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 588.36it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 621.56it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 645.88it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 632.59it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.3132, 'grad_norm': 0.1262592375278473, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.529705286026001, 'eval_runtime': 7.8949, 'eval_samples_per_second': 126.538, 'eval_steps_per_second': 7.98, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.07it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 465.22it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.88it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 588.31it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 621.00it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 645.41it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 632.56it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.7364, 'grad_norm': 0.10015113651752472, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 2.1535592079162598, 'eval_runtime': 7.8995, 'eval_samples_per_second': 126.464, 'eval_steps_per_second': 7.975, 'epoch': 0.08}
{'loss': 2.0034, 'grad_norm': 0.07441962510347366, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8512455224990845, 'eval_runtime': 7.8857, 'eval_samples_per_second': 126.686, 'eval_steps_per_second': 7.989, 'epoch': 0.12}
{'loss': 1.8428, 'grad_norm': 0.07481379806995392, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7310906648635864, 'eval_runtime': 7.8709, 'eval_samples_per_second': 126.924, 'eval_steps_per_second': 8.004, 'epoch': 0.16}
{'loss': 1.7763, 'grad_norm': 0.05426936596632004, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.670820951461792, 'eval_runtime': 7.8741, 'eval_samples_per_second': 126.872, 'eval_steps_per_second': 8.001, 'epoch': 0.2}
{'loss': 1.69, 'grad_norm': 0.06518499553203583, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6424421072006226, 'eval_runtime': 7.8867, 'eval_samples_per_second': 126.669, 'eval_steps_per_second': 7.988, 'epoch': 0.24}
{'loss': 1.6446, 'grad_norm': 0.0697401836514473, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6244909763336182, 'eval_runtime': 7.8962, 'eval_samples_per_second': 126.517, 'eval_steps_per_second': 7.979, 'epoch': 0.28}
{'loss': 1.6778, 'grad_norm': 0.07606682181358337, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6102582216262817, 'eval_runtime': 7.9178, 'eval_samples_per_second': 126.172, 'eval_steps_per_second': 7.957, 'epoch': 0.32}
{'loss': 1.5566, 'grad_norm': 0.0526559054851532, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5979797840118408, 'eval_runtime': 7.9786, 'eval_samples_per_second': 125.209, 'eval_steps_per_second': 7.896, 'epoch': 0.36}
{'loss': 1.6719, 'grad_norm': 0.11545814573764801, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5836116075515747, 'eval_runtime': 7.9719, 'eval_samples_per_second': 125.315, 'eval_steps_per_second': 7.903, 'epoch': 0.4}
{'loss': 1.569, 'grad_norm': 0.08266361057758331, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5700769424438477, 'eval_runtime': 7.957, 'eval_samples_per_second': 125.55, 'eval_steps_per_second': 7.918, 'epoch': 0.44}
{'loss': 1.6317, 'grad_norm': 0.06779985874891281, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5538748502731323, 'eval_runtime': 7.9553, 'eval_samples_per_second': 125.577, 'eval_steps_per_second': 7.919, 'epoch': 0.48}
{'loss': 1.5712, 'grad_norm': 0.1301630735397339, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5320051908493042, 'eval_runtime': 7.954, 'eval_samples_per_second': 125.596, 'eval_steps_per_second': 7.92, 'epoch': 0.52}
{'loss': 1.4797, 'grad_norm': 0.07750775665044785, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.481777310371399, 'eval_runtime': 7.9492, 'eval_samples_per_second': 125.674, 'eval_steps_per_second': 7.925, 'epoch': 0.56}
{'loss': 1.5093, 'grad_norm': 0.08698651939630508, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4667409658432007, 'eval_runtime': 7.9634, 'eval_samples_per_second': 125.449, 'eval_steps_per_second': 7.911, 'epoch': 0.6}
{'loss': 1.4442, 'grad_norm': 0.06692073494195938, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4510343074798584, 'eval_runtime': 7.9436, 'eval_samples_per_second': 125.761, 'eval_steps_per_second': 7.931, 'epoch': 0.64}
{'loss': 1.4303, 'grad_norm': 0.09368186444044113, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.435407042503357, 'eval_runtime': 7.9489, 'eval_samples_per_second': 125.678, 'eval_steps_per_second': 7.926, 'epoch': 0.68}
{'loss': 1.4728, 'grad_norm': 0.07254233956336975, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4180872440338135, 'eval_runtime': 7.9392, 'eval_samples_per_second': 125.831, 'eval_steps_per_second': 7.935, 'epoch': 0.72}
{'loss': 1.4546, 'grad_norm': 0.07510018348693848, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3980717658996582, 'eval_runtime': 7.9515, 'eval_samples_per_second': 125.637, 'eval_steps_per_second': 7.923, 'epoch': 0.76}
{'loss': 1.3534, 'grad_norm': 0.10057838261127472, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3812285661697388, 'eval_runtime': 7.9492, 'eval_samples_per_second': 125.673, 'eval_steps_per_second': 7.925, 'epoch': 0.8}
{'loss': 1.3269, 'grad_norm': 0.07817279547452927, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.367806077003479, 'eval_runtime': 7.9533, 'eval_samples_per_second': 125.607, 'eval_steps_per_second': 7.921, 'epoch': 0.84}
{'loss': 1.4237, 'grad_norm': 0.08087080717086792, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3579471111297607, 'eval_runtime': 7.9571, 'eval_samples_per_second': 125.548, 'eval_steps_per_second': 7.917, 'epoch': 0.88}
{'loss': 1.4864, 'grad_norm': 0.0617169514298439, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.353474736213684, 'eval_runtime': 7.9487, 'eval_samples_per_second': 125.681, 'eval_steps_per_second': 7.926, 'epoch': 0.92}
{'loss': 1.2964, 'grad_norm': 0.07508534938097, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.351530909538269, 'eval_runtime': 7.9539, 'eval_samples_per_second': 125.599, 'eval_steps_per_second': 7.921, 'epoch': 0.96}
{'loss': 1.2999, 'grad_norm': 0.10628897696733475, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3504095077514648, 'eval_runtime': 7.951, 'eval_samples_per_second': 125.644, 'eval_steps_per_second': 7.923, 'epoch': 1.0}
{'train_runtime': 409.1838, 'train_samples_per_second': 24.436, 'train_steps_per_second': 1.527, 'train_loss': 1.7064968017578126, 'epoch': 1.0}
train_results:  {'eval_loss': [3.529705286026001, 2.1535592079162598, 1.8512455224990845, 1.7310906648635864, 1.670820951461792, 1.6424421072006226, 1.6244909763336182, 1.6102582216262817, 1.5979797840118408, 1.5836116075515747, 1.5700769424438477, 1.5538748502731323, 1.5320051908493042, 1.481777310371399, 1.4667409658432007, 1.4510343074798584, 1.435407042503357, 1.4180872440338135, 1.3980717658996582, 1.3812285661697388, 1.367806077003479, 1.3579471111297607, 1.353474736213684, 1.351530909538269, 1.3504095077514648], 'performance': [0.75, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<06:28,  1.28it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 101.90it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 186.55it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 257.96it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 315.52it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 364.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 285.92it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8484109044075012
current iteration best possible performance (full train run):  0.777
max performance so far:  0.798
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7002 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 0.845542782927832
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.8436010629760629
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.8460947729708037
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.8329453227919109
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 0.8412720899667848
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5821, dtype=torch.float64), tensor(0.3755, dtype=torch.float64), 0, 0, tensor(0.0424, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(4.9686e-18, dtype=torch.float64), tensor(2.5331e-17, dtype=torch.float64), tensor(0.5821, dtype=torch.float64), tensor(0.3755, dtype=torch.float64), tensor(6.4636e-17, dtype=torch.float64), tensor(3.5811e-17, dtype=torch.float64), tensor(0.0424, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.8953e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.582
  sciq: 0.376
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.042
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.45it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.80it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.85it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 588.99it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 616.06it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 642.63it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 630.78it/s]
Evaluation performance at step 25: 0.73
{'loss': 4.2373, 'grad_norm': 0.14732657372951508, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.5501928329467773, 'eval_runtime': 9.9176, 'eval_samples_per_second': 100.73, 'eval_steps_per_second': 6.352, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 278.72it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.20it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 540.70it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 586.45it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 618.98it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 643.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 630.88it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.8188, 'grad_norm': 0.08062945306301117, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.2770333290100098, 'eval_runtime': 9.9314, 'eval_samples_per_second': 100.59, 'eval_steps_per_second': 6.344, 'epoch': 0.08}
{'loss': 2.0997, 'grad_norm': 0.05413743481040001, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9738937616348267, 'eval_runtime': 9.9189, 'eval_samples_per_second': 100.717, 'eval_steps_per_second': 6.351, 'epoch': 0.12}
{'loss': 1.8752, 'grad_norm': 0.04852087050676346, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8725924491882324, 'eval_runtime': 9.9676, 'eval_samples_per_second': 100.225, 'eval_steps_per_second': 6.32, 'epoch': 0.16}
{'loss': 1.826, 'grad_norm': 0.0446271114051342, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.836830973625183, 'eval_runtime': 9.9829, 'eval_samples_per_second': 100.071, 'eval_steps_per_second': 6.311, 'epoch': 0.2}
{'loss': 1.8151, 'grad_norm': 0.046669892966747284, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.815864086151123, 'eval_runtime': 10.0216, 'eval_samples_per_second': 99.684, 'eval_steps_per_second': 6.286, 'epoch': 0.24}
{'loss': 1.7648, 'grad_norm': 0.05125880241394043, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8017916679382324, 'eval_runtime': 10.0466, 'eval_samples_per_second': 99.436, 'eval_steps_per_second': 6.271, 'epoch': 0.28}
{'loss': 1.767, 'grad_norm': 0.04585718363523483, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.78807532787323, 'eval_runtime': 10.1108, 'eval_samples_per_second': 98.805, 'eval_steps_per_second': 6.231, 'epoch': 0.32}
{'loss': 1.7378, 'grad_norm': 0.04444878548383713, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7765018939971924, 'eval_runtime': 10.1141, 'eval_samples_per_second': 98.773, 'eval_steps_per_second': 6.229, 'epoch': 0.36}
{'loss': 1.7955, 'grad_norm': 0.04284406825900078, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7646440267562866, 'eval_runtime': 10.0307, 'eval_samples_per_second': 99.594, 'eval_steps_per_second': 6.281, 'epoch': 0.4}
{'loss': 1.7442, 'grad_norm': 0.050333086401224136, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.754358172416687, 'eval_runtime': 10.032, 'eval_samples_per_second': 99.581, 'eval_steps_per_second': 6.28, 'epoch': 0.44}
{'loss': 1.724, 'grad_norm': 0.048693642020225525, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7417304515838623, 'eval_runtime': 10.0229, 'eval_samples_per_second': 99.672, 'eval_steps_per_second': 6.286, 'epoch': 0.48}
{'loss': 1.7019, 'grad_norm': 0.046641554683446884, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7277456521987915, 'eval_runtime': 10.0095, 'eval_samples_per_second': 99.805, 'eval_steps_per_second': 6.294, 'epoch': 0.52}
{'loss': 1.6561, 'grad_norm': 0.046071745455265045, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7080252170562744, 'eval_runtime': 10.0086, 'eval_samples_per_second': 99.814, 'eval_steps_per_second': 6.295, 'epoch': 0.56}
{'loss': 1.6699, 'grad_norm': 0.052137766033411026, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6936372518539429, 'eval_runtime': 10.0031, 'eval_samples_per_second': 99.869, 'eval_steps_per_second': 6.298, 'epoch': 0.6}
{'loss': 1.6629, 'grad_norm': 0.05462879315018654, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6889444589614868, 'eval_runtime': 10.0092, 'eval_samples_per_second': 99.808, 'eval_steps_per_second': 6.294, 'epoch': 0.64}
{'loss': 1.6564, 'grad_norm': 0.049555785953998566, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6841728687286377, 'eval_runtime': 10.0024, 'eval_samples_per_second': 99.876, 'eval_steps_per_second': 6.298, 'epoch': 0.68}
{'loss': 1.6784, 'grad_norm': 0.043097566813230515, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.680179476737976, 'eval_runtime': 9.9827, 'eval_samples_per_second': 100.073, 'eval_steps_per_second': 6.311, 'epoch': 0.72}
{'loss': 1.7091, 'grad_norm': 0.0603751502931118, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.676706314086914, 'eval_runtime': 10.0019, 'eval_samples_per_second': 99.881, 'eval_steps_per_second': 6.299, 'epoch': 0.76}
{'loss': 1.6766, 'grad_norm': 0.047517091035842896, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.673554539680481, 'eval_runtime': 9.9934, 'eval_samples_per_second': 99.966, 'eval_steps_per_second': 6.304, 'epoch': 0.8}
{'loss': 1.623, 'grad_norm': 0.05303210765123367, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6706472635269165, 'eval_runtime': 9.9882, 'eval_samples_per_second': 100.018, 'eval_steps_per_second': 6.307, 'epoch': 0.84}
{'loss': 1.6611, 'grad_norm': 0.05933832749724388, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6686662435531616, 'eval_runtime': 10.0103, 'eval_samples_per_second': 99.797, 'eval_steps_per_second': 6.294, 'epoch': 0.88}
{'loss': 1.6735, 'grad_norm': 0.05070973187685013, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6671192646026611, 'eval_runtime': 10.0236, 'eval_samples_per_second': 99.665, 'eval_steps_per_second': 6.285, 'epoch': 0.92}
{'loss': 1.6657, 'grad_norm': 0.05448519065976143, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6658532619476318, 'eval_runtime': 10.0148, 'eval_samples_per_second': 99.752, 'eval_steps_per_second': 6.291, 'epoch': 0.96}
{'loss': 1.6666, 'grad_norm': 0.057679433375597, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.66552734375, 'eval_runtime': 10.0135, 'eval_samples_per_second': 99.765, 'eval_steps_per_second': 6.292, 'epoch': 1.0}
{'train_runtime': 506.579, 'train_samples_per_second': 19.736, 'train_steps_per_second': 1.234, 'train_loss': 1.876259228515625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5501928329467773, 2.2770333290100098, 1.9738937616348267, 1.8725924491882324, 1.836830973625183, 1.815864086151123, 1.8017916679382324, 1.78807532787323, 1.7765018939971924, 1.7646440267562866, 1.754358172416687, 1.7417304515838623, 1.7277456521987915, 1.7080252170562744, 1.6936372518539429, 1.6889444589614868, 1.6841728687286377, 1.680179476737976, 1.676706314086914, 1.673554539680481, 1.6706472635269165, 1.6686662435531616, 1.6671192646026611, 1.6658532619476318, 1.66552734375], 'performance': [0.73, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<06:13,  1.34it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 96.68it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 179.83it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 250.62it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 309.00it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 358.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 280.60it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8477435111999512
current iteration best possible performance (full train run):  0.756
max performance so far:  0.798
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8030 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 0.8352752079595145
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.81309535332579
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 0.8415823556881827
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.8392932543066564
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 0.8255104099897286
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.2332, dtype=torch.float64), tensor(0.3475, dtype=torch.float64), 0, tensor(0.4193, dtype=torch.float64), 8, 0, 1, 1, 1, 0, 128, 0.1, 1.4800000190734903, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.6901e-17, dtype=torch.float64), tensor(9.8733e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3964e-17, dtype=torch.float64), tensor(0.2332, dtype=torch.float64), tensor(0.3475, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4193, dtype=torch.float64), tensor(0.2395, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.233
  wikitext: 0.348
  mmlu: 0
  arc_challenge: 0.419

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (8,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734903,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  8
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734903
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 42,991,616 || all params: 8,073,252,864 || trainable%: 0.5325
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 305.70it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 507.23it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 591.67it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 644.20it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 680.40it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 708.99it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 693.77it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.8785, 'grad_norm': 0.45204252004623413, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.4486541748046875, 'eval_runtime': 7.4725, 'eval_samples_per_second': 133.69, 'eval_steps_per_second': 8.431, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 305.90it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 508.65it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 593.16it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 642.29it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 676.54it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 706.22it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 692.24it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.8024, 'grad_norm': 0.5950327515602112, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 2.324066638946533, 'eval_runtime': 7.4703, 'eval_samples_per_second': 133.729, 'eval_steps_per_second': 8.433, 'epoch': 0.08}
{'loss': 1.9338, 'grad_norm': 0.1647847294807434, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7019944190979004, 'eval_runtime': 7.5099, 'eval_samples_per_second': 133.024, 'eval_steps_per_second': 8.389, 'epoch': 0.12}
{'loss': 1.555, 'grad_norm': 0.12702523171901703, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5764079093933105, 'eval_runtime': 7.5001, 'eval_samples_per_second': 133.198, 'eval_steps_per_second': 8.4, 'epoch': 0.16}
{'loss': 1.5018, 'grad_norm': 0.10111907124519348, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.503242015838623, 'eval_runtime': 7.5189, 'eval_samples_per_second': 132.866, 'eval_steps_per_second': 8.379, 'epoch': 0.2}
{'loss': 1.4883, 'grad_norm': 0.14177025854587555, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4615070819854736, 'eval_runtime': 7.5188, 'eval_samples_per_second': 132.867, 'eval_steps_per_second': 8.379, 'epoch': 0.24}
{'loss': 1.4024, 'grad_norm': 0.09331116825342178, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4089609384536743, 'eval_runtime': 7.536, 'eval_samples_per_second': 132.563, 'eval_steps_per_second': 8.36, 'epoch': 0.28}
{'loss': 1.3533, 'grad_norm': 0.08727812021970749, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3730583190917969, 'eval_runtime': 7.5347, 'eval_samples_per_second': 132.587, 'eval_steps_per_second': 8.361, 'epoch': 0.32}
{'loss': 1.2662, 'grad_norm': 0.08795620501041412, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.351102352142334, 'eval_runtime': 7.5175, 'eval_samples_per_second': 132.891, 'eval_steps_per_second': 8.38, 'epoch': 0.36}
{'loss': 1.3562, 'grad_norm': 0.09302634000778198, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3208979368209839, 'eval_runtime': 7.5219, 'eval_samples_per_second': 132.813, 'eval_steps_per_second': 8.376, 'epoch': 0.4}
{'loss': 1.2963, 'grad_norm': 0.07513585686683655, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2857332229614258, 'eval_runtime': 7.5139, 'eval_samples_per_second': 132.954, 'eval_steps_per_second': 8.384, 'epoch': 0.44}
{'loss': 1.325, 'grad_norm': 0.06854978203773499, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.25734281539917, 'eval_runtime': 7.5158, 'eval_samples_per_second': 132.92, 'eval_steps_per_second': 8.382, 'epoch': 0.48}
{'loss': 1.232, 'grad_norm': 0.07647189497947693, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2482988834381104, 'eval_runtime': 7.4996, 'eval_samples_per_second': 133.206, 'eval_steps_per_second': 8.4, 'epoch': 0.52}
{'loss': 1.2286, 'grad_norm': 0.12796330451965332, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2409716844558716, 'eval_runtime': 7.4777, 'eval_samples_per_second': 133.597, 'eval_steps_per_second': 8.425, 'epoch': 0.56}
{'loss': 1.236, 'grad_norm': 0.07934074103832245, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.235722303390503, 'eval_runtime': 7.4635, 'eval_samples_per_second': 133.851, 'eval_steps_per_second': 8.441, 'epoch': 0.6}
{'loss': 1.2387, 'grad_norm': 0.07293359190225601, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2322425842285156, 'eval_runtime': 7.479, 'eval_samples_per_second': 133.575, 'eval_steps_per_second': 8.424, 'epoch': 0.64}
{'loss': 1.2041, 'grad_norm': 0.09010720998048782, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2282390594482422, 'eval_runtime': 7.4759, 'eval_samples_per_second': 133.629, 'eval_steps_per_second': 8.427, 'epoch': 0.68}
{'loss': 1.2776, 'grad_norm': 0.07695403695106506, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2241566181182861, 'eval_runtime': 7.4872, 'eval_samples_per_second': 133.428, 'eval_steps_per_second': 8.414, 'epoch': 0.72}
{'loss': 1.2784, 'grad_norm': 0.07411038875579834, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2217143774032593, 'eval_runtime': 7.4793, 'eval_samples_per_second': 133.569, 'eval_steps_per_second': 8.423, 'epoch': 0.76}
{'loss': 1.2186, 'grad_norm': 0.10017804801464081, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2179032564163208, 'eval_runtime': 7.4775, 'eval_samples_per_second': 133.601, 'eval_steps_per_second': 8.425, 'epoch': 0.8}
{'loss': 1.1708, 'grad_norm': 0.09580716490745544, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2153313159942627, 'eval_runtime': 7.4844, 'eval_samples_per_second': 133.477, 'eval_steps_per_second': 8.417, 'epoch': 0.84}
{'loss': 1.1985, 'grad_norm': 0.10114412754774094, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2139712572097778, 'eval_runtime': 7.477, 'eval_samples_per_second': 133.609, 'eval_steps_per_second': 8.426, 'epoch': 0.88}
{'loss': 1.2504, 'grad_norm': 0.08760813623666763, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2123421430587769, 'eval_runtime': 7.4697, 'eval_samples_per_second': 133.74, 'eval_steps_per_second': 8.434, 'epoch': 0.92}
{'loss': 1.1794, 'grad_norm': 0.12169624865055084, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2118016481399536, 'eval_runtime': 7.4819, 'eval_samples_per_second': 133.522, 'eval_steps_per_second': 8.42, 'epoch': 0.96}
{'loss': 1.2891, 'grad_norm': 0.08477780967950821, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.211287021636963, 'eval_runtime': 7.4714, 'eval_samples_per_second': 133.71, 'eval_steps_per_second': 8.432, 'epoch': 1.0}
{'train_runtime': 383.0715, 'train_samples_per_second': 26.1, 'train_steps_per_second': 1.632, 'train_loss': 1.4864543029785156, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4486541748046875, 2.324066638946533, 1.7019944190979004, 1.5764079093933105, 1.503242015838623, 1.4615070819854736, 1.4089609384536743, 1.3730583190917969, 1.351102352142334, 1.3208979368209839, 1.2857332229614258, 1.25734281539917, 1.2482988834381104, 1.2409716844558716, 1.235722303390503, 1.2322425842285156, 1.2282390594482422, 1.2241566181182861, 1.2217143774032593, 1.2179032564163208, 1.2153313159942627, 1.2139712572097778, 1.2123421430587769, 1.2118016481399536, 1.211287021636963], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:16,  1.58it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 117.33it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 212.38it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 289.56it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 348.59it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 401.23it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 323.34it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.837490439414978
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1482 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 0.8388218255419728
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.8352753251303793
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 0.8386818676435646
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.835983637394073
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.8344945854530801
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(1.0000, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(8.6613e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.6048e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(6.8084e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 1.0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.12it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.92it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 502.67it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 546.30it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 576.43it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 599.23it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 587.01it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.9235, 'grad_norm': 0.41277578473091125, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2722182273864746, 'eval_runtime': 3.8118, 'eval_samples_per_second': 262.08, 'eval_steps_per_second': 16.528, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.41it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 431.29it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 502.70it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 545.93it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 576.02it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 598.07it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 586.49it/s]
Evaluation performance at step 50: 0.75
{'loss': 1.9458, 'grad_norm': 0.3064693510532379, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.1855573654174805, 'eval_runtime': 3.8107, 'eval_samples_per_second': 262.155, 'eval_steps_per_second': 16.532, 'epoch': 0.08}
{'loss': 1.0378, 'grad_norm': 0.12040724605321884, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9026056528091431, 'eval_runtime': 3.793, 'eval_samples_per_second': 263.379, 'eval_steps_per_second': 16.61, 'epoch': 0.12}
{'loss': 0.8228, 'grad_norm': 0.0777781531214714, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7433145642280579, 'eval_runtime': 3.7994, 'eval_samples_per_second': 262.937, 'eval_steps_per_second': 16.582, 'epoch': 0.16}
{'loss': 0.7297, 'grad_norm': 0.07497484236955643, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.712182343006134, 'eval_runtime': 3.8006, 'eval_samples_per_second': 262.85, 'eval_steps_per_second': 16.576, 'epoch': 0.2}
{'loss': 0.7015, 'grad_norm': 0.05880039930343628, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6778603792190552, 'eval_runtime': 3.7981, 'eval_samples_per_second': 263.024, 'eval_steps_per_second': 16.587, 'epoch': 0.24}
{'loss': 0.6929, 'grad_norm': 0.07182777673006058, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6470133066177368, 'eval_runtime': 3.7946, 'eval_samples_per_second': 263.269, 'eval_steps_per_second': 16.603, 'epoch': 0.28}
{'loss': 0.628, 'grad_norm': 0.0816158875823021, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6077090501785278, 'eval_runtime': 3.7945, 'eval_samples_per_second': 263.276, 'eval_steps_per_second': 16.603, 'epoch': 0.32}
{'loss': 0.5994, 'grad_norm': 0.0883292630314827, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5647753477096558, 'eval_runtime': 3.7991, 'eval_samples_per_second': 262.956, 'eval_steps_per_second': 16.583, 'epoch': 0.36}
{'loss': 0.5415, 'grad_norm': 0.10911896824836731, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5198945999145508, 'eval_runtime': 3.8014, 'eval_samples_per_second': 262.799, 'eval_steps_per_second': 16.573, 'epoch': 0.4}
{'loss': 0.5312, 'grad_norm': 0.12459728866815567, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4711313545703888, 'eval_runtime': 3.8077, 'eval_samples_per_second': 262.364, 'eval_steps_per_second': 16.545, 'epoch': 0.44}
{'loss': 0.4628, 'grad_norm': 0.1190100684762001, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4172646701335907, 'eval_runtime': 3.7982, 'eval_samples_per_second': 263.019, 'eval_steps_per_second': 16.587, 'epoch': 0.48}
{'loss': 0.4223, 'grad_norm': 0.12063329666852951, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.37279343605041504, 'eval_runtime': 3.8031, 'eval_samples_per_second': 262.679, 'eval_steps_per_second': 16.565, 'epoch': 0.52}
{'loss': 0.3654, 'grad_norm': 0.14795821905136108, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3313899338245392, 'eval_runtime': 3.8042, 'eval_samples_per_second': 262.606, 'eval_steps_per_second': 16.561, 'epoch': 0.56}
{'loss': 0.3287, 'grad_norm': 0.16883482038974762, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3003809452056885, 'eval_runtime': 3.7999, 'eval_samples_per_second': 262.901, 'eval_steps_per_second': 16.579, 'epoch': 0.6}
{'loss': 0.3211, 'grad_norm': 0.13721922039985657, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.27918383479118347, 'eval_runtime': 3.7982, 'eval_samples_per_second': 263.02, 'eval_steps_per_second': 16.587, 'epoch': 0.64}
{'loss': 0.2677, 'grad_norm': 0.12123675644397736, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2478143721818924, 'eval_runtime': 3.7936, 'eval_samples_per_second': 263.34, 'eval_steps_per_second': 16.607, 'epoch': 0.68}
{'loss': 0.2538, 'grad_norm': 0.1378900408744812, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.22871443629264832, 'eval_runtime': 3.7913, 'eval_samples_per_second': 263.499, 'eval_steps_per_second': 16.617, 'epoch': 0.72}
{'loss': 0.2381, 'grad_norm': 0.1448175609111786, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.21270716190338135, 'eval_runtime': 3.7937, 'eval_samples_per_second': 263.331, 'eval_steps_per_second': 16.606, 'epoch': 0.76}
{'loss': 0.2272, 'grad_norm': 0.15639109909534454, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.19925834238529205, 'eval_runtime': 3.7947, 'eval_samples_per_second': 263.261, 'eval_steps_per_second': 16.602, 'epoch': 0.8}
{'loss': 0.2015, 'grad_norm': 0.13462544977664948, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.18641212582588196, 'eval_runtime': 3.7923, 'eval_samples_per_second': 263.427, 'eval_steps_per_second': 16.612, 'epoch': 0.84}
{'loss': 0.1895, 'grad_norm': 0.14150649309158325, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.17983630299568176, 'eval_runtime': 3.7939, 'eval_samples_per_second': 263.315, 'eval_steps_per_second': 16.605, 'epoch': 0.88}
{'loss': 0.1848, 'grad_norm': 0.16155286133289337, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.17373880743980408, 'eval_runtime': 3.8023, 'eval_samples_per_second': 262.734, 'eval_steps_per_second': 16.569, 'epoch': 0.92}
{'loss': 0.1794, 'grad_norm': 0.1318865269422531, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.168738454580307, 'eval_runtime': 3.8095, 'eval_samples_per_second': 262.236, 'eval_steps_per_second': 16.537, 'epoch': 0.96}
{'loss': 0.1767, 'grad_norm': 0.12277086824178696, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.16706374287605286, 'eval_runtime': 3.8118, 'eval_samples_per_second': 262.078, 'eval_steps_per_second': 16.527, 'epoch': 1.0}
{'train_runtime': 221.5027, 'train_samples_per_second': 45.142, 'train_steps_per_second': 2.822, 'train_loss': 0.6789298927307129, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2722182273864746, 1.1855573654174805, 0.9026056528091431, 0.7433145642280579, 0.712182343006134, 0.6778603792190552, 0.6470133066177368, 0.6077090501785278, 0.5647753477096558, 0.5198945999145508, 0.4711313545703888, 0.4172646701335907, 0.37279343605041504, 0.3313899338245392, 0.3003809452056885, 0.27918383479118347, 0.2478143721818924, 0.22871443629264832, 0.21270716190338135, 0.19925834238529205, 0.18641212582588196, 0.17983630299568176, 0.17373880743980408, 0.168738454580307, 0.16706374287605286], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:51,  4.47it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 231.25it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 316.83it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 363.36it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 392.72it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 417.59it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 419.96it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8483384847640991
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.8085000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0384 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.7991349964401658
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.8327450041198174
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 0.8381034940759268
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 0.8379725936343301
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 0.8381053214753854
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.0138, dtype=torch.float64), tensor(0.9862, dtype=torch.float64), 0, 0, 1, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(4.4090e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0138, dtype=torch.float64), tensor(0.9862, dtype=torch.float64), tensor(9.1293e-18, dtype=torch.float64), tensor(4.3989e-17, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.014
  wikitext: 0.986
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 3,014,656 || all params: 8,033,275,904 || trainable%: 0.0375
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 319.92it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 533.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 621.87it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 675.85it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 714.97it/s]Running loglikelihood requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 446/500 [00:00<00:00, 758.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 728.25it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.186, 'grad_norm': 0.031476106494665146, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.138996124267578, 'eval_runtime': 8.1885, 'eval_samples_per_second': 122.001, 'eval_steps_per_second': 7.694, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 319.37it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 531.89it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 619.30it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 672.19it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 710.62it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 737.92it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 723.67it/s]
Evaluation performance at step 50: 0.75
{'loss': 3.1016, 'grad_norm': 0.08794741332530975, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.8783323764801025, 'eval_runtime': 8.1979, 'eval_samples_per_second': 121.861, 'eval_steps_per_second': 7.685, 'epoch': 0.08}
{'loss': 2.7451, 'grad_norm': 0.09857839345932007, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.606663227081299, 'eval_runtime': 8.2352, 'eval_samples_per_second': 121.308, 'eval_steps_per_second': 7.65, 'epoch': 0.12}
{'loss': 2.6234, 'grad_norm': 0.07838904112577438, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.542687177658081, 'eval_runtime': 8.2436, 'eval_samples_per_second': 121.185, 'eval_steps_per_second': 7.642, 'epoch': 0.16}
{'loss': 2.5364, 'grad_norm': 0.09434328228235245, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.506774425506592, 'eval_runtime': 8.2566, 'eval_samples_per_second': 120.994, 'eval_steps_per_second': 7.63, 'epoch': 0.2}
{'loss': 2.569, 'grad_norm': 0.08565063029527664, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.4818015098571777, 'eval_runtime': 8.2736, 'eval_samples_per_second': 120.746, 'eval_steps_per_second': 7.615, 'epoch': 0.24}
{'loss': 2.5192, 'grad_norm': 0.08413687348365784, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.463106393814087, 'eval_runtime': 8.2996, 'eval_samples_per_second': 120.367, 'eval_steps_per_second': 7.591, 'epoch': 0.28}
{'loss': 2.4994, 'grad_norm': 0.09665130078792572, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.4487016201019287, 'eval_runtime': 8.3105, 'eval_samples_per_second': 120.209, 'eval_steps_per_second': 7.581, 'epoch': 0.32}
{'loss': 2.4482, 'grad_norm': 0.08814916014671326, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.4366085529327393, 'eval_runtime': 8.3126, 'eval_samples_per_second': 120.178, 'eval_steps_per_second': 7.579, 'epoch': 0.36}
{'loss': 2.4495, 'grad_norm': 0.12119633704423904, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.4249165058135986, 'eval_runtime': 8.3121, 'eval_samples_per_second': 120.186, 'eval_steps_per_second': 7.579, 'epoch': 0.4}
{'loss': 2.4364, 'grad_norm': 0.10468332469463348, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.417341709136963, 'eval_runtime': 8.317, 'eval_samples_per_second': 120.115, 'eval_steps_per_second': 7.575, 'epoch': 0.44}
{'loss': 2.4709, 'grad_norm': 0.09289173781871796, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.4069175720214844, 'eval_runtime': 8.3053, 'eval_samples_per_second': 120.284, 'eval_steps_per_second': 7.585, 'epoch': 0.48}
{'loss': 2.4509, 'grad_norm': 0.09023257344961166, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.3999838829040527, 'eval_runtime': 8.3202, 'eval_samples_per_second': 120.069, 'eval_steps_per_second': 7.572, 'epoch': 0.52}
{'loss': 2.4424, 'grad_norm': 0.07279536873102188, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.3923635482788086, 'eval_runtime': 8.3085, 'eval_samples_per_second': 120.238, 'eval_steps_per_second': 7.583, 'epoch': 0.56}
{'loss': 2.4082, 'grad_norm': 0.09209553152322769, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.387389898300171, 'eval_runtime': 8.3104, 'eval_samples_per_second': 120.211, 'eval_steps_per_second': 7.581, 'epoch': 0.6}
{'loss': 2.4052, 'grad_norm': 0.09219661355018616, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.3815486431121826, 'eval_runtime': 8.3156, 'eval_samples_per_second': 120.136, 'eval_steps_per_second': 7.576, 'epoch': 0.64}
{'loss': 2.435, 'grad_norm': 0.08534106612205505, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.37734317779541, 'eval_runtime': 8.3175, 'eval_samples_per_second': 120.109, 'eval_steps_per_second': 7.574, 'epoch': 0.68}
{'loss': 2.4536, 'grad_norm': 0.08269324898719788, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.3734545707702637, 'eval_runtime': 8.3143, 'eval_samples_per_second': 120.154, 'eval_steps_per_second': 7.577, 'epoch': 0.72}
{'loss': 2.4178, 'grad_norm': 0.10293947905302048, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.370015859603882, 'eval_runtime': 8.3264, 'eval_samples_per_second': 119.98, 'eval_steps_per_second': 7.566, 'epoch': 0.76}
{'loss': 2.3847, 'grad_norm': 0.09364805370569229, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.3681445121765137, 'eval_runtime': 8.3415, 'eval_samples_per_second': 119.763, 'eval_steps_per_second': 7.553, 'epoch': 0.8}
{'loss': 2.3618, 'grad_norm': 0.08438624441623688, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.3660318851470947, 'eval_runtime': 8.3175, 'eval_samples_per_second': 120.109, 'eval_steps_per_second': 7.574, 'epoch': 0.84}
{'loss': 2.3368, 'grad_norm': 0.08632910251617432, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.3643341064453125, 'eval_runtime': 8.3364, 'eval_samples_per_second': 119.836, 'eval_steps_per_second': 7.557, 'epoch': 0.88}
{'loss': 2.4328, 'grad_norm': 0.09916770458221436, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.3631038665771484, 'eval_runtime': 8.3459, 'eval_samples_per_second': 119.699, 'eval_steps_per_second': 7.549, 'epoch': 0.92}
{'loss': 2.402, 'grad_norm': 0.07503678649663925, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.3626580238342285, 'eval_runtime': 8.3274, 'eval_samples_per_second': 119.966, 'eval_steps_per_second': 7.565, 'epoch': 0.96}
{'loss': 2.4402, 'grad_norm': 0.08764214813709259, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.3624958992004395, 'eval_runtime': 8.3231, 'eval_samples_per_second': 120.027, 'eval_steps_per_second': 7.569, 'epoch': 1.0}
{'train_runtime': 332.1919, 'train_samples_per_second': 30.1, 'train_steps_per_second': 1.881, 'train_loss': 2.518260284423828, 'epoch': 1.0}
train_results:  {'eval_loss': [3.138996124267578, 2.8783323764801025, 2.606663227081299, 2.542687177658081, 2.506774425506592, 2.4818015098571777, 2.463106393814087, 2.4487016201019287, 2.4366085529327393, 2.4249165058135986, 2.417341709136963, 2.4069175720214844, 2.3999838829040527, 2.3923635482788086, 2.387389898300171, 2.3815486431121826, 2.37734317779541, 2.3734545707702637, 2.370015859603882, 2.3681445121765137, 2.3660318851470947, 2.3643341064453125, 2.3631038665771484, 2.3626580238342285, 2.3624958992004395], 'performance': [0.75, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:32,  5.41it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 278.02it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 380.15it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 435.82it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 470.90it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:00<00:00, 499.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 505.09it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8343195915222168
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8085000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6324 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.8313706543023605
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.8213599558769974
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 0.8322729050358237
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7900240476494145
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.8258446820750077
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.0698, dtype=torch.float64), 0, tensor(0.9302, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 14, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0005e-18, dtype=torch.float64), tensor(9.1325e-18, dtype=torch.float64), tensor(1.2579e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0698, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9302, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1127, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.07
  mmlu: 0
  arc_challenge: 0.93

LoRA Parameters:
  lora_r: (14,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  14
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 22,478,848 || all params: 8,052,740,096 || trainable%: 0.2791
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 254.77it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 423.00it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 494.12it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 536.94it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 567.49it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 590.19it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 577.94it/s]
Evaluation performance at step 25: 0.73
{'loss': 2.2807, 'grad_norm': 1.2883739471435547, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 1.0288623571395874, 'eval_runtime': 7.7127, 'eval_samples_per_second': 129.526, 'eval_steps_per_second': 8.168, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 254.69it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 424.57it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 494.49it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 536.94it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 567.43it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 590.01it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 578.17it/s]
Evaluation performance at step 50: 0.74
{'loss': 0.9828, 'grad_norm': 0.8323715925216675, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 0.8385083079338074, 'eval_runtime': 7.6919, 'eval_samples_per_second': 129.877, 'eval_steps_per_second': 8.19, 'epoch': 0.08}
{'loss': 0.8498, 'grad_norm': 1.000087857246399, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7367832660675049, 'eval_runtime': 7.6675, 'eval_samples_per_second': 130.29, 'eval_steps_per_second': 8.216, 'epoch': 0.12}
{'loss': 0.7134, 'grad_norm': 1.0249496698379517, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6557387709617615, 'eval_runtime': 7.6784, 'eval_samples_per_second': 130.106, 'eval_steps_per_second': 8.205, 'epoch': 0.16}
{'loss': 0.6392, 'grad_norm': 0.8756588697433472, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.5697847604751587, 'eval_runtime': 7.6878, 'eval_samples_per_second': 129.946, 'eval_steps_per_second': 8.195, 'epoch': 0.2}
{'loss': 0.5698, 'grad_norm': 1.0195879936218262, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.49364781379699707, 'eval_runtime': 7.6846, 'eval_samples_per_second': 130.0, 'eval_steps_per_second': 8.198, 'epoch': 0.24}
{'loss': 0.494, 'grad_norm': 1.4770078659057617, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.4291563332080841, 'eval_runtime': 7.6961, 'eval_samples_per_second': 129.806, 'eval_steps_per_second': 8.186, 'epoch': 0.28}
{'loss': 0.457, 'grad_norm': 0.9573046565055847, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.37747564911842346, 'eval_runtime': 7.6938, 'eval_samples_per_second': 129.845, 'eval_steps_per_second': 8.188, 'epoch': 0.32}
{'loss': 0.372, 'grad_norm': 0.9569956064224243, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.3243042230606079, 'eval_runtime': 7.6954, 'eval_samples_per_second': 129.818, 'eval_steps_per_second': 8.187, 'epoch': 0.36}
{'loss': 0.3891, 'grad_norm': 0.9303073287010193, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.2845457196235657, 'eval_runtime': 7.7054, 'eval_samples_per_second': 129.649, 'eval_steps_per_second': 8.176, 'epoch': 0.4}
{'loss': 0.326, 'grad_norm': 1.0608201026916504, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.24788647890090942, 'eval_runtime': 7.7087, 'eval_samples_per_second': 129.593, 'eval_steps_per_second': 8.173, 'epoch': 0.44}
{'loss': 0.3571, 'grad_norm': 1.1896613836288452, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.22621136903762817, 'eval_runtime': 7.7033, 'eval_samples_per_second': 129.685, 'eval_steps_per_second': 8.178, 'epoch': 0.48}
{'loss': 0.2635, 'grad_norm': 0.8839541077613831, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.20611123740673065, 'eval_runtime': 7.7119, 'eval_samples_per_second': 129.54, 'eval_steps_per_second': 8.169, 'epoch': 0.52}
{'loss': 0.2459, 'grad_norm': 0.8431586623191833, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.19083087146282196, 'eval_runtime': 7.7139, 'eval_samples_per_second': 129.506, 'eval_steps_per_second': 8.167, 'epoch': 0.56}
{'loss': 0.2069, 'grad_norm': 0.8705506324768066, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.17597147822380066, 'eval_runtime': 7.7025, 'eval_samples_per_second': 129.699, 'eval_steps_per_second': 8.179, 'epoch': 0.6}
{'loss': 0.2292, 'grad_norm': 0.7343533635139465, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.16620047390460968, 'eval_runtime': 7.7044, 'eval_samples_per_second': 129.666, 'eval_steps_per_second': 8.177, 'epoch': 0.64}
{'loss': 0.2038, 'grad_norm': 0.498735249042511, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.15939031541347504, 'eval_runtime': 7.7162, 'eval_samples_per_second': 129.467, 'eval_steps_per_second': 8.165, 'epoch': 0.68}
{'loss': 0.2037, 'grad_norm': 0.4714491367340088, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.15209822356700897, 'eval_runtime': 7.759, 'eval_samples_per_second': 128.753, 'eval_steps_per_second': 8.12, 'epoch': 0.72}
{'loss': 0.2623, 'grad_norm': 0.5234243869781494, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.14689594507217407, 'eval_runtime': 7.7629, 'eval_samples_per_second': 128.689, 'eval_steps_per_second': 8.115, 'epoch': 0.76}
{'loss': 0.168, 'grad_norm': 0.2485617697238922, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.14238998293876648, 'eval_runtime': 7.7573, 'eval_samples_per_second': 128.782, 'eval_steps_per_second': 8.121, 'epoch': 0.8}
{'loss': 0.1503, 'grad_norm': 0.7226855158805847, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.13942018151283264, 'eval_runtime': 7.7506, 'eval_samples_per_second': 128.894, 'eval_steps_per_second': 8.128, 'epoch': 0.84}
{'loss': 0.1775, 'grad_norm': 0.6546722054481506, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.13612037897109985, 'eval_runtime': 7.7521, 'eval_samples_per_second': 128.869, 'eval_steps_per_second': 8.127, 'epoch': 0.88}
{'loss': 0.2136, 'grad_norm': 0.31149712204933167, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.13358145952224731, 'eval_runtime': 7.7606, 'eval_samples_per_second': 128.728, 'eval_steps_per_second': 8.118, 'epoch': 0.92}
{'loss': 0.1206, 'grad_norm': 0.3988100588321686, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.13263285160064697, 'eval_runtime': 7.7537, 'eval_samples_per_second': 128.842, 'eval_steps_per_second': 8.125, 'epoch': 0.96}
{'loss': 0.1526, 'grad_norm': 0.40102094411849976, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.13233771920204163, 'eval_runtime': 7.7527, 'eval_samples_per_second': 128.858, 'eval_steps_per_second': 8.126, 'epoch': 1.0}
{'train_runtime': 417.9683, 'train_samples_per_second': 23.923, 'train_steps_per_second': 1.495, 'train_loss': 0.44114768676757815, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0288623571395874, 0.8385083079338074, 0.7367832660675049, 0.6557387709617615, 0.5697847604751587, 0.49364781379699707, 0.4291563332080841, 0.37747564911842346, 0.3243042230606079, 0.2845457196235657, 0.24788647890090942, 0.22621136903762817, 0.20611123740673065, 0.19083087146282196, 0.17597147822380066, 0.16620047390460968, 0.15939031541347504, 0.15209822356700897, 0.14689594507217407, 0.14238998293876648, 0.13942018151283264, 0.13612037897109985, 0.13358145952224731, 0.13263285160064697, 0.13233771920204163], 'performance': [0.73, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:52,  4.43it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 229.12it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 314.28it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 360.59it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 390.10it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 413.51it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 417.74it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8145298957824707
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8085000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6764 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.8275072130455683
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.8045455998495293
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.8252890916124267
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.7839510533890965
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.8258088375877601
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0449, dtype=torch.float64), tensor(0.1403, dtype=torch.float64), 0, tensor(0.5430, dtype=torch.float64), tensor(0.2718, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 1, 0, 78, 0.1, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(7.5212e-17, dtype=torch.float64), tensor(5.0441e-18, dtype=torch.float64), tensor(0.0449, dtype=torch.float64), tensor(0.1403, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5430, dtype=torch.float64), tensor(0.2718, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.8130e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6085, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.045
  sciq: 0.14
  triviaqa: 0
  truthfulqa_gen: 0.543
  wikitext: 0.272
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (78,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  78
lora dropout:  0.1
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 125,239,296 || all params: 8,155,500,544 || trainable%: 1.5356
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 249.84it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 416.15it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 483.75it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 525.11it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 554.63it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 576.71it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 565.47it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.4455, 'grad_norm': 0.4963599443435669, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.3519749641418457, 'eval_runtime': 8.2386, 'eval_samples_per_second': 121.258, 'eval_steps_per_second': 7.647, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 249.74it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 415.97it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 483.88it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 523.97it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 553.53it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 575.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 564.71it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.3874, 'grad_norm': 0.3480912148952484, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8068739175796509, 'eval_runtime': 8.2429, 'eval_samples_per_second': 121.195, 'eval_steps_per_second': 7.643, 'epoch': 0.08}
{'loss': 1.6231, 'grad_norm': 0.11226394772529602, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.530853033065796, 'eval_runtime': 8.2853, 'eval_samples_per_second': 120.574, 'eval_steps_per_second': 7.604, 'epoch': 0.12}
{'loss': 1.4718, 'grad_norm': 0.08979692310094833, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3941439390182495, 'eval_runtime': 8.2653, 'eval_samples_per_second': 120.867, 'eval_steps_per_second': 7.622, 'epoch': 0.16}
{'loss': 1.4047, 'grad_norm': 0.09294041246175766, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2957353591918945, 'eval_runtime': 8.253, 'eval_samples_per_second': 121.047, 'eval_steps_per_second': 7.634, 'epoch': 0.2}
{'loss': 1.3256, 'grad_norm': 0.07442072033882141, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.253612995147705, 'eval_runtime': 8.2718, 'eval_samples_per_second': 120.772, 'eval_steps_per_second': 7.616, 'epoch': 0.24}
{'loss': 1.2724, 'grad_norm': 0.10491162538528442, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.225887417793274, 'eval_runtime': 8.3462, 'eval_samples_per_second': 119.695, 'eval_steps_per_second': 7.548, 'epoch': 0.28}
{'loss': 1.1956, 'grad_norm': 0.0899975523352623, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2089767456054688, 'eval_runtime': 8.3396, 'eval_samples_per_second': 119.79, 'eval_steps_per_second': 7.554, 'epoch': 0.32}
{'loss': 1.2791, 'grad_norm': 0.12026231735944748, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1895915269851685, 'eval_runtime': 8.3246, 'eval_samples_per_second': 120.005, 'eval_steps_per_second': 7.568, 'epoch': 0.36}
{'loss': 1.1578, 'grad_norm': 0.10063084959983826, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1718578338623047, 'eval_runtime': 8.3261, 'eval_samples_per_second': 119.984, 'eval_steps_per_second': 7.567, 'epoch': 0.4}
{'loss': 1.2037, 'grad_norm': 0.12449753284454346, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1598013639450073, 'eval_runtime': 8.313, 'eval_samples_per_second': 120.174, 'eval_steps_per_second': 7.579, 'epoch': 0.44}
{'loss': 1.2444, 'grad_norm': 0.11008763313293457, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1433980464935303, 'eval_runtime': 8.3066, 'eval_samples_per_second': 120.265, 'eval_steps_per_second': 7.584, 'epoch': 0.48}
{'loss': 1.2148, 'grad_norm': 0.11986663192510605, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1270934343338013, 'eval_runtime': 8.3045, 'eval_samples_per_second': 120.297, 'eval_steps_per_second': 7.586, 'epoch': 0.52}
{'loss': 1.1719, 'grad_norm': 0.13331754505634308, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1110143661499023, 'eval_runtime': 8.3168, 'eval_samples_per_second': 120.118, 'eval_steps_per_second': 7.575, 'epoch': 0.56}
{'loss': 1.2215, 'grad_norm': 0.11316698789596558, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0964070558547974, 'eval_runtime': 8.3112, 'eval_samples_per_second': 120.2, 'eval_steps_per_second': 7.58, 'epoch': 0.6}
{'loss': 1.1011, 'grad_norm': 0.1811072677373886, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.081355333328247, 'eval_runtime': 8.3057, 'eval_samples_per_second': 120.278, 'eval_steps_per_second': 7.585, 'epoch': 0.64}
{'loss': 1.276, 'grad_norm': 0.11043373495340347, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0679041147232056, 'eval_runtime': 8.3342, 'eval_samples_per_second': 119.868, 'eval_steps_per_second': 7.559, 'epoch': 0.68}
{'loss': 1.1389, 'grad_norm': 0.16606588661670685, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0573160648345947, 'eval_runtime': 8.3225, 'eval_samples_per_second': 120.037, 'eval_steps_per_second': 7.57, 'epoch': 0.72}
{'loss': 1.0433, 'grad_norm': 0.12522390484809875, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.043606162071228, 'eval_runtime': 8.3161, 'eval_samples_per_second': 120.128, 'eval_steps_per_second': 7.576, 'epoch': 0.76}
{'loss': 1.1031, 'grad_norm': 0.13827988505363464, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0330967903137207, 'eval_runtime': 8.3185, 'eval_samples_per_second': 120.094, 'eval_steps_per_second': 7.573, 'epoch': 0.8}
{'loss': 1.1557, 'grad_norm': 0.15424762666225433, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0172219276428223, 'eval_runtime': 8.2965, 'eval_samples_per_second': 120.412, 'eval_steps_per_second': 7.594, 'epoch': 0.84}
{'loss': 1.1815, 'grad_norm': 0.1413947492837906, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.009533166885376, 'eval_runtime': 8.2967, 'eval_samples_per_second': 120.409, 'eval_steps_per_second': 7.593, 'epoch': 0.88}
{'loss': 1.1252, 'grad_norm': 0.18744239211082458, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.002882719039917, 'eval_runtime': 8.3093, 'eval_samples_per_second': 120.226, 'eval_steps_per_second': 7.582, 'epoch': 0.92}
{'loss': 1.1191, 'grad_norm': 0.14208388328552246, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9974594116210938, 'eval_runtime': 8.3163, 'eval_samples_per_second': 120.126, 'eval_steps_per_second': 7.575, 'epoch': 0.96}
{'loss': 1.0666, 'grad_norm': 0.19095292687416077, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9960481524467468, 'eval_runtime': 8.3089, 'eval_samples_per_second': 120.232, 'eval_steps_per_second': 7.582, 'epoch': 1.0}
{'train_runtime': 450.018, 'train_samples_per_second': 22.217, 'train_steps_per_second': 1.389, 'train_loss': 1.397186895751953, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3519749641418457, 1.8068739175796509, 1.530853033065796, 1.3941439390182495, 1.2957353591918945, 1.253612995147705, 1.225887417793274, 1.2089767456054688, 1.1895915269851685, 1.1718578338623047, 1.1598013639450073, 1.1433980464935303, 1.1270934343338013, 1.1110143661499023, 1.0964070558547974, 1.081355333328247, 1.0679041147232056, 1.0573160648345947, 1.043606162071228, 1.0330967903137207, 1.0172219276428223, 1.009533166885376, 1.002882719039917, 0.9974594116210938, 0.9960481524467468], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:55,  4.32it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 224.14it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 307.78it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 353.24it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 382.02it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 406.13it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 409.16it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8141338229179382
current iteration best possible performance (full train run):  0.756
max performance so far:  0.8085000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3079 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.8230403465504071
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.8173814660408113
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.7975039907647354
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.823769750410111
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.823783786055163
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0832, dtype=torch.float64), tensor(0.2430, dtype=torch.float64), 0, tensor(0.1697, dtype=torch.float64), tensor(0.1188, dtype=torch.float64), tensor(0.3853, dtype=torch.float64), 0, 32, 1, 1, 1, 0, 1, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(5.9300e-17, dtype=torch.float64), tensor(0.0832, dtype=torch.float64), tensor(0.2430, dtype=torch.float64), tensor(1.9764e-17, dtype=torch.float64), tensor(0.1697, dtype=torch.float64), tensor(0.1188, dtype=torch.float64), tensor(0.3853, dtype=torch.float64), tensor(2.2731e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.083
  sciq: 0.243
  triviaqa: 0
  truthfulqa_gen: 0.17
  wikitext: 0.119
  mmlu: 0.385
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 262.00it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 437.04it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 508.44it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 550.89it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 581.23it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 604.11it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 592.58it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.9928, 'grad_norm': 0.13194045424461365, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.0424087047576904, 'eval_runtime': 9.9971, 'eval_samples_per_second': 99.929, 'eval_steps_per_second': 6.302, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 261.97it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 436.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 507.28it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 550.89it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 581.65it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 604.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 592.68it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.3112, 'grad_norm': 0.06322666257619858, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.9603246450424194, 'eval_runtime': 10.0082, 'eval_samples_per_second': 99.818, 'eval_steps_per_second': 6.295, 'epoch': 0.08}
{'loss': 1.798, 'grad_norm': 0.06438518315553665, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6875979900360107, 'eval_runtime': 9.9852, 'eval_samples_per_second': 100.048, 'eval_steps_per_second': 6.309, 'epoch': 0.12}
{'loss': 1.67, 'grad_norm': 0.0383816622197628, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6081486940383911, 'eval_runtime': 10.0254, 'eval_samples_per_second': 99.647, 'eval_steps_per_second': 6.284, 'epoch': 0.16}
{'loss': 1.5985, 'grad_norm': 0.04699745774269104, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.563847303390503, 'eval_runtime': 10.0263, 'eval_samples_per_second': 99.638, 'eval_steps_per_second': 6.283, 'epoch': 0.2}
{'loss': 1.5751, 'grad_norm': 0.05198562890291214, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5277431011199951, 'eval_runtime': 10.0481, 'eval_samples_per_second': 99.421, 'eval_steps_per_second': 6.27, 'epoch': 0.24}
{'loss': 1.5871, 'grad_norm': 0.05575455725193024, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4999001026153564, 'eval_runtime': 10.1085, 'eval_samples_per_second': 98.828, 'eval_steps_per_second': 6.232, 'epoch': 0.28}
{'loss': 1.4569, 'grad_norm': 0.05198954790830612, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4718365669250488, 'eval_runtime': 10.1246, 'eval_samples_per_second': 98.671, 'eval_steps_per_second': 6.222, 'epoch': 0.32}
{'loss': 1.4818, 'grad_norm': 0.05858549848198891, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.437301754951477, 'eval_runtime': 10.114, 'eval_samples_per_second': 98.774, 'eval_steps_per_second': 6.229, 'epoch': 0.36}
{'loss': 1.399, 'grad_norm': 0.05445392429828644, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3776105642318726, 'eval_runtime': 10.0952, 'eval_samples_per_second': 98.958, 'eval_steps_per_second': 6.241, 'epoch': 0.4}
{'loss': 1.4131, 'grad_norm': 0.05468608811497688, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3585964441299438, 'eval_runtime': 10.096, 'eval_samples_per_second': 98.95, 'eval_steps_per_second': 6.24, 'epoch': 0.44}
{'loss': 1.3611, 'grad_norm': 0.04920761287212372, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3392105102539062, 'eval_runtime': 10.0905, 'eval_samples_per_second': 99.004, 'eval_steps_per_second': 6.243, 'epoch': 0.48}
{'loss': 1.3343, 'grad_norm': 0.0583990253508091, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3105822801589966, 'eval_runtime': 10.0886, 'eval_samples_per_second': 99.023, 'eval_steps_per_second': 6.245, 'epoch': 0.52}
{'loss': 1.3103, 'grad_norm': 0.06787019968032837, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2942790985107422, 'eval_runtime': 10.0944, 'eval_samples_per_second': 98.965, 'eval_steps_per_second': 6.241, 'epoch': 0.56}
{'loss': 1.3302, 'grad_norm': 0.05584893748164177, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.285752534866333, 'eval_runtime': 10.0861, 'eval_samples_per_second': 99.048, 'eval_steps_per_second': 6.246, 'epoch': 0.6}
{'loss': 1.284, 'grad_norm': 0.04471792280673981, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2812237739562988, 'eval_runtime': 10.0935, 'eval_samples_per_second': 98.975, 'eval_steps_per_second': 6.242, 'epoch': 0.64}
{'loss': 1.3423, 'grad_norm': 0.05521281063556671, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2762833833694458, 'eval_runtime': 10.0885, 'eval_samples_per_second': 99.024, 'eval_steps_per_second': 6.245, 'epoch': 0.68}
{'loss': 1.2747, 'grad_norm': 0.07071042060852051, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2720950841903687, 'eval_runtime': 10.0938, 'eval_samples_per_second': 98.972, 'eval_steps_per_second': 6.241, 'epoch': 0.72}
{'loss': 1.3311, 'grad_norm': 0.05312894284725189, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2679840326309204, 'eval_runtime': 10.0811, 'eval_samples_per_second': 99.096, 'eval_steps_per_second': 6.249, 'epoch': 0.76}
{'loss': 1.2859, 'grad_norm': 0.05502571165561676, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2647002935409546, 'eval_runtime': 10.0852, 'eval_samples_per_second': 99.056, 'eval_steps_per_second': 6.247, 'epoch': 0.8}
{'loss': 1.3336, 'grad_norm': 0.05543392524123192, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.261836051940918, 'eval_runtime': 10.0889, 'eval_samples_per_second': 99.02, 'eval_steps_per_second': 6.244, 'epoch': 0.84}
{'loss': 1.278, 'grad_norm': 0.05935895815491676, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.259350299835205, 'eval_runtime': 10.0964, 'eval_samples_per_second': 98.946, 'eval_steps_per_second': 6.24, 'epoch': 0.88}
{'loss': 1.2933, 'grad_norm': 0.05980067700147629, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.25721275806427, 'eval_runtime': 10.0969, 'eval_samples_per_second': 98.941, 'eval_steps_per_second': 6.24, 'epoch': 0.92}
{'loss': 1.3037, 'grad_norm': 0.0703047439455986, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2563731670379639, 'eval_runtime': 10.0904, 'eval_samples_per_second': 99.005, 'eval_steps_per_second': 6.244, 'epoch': 0.96}
{'loss': 1.2641, 'grad_norm': 0.0669073686003685, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2558588981628418, 'eval_runtime': 10.0911, 'eval_samples_per_second': 98.998, 'eval_steps_per_second': 6.243, 'epoch': 1.0}
{'train_runtime': 511.0793, 'train_samples_per_second': 19.563, 'train_steps_per_second': 1.223, 'train_loss': 1.544405093383789, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0424087047576904, 1.9603246450424194, 1.6875979900360107, 1.6081486940383911, 1.563847303390503, 1.5277431011199951, 1.4999001026153564, 1.4718365669250488, 1.437301754951477, 1.3776105642318726, 1.3585964441299438, 1.3392105102539062, 1.3105822801589966, 1.2942790985107422, 1.285752534866333, 1.2812237739562988, 1.2762833833694458, 1.2720950841903687, 1.2679840326309204, 1.2647002935409546, 1.261836051940918, 1.259350299835205, 1.25721275806427, 1.2563731670379639, 1.2558588981628418], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:43,  1.45it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 87.15it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 163.10it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 229.64it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 285.80it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 334.35it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 263.76it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8473111391067505
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4424 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.8194710003954169
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.8194840088828685
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.8194851748937448
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.7798653431131587
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.8194796989241013
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.8378, dtype=torch.float64), 0, tensor(0.1622, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 5.204170427930422e-19, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(3.8974e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.2480e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8378, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1622, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.2042e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.838
  mmlu: 0
  arc_challenge: 0.162

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.204170427930422e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  5.204170427930422e-19
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 285.24it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 475.73it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 552.83it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 601.56it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 634.77it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 660.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 646.97it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.3601, 'grad_norm': 0.13716521859169006, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.0948479175567627, 'eval_runtime': 8.9851, 'eval_samples_per_second': 111.185, 'eval_steps_per_second': 7.012, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 285.86it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 475.08it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 553.93it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 602.45it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 636.74it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 661.64it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 648.24it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.7027, 'grad_norm': 0.1392183154821396, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.2182044982910156, 'eval_runtime': 8.9742, 'eval_samples_per_second': 111.32, 'eval_steps_per_second': 7.02, 'epoch': 0.08}
{'loss': 1.955, 'grad_norm': 0.10361940413713455, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9860602617263794, 'eval_runtime': 8.9778, 'eval_samples_per_second': 111.275, 'eval_steps_per_second': 7.017, 'epoch': 0.12}
{'loss': 1.9315, 'grad_norm': 0.06943400949239731, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9346164464950562, 'eval_runtime': 9.001, 'eval_samples_per_second': 110.988, 'eval_steps_per_second': 6.999, 'epoch': 0.16}
{'loss': 1.8417, 'grad_norm': 0.11359331011772156, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9017080068588257, 'eval_runtime': 9.0355, 'eval_samples_per_second': 110.564, 'eval_steps_per_second': 6.972, 'epoch': 0.2}
{'loss': 1.8448, 'grad_norm': 0.2073076218366623, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8853836059570312, 'eval_runtime': 9.0858, 'eval_samples_per_second': 109.951, 'eval_steps_per_second': 6.934, 'epoch': 0.24}
{'loss': 1.7638, 'grad_norm': 0.20003898441791534, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8704053163528442, 'eval_runtime': 9.0515, 'eval_samples_per_second': 110.368, 'eval_steps_per_second': 6.96, 'epoch': 0.28}
{'loss': 1.8094, 'grad_norm': 0.14875733852386475, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8637027740478516, 'eval_runtime': 9.0644, 'eval_samples_per_second': 110.211, 'eval_steps_per_second': 6.95, 'epoch': 0.32}
{'loss': 1.8051, 'grad_norm': 0.1490861028432846, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.85881769657135, 'eval_runtime': 9.0666, 'eval_samples_per_second': 110.185, 'eval_steps_per_second': 6.949, 'epoch': 0.36}
{'loss': 1.7929, 'grad_norm': 0.1137714684009552, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8476215600967407, 'eval_runtime': 9.0372, 'eval_samples_per_second': 110.543, 'eval_steps_per_second': 6.971, 'epoch': 0.4}
{'loss': 1.7625, 'grad_norm': 0.07850456982851028, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8439582586288452, 'eval_runtime': 9.0187, 'eval_samples_per_second': 110.769, 'eval_steps_per_second': 6.985, 'epoch': 0.44}
{'loss': 1.7745, 'grad_norm': 0.07825499773025513, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8350814580917358, 'eval_runtime': 8.9944, 'eval_samples_per_second': 111.069, 'eval_steps_per_second': 7.004, 'epoch': 0.48}
{'loss': 1.7987, 'grad_norm': 0.07336164265871048, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8353958129882812, 'eval_runtime': 8.9958, 'eval_samples_per_second': 111.051, 'eval_steps_per_second': 7.003, 'epoch': 0.52}
{'loss': 1.8599, 'grad_norm': 0.2067273110151291, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8309783935546875, 'eval_runtime': 8.9853, 'eval_samples_per_second': 111.181, 'eval_steps_per_second': 7.011, 'epoch': 0.56}
{'loss': 1.7231, 'grad_norm': 0.10446146130561829, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.8260529041290283, 'eval_runtime': 8.9991, 'eval_samples_per_second': 111.012, 'eval_steps_per_second': 7.001, 'epoch': 0.6}
{'loss': 1.7654, 'grad_norm': 0.14947722852230072, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8233916759490967, 'eval_runtime': 9.017, 'eval_samples_per_second': 110.791, 'eval_steps_per_second': 6.987, 'epoch': 0.64}
{'loss': 1.8489, 'grad_norm': 0.11103545129299164, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8197402954101562, 'eval_runtime': 9.0332, 'eval_samples_per_second': 110.592, 'eval_steps_per_second': 6.974, 'epoch': 0.68}
{'loss': 1.8441, 'grad_norm': 0.12139714509248734, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8186198472976685, 'eval_runtime': 9.0372, 'eval_samples_per_second': 110.543, 'eval_steps_per_second': 6.971, 'epoch': 0.72}
{'loss': 1.7791, 'grad_norm': 0.11652826517820358, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.816640019416809, 'eval_runtime': 9.0365, 'eval_samples_per_second': 110.551, 'eval_steps_per_second': 6.972, 'epoch': 0.76}
{'loss': 1.749, 'grad_norm': 0.06241023167967796, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.814139723777771, 'eval_runtime': 9.0414, 'eval_samples_per_second': 110.491, 'eval_steps_per_second': 6.968, 'epoch': 0.8}
{'loss': 1.791, 'grad_norm': 0.06637225300073624, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8119374513626099, 'eval_runtime': 9.0384, 'eval_samples_per_second': 110.528, 'eval_steps_per_second': 6.97, 'epoch': 0.84}
{'loss': 1.8006, 'grad_norm': 0.08038341999053955, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8109714984893799, 'eval_runtime': 9.036, 'eval_samples_per_second': 110.558, 'eval_steps_per_second': 6.972, 'epoch': 0.88}
{'loss': 1.848, 'grad_norm': 0.09705156832933426, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.809164047241211, 'eval_runtime': 9.0288, 'eval_samples_per_second': 110.645, 'eval_steps_per_second': 6.978, 'epoch': 0.92}
{'loss': 1.8744, 'grad_norm': 0.3251934051513672, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8085626363754272, 'eval_runtime': 8.9904, 'eval_samples_per_second': 111.119, 'eval_steps_per_second': 7.008, 'epoch': 0.96}
{'loss': 1.7271, 'grad_norm': 0.11019685119390488, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8082364797592163, 'eval_runtime': 8.9853, 'eval_samples_per_second': 111.181, 'eval_steps_per_second': 7.011, 'epoch': 1.0}
{'train_runtime': 461.712, 'train_samples_per_second': 21.656, 'train_steps_per_second': 1.354, 'train_loss': 1.9101390319824219, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0948479175567627, 2.2182044982910156, 1.9860602617263794, 1.9346164464950562, 1.9017080068588257, 1.8853836059570312, 1.8704053163528442, 1.8637027740478516, 1.85881769657135, 1.8476215600967407, 1.8439582586288452, 1.8350814580917358, 1.8353958129882812, 1.8309783935546875, 1.8260529041290283, 1.8233916759490967, 1.8197402954101562, 1.8186198472976685, 1.816640019416809, 1.814139723777771, 1.8119374513626099, 1.8109714984893799, 1.809164047241211, 1.8085626363754272, 1.8082364797592163], 'performance': [0.75, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:42,  4.89it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 165.49it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 264.97it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 334.11it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 382.47it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 422.18it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 398.43it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.8472181558609009
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4759 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.8059215535086215
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.8166827654403683
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.767140676553601
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.8143722395128342
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8177171490053867
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2425, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0888, dtype=torch.float64), 0, tensor(0.6687, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 2, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2425, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9820e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1649e-17, dtype=torch.float64), tensor(0.0888, dtype=torch.float64), tensor(2.1103e-17, dtype=torch.float64), tensor(0.6687, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.242
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.089
  mmlu: 0
  arc_challenge: 0.669

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,211,264 || all params: 8,033,472,512 || trainable%: 0.0400
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 253.34it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 422.84it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 492.10it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 533.91it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 563.51it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 584.11it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 573.72it/s]
Evaluation performance at step 25: 0.76
{'loss': 2.1422, 'grad_norm': 3.0039122104644775, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 1.0918571949005127, 'eval_runtime': 10.1585, 'eval_samples_per_second': 98.341, 'eval_steps_per_second': 6.202, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 253.28it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 421.72it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 490.63it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 532.77it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 562.87it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 585.75it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 573.81it/s]
Evaluation performance at step 50: 0.72
{'loss': 1.0138, 'grad_norm': 1.9302797317504883, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 0.9335551261901855, 'eval_runtime': 10.1759, 'eval_samples_per_second': 98.174, 'eval_steps_per_second': 6.191, 'epoch': 0.08}
{'loss': 0.9364, 'grad_norm': 2.1161816120147705, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8842933773994446, 'eval_runtime': 10.1738, 'eval_samples_per_second': 98.194, 'eval_steps_per_second': 6.192, 'epoch': 0.12}
{'loss': 0.867, 'grad_norm': 1.8355504274368286, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8415589928627014, 'eval_runtime': 10.1541, 'eval_samples_per_second': 98.384, 'eval_steps_per_second': 6.204, 'epoch': 0.16}
{'loss': 0.8441, 'grad_norm': 2.207247018814087, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8073928356170654, 'eval_runtime': 10.1415, 'eval_samples_per_second': 98.507, 'eval_steps_per_second': 6.212, 'epoch': 0.2}
{'loss': 0.801, 'grad_norm': 1.660864233970642, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7667898535728455, 'eval_runtime': 10.148, 'eval_samples_per_second': 98.443, 'eval_steps_per_second': 6.208, 'epoch': 0.24}
{'loss': 0.7783, 'grad_norm': 1.9018502235412598, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7333735227584839, 'eval_runtime': 10.1661, 'eval_samples_per_second': 98.268, 'eval_steps_per_second': 6.197, 'epoch': 0.28}
{'loss': 0.7437, 'grad_norm': 1.87079656124115, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6981223225593567, 'eval_runtime': 10.2276, 'eval_samples_per_second': 97.677, 'eval_steps_per_second': 6.16, 'epoch': 0.32}
{'loss': 0.7062, 'grad_norm': 2.2780771255493164, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.669554591178894, 'eval_runtime': 10.2387, 'eval_samples_per_second': 97.571, 'eval_steps_per_second': 6.153, 'epoch': 0.36}
{'loss': 0.6578, 'grad_norm': 2.70798921585083, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6408242583274841, 'eval_runtime': 10.2354, 'eval_samples_per_second': 97.602, 'eval_steps_per_second': 6.155, 'epoch': 0.4}
{'loss': 0.6782, 'grad_norm': 2.2512972354888916, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6163696646690369, 'eval_runtime': 10.2486, 'eval_samples_per_second': 97.477, 'eval_steps_per_second': 6.147, 'epoch': 0.44}
{'loss': 0.6528, 'grad_norm': 2.336717128753662, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5964853167533875, 'eval_runtime': 10.2415, 'eval_samples_per_second': 97.544, 'eval_steps_per_second': 6.151, 'epoch': 0.48}
{'loss': 0.5951, 'grad_norm': 2.047790288925171, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5710427165031433, 'eval_runtime': 10.246, 'eval_samples_per_second': 97.502, 'eval_steps_per_second': 6.149, 'epoch': 0.52}
{'loss': 0.6076, 'grad_norm': 2.213641881942749, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5524718761444092, 'eval_runtime': 10.2376, 'eval_samples_per_second': 97.582, 'eval_steps_per_second': 6.154, 'epoch': 0.56}
{'loss': 0.5909, 'grad_norm': 2.023113489151001, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5337727069854736, 'eval_runtime': 10.2526, 'eval_samples_per_second': 97.439, 'eval_steps_per_second': 6.145, 'epoch': 0.6}
{'loss': 0.6037, 'grad_norm': 2.164693832397461, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5217150449752808, 'eval_runtime': 10.2121, 'eval_samples_per_second': 97.825, 'eval_steps_per_second': 6.169, 'epoch': 0.64}
{'loss': 0.5523, 'grad_norm': 2.4366540908813477, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5026659369468689, 'eval_runtime': 10.1777, 'eval_samples_per_second': 98.156, 'eval_steps_per_second': 6.19, 'epoch': 0.68}
{'loss': 0.5046, 'grad_norm': 2.6732475757598877, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.49011510610580444, 'eval_runtime': 10.1784, 'eval_samples_per_second': 98.149, 'eval_steps_per_second': 6.19, 'epoch': 0.72}
{'loss': 0.52, 'grad_norm': 1.888737678527832, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4769585132598877, 'eval_runtime': 10.2579, 'eval_samples_per_second': 97.388, 'eval_steps_per_second': 6.142, 'epoch': 0.76}
{'loss': 0.504, 'grad_norm': 1.656242847442627, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4689734876155853, 'eval_runtime': 10.2671, 'eval_samples_per_second': 97.302, 'eval_steps_per_second': 6.136, 'epoch': 0.8}
{'loss': 0.5462, 'grad_norm': 2.4074103832244873, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4552502930164337, 'eval_runtime': 10.3155, 'eval_samples_per_second': 96.844, 'eval_steps_per_second': 6.107, 'epoch': 0.84}
{'loss': 0.4695, 'grad_norm': 2.220017910003662, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.4495908319950104, 'eval_runtime': 10.3063, 'eval_samples_per_second': 96.931, 'eval_steps_per_second': 6.113, 'epoch': 0.88}
{'loss': 0.5273, 'grad_norm': 1.9333343505859375, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.44446641206741333, 'eval_runtime': 10.3277, 'eval_samples_per_second': 96.73, 'eval_steps_per_second': 6.1, 'epoch': 0.92}
{'loss': 0.4665, 'grad_norm': 1.7273259162902832, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.44025009870529175, 'eval_runtime': 10.3023, 'eval_samples_per_second': 96.968, 'eval_steps_per_second': 6.115, 'epoch': 0.96}
{'loss': 0.5202, 'grad_norm': 2.859740972518921, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.4388747215270996, 'eval_runtime': 10.2942, 'eval_samples_per_second': 97.045, 'eval_steps_per_second': 6.12, 'epoch': 1.0}
{'train_runtime': 527.5798, 'train_samples_per_second': 18.951, 'train_steps_per_second': 1.185, 'train_loss': 0.7131777236938477, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0918571949005127, 0.9335551261901855, 0.8842933773994446, 0.8415589928627014, 0.8073928356170654, 0.7667898535728455, 0.7333735227584839, 0.6981223225593567, 0.669554591178894, 0.6408242583274841, 0.6163696646690369, 0.5964853167533875, 0.5710427165031433, 0.5524718761444092, 0.5337727069854736, 0.5217150449752808, 0.5026659369468689, 0.49011510610580444, 0.4769585132598877, 0.4689734876155853, 0.4552502930164337, 0.4495908319950104, 0.44446641206741333, 0.44025009870529175, 0.4388747215270996], 'performance': [0.76, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:53,  4.39it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 163.14it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 253.63it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 313.12it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 353.96it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 385.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 370.68it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  0.8321012854576111
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4782 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7688419305559183
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.8060189210000284
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.8222201855739742
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.7741173845908202
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8246864170744332
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.1136, dtype=torch.float64), tensor(0.8864, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 21.96940284060174, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(5.0825e-17, dtype=torch.float64), tensor(6.8172e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1136, dtype=torch.float64), tensor(0.8864, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.9090e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4577, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.114
  wikitext: 0.886
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (21.96940284060174,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  21.96940284060174
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.15it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.47it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 501.74it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 544.70it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 574.88it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 597.88it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 585.69it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.001, 'grad_norm': 0.7611591815948486, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.307121992111206, 'eval_runtime': 9.8202, 'eval_samples_per_second': 101.729, 'eval_steps_per_second': 6.415, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 256.98it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 426.91it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 497.97it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 541.67it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 572.05it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 595.41it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 582.90it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.1125, 'grad_norm': 0.6419879198074341, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 2.0526974201202393, 'eval_runtime': 9.8128, 'eval_samples_per_second': 101.806, 'eval_steps_per_second': 6.42, 'epoch': 0.08}
{'loss': 2.0531, 'grad_norm': 0.3616012930870056, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9919676780700684, 'eval_runtime': 9.8043, 'eval_samples_per_second': 101.894, 'eval_steps_per_second': 6.426, 'epoch': 0.12}
{'loss': 2.0158, 'grad_norm': 0.8156895041465759, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9576998949050903, 'eval_runtime': 9.7942, 'eval_samples_per_second': 101.999, 'eval_steps_per_second': 6.432, 'epoch': 0.16}
{'loss': 2.0255, 'grad_norm': 0.3187749683856964, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.940201997756958, 'eval_runtime': 9.7864, 'eval_samples_per_second': 102.08, 'eval_steps_per_second': 6.437, 'epoch': 0.2}
{'loss': 1.9692, 'grad_norm': 0.3524799048900604, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9148179292678833, 'eval_runtime': 9.8153, 'eval_samples_per_second': 101.78, 'eval_steps_per_second': 6.419, 'epoch': 0.24}
{'loss': 2.1129, 'grad_norm': 0.33289068937301636, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.900659203529358, 'eval_runtime': 9.8175, 'eval_samples_per_second': 101.758, 'eval_steps_per_second': 6.417, 'epoch': 0.28}
{'loss': 1.9776, 'grad_norm': 0.2609406113624573, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8826223611831665, 'eval_runtime': 9.8422, 'eval_samples_per_second': 101.502, 'eval_steps_per_second': 6.401, 'epoch': 0.32}
{'loss': 1.9957, 'grad_norm': 1.045224905014038, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.873855471611023, 'eval_runtime': 9.8523, 'eval_samples_per_second': 101.398, 'eval_steps_per_second': 6.394, 'epoch': 0.36}
{'loss': 1.9464, 'grad_norm': 0.645200252532959, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8403738737106323, 'eval_runtime': 9.8552, 'eval_samples_per_second': 101.368, 'eval_steps_per_second': 6.393, 'epoch': 0.4}
{'loss': 1.9339, 'grad_norm': 0.361948162317276, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.821456789970398, 'eval_runtime': 9.8531, 'eval_samples_per_second': 101.39, 'eval_steps_per_second': 6.394, 'epoch': 0.44}
{'loss': 1.9029, 'grad_norm': 0.28092217445373535, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8046890497207642, 'eval_runtime': 9.8491, 'eval_samples_per_second': 101.43, 'eval_steps_per_second': 6.397, 'epoch': 0.48}
{'loss': 1.8965, 'grad_norm': 0.4022601246833801, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7920266389846802, 'eval_runtime': 9.8637, 'eval_samples_per_second': 101.281, 'eval_steps_per_second': 6.387, 'epoch': 0.52}
{'loss': 1.8482, 'grad_norm': 0.2626754343509674, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7799711227416992, 'eval_runtime': 9.8409, 'eval_samples_per_second': 101.515, 'eval_steps_per_second': 6.402, 'epoch': 0.56}
{'loss': 1.9025, 'grad_norm': 0.6001999974250793, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7693651914596558, 'eval_runtime': 9.8299, 'eval_samples_per_second': 101.629, 'eval_steps_per_second': 6.409, 'epoch': 0.6}
{'loss': 1.8886, 'grad_norm': 0.3054195046424866, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7525237798690796, 'eval_runtime': 9.8002, 'eval_samples_per_second': 101.936, 'eval_steps_per_second': 6.428, 'epoch': 0.64}
{'loss': 1.9315, 'grad_norm': 0.33873769640922546, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7362920045852661, 'eval_runtime': 9.7948, 'eval_samples_per_second': 101.993, 'eval_steps_per_second': 6.432, 'epoch': 0.68}
{'loss': 1.8784, 'grad_norm': 0.2860366702079773, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7282074689865112, 'eval_runtime': 9.8068, 'eval_samples_per_second': 101.868, 'eval_steps_per_second': 6.424, 'epoch': 0.72}
{'loss': 1.8356, 'grad_norm': 0.2998013496398926, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7144391536712646, 'eval_runtime': 9.804, 'eval_samples_per_second': 101.898, 'eval_steps_per_second': 6.426, 'epoch': 0.76}
{'loss': 1.8481, 'grad_norm': 0.3425157368183136, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7049579620361328, 'eval_runtime': 9.797, 'eval_samples_per_second': 101.97, 'eval_steps_per_second': 6.431, 'epoch': 0.8}
{'loss': 1.8366, 'grad_norm': 0.397114634513855, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.698426604270935, 'eval_runtime': 9.7878, 'eval_samples_per_second': 102.066, 'eval_steps_per_second': 6.437, 'epoch': 0.84}
{'loss': 1.8365, 'grad_norm': 0.41249459981918335, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6909540891647339, 'eval_runtime': 9.7898, 'eval_samples_per_second': 102.045, 'eval_steps_per_second': 6.435, 'epoch': 0.88}
{'loss': 1.8319, 'grad_norm': 0.36620932817459106, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6856836080551147, 'eval_runtime': 9.8417, 'eval_samples_per_second': 101.507, 'eval_steps_per_second': 6.401, 'epoch': 0.92}
{'loss': 1.869, 'grad_norm': 0.25760167837142944, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6823421716690063, 'eval_runtime': 9.8788, 'eval_samples_per_second': 101.126, 'eval_steps_per_second': 6.377, 'epoch': 0.96}
{'loss': 1.8626, 'grad_norm': 0.40188443660736084, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6819326877593994, 'eval_runtime': 9.8631, 'eval_samples_per_second': 101.287, 'eval_steps_per_second': 6.387, 'epoch': 1.0}
{'train_runtime': 504.0146, 'train_samples_per_second': 19.839, 'train_steps_per_second': 1.24, 'train_loss': 1.9725018493652344, 'epoch': 1.0}
train_results:  {'eval_loss': [2.307121992111206, 2.0526974201202393, 1.9919676780700684, 1.9576998949050903, 1.940201997756958, 1.9148179292678833, 1.900659203529358, 1.8826223611831665, 1.873855471611023, 1.8403738737106323, 1.821456789970398, 1.8046890497207642, 1.7920266389846802, 1.7799711227416992, 1.7693651914596558, 1.7525237798690796, 1.7362920045852661, 1.7282074689865112, 1.7144391536712646, 1.7049579620361328, 1.698426604270935, 1.6909540891647339, 1.6856836080551147, 1.6823421716690063, 1.6819326877593994], 'performance': [0.74, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:51,  4.48it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 231.10it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 317.38it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 363.97it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 393.20it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 417.83it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 421.67it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.8389402031898499
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9770 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.8158914871651713
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.7566283258819697
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.8212826079879366
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.8195342095144709
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.7673643486168457
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.6240, dtype=torch.float64), tensor(0.0645, dtype=torch.float64), 0, tensor(0.3115, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 1.4796357832869638e-18, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.8204e-17, dtype=torch.float64), tensor(0.6240, dtype=torch.float64), tensor(0.0645, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3115, dtype=torch.float64), tensor(3.7784e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.4796e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.624
  sciq: 0.064
  triviaqa: 0
  truthfulqa_gen: 0.312
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.4796357832869638e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  1.4796357832869638e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.40it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 431.21it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 502.53it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 545.05it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 574.62it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 597.28it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 585.82it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.1727, 'grad_norm': 0.18961797654628754, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2821414470672607, 'eval_runtime': 10.6478, 'eval_samples_per_second': 93.822, 'eval_steps_per_second': 5.917, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 256.38it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 427.75it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 497.72it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 541.35it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 572.03it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 595.95it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 583.14it/s]
Evaluation performance at step 50: 0.72
{'loss': 2.59, 'grad_norm': 0.21510636806488037, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 2.0822339057922363, 'eval_runtime': 10.6736, 'eval_samples_per_second': 93.596, 'eval_steps_per_second': 5.902, 'epoch': 0.08}
{'loss': 1.8788, 'grad_norm': 0.039376337081193924, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8027021884918213, 'eval_runtime': 10.683, 'eval_samples_per_second': 93.513, 'eval_steps_per_second': 5.897, 'epoch': 0.12}
{'loss': 1.7272, 'grad_norm': 0.0720209851861, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7033014297485352, 'eval_runtime': 10.7199, 'eval_samples_per_second': 93.191, 'eval_steps_per_second': 5.877, 'epoch': 0.16}
{'loss': 1.6768, 'grad_norm': 0.04415464773774147, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6729121208190918, 'eval_runtime': 10.7321, 'eval_samples_per_second': 93.085, 'eval_steps_per_second': 5.87, 'epoch': 0.2}
{'loss': 1.6227, 'grad_norm': 0.04823135584592819, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6573259830474854, 'eval_runtime': 10.6944, 'eval_samples_per_second': 93.414, 'eval_steps_per_second': 5.891, 'epoch': 0.24}
{'loss': 1.6677, 'grad_norm': 0.0441962406039238, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6456084251403809, 'eval_runtime': 10.6541, 'eval_samples_per_second': 93.766, 'eval_steps_per_second': 5.913, 'epoch': 0.28}
{'loss': 1.6347, 'grad_norm': 0.04684075340628624, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6365920305252075, 'eval_runtime': 10.6546, 'eval_samples_per_second': 93.762, 'eval_steps_per_second': 5.913, 'epoch': 0.32}
{'loss': 1.6177, 'grad_norm': 0.049934279173612595, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.630628228187561, 'eval_runtime': 10.6634, 'eval_samples_per_second': 93.685, 'eval_steps_per_second': 5.908, 'epoch': 0.36}
{'loss': 1.6144, 'grad_norm': 0.04702191427350044, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6244953870773315, 'eval_runtime': 10.7296, 'eval_samples_per_second': 93.107, 'eval_steps_per_second': 5.872, 'epoch': 0.4}
{'loss': 1.5876, 'grad_norm': 0.04363008216023445, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6198780536651611, 'eval_runtime': 10.7876, 'eval_samples_per_second': 92.606, 'eval_steps_per_second': 5.84, 'epoch': 0.44}
{'loss': 1.6206, 'grad_norm': 0.04830896109342575, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6142138242721558, 'eval_runtime': 10.7543, 'eval_samples_per_second': 92.893, 'eval_steps_per_second': 5.858, 'epoch': 0.48}
{'loss': 1.6072, 'grad_norm': 0.04513902589678764, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6082810163497925, 'eval_runtime': 10.7496, 'eval_samples_per_second': 92.934, 'eval_steps_per_second': 5.861, 'epoch': 0.52}
{'loss': 1.5656, 'grad_norm': 0.04281943291425705, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.604132890701294, 'eval_runtime': 10.7297, 'eval_samples_per_second': 93.106, 'eval_steps_per_second': 5.872, 'epoch': 0.56}
{'loss': 1.5983, 'grad_norm': 0.049755875021219254, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6007559299468994, 'eval_runtime': 10.7561, 'eval_samples_per_second': 92.877, 'eval_steps_per_second': 5.857, 'epoch': 0.6}
{'loss': 1.5759, 'grad_norm': 0.060555893927812576, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5975511074066162, 'eval_runtime': 10.7432, 'eval_samples_per_second': 92.989, 'eval_steps_per_second': 5.864, 'epoch': 0.64}
{'loss': 1.5701, 'grad_norm': 0.047710176557302475, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5948443412780762, 'eval_runtime': 10.7417, 'eval_samples_per_second': 93.002, 'eval_steps_per_second': 5.865, 'epoch': 0.68}
{'loss': 1.5622, 'grad_norm': 0.0446891188621521, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5923992395401, 'eval_runtime': 10.7474, 'eval_samples_per_second': 92.953, 'eval_steps_per_second': 5.862, 'epoch': 0.72}
{'loss': 1.6101, 'grad_norm': 0.05583074316382408, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5885844230651855, 'eval_runtime': 10.7835, 'eval_samples_per_second': 92.642, 'eval_steps_per_second': 5.842, 'epoch': 0.76}
{'loss': 1.5863, 'grad_norm': 0.04984060674905777, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.586854100227356, 'eval_runtime': 10.7651, 'eval_samples_per_second': 92.8, 'eval_steps_per_second': 5.852, 'epoch': 0.8}
{'loss': 1.5559, 'grad_norm': 0.05125589296221733, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.584924578666687, 'eval_runtime': 10.7678, 'eval_samples_per_second': 92.777, 'eval_steps_per_second': 5.851, 'epoch': 0.84}
{'loss': 1.5632, 'grad_norm': 0.06670062988996506, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.5828953981399536, 'eval_runtime': 10.7737, 'eval_samples_per_second': 92.725, 'eval_steps_per_second': 5.848, 'epoch': 0.88}
{'loss': 1.5627, 'grad_norm': 0.05337398499250412, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.58196222782135, 'eval_runtime': 10.7648, 'eval_samples_per_second': 92.803, 'eval_steps_per_second': 5.852, 'epoch': 0.92}
{'loss': 1.5592, 'grad_norm': 0.0465533584356308, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.5812466144561768, 'eval_runtime': 10.7882, 'eval_samples_per_second': 92.602, 'eval_steps_per_second': 5.84, 'epoch': 0.96}
{'loss': 1.6032, 'grad_norm': 0.05315274000167847, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.5807169675827026, 'eval_runtime': 10.753, 'eval_samples_per_second': 92.904, 'eval_steps_per_second': 5.859, 'epoch': 1.0}
{'train_runtime': 550.7484, 'train_samples_per_second': 18.153, 'train_steps_per_second': 1.135, 'train_loss': 1.7572315002441405, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2821414470672607, 2.0822339057922363, 1.8027021884918213, 1.7033014297485352, 1.6729121208190918, 1.6573259830474854, 1.6456084251403809, 1.6365920305252075, 1.630628228187561, 1.6244953870773315, 1.6198780536651611, 1.6142138242721558, 1.6082810163497925, 1.604132890701294, 1.6007559299468994, 1.5975511074066162, 1.5948443412780762, 1.5923992395401, 1.5885844230651855, 1.586854100227356, 1.584924578666687, 1.5828953981399536, 1.58196222782135, 1.5812466144561768, 1.5807169675827026], 'performance': [0.74, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:51,  4.46it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:08, 47.98it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:03, 99.78it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:02<00:01, 154.20it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:02<00:00, 208.12it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:02<00:00, 260.74it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 198.15it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  0.8474618792533875
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7290 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.8200337751228802
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.8201053417406075
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.8184152498200242
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.8138975922131103
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.8202772622617847
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, 0, tensor(1.0000, dtype=torch.float64), 32, 1, 1, 0, 1, 0, 128, 0.08518208392622047, 1.4800000190734908, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6653e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.8723e-17, dtype=torch.float64), tensor(4.8072e-17, dtype=torch.float64), tensor(5.3948e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8518, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 1.0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.08518208392622047,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734908,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.08518208392622047
lora alpha:  1.4800000190734908
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 275.56it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 459.96it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 535.94it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 581.67it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 614.04it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 638.06it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 625.45it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.82, 'grad_norm': 0.3178843855857849, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.7987546920776367, 'eval_runtime': 7.4944, 'eval_samples_per_second': 133.299, 'eval_steps_per_second': 8.406, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 275.71it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 459.67it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 536.13it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 582.49it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 614.19it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 638.39it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 625.76it/s]
Evaluation performance at step 50: 0.74
{'loss': 1.8262, 'grad_norm': 0.25311723351478577, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.1601530313491821, 'eval_runtime': 7.0831, 'eval_samples_per_second': 141.039, 'eval_steps_per_second': 8.894, 'epoch': 0.08}
{'loss': 1.0242, 'grad_norm': 0.03960190713405609, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9281578660011292, 'eval_runtime': 7.0937, 'eval_samples_per_second': 140.829, 'eval_steps_per_second': 8.881, 'epoch': 0.12}
{'loss': 0.8871, 'grad_norm': 0.08598105609416962, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.850091814994812, 'eval_runtime': 7.0774, 'eval_samples_per_second': 141.154, 'eval_steps_per_second': 8.902, 'epoch': 0.16}
{'loss': 0.8353, 'grad_norm': 0.04832516610622406, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8233745694160461, 'eval_runtime': 7.0937, 'eval_samples_per_second': 140.83, 'eval_steps_per_second': 8.881, 'epoch': 0.2}
{'loss': 0.8472, 'grad_norm': 0.04141963645815849, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8130520582199097, 'eval_runtime': 7.1164, 'eval_samples_per_second': 140.38, 'eval_steps_per_second': 8.853, 'epoch': 0.24}
{'loss': 0.8245, 'grad_norm': 0.038952670991420746, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8042588829994202, 'eval_runtime': 7.1102, 'eval_samples_per_second': 140.502, 'eval_steps_per_second': 8.86, 'epoch': 0.28}
{'loss': 0.8109, 'grad_norm': 0.04524451121687889, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7986223101615906, 'eval_runtime': 7.1146, 'eval_samples_per_second': 140.416, 'eval_steps_per_second': 8.855, 'epoch': 0.32}
{'loss': 0.828, 'grad_norm': 0.04242555424571037, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7911428213119507, 'eval_runtime': 7.1143, 'eval_samples_per_second': 140.422, 'eval_steps_per_second': 8.855, 'epoch': 0.36}
{'loss': 0.8002, 'grad_norm': 0.046695563942193985, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7826070785522461, 'eval_runtime': 7.123, 'eval_samples_per_second': 140.251, 'eval_steps_per_second': 8.845, 'epoch': 0.4}
{'loss': 0.7919, 'grad_norm': 0.053407471626996994, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7765854001045227, 'eval_runtime': 7.1396, 'eval_samples_per_second': 139.924, 'eval_steps_per_second': 8.824, 'epoch': 0.44}
{'loss': 0.7825, 'grad_norm': 0.05288537219166756, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7678491473197937, 'eval_runtime': 7.1386, 'eval_samples_per_second': 139.944, 'eval_steps_per_second': 8.825, 'epoch': 0.48}
{'loss': 0.7703, 'grad_norm': 0.06454584002494812, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.762611985206604, 'eval_runtime': 7.1257, 'eval_samples_per_second': 140.198, 'eval_steps_per_second': 8.841, 'epoch': 0.52}
{'loss': 0.7593, 'grad_norm': 0.06358584761619568, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7553442120552063, 'eval_runtime': 7.12, 'eval_samples_per_second': 140.308, 'eval_steps_per_second': 8.848, 'epoch': 0.56}
{'loss': 0.7521, 'grad_norm': 0.06200133264064789, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7499810457229614, 'eval_runtime': 7.1317, 'eval_samples_per_second': 140.078, 'eval_steps_per_second': 8.834, 'epoch': 0.6}
{'loss': 0.7708, 'grad_norm': 0.06313662230968475, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7436454892158508, 'eval_runtime': 7.1213, 'eval_samples_per_second': 140.283, 'eval_steps_per_second': 8.847, 'epoch': 0.64}
{'loss': 0.738, 'grad_norm': 0.06703279912471771, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7384739518165588, 'eval_runtime': 7.1228, 'eval_samples_per_second': 140.254, 'eval_steps_per_second': 8.845, 'epoch': 0.68}
{'loss': 0.7535, 'grad_norm': 0.06142165884375572, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7327315211296082, 'eval_runtime': 7.1131, 'eval_samples_per_second': 140.444, 'eval_steps_per_second': 8.857, 'epoch': 0.72}
{'loss': 0.7484, 'grad_norm': 0.08261723816394806, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7284470796585083, 'eval_runtime': 7.1233, 'eval_samples_per_second': 140.244, 'eval_steps_per_second': 8.844, 'epoch': 0.76}
{'loss': 0.7444, 'grad_norm': 0.07263100892305374, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7234277725219727, 'eval_runtime': 7.114, 'eval_samples_per_second': 140.428, 'eval_steps_per_second': 8.856, 'epoch': 0.8}
{'loss': 0.7378, 'grad_norm': 0.07145612686872482, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7183235287666321, 'eval_runtime': 7.1371, 'eval_samples_per_second': 139.973, 'eval_steps_per_second': 8.827, 'epoch': 0.84}
{'loss': 0.7421, 'grad_norm': 0.07406704127788544, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7155188918113708, 'eval_runtime': 7.1589, 'eval_samples_per_second': 139.546, 'eval_steps_per_second': 8.8, 'epoch': 0.88}
{'loss': 0.7023, 'grad_norm': 0.08445335924625397, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7126883864402771, 'eval_runtime': 7.2047, 'eval_samples_per_second': 138.66, 'eval_steps_per_second': 8.744, 'epoch': 0.92}
{'loss': 0.743, 'grad_norm': 0.08549383282661438, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7109217047691345, 'eval_runtime': 7.2202, 'eval_samples_per_second': 138.362, 'eval_steps_per_second': 8.726, 'epoch': 0.96}
{'loss': 0.7147, 'grad_norm': 0.085979163646698, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7103171348571777, 'eval_runtime': 7.1958, 'eval_samples_per_second': 138.832, 'eval_steps_per_second': 8.755, 'epoch': 1.0}
{'train_runtime': 387.1243, 'train_samples_per_second': 25.829, 'train_steps_per_second': 1.614, 'train_loss': 0.9502019653320313, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7987546920776367, 1.1601530313491821, 0.9281578660011292, 0.850091814994812, 0.8233745694160461, 0.8130520582199097, 0.8042588829994202, 0.7986223101615906, 0.7911428213119507, 0.7826070785522461, 0.7765854001045227, 0.7678491473197937, 0.762611985206604, 0.7553442120552063, 0.7499810457229614, 0.7436454892158508, 0.7384739518165588, 0.7327315211296082, 0.7284470796585083, 0.7234277725219727, 0.7183235287666321, 0.7155188918113708, 0.7126883864402771, 0.7109217047691345, 0.7103171348571777], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:48,  2.19it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 121.31it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 211.16it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 281.49it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 335.56it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 380.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 325.53it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8474158644676208
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.2888 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.8155332031790524
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.8030763203945963
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.811410481258097
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.8254653608728978
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.7938049296770214
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.8572, dtype=torch.float64), 0, 0, 0, tensor(0.1428, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(2.3027e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8572, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1428, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.857
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.143
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 267.15it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 445.16it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 518.76it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 562.50it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 594.14it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 618.53it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 605.82it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.9018, 'grad_norm': 0.17298942804336548, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2738192081451416, 'eval_runtime': 10.4546, 'eval_samples_per_second': 95.556, 'eval_steps_per_second': 6.026, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 266.10it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 444.01it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 517.38it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 562.02it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 592.58it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 616.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 603.67it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.6652, 'grad_norm': 0.15133753418922424, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 2.176917314529419, 'eval_runtime': 10.4191, 'eval_samples_per_second': 95.882, 'eval_steps_per_second': 6.047, 'epoch': 0.08}
{'loss': 2.0477, 'grad_norm': 0.04629999026656151, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.931359052658081, 'eval_runtime': 10.45, 'eval_samples_per_second': 95.598, 'eval_steps_per_second': 6.029, 'epoch': 0.12}
{'loss': 1.874, 'grad_norm': 0.06935135275125504, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8583400249481201, 'eval_runtime': 10.4626, 'eval_samples_per_second': 95.483, 'eval_steps_per_second': 6.021, 'epoch': 0.16}
{'loss': 1.8676, 'grad_norm': 0.04300874099135399, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8221122026443481, 'eval_runtime': 10.4385, 'eval_samples_per_second': 95.703, 'eval_steps_per_second': 6.035, 'epoch': 0.2}
{'loss': 1.8205, 'grad_norm': 0.04559246450662613, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.8045059442520142, 'eval_runtime': 10.4904, 'eval_samples_per_second': 95.229, 'eval_steps_per_second': 6.005, 'epoch': 0.24}
{'loss': 1.8286, 'grad_norm': 0.04114066809415817, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7921571731567383, 'eval_runtime': 10.5003, 'eval_samples_per_second': 95.141, 'eval_steps_per_second': 6.0, 'epoch': 0.28}
{'loss': 1.8139, 'grad_norm': 0.0458928607404232, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7799538373947144, 'eval_runtime': 10.4987, 'eval_samples_per_second': 95.155, 'eval_steps_per_second': 6.001, 'epoch': 0.32}
{'loss': 1.7885, 'grad_norm': 0.041708990931510925, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7691130638122559, 'eval_runtime': 10.494, 'eval_samples_per_second': 95.197, 'eval_steps_per_second': 6.003, 'epoch': 0.36}
{'loss': 1.7732, 'grad_norm': 0.04336608946323395, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7621378898620605, 'eval_runtime': 10.4977, 'eval_samples_per_second': 95.164, 'eval_steps_per_second': 6.001, 'epoch': 0.4}
{'loss': 1.7874, 'grad_norm': 0.04815443605184555, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7545567750930786, 'eval_runtime': 10.4897, 'eval_samples_per_second': 95.236, 'eval_steps_per_second': 6.006, 'epoch': 0.44}
{'loss': 1.814, 'grad_norm': 0.04018939658999443, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7501389980316162, 'eval_runtime': 10.4921, 'eval_samples_per_second': 95.215, 'eval_steps_per_second': 6.005, 'epoch': 0.48}
{'loss': 1.7729, 'grad_norm': 0.045163147151470184, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.747230887413025, 'eval_runtime': 10.4983, 'eval_samples_per_second': 95.159, 'eval_steps_per_second': 6.001, 'epoch': 0.52}
{'loss': 1.7481, 'grad_norm': 0.045152682811021805, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7423522472381592, 'eval_runtime': 10.4905, 'eval_samples_per_second': 95.229, 'eval_steps_per_second': 6.005, 'epoch': 0.56}
{'loss': 1.7448, 'grad_norm': 0.04545489698648453, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.7380727529525757, 'eval_runtime': 10.4733, 'eval_samples_per_second': 95.385, 'eval_steps_per_second': 6.015, 'epoch': 0.6}
{'loss': 1.7287, 'grad_norm': 0.04031916335225105, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.7352582216262817, 'eval_runtime': 10.423, 'eval_samples_per_second': 95.846, 'eval_steps_per_second': 6.044, 'epoch': 0.64}
{'loss': 1.7611, 'grad_norm': 0.043737299740314484, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.7329797744750977, 'eval_runtime': 10.4243, 'eval_samples_per_second': 95.833, 'eval_steps_per_second': 6.044, 'epoch': 0.68}
{'loss': 1.7421, 'grad_norm': 0.048607539385557175, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.7295308113098145, 'eval_runtime': 10.4315, 'eval_samples_per_second': 95.768, 'eval_steps_per_second': 6.039, 'epoch': 0.72}
{'loss': 1.7261, 'grad_norm': 0.047944772988557816, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.7275190353393555, 'eval_runtime': 10.4383, 'eval_samples_per_second': 95.705, 'eval_steps_per_second': 6.035, 'epoch': 0.76}
{'loss': 1.7365, 'grad_norm': 0.04275050759315491, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.7255077362060547, 'eval_runtime': 10.464, 'eval_samples_per_second': 95.471, 'eval_steps_per_second': 6.021, 'epoch': 0.8}
{'loss': 1.7297, 'grad_norm': 0.0428035631775856, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.7239854335784912, 'eval_runtime': 10.5253, 'eval_samples_per_second': 94.914, 'eval_steps_per_second': 5.986, 'epoch': 0.84}
{'loss': 1.7595, 'grad_norm': 0.04482123628258705, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.7222254276275635, 'eval_runtime': 10.5083, 'eval_samples_per_second': 95.068, 'eval_steps_per_second': 5.995, 'epoch': 0.88}
{'loss': 1.7363, 'grad_norm': 0.04524858668446541, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.721356749534607, 'eval_runtime': 10.5188, 'eval_samples_per_second': 94.973, 'eval_steps_per_second': 5.989, 'epoch': 0.92}
{'loss': 1.7328, 'grad_norm': 0.04195164144039154, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.7205450534820557, 'eval_runtime': 10.5218, 'eval_samples_per_second': 94.945, 'eval_steps_per_second': 5.988, 'epoch': 0.96}
{'loss': 1.7524, 'grad_norm': 0.04732312262058258, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.720145583152771, 'eval_runtime': 10.5234, 'eval_samples_per_second': 94.932, 'eval_steps_per_second': 5.987, 'epoch': 1.0}
{'train_runtime': 532.0981, 'train_samples_per_second': 18.792, 'train_steps_per_second': 1.175, 'train_loss': 1.9061400451660155, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2738192081451416, 2.176917314529419, 1.931359052658081, 1.8583400249481201, 1.8221122026443481, 1.8045059442520142, 1.7921571731567383, 1.7799538373947144, 1.7691130638122559, 1.7621378898620605, 1.7545567750930786, 1.7501389980316162, 1.747230887413025, 1.7423522472381592, 1.7380727529525757, 1.7352582216262817, 1.7329797744750977, 1.7295308113098145, 1.7275190353393555, 1.7255077362060547, 1.7239854335784912, 1.7222254276275635, 1.721356749534607, 1.7205450534820557, 1.720145583152771], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:14,  3.71it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 214.19it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 306.94it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 359.65it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 394.11it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 422.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 414.68it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8458070755004883
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4367 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.8188495147787066
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.8208739925609926
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.8069989579745264
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.8241488342906875
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.8241013135008372
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1620, dtype=torch.float64), 0, tensor(0.3690, dtype=torch.float64), 0, tensor(0.4689, dtype=torch.float64), 0, 32, 1, 1, 1, 0, 0, 128, 3.2200804522819496e-18, 1.4800000190734899, 1]
normalized proposed parameters for next round by BO: [tensor(1.0200e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1620, dtype=torch.float64), tensor(1.1494e-16, dtype=torch.float64), tensor(0.3690, dtype=torch.float64), tensor(4.1633e-17, dtype=torch.float64), tensor(0.4689, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.2201e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.162
  triviaqa: 0
  truthfulqa_gen: 0.369
  wikitext: 0
  mmlu: 0.469
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.2200804522819496e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734899,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  3.2200804522819496e-18
lora alpha:  1.4800000190734899
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 278.41it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.69it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 588.12it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 619.30it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 643.95it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 631.14it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.0631, 'grad_norm': 0.23770561814308167, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.130750894546509, 'eval_runtime': 8.8907, 'eval_samples_per_second': 112.364, 'eval_steps_per_second': 7.086, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.65it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 466.05it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 542.63it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 587.52it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 618.61it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 643.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 631.79it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.2953, 'grad_norm': 0.09481372684240341, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.7542518377304077, 'eval_runtime': 8.9073, 'eval_samples_per_second': 112.155, 'eval_steps_per_second': 7.073, 'epoch': 0.08}
{'loss': 1.6095, 'grad_norm': 0.07343648374080658, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5039904117584229, 'eval_runtime': 8.9263, 'eval_samples_per_second': 111.916, 'eval_steps_per_second': 7.058, 'epoch': 0.12}
{'loss': 1.426, 'grad_norm': 0.040169261395931244, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4159010648727417, 'eval_runtime': 8.9605, 'eval_samples_per_second': 111.489, 'eval_steps_per_second': 7.031, 'epoch': 0.16}
{'loss': 1.3903, 'grad_norm': 0.05309153348207474, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3771090507507324, 'eval_runtime': 8.9774, 'eval_samples_per_second': 111.279, 'eval_steps_per_second': 7.018, 'epoch': 0.2}
{'loss': 1.3367, 'grad_norm': 0.04326874762773514, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.33445405960083, 'eval_runtime': 8.9983, 'eval_samples_per_second': 111.021, 'eval_steps_per_second': 7.001, 'epoch': 0.24}
{'loss': 1.2963, 'grad_norm': 0.049124304205179214, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3100720643997192, 'eval_runtime': 9.0207, 'eval_samples_per_second': 110.746, 'eval_steps_per_second': 6.984, 'epoch': 0.28}
{'loss': 1.2603, 'grad_norm': 0.061748579144477844, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2882182598114014, 'eval_runtime': 9.0022, 'eval_samples_per_second': 110.973, 'eval_steps_per_second': 6.998, 'epoch': 0.32}
{'loss': 1.2453, 'grad_norm': 0.06117526441812515, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.265703558921814, 'eval_runtime': 8.9857, 'eval_samples_per_second': 111.177, 'eval_steps_per_second': 7.011, 'epoch': 0.36}
{'loss': 1.2504, 'grad_norm': 0.05165654048323631, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2393494844436646, 'eval_runtime': 8.9668, 'eval_samples_per_second': 111.411, 'eval_steps_per_second': 7.026, 'epoch': 0.4}
{'loss': 1.2153, 'grad_norm': 0.06815578788518906, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1909377574920654, 'eval_runtime': 8.9793, 'eval_samples_per_second': 111.256, 'eval_steps_per_second': 7.016, 'epoch': 0.44}
{'loss': 1.1337, 'grad_norm': 0.05632496625185013, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1503429412841797, 'eval_runtime': 8.98, 'eval_samples_per_second': 111.248, 'eval_steps_per_second': 7.016, 'epoch': 0.48}
{'loss': 1.1479, 'grad_norm': 0.05445583537220955, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1293878555297852, 'eval_runtime': 8.9781, 'eval_samples_per_second': 111.27, 'eval_steps_per_second': 7.017, 'epoch': 0.52}
{'loss': 1.1305, 'grad_norm': 0.059573255479335785, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1049532890319824, 'eval_runtime': 8.9735, 'eval_samples_per_second': 111.328, 'eval_steps_per_second': 7.021, 'epoch': 0.56}
{'loss': 1.1201, 'grad_norm': 0.056769076734781265, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.077707290649414, 'eval_runtime': 8.9789, 'eval_samples_per_second': 111.261, 'eval_steps_per_second': 7.016, 'epoch': 0.6}
{'loss': 1.0557, 'grad_norm': 0.060218725353479385, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0570564270019531, 'eval_runtime': 8.9872, 'eval_samples_per_second': 111.157, 'eval_steps_per_second': 7.01, 'epoch': 0.64}
{'loss': 1.1018, 'grad_norm': 0.05758098140358925, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0502163171768188, 'eval_runtime': 8.9831, 'eval_samples_per_second': 111.209, 'eval_steps_per_second': 7.013, 'epoch': 0.68}
{'loss': 1.055, 'grad_norm': 0.06786586344242096, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.044283151626587, 'eval_runtime': 8.9744, 'eval_samples_per_second': 111.316, 'eval_steps_per_second': 7.02, 'epoch': 0.72}
{'loss': 1.0711, 'grad_norm': 0.05912322178483009, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0407147407531738, 'eval_runtime': 9.0194, 'eval_samples_per_second': 110.762, 'eval_steps_per_second': 6.985, 'epoch': 0.76}
{'loss': 1.0352, 'grad_norm': 0.05758767947554588, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0375487804412842, 'eval_runtime': 9.0214, 'eval_samples_per_second': 110.737, 'eval_steps_per_second': 6.983, 'epoch': 0.8}
{'loss': 1.1093, 'grad_norm': 0.06921481341123581, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0352965593338013, 'eval_runtime': 9.0223, 'eval_samples_per_second': 110.725, 'eval_steps_per_second': 6.983, 'epoch': 0.84}
{'loss': 1.05, 'grad_norm': 0.06843644380569458, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0323412418365479, 'eval_runtime': 9.0184, 'eval_samples_per_second': 110.774, 'eval_steps_per_second': 6.986, 'epoch': 0.88}
{'loss': 0.993, 'grad_norm': 0.06964676082134247, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0289541482925415, 'eval_runtime': 9.039, 'eval_samples_per_second': 110.521, 'eval_steps_per_second': 6.97, 'epoch': 0.92}
{'loss': 1.008, 'grad_norm': 0.06980454921722412, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0280147790908813, 'eval_runtime': 9.0297, 'eval_samples_per_second': 110.635, 'eval_steps_per_second': 6.977, 'epoch': 0.96}
{'loss': 1.0668, 'grad_norm': 0.06626966595649719, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0274933576583862, 'eval_runtime': 9.0254, 'eval_samples_per_second': 110.688, 'eval_steps_per_second': 6.98, 'epoch': 1.0}
{'train_runtime': 460.0707, 'train_samples_per_second': 21.734, 'train_steps_per_second': 1.358, 'train_loss': 1.3386670166015624, 'epoch': 1.0}
train_results:  {'eval_loss': [3.130750894546509, 1.7542518377304077, 1.5039904117584229, 1.4159010648727417, 1.3771090507507324, 1.33445405960083, 1.3100720643997192, 1.2882182598114014, 1.265703558921814, 1.2393494844436646, 1.1909377574920654, 1.1503429412841797, 1.1293878555297852, 1.1049532890319824, 1.077707290649414, 1.0570564270019531, 1.0502163171768188, 1.044283151626587, 1.0407147407531738, 1.0375487804412842, 1.0352965593338013, 1.0323412418365479, 1.0289541482925415, 1.0280147790908813, 1.0274933576583862], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:01,  2.07it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 134.55it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 228.59it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 299.46it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 351.92it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 394.91it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 340.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8477532863616943
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.5855 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.779866170691726
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.8228840064744941
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.7608363693327569
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.8200318462394639
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.7962996599369317
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.2296, dtype=torch.float64), 0, tensor(0.3800, dtype=torch.float64), tensor(0.3904, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 0, 1, 128, 0.03852099331783329, 1.4800000190734914, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1449e-16, dtype=torch.float64), tensor(0.2296, dtype=torch.float64), tensor(7.1348e-17, dtype=torch.float64), tensor(0.3800, dtype=torch.float64), tensor(0.3904, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5468e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3852, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.23
  triviaqa: 0
  truthfulqa_gen: 0.38
  wikitext: 0.39
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03852099331783329,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734914,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.03852099331783329
lora alpha:  1.4800000190734914
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 260.05it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 433.17it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 503.83it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 537.07it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 569.63it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 593.88it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 582.68it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.1885, 'grad_norm': 0.16549086570739746, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.4153826236724854, 'eval_runtime': 7.8532, 'eval_samples_per_second': 127.21, 'eval_steps_per_second': 8.022, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 262.46it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 437.86it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 509.35it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 552.47it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 581.98it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 604.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 593.61it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.5355, 'grad_norm': 0.05605451762676239, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 2.0614848136901855, 'eval_runtime': 7.8337, 'eval_samples_per_second': 127.526, 'eval_steps_per_second': 8.042, 'epoch': 0.08}
{'loss': 1.9306, 'grad_norm': 0.053610362112522125, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7619926929473877, 'eval_runtime': 7.8425, 'eval_samples_per_second': 127.384, 'eval_steps_per_second': 8.033, 'epoch': 0.12}
{'loss': 1.7816, 'grad_norm': 0.05301853269338608, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6488373279571533, 'eval_runtime': 7.8653, 'eval_samples_per_second': 127.013, 'eval_steps_per_second': 8.01, 'epoch': 0.16}
{'loss': 1.6431, 'grad_norm': 0.06254695355892181, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.589482069015503, 'eval_runtime': 7.8808, 'eval_samples_per_second': 126.763, 'eval_steps_per_second': 7.994, 'epoch': 0.2}
{'loss': 1.6193, 'grad_norm': 0.054262176156044006, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5570558309555054, 'eval_runtime': 7.8734, 'eval_samples_per_second': 126.884, 'eval_steps_per_second': 8.002, 'epoch': 0.24}
{'loss': 1.5642, 'grad_norm': 0.05998776853084564, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5271763801574707, 'eval_runtime': 7.8782, 'eval_samples_per_second': 126.805, 'eval_steps_per_second': 7.997, 'epoch': 0.28}
{'loss': 1.6135, 'grad_norm': 0.059017300605773926, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5014106035232544, 'eval_runtime': 7.8897, 'eval_samples_per_second': 126.621, 'eval_steps_per_second': 7.985, 'epoch': 0.32}
{'loss': 1.47, 'grad_norm': 0.0645245611667633, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.471146821975708, 'eval_runtime': 7.9029, 'eval_samples_per_second': 126.409, 'eval_steps_per_second': 7.972, 'epoch': 0.36}
{'loss': 1.5589, 'grad_norm': 0.08377481251955032, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4385660886764526, 'eval_runtime': 7.8897, 'eval_samples_per_second': 126.621, 'eval_steps_per_second': 7.985, 'epoch': 0.4}
{'loss': 1.4813, 'grad_norm': 0.09297486394643784, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3778886795043945, 'eval_runtime': 7.8673, 'eval_samples_per_second': 126.981, 'eval_steps_per_second': 8.008, 'epoch': 0.44}
{'loss': 1.4293, 'grad_norm': 0.08288030326366425, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.341465711593628, 'eval_runtime': 7.8631, 'eval_samples_per_second': 127.049, 'eval_steps_per_second': 8.012, 'epoch': 0.48}
{'loss': 1.2986, 'grad_norm': 0.08459662646055222, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.316860556602478, 'eval_runtime': 7.833, 'eval_samples_per_second': 127.537, 'eval_steps_per_second': 8.043, 'epoch': 0.52}
{'loss': 1.3401, 'grad_norm': 0.0777859017252922, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2876642942428589, 'eval_runtime': 7.8352, 'eval_samples_per_second': 127.501, 'eval_steps_per_second': 8.041, 'epoch': 0.56}
{'loss': 1.4097, 'grad_norm': 0.0866779312491417, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2515745162963867, 'eval_runtime': 7.8328, 'eval_samples_per_second': 127.54, 'eval_steps_per_second': 8.043, 'epoch': 0.6}
{'loss': 1.1935, 'grad_norm': 0.09686197340488434, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1962778568267822, 'eval_runtime': 7.8272, 'eval_samples_per_second': 127.631, 'eval_steps_per_second': 8.049, 'epoch': 0.64}
{'loss': 1.2759, 'grad_norm': 0.08662015199661255, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1830717325210571, 'eval_runtime': 7.8492, 'eval_samples_per_second': 127.274, 'eval_steps_per_second': 8.026, 'epoch': 0.68}
{'loss': 1.2119, 'grad_norm': 0.09987583011388779, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1676329374313354, 'eval_runtime': 7.8281, 'eval_samples_per_second': 127.617, 'eval_steps_per_second': 8.048, 'epoch': 0.72}
{'loss': 1.2269, 'grad_norm': 0.07940289378166199, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1572699546813965, 'eval_runtime': 7.8605, 'eval_samples_per_second': 127.091, 'eval_steps_per_second': 8.015, 'epoch': 0.76}
{'loss': 1.1869, 'grad_norm': 0.08680925518274307, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1463596820831299, 'eval_runtime': 7.8757, 'eval_samples_per_second': 126.847, 'eval_steps_per_second': 7.999, 'epoch': 0.8}
{'loss': 1.1388, 'grad_norm': 0.08173175156116486, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1374022960662842, 'eval_runtime': 7.8799, 'eval_samples_per_second': 126.778, 'eval_steps_per_second': 7.995, 'epoch': 0.84}
{'loss': 1.1942, 'grad_norm': 0.11336825788021088, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.130577802658081, 'eval_runtime': 7.8795, 'eval_samples_per_second': 126.785, 'eval_steps_per_second': 7.995, 'epoch': 0.88}
{'loss': 1.1742, 'grad_norm': 0.08101636916399002, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.124059796333313, 'eval_runtime': 7.8895, 'eval_samples_per_second': 126.623, 'eval_steps_per_second': 7.985, 'epoch': 0.92}
{'loss': 1.0986, 'grad_norm': 0.08854254335165024, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1207702159881592, 'eval_runtime': 7.8863, 'eval_samples_per_second': 126.675, 'eval_steps_per_second': 7.989, 'epoch': 0.96}
{'loss': 1.2535, 'grad_norm': 0.09935303777456284, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1193132400512695, 'eval_runtime': 7.8829, 'eval_samples_per_second': 126.73, 'eval_steps_per_second': 7.992, 'epoch': 1.0}
{'train_runtime': 427.6034, 'train_samples_per_second': 23.384, 'train_steps_per_second': 1.462, 'train_loss': 1.5527432922363282, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4153826236724854, 2.0614848136901855, 1.7619926929473877, 1.6488373279571533, 1.589482069015503, 1.5570558309555054, 1.5271763801574707, 1.5014106035232544, 1.471146821975708, 1.4385660886764526, 1.3778886795043945, 1.341465711593628, 1.316860556602478, 1.2876642942428589, 1.2515745162963867, 1.1962778568267822, 1.1830717325210571, 1.1676329374313354, 1.1572699546813965, 1.1463596820831299, 1.1374022960662842, 1.130577802658081, 1.124059796333313, 1.1207702159881592, 1.1193132400512695], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:03,  2.72it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 179.02it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 274.06it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 333.28it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 373.31it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 405.65it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 379.61it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8479848504066467
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0885 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.7477934600694472
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.8226097310143394
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.7900182763511752
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.8124344787847019
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.7705205166914727
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.6180, dtype=torch.float64), 0, tensor(0.3380, dtype=torch.float64), tensor(0.0440, dtype=torch.float64), 0, 0, 32, 0, 0, 1, 1, 0, 128, 0.027222360930444146, 1.4800000190734872, 1]
normalized proposed parameters for next round by BO: [tensor(1.7491e-17, dtype=torch.float64), tensor(5.7478e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6180, dtype=torch.float64), tensor(5.6721e-18, dtype=torch.float64), tensor(0.3380, dtype=torch.float64), tensor(0.0440, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2722, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.618
  triviaqa: 0
  truthfulqa_gen: 0.338
  wikitext: 0.044
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.027222360930444146,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.027222360930444146
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 274.78it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 459.24it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 535.89it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 582.73it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 615.87it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 641.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 627.66it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.9854, 'grad_norm': 0.41650390625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.4644458293914795, 'eval_runtime': 4.4056, 'eval_samples_per_second': 226.755, 'eval_steps_per_second': 14.3, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 276.81it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 461.06it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 536.55it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 582.97it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 615.71it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 640.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 627.97it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.0778, 'grad_norm': 0.33618462085723877, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.3417409658432007, 'eval_runtime': 4.4034, 'eval_samples_per_second': 226.868, 'eval_steps_per_second': 14.307, 'epoch': 0.08}
{'loss': 1.2381, 'grad_norm': 0.1514742374420166, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0705407857894897, 'eval_runtime': 4.4167, 'eval_samples_per_second': 226.187, 'eval_steps_per_second': 14.264, 'epoch': 0.12}
{'loss': 1.0563, 'grad_norm': 0.06806135922670364, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9176266193389893, 'eval_runtime': 4.4153, 'eval_samples_per_second': 226.256, 'eval_steps_per_second': 14.268, 'epoch': 0.16}
{'loss': 0.921, 'grad_norm': 0.05605198070406914, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8991303443908691, 'eval_runtime': 4.4075, 'eval_samples_per_second': 226.657, 'eval_steps_per_second': 14.294, 'epoch': 0.2}
{'loss': 0.9122, 'grad_norm': 0.05674601346254349, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8839707374572754, 'eval_runtime': 4.4094, 'eval_samples_per_second': 226.561, 'eval_steps_per_second': 14.288, 'epoch': 0.24}
{'loss': 0.8511, 'grad_norm': 0.06218814104795456, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8727017641067505, 'eval_runtime': 4.4173, 'eval_samples_per_second': 226.155, 'eval_steps_per_second': 14.262, 'epoch': 0.28}
{'loss': 0.8691, 'grad_norm': 0.08623836934566498, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8612520098686218, 'eval_runtime': 4.4188, 'eval_samples_per_second': 226.08, 'eval_steps_per_second': 14.257, 'epoch': 0.32}
{'loss': 0.8979, 'grad_norm': 0.07032234221696854, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.85505610704422, 'eval_runtime': 4.4266, 'eval_samples_per_second': 225.683, 'eval_steps_per_second': 14.232, 'epoch': 0.36}
{'loss': 0.8795, 'grad_norm': 0.05432599410414696, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8442227840423584, 'eval_runtime': 4.424, 'eval_samples_per_second': 225.811, 'eval_steps_per_second': 14.24, 'epoch': 0.4}
{'loss': 0.8343, 'grad_norm': 0.06189018487930298, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8329871892929077, 'eval_runtime': 4.4291, 'eval_samples_per_second': 225.556, 'eval_steps_per_second': 14.224, 'epoch': 0.44}
{'loss': 0.8561, 'grad_norm': 0.0722552090883255, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8282227516174316, 'eval_runtime': 4.4164, 'eval_samples_per_second': 226.203, 'eval_steps_per_second': 14.265, 'epoch': 0.48}
{'loss': 0.8426, 'grad_norm': 0.06133117154240608, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8185222148895264, 'eval_runtime': 4.4223, 'eval_samples_per_second': 225.901, 'eval_steps_per_second': 14.246, 'epoch': 0.52}
{'loss': 0.7919, 'grad_norm': 0.057767268270254135, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8118225932121277, 'eval_runtime': 4.4138, 'eval_samples_per_second': 226.337, 'eval_steps_per_second': 14.274, 'epoch': 0.56}
{'loss': 0.8381, 'grad_norm': 0.06773044914007187, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8066380620002747, 'eval_runtime': 4.407, 'eval_samples_per_second': 226.685, 'eval_steps_per_second': 14.295, 'epoch': 0.6}
{'loss': 0.8435, 'grad_norm': 0.06271098554134369, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.800544261932373, 'eval_runtime': 4.4079, 'eval_samples_per_second': 226.637, 'eval_steps_per_second': 14.292, 'epoch': 0.64}
{'loss': 0.761, 'grad_norm': 0.0843823254108429, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7954230904579163, 'eval_runtime': 4.4066, 'eval_samples_per_second': 226.707, 'eval_steps_per_second': 14.297, 'epoch': 0.68}
{'loss': 0.8244, 'grad_norm': 0.08441892266273499, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7894238233566284, 'eval_runtime': 4.4106, 'eval_samples_per_second': 226.501, 'eval_steps_per_second': 14.284, 'epoch': 0.72}
{'loss': 0.8169, 'grad_norm': 0.06072494015097618, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7855136394500732, 'eval_runtime': 4.4143, 'eval_samples_per_second': 226.31, 'eval_steps_per_second': 14.272, 'epoch': 0.76}
{'loss': 0.7496, 'grad_norm': 0.07698014378547668, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.78019118309021, 'eval_runtime': 4.4073, 'eval_samples_per_second': 226.671, 'eval_steps_per_second': 14.295, 'epoch': 0.8}
{'loss': 0.7658, 'grad_norm': 0.0764700248837471, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7763777375221252, 'eval_runtime': 4.41, 'eval_samples_per_second': 226.533, 'eval_steps_per_second': 14.286, 'epoch': 0.84}
{'loss': 0.7984, 'grad_norm': 0.07768694311380386, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7735661268234253, 'eval_runtime': 4.4095, 'eval_samples_per_second': 226.558, 'eval_steps_per_second': 14.287, 'epoch': 0.88}
{'loss': 0.7802, 'grad_norm': 0.08292078971862793, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7704419493675232, 'eval_runtime': 4.4081, 'eval_samples_per_second': 226.629, 'eval_steps_per_second': 14.292, 'epoch': 0.92}
{'loss': 0.7142, 'grad_norm': 0.06680531054735184, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7688876986503601, 'eval_runtime': 4.4118, 'eval_samples_per_second': 226.437, 'eval_steps_per_second': 14.28, 'epoch': 0.96}
{'loss': 0.7949, 'grad_norm': 0.07481539249420166, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7685734033584595, 'eval_runtime': 4.4099, 'eval_samples_per_second': 226.533, 'eval_steps_per_second': 14.286, 'epoch': 1.0}
{'train_runtime': 245.5862, 'train_samples_per_second': 40.715, 'train_steps_per_second': 2.545, 'train_loss': 1.068011050415039, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4644458293914795, 1.3417409658432007, 1.0705407857894897, 0.9176266193389893, 0.8991303443908691, 0.8839707374572754, 0.8727017641067505, 0.8612520098686218, 0.85505610704422, 0.8442227840423584, 0.8329871892929077, 0.8282227516174316, 0.8185222148895264, 0.8118225932121277, 0.8066380620002747, 0.800544261932373, 0.7954230904579163, 0.7894238233566284, 0.7855136394500732, 0.78019118309021, 0.7763777375221252, 0.7735661268234253, 0.7704419493675232, 0.7688876986503601, 0.7685734033584595], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:25,  1.53it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 113.10it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 200.96it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 271.74it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 327.28it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 374.00it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 305.35it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8482716083526611
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8763 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.8187706951780892
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.806222351487194
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.8230270851269054
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.7834591844524361
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.7145707815603252
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3754, dtype=torch.float64), 0, tensor(0.5975, dtype=torch.float64), 0, 0, tensor(0.0271, dtype=torch.float64), 32, 1, 1, 1, 0, 0, 128, 0.0, 1.480000019073488, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.9779e-17, dtype=torch.float64), tensor(9.7946e-18, dtype=torch.float64), tensor(0.3754, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5975, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.3267e-17, dtype=torch.float64), tensor(0.0271, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.375
  triviaqa: 0
  truthfulqa_gen: 0.598
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.027

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.26it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 465.47it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 543.37it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 590.19it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 623.07it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 647.39it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 634.34it/s]
Evaluation performance at step 25: 0.73
{'loss': 5.0874, 'grad_norm': 0.2755913734436035, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.6504340171813965, 'eval_runtime': 4.0083, 'eval_samples_per_second': 249.234, 'eval_steps_per_second': 15.717, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.42it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.10it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.50it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 588.08it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 619.47it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 644.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 631.88it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.3888, 'grad_norm': 0.09499720484018326, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.636152982711792, 'eval_runtime': 4.0012, 'eval_samples_per_second': 249.677, 'eval_steps_per_second': 15.745, 'epoch': 0.08}
{'loss': 1.4551, 'grad_norm': 0.10446754842996597, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2732101678848267, 'eval_runtime': 4.0177, 'eval_samples_per_second': 248.649, 'eval_steps_per_second': 15.681, 'epoch': 0.12}
{'loss': 1.2676, 'grad_norm': 0.05834781751036644, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1669865846633911, 'eval_runtime': 4.0304, 'eval_samples_per_second': 247.867, 'eval_steps_per_second': 15.631, 'epoch': 0.16}
{'loss': 1.158, 'grad_norm': 0.0579523891210556, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.121452808380127, 'eval_runtime': 4.0421, 'eval_samples_per_second': 247.151, 'eval_steps_per_second': 15.586, 'epoch': 0.2}
{'loss': 1.1142, 'grad_norm': 0.06687179207801819, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0846062898635864, 'eval_runtime': 4.028, 'eval_samples_per_second': 248.013, 'eval_steps_per_second': 15.64, 'epoch': 0.24}
{'loss': 1.0789, 'grad_norm': 0.07887021452188492, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0518555641174316, 'eval_runtime': 4.032, 'eval_samples_per_second': 247.768, 'eval_steps_per_second': 15.625, 'epoch': 0.28}
{'loss': 1.046, 'grad_norm': 0.0758361667394638, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0156863927841187, 'eval_runtime': 4.0319, 'eval_samples_per_second': 247.776, 'eval_steps_per_second': 15.626, 'epoch': 0.32}
{'loss': 0.9889, 'grad_norm': 0.07660598307847977, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9693875312805176, 'eval_runtime': 4.036, 'eval_samples_per_second': 247.519, 'eval_steps_per_second': 15.609, 'epoch': 0.36}
{'loss': 0.9414, 'grad_norm': 0.08214180171489716, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8933240175247192, 'eval_runtime': 4.0409, 'eval_samples_per_second': 247.223, 'eval_steps_per_second': 15.591, 'epoch': 0.4}
{'loss': 0.8575, 'grad_norm': 0.07146710902452469, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8241962790489197, 'eval_runtime': 4.0355, 'eval_samples_per_second': 247.553, 'eval_steps_per_second': 15.611, 'epoch': 0.44}
{'loss': 0.8253, 'grad_norm': 0.07923012971878052, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7799710035324097, 'eval_runtime': 4.0562, 'eval_samples_per_second': 246.292, 'eval_steps_per_second': 15.532, 'epoch': 0.48}
{'loss': 0.7975, 'grad_norm': 0.09134940803050995, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7303242683410645, 'eval_runtime': 4.0418, 'eval_samples_per_second': 247.166, 'eval_steps_per_second': 15.587, 'epoch': 0.52}
{'loss': 0.7227, 'grad_norm': 0.08107677102088928, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6872895956039429, 'eval_runtime': 4.0497, 'eval_samples_per_second': 246.684, 'eval_steps_per_second': 15.557, 'epoch': 0.56}
{'loss': 0.7246, 'grad_norm': 0.08481224626302719, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6714543104171753, 'eval_runtime': 4.0249, 'eval_samples_per_second': 248.203, 'eval_steps_per_second': 15.652, 'epoch': 0.6}
{'loss': 0.7102, 'grad_norm': 0.08162419497966766, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6578254103660583, 'eval_runtime': 4.021, 'eval_samples_per_second': 248.445, 'eval_steps_per_second': 15.668, 'epoch': 0.64}
{'loss': 0.6905, 'grad_norm': 0.08873886615037918, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6501502990722656, 'eval_runtime': 4.0115, 'eval_samples_per_second': 249.034, 'eval_steps_per_second': 15.705, 'epoch': 0.68}
{'loss': 0.6893, 'grad_norm': 0.0994889959692955, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6357403993606567, 'eval_runtime': 4.0112, 'eval_samples_per_second': 249.05, 'eval_steps_per_second': 15.706, 'epoch': 0.72}
{'loss': 0.6789, 'grad_norm': 0.09758428484201431, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6261701583862305, 'eval_runtime': 4.0166, 'eval_samples_per_second': 248.718, 'eval_steps_per_second': 15.685, 'epoch': 0.76}
{'loss': 0.6406, 'grad_norm': 0.10636697709560394, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6149992346763611, 'eval_runtime': 4.0087, 'eval_samples_per_second': 249.206, 'eval_steps_per_second': 15.716, 'epoch': 0.8}
{'loss': 0.6861, 'grad_norm': 0.1027783527970314, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6068105697631836, 'eval_runtime': 4.0166, 'eval_samples_per_second': 248.721, 'eval_steps_per_second': 15.685, 'epoch': 0.84}
{'loss': 0.6431, 'grad_norm': 0.10219898819923401, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.599536657333374, 'eval_runtime': 4.0118, 'eval_samples_per_second': 249.017, 'eval_steps_per_second': 15.704, 'epoch': 0.88}
{'loss': 0.62, 'grad_norm': 0.11349537968635559, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5943304896354675, 'eval_runtime': 4.008, 'eval_samples_per_second': 249.253, 'eval_steps_per_second': 15.719, 'epoch': 0.92}
{'loss': 0.6312, 'grad_norm': 0.11377394199371338, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5905007719993591, 'eval_runtime': 4.0088, 'eval_samples_per_second': 249.201, 'eval_steps_per_second': 15.715, 'epoch': 0.96}
{'loss': 0.6364, 'grad_norm': 0.12041160464286804, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5893762707710266, 'eval_runtime': 4.0076, 'eval_samples_per_second': 249.276, 'eval_steps_per_second': 15.72, 'epoch': 1.0}
{'train_runtime': 228.3588, 'train_samples_per_second': 43.786, 'train_steps_per_second': 2.737, 'train_loss': 1.083207273864746, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6504340171813965, 1.636152982711792, 1.2732101678848267, 1.1669865846633911, 1.121452808380127, 1.0846062898635864, 1.0518555641174316, 1.0156863927841187, 0.9693875312805176, 0.8933240175247192, 0.8241962790489197, 0.7799710035324097, 0.7303242683410645, 0.6872895956039429, 0.6714543104171753, 0.6578254103660583, 0.6501502990722656, 0.6357403993606567, 0.6261701583862305, 0.6149992346763611, 0.6068105697631836, 0.599536657333374, 0.5943304896354675, 0.5905007719993591, 0.5893762707710266], 'performance': [0.73, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:44,  4.80it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 248.09it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 340.40it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 390.38it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 422.66it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 449.47it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 452.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8485893607139587
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3284 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.7856261257706058
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.8236167424935565
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.8231056428520307
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.8236216298519548
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.7920696500047492
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1078, dtype=torch.float64), 0, 0, tensor(0.2483, dtype=torch.float64), 0, tensor(0.3976, dtype=torch.float64), tensor(0.1352, dtype=torch.float64), 0, tensor(0.1112, dtype=torch.float64), 32, 1, 1, 1, 0, 0, 128, 0.08105546686148014, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.1078, dtype=torch.float64), tensor(2.3883e-16, dtype=torch.float64), tensor(4.6879e-17, dtype=torch.float64), tensor(0.2483, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3976, dtype=torch.float64), tensor(0.1352, dtype=torch.float64), tensor(4.5527e-16, dtype=torch.float64), tensor(0.1112, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8106, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.108
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.248
  triviaqa: 0
  truthfulqa_gen: 0.398
  wikitext: 0.135
  mmlu: 0
  arc_challenge: 0.111

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.08105546686148014,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.08105546686148014
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.89it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 465.10it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 542.52it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 589.33it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 620.84it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 645.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 632.62it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.6111, 'grad_norm': 0.19073979556560516, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.53346848487854, 'eval_runtime': 7.3992, 'eval_samples_per_second': 135.014, 'eval_steps_per_second': 8.514, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.84it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 466.38it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 543.43it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 589.77it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 621.94it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 646.47it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 633.75it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.4949, 'grad_norm': 0.1834774613380432, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8134316205978394, 'eval_runtime': 6.3992, 'eval_samples_per_second': 156.113, 'eval_steps_per_second': 9.845, 'epoch': 0.08}
{'loss': 1.7049, 'grad_norm': 0.08975252509117126, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4380906820297241, 'eval_runtime': 6.4389, 'eval_samples_per_second': 155.151, 'eval_steps_per_second': 9.784, 'epoch': 0.12}
{'loss': 1.4482, 'grad_norm': 0.05213212966918945, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.338937759399414, 'eval_runtime': 6.4816, 'eval_samples_per_second': 154.129, 'eval_steps_per_second': 9.72, 'epoch': 0.16}
{'loss': 1.3823, 'grad_norm': 0.05191413685679436, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2997905015945435, 'eval_runtime': 6.473, 'eval_samples_per_second': 154.333, 'eval_steps_per_second': 9.733, 'epoch': 0.2}
{'loss': 1.3094, 'grad_norm': 0.04629216343164444, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2697595357894897, 'eval_runtime': 6.4814, 'eval_samples_per_second': 154.132, 'eval_steps_per_second': 9.72, 'epoch': 0.24}
{'loss': 1.1855, 'grad_norm': 0.0968189537525177, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2465543746948242, 'eval_runtime': 6.5072, 'eval_samples_per_second': 153.522, 'eval_steps_per_second': 9.682, 'epoch': 0.28}
{'loss': 1.208, 'grad_norm': 0.060900911688804626, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2216846942901611, 'eval_runtime': 6.4982, 'eval_samples_per_second': 153.736, 'eval_steps_per_second': 9.695, 'epoch': 0.32}
{'loss': 1.2302, 'grad_norm': 0.05515608564019203, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.193735957145691, 'eval_runtime': 6.4914, 'eval_samples_per_second': 153.896, 'eval_steps_per_second': 9.705, 'epoch': 0.36}
{'loss': 1.242, 'grad_norm': 0.06053008511662483, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1674903631210327, 'eval_runtime': 6.4933, 'eval_samples_per_second': 153.851, 'eval_steps_per_second': 9.702, 'epoch': 0.4}
{'loss': 1.1991, 'grad_norm': 0.07626283168792725, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1397422552108765, 'eval_runtime': 6.4971, 'eval_samples_per_second': 153.761, 'eval_steps_per_second': 9.697, 'epoch': 0.44}
{'loss': 1.139, 'grad_norm': 0.07647911459207535, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1062010526657104, 'eval_runtime': 6.5197, 'eval_samples_per_second': 153.228, 'eval_steps_per_second': 9.663, 'epoch': 0.48}
{'loss': 1.144, 'grad_norm': 0.08701765537261963, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0620018243789673, 'eval_runtime': 6.5013, 'eval_samples_per_second': 153.662, 'eval_steps_per_second': 9.69, 'epoch': 0.52}
{'loss': 1.1167, 'grad_norm': 0.0784430131316185, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0357604026794434, 'eval_runtime': 6.4997, 'eval_samples_per_second': 153.699, 'eval_steps_per_second': 9.693, 'epoch': 0.56}
{'loss': 1.147, 'grad_norm': 0.09832461178302765, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.017466425895691, 'eval_runtime': 6.497, 'eval_samples_per_second': 153.763, 'eval_steps_per_second': 9.697, 'epoch': 0.6}
{'loss': 0.9822, 'grad_norm': 0.08158658444881439, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0056657791137695, 'eval_runtime': 6.4868, 'eval_samples_per_second': 154.006, 'eval_steps_per_second': 9.712, 'epoch': 0.64}
{'loss': 1.0551, 'grad_norm': 0.07375429570674896, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9908963441848755, 'eval_runtime': 6.4905, 'eval_samples_per_second': 153.917, 'eval_steps_per_second': 9.707, 'epoch': 0.68}
{'loss': 0.9726, 'grad_norm': 0.0845024436712265, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9735807776451111, 'eval_runtime': 6.4887, 'eval_samples_per_second': 153.959, 'eval_steps_per_second': 9.709, 'epoch': 0.72}
{'loss': 0.9998, 'grad_norm': 0.10372024029493332, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9586499929428101, 'eval_runtime': 6.4887, 'eval_samples_per_second': 153.96, 'eval_steps_per_second': 9.709, 'epoch': 0.76}
{'loss': 1.0372, 'grad_norm': 0.07522424310445786, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9463909864425659, 'eval_runtime': 6.4924, 'eval_samples_per_second': 153.873, 'eval_steps_per_second': 9.704, 'epoch': 0.8}
{'loss': 0.9259, 'grad_norm': 0.11692290008068085, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9289155006408691, 'eval_runtime': 6.4732, 'eval_samples_per_second': 154.329, 'eval_steps_per_second': 9.732, 'epoch': 0.84}
{'loss': 1.033, 'grad_norm': 0.07144400477409363, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9177901744842529, 'eval_runtime': 6.461, 'eval_samples_per_second': 154.62, 'eval_steps_per_second': 9.751, 'epoch': 0.88}
{'loss': 0.9462, 'grad_norm': 0.07211045175790787, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9116615653038025, 'eval_runtime': 6.4423, 'eval_samples_per_second': 155.07, 'eval_steps_per_second': 9.779, 'epoch': 0.92}
{'loss': 0.926, 'grad_norm': 0.0838194414973259, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9081428050994873, 'eval_runtime': 6.4446, 'eval_samples_per_second': 155.014, 'eval_steps_per_second': 9.776, 'epoch': 0.96}
{'loss': 0.9503, 'grad_norm': 0.07793990522623062, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9070146679878235, 'eval_runtime': 6.4448, 'eval_samples_per_second': 155.008, 'eval_steps_per_second': 9.775, 'epoch': 1.0}
{'train_runtime': 352.3005, 'train_samples_per_second': 28.376, 'train_steps_per_second': 1.774, 'train_loss': 1.3356232849121095, 'epoch': 1.0}
train_results:  {'eval_loss': [3.53346848487854, 1.8134316205978394, 1.4380906820297241, 1.338937759399414, 1.2997905015945435, 1.2697595357894897, 1.2465543746948242, 1.2216846942901611, 1.193735957145691, 1.1674903631210327, 1.1397422552108765, 1.1062010526657104, 1.0620018243789673, 1.0357604026794434, 1.017466425895691, 1.0056657791137695, 0.9908963441848755, 0.9735807776451111, 0.9586499929428101, 0.9463909864425659, 0.9289155006408691, 0.9177901744842529, 0.9116615653038025, 0.9081428050994873, 0.9070146679878235], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:43,  4.81it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 146.82it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 242.90it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 312.88it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 363.71it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 404.06it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 375.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8483144044876099
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8528 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.8166382608237643
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.7689435466868664
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.823532917330586
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.7980251432534713
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.8221571600365355
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0827, dtype=torch.float64), 0, 0, tensor(0.4116, dtype=torch.float64), 0, tensor(0.1629, dtype=torch.float64), 0, 0, tensor(0.3427, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 128, 0.004062062420920336, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0827, dtype=torch.float64), tensor(1.2995e-17, dtype=torch.float64), tensor(1.4572e-16, dtype=torch.float64), tensor(0.4116, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1629, dtype=torch.float64), tensor(8.5731e-18, dtype=torch.float64), tensor(1.4864e-17, dtype=torch.float64), tensor(0.3427, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0406, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.083
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.412
  triviaqa: 0
  truthfulqa_gen: 0.163
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.343

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.004062062420920336,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.004062062420920336
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 256.98it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 429.45it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 499.56it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 543.51it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 573.71it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 596.82it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 584.60it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.3569, 'grad_norm': 0.2813510596752167, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.0441436767578125, 'eval_runtime': 6.7864, 'eval_samples_per_second': 147.205, 'eval_steps_per_second': 9.283, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.34it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.76it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 501.85it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 544.10it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 573.56it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 593.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 583.76it/s]
Evaluation performance at step 50: 0.73
{'loss': 1.9607, 'grad_norm': 0.29981565475463867, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.2381166219711304, 'eval_runtime': 6.5185, 'eval_samples_per_second': 153.256, 'eval_steps_per_second': 9.665, 'epoch': 0.08}
{'loss': 1.1009, 'grad_norm': 0.08787357807159424, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0063209533691406, 'eval_runtime': 6.5392, 'eval_samples_per_second': 152.77, 'eval_steps_per_second': 9.634, 'epoch': 0.12}
{'loss': 0.9532, 'grad_norm': 0.07367324084043503, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8847178816795349, 'eval_runtime': 6.5375, 'eval_samples_per_second': 152.812, 'eval_steps_per_second': 9.637, 'epoch': 0.16}
{'loss': 0.873, 'grad_norm': 0.046599917113780975, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8649915456771851, 'eval_runtime': 6.5283, 'eval_samples_per_second': 153.025, 'eval_steps_per_second': 9.65, 'epoch': 0.2}
{'loss': 0.8646, 'grad_norm': 0.051752883940935135, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8521002531051636, 'eval_runtime': 6.5487, 'eval_samples_per_second': 152.55, 'eval_steps_per_second': 9.62, 'epoch': 0.24}
{'loss': 0.8274, 'grad_norm': 0.05684731900691986, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8430054187774658, 'eval_runtime': 6.5515, 'eval_samples_per_second': 152.485, 'eval_steps_per_second': 9.616, 'epoch': 0.28}
{'loss': 0.8159, 'grad_norm': 0.045247845351696014, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8349217176437378, 'eval_runtime': 6.5409, 'eval_samples_per_second': 152.732, 'eval_steps_per_second': 9.632, 'epoch': 0.32}
{'loss': 0.8405, 'grad_norm': 0.0658404603600502, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8275064826011658, 'eval_runtime': 6.5513, 'eval_samples_per_second': 152.489, 'eval_steps_per_second': 9.616, 'epoch': 0.36}
{'loss': 0.8355, 'grad_norm': 0.04748976230621338, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8210734724998474, 'eval_runtime': 6.55, 'eval_samples_per_second': 152.52, 'eval_steps_per_second': 9.618, 'epoch': 0.4}
{'loss': 0.8275, 'grad_norm': 0.0522337444126606, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8130351901054382, 'eval_runtime': 6.5524, 'eval_samples_per_second': 152.463, 'eval_steps_per_second': 9.615, 'epoch': 0.44}
{'loss': 0.8042, 'grad_norm': 0.045595187693834305, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8071557879447937, 'eval_runtime': 6.5429, 'eval_samples_per_second': 152.686, 'eval_steps_per_second': 9.629, 'epoch': 0.48}
{'loss': 0.8299, 'grad_norm': 0.05996186286211014, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8011418581008911, 'eval_runtime': 6.5509, 'eval_samples_per_second': 152.499, 'eval_steps_per_second': 9.617, 'epoch': 0.52}
{'loss': 0.8206, 'grad_norm': 0.06335411965847015, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7949262857437134, 'eval_runtime': 6.5564, 'eval_samples_per_second': 152.37, 'eval_steps_per_second': 9.609, 'epoch': 0.56}
{'loss': 0.7946, 'grad_norm': 0.05834343656897545, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7903019189834595, 'eval_runtime': 6.5653, 'eval_samples_per_second': 152.165, 'eval_steps_per_second': 9.596, 'epoch': 0.6}
{'loss': 0.8018, 'grad_norm': 0.04986429587006569, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7853353023529053, 'eval_runtime': 6.5648, 'eval_samples_per_second': 152.174, 'eval_steps_per_second': 9.597, 'epoch': 0.64}
{'loss': 0.7899, 'grad_norm': 0.0601019561290741, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.780013918876648, 'eval_runtime': 6.5572, 'eval_samples_per_second': 152.352, 'eval_steps_per_second': 9.608, 'epoch': 0.68}
{'loss': 0.7776, 'grad_norm': 0.06052577495574951, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7759450078010559, 'eval_runtime': 6.5529, 'eval_samples_per_second': 152.451, 'eval_steps_per_second': 9.614, 'epoch': 0.72}
{'loss': 0.7766, 'grad_norm': 0.06206994503736496, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7720484137535095, 'eval_runtime': 6.5499, 'eval_samples_per_second': 152.523, 'eval_steps_per_second': 9.619, 'epoch': 0.76}
{'loss': 0.8078, 'grad_norm': 0.058497052639722824, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.768665611743927, 'eval_runtime': 6.5595, 'eval_samples_per_second': 152.299, 'eval_steps_per_second': 9.604, 'epoch': 0.8}
{'loss': 0.7994, 'grad_norm': 0.06895951926708221, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7663931250572205, 'eval_runtime': 6.556, 'eval_samples_per_second': 152.379, 'eval_steps_per_second': 9.61, 'epoch': 0.84}
{'loss': 0.7857, 'grad_norm': 0.07650652527809143, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7631129622459412, 'eval_runtime': 6.5549, 'eval_samples_per_second': 152.405, 'eval_steps_per_second': 9.611, 'epoch': 0.88}
{'loss': 0.7989, 'grad_norm': 0.06551599502563477, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7610780596733093, 'eval_runtime': 6.5516, 'eval_samples_per_second': 152.482, 'eval_steps_per_second': 9.616, 'epoch': 0.92}
{'loss': 0.7844, 'grad_norm': 0.06871296465396881, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7597590088844299, 'eval_runtime': 6.5487, 'eval_samples_per_second': 152.549, 'eval_steps_per_second': 9.62, 'epoch': 0.96}
{'loss': 0.7938, 'grad_norm': 0.06035255268216133, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.759477436542511, 'eval_runtime': 6.5501, 'eval_samples_per_second': 152.518, 'eval_steps_per_second': 9.618, 'epoch': 1.0}
{'train_runtime': 360.8183, 'train_samples_per_second': 27.712, 'train_steps_per_second': 1.732, 'train_loss': 1.0168609222412108, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0441436767578125, 1.2381166219711304, 1.0063209533691406, 0.8847178816795349, 0.8649915456771851, 0.8521002531051636, 0.8430054187774658, 0.8349217176437378, 0.8275064826011658, 0.8210734724998474, 0.8130351901054382, 0.8071557879447937, 0.8011418581008911, 0.7949262857437134, 0.7903019189834595, 0.7853353023529053, 0.780013918876648, 0.7759450078010559, 0.7720484137535095, 0.768665611743927, 0.7663931250572205, 0.7631129622459412, 0.7610780596733093, 0.7597590088844299, 0.759477436542511], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:13,  2.58it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 172.84it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 263.83it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 323.13it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 363.04it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 395.49it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 368.08it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8481737375259399
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099, 0.8481737375259399]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5367 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.8159119198195423
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.7857387664708917
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.8240927082730878
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.8232733833621076
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.8189956872332756
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1208, dtype=torch.float64), 0, 0, tensor(0.0523, dtype=torch.float64), 0, tensor(0.4862, dtype=torch.float64), tensor(0.1312, dtype=torch.float64), tensor(0.1856, dtype=torch.float64), tensor(0.0239, dtype=torch.float64), 32, 0, 0, 1, 0, 0, 128, 0.047236545243694375, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.1208, dtype=torch.float64), tensor(1.7835e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0523, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4862, dtype=torch.float64), tensor(0.1312, dtype=torch.float64), tensor(0.1856, dtype=torch.float64), tensor(0.0239, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4724, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.121
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.052
  triviaqa: 0
  truthfulqa_gen: 0.486
  wikitext: 0.131
  mmlu: 0.186
  arc_challenge: 0.024

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.047236545243694375,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.047236545243694375
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 301.22it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 498.18it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 578.41it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 630.98it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 668.19it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 695.70it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 680.89it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.5153, 'grad_norm': 0.19904205203056335, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.6173434257507324, 'eval_runtime': 7.2163, 'eval_samples_per_second': 138.437, 'eval_steps_per_second': 8.73, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 299.26it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 497.23it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 575.85it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 626.17it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 662.73it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 689.61it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 675.94it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.6288, 'grad_norm': 0.07320108264684677, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.990853190422058, 'eval_runtime': 7.2158, 'eval_samples_per_second': 138.446, 'eval_steps_per_second': 8.731, 'epoch': 0.08}
{'loss': 1.7173, 'grad_norm': 0.12161780148744583, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6135063171386719, 'eval_runtime': 7.2264, 'eval_samples_per_second': 138.243, 'eval_steps_per_second': 8.718, 'epoch': 0.12}
{'loss': 1.5417, 'grad_norm': 0.09171265363693237, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4800076484680176, 'eval_runtime': 7.2833, 'eval_samples_per_second': 137.162, 'eval_steps_per_second': 8.65, 'epoch': 0.16}
{'loss': 1.4649, 'grad_norm': 0.053497493267059326, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4273674488067627, 'eval_runtime': 7.282, 'eval_samples_per_second': 137.187, 'eval_steps_per_second': 8.651, 'epoch': 0.2}
{'loss': 1.4595, 'grad_norm': 0.050859056413173676, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3960168361663818, 'eval_runtime': 7.2929, 'eval_samples_per_second': 136.983, 'eval_steps_per_second': 8.639, 'epoch': 0.24}
{'loss': 1.4231, 'grad_norm': 0.05965902656316757, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.372679352760315, 'eval_runtime': 7.308, 'eval_samples_per_second': 136.7, 'eval_steps_per_second': 8.621, 'epoch': 0.28}
{'loss': 1.3981, 'grad_norm': 0.05177588015794754, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3499740362167358, 'eval_runtime': 7.3131, 'eval_samples_per_second': 136.604, 'eval_steps_per_second': 8.615, 'epoch': 0.32}
{'loss': 1.3407, 'grad_norm': 0.05223262310028076, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.331908106803894, 'eval_runtime': 7.2866, 'eval_samples_per_second': 137.1, 'eval_steps_per_second': 8.646, 'epoch': 0.36}
{'loss': 1.3672, 'grad_norm': 0.07030168175697327, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3119736909866333, 'eval_runtime': 7.3041, 'eval_samples_per_second': 136.773, 'eval_steps_per_second': 8.625, 'epoch': 0.4}
{'loss': 1.3843, 'grad_norm': 0.05495423078536987, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.298250436782837, 'eval_runtime': 7.3132, 'eval_samples_per_second': 136.603, 'eval_steps_per_second': 8.615, 'epoch': 0.44}
{'loss': 1.3435, 'grad_norm': 0.04980828985571861, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2760207653045654, 'eval_runtime': 7.3134, 'eval_samples_per_second': 136.598, 'eval_steps_per_second': 8.614, 'epoch': 0.48}
{'loss': 1.2899, 'grad_norm': 0.06643534451723099, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2585575580596924, 'eval_runtime': 7.3102, 'eval_samples_per_second': 136.658, 'eval_steps_per_second': 8.618, 'epoch': 0.52}
{'loss': 1.3194, 'grad_norm': 0.06702487170696259, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.240264892578125, 'eval_runtime': 7.3002, 'eval_samples_per_second': 136.845, 'eval_steps_per_second': 8.63, 'epoch': 0.56}
{'loss': 1.2912, 'grad_norm': 0.06507903337478638, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2204408645629883, 'eval_runtime': 7.3067, 'eval_samples_per_second': 136.723, 'eval_steps_per_second': 8.622, 'epoch': 0.6}
{'loss': 1.2236, 'grad_norm': 0.0654660314321518, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1980867385864258, 'eval_runtime': 7.3153, 'eval_samples_per_second': 136.562, 'eval_steps_per_second': 8.612, 'epoch': 0.64}
{'loss': 1.2293, 'grad_norm': 0.09882039576768875, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1699007749557495, 'eval_runtime': 7.2959, 'eval_samples_per_second': 136.926, 'eval_steps_per_second': 8.635, 'epoch': 0.68}
{'loss': 1.1589, 'grad_norm': 0.08676548302173615, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1384607553482056, 'eval_runtime': 7.3076, 'eval_samples_per_second': 136.708, 'eval_steps_per_second': 8.621, 'epoch': 0.72}
{'loss': 1.0842, 'grad_norm': 0.08243262767791748, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1199816465377808, 'eval_runtime': 7.2987, 'eval_samples_per_second': 136.873, 'eval_steps_per_second': 8.632, 'epoch': 0.76}
{'loss': 1.1453, 'grad_norm': 0.0749504342675209, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.110314130783081, 'eval_runtime': 7.3025, 'eval_samples_per_second': 136.802, 'eval_steps_per_second': 8.627, 'epoch': 0.8}
{'loss': 1.1682, 'grad_norm': 0.08266974985599518, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1027976274490356, 'eval_runtime': 7.2918, 'eval_samples_per_second': 137.004, 'eval_steps_per_second': 8.64, 'epoch': 0.84}
{'loss': 1.1556, 'grad_norm': 0.0700250193476677, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0967613458633423, 'eval_runtime': 7.29, 'eval_samples_per_second': 137.038, 'eval_steps_per_second': 8.642, 'epoch': 0.88}
{'loss': 1.1309, 'grad_norm': 0.06886599957942963, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0921133756637573, 'eval_runtime': 7.2802, 'eval_samples_per_second': 137.221, 'eval_steps_per_second': 8.654, 'epoch': 0.92}
{'loss': 1.1599, 'grad_norm': 0.07898710668087006, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0890235900878906, 'eval_runtime': 7.3007, 'eval_samples_per_second': 136.836, 'eval_steps_per_second': 8.629, 'epoch': 0.96}
{'loss': 1.1231, 'grad_norm': 0.07698704302310944, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0881613492965698, 'eval_runtime': 7.2951, 'eval_samples_per_second': 136.941, 'eval_steps_per_second': 8.636, 'epoch': 1.0}
{'train_runtime': 380.1542, 'train_samples_per_second': 26.3, 'train_steps_per_second': 1.644, 'train_loss': 1.4825562072753906, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6173434257507324, 1.990853190422058, 1.6135063171386719, 1.4800076484680176, 1.4273674488067627, 1.3960168361663818, 1.372679352760315, 1.3499740362167358, 1.331908106803894, 1.3119736909866333, 1.298250436782837, 1.2760207653045654, 1.2585575580596924, 1.240264892578125, 1.2204408645629883, 1.1980867385864258, 1.1699007749557495, 1.1384607553482056, 1.1199816465377808, 1.110314130783081, 1.1027976274490356, 1.0967613458633423, 1.0921133756637573, 1.0890235900878906, 1.0881613492965698], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<07:34,  1.10it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 99.66it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 186.62it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 261.05it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 322.98it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 376.00it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 283.52it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8477031588554382
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099, 0.8481737375259399, 0.8477031588554382]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6735 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.8029341887651437
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.8186285422222037
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.7898741116449304
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.8187263731026774
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.8084536417154841
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1933, dtype=torch.float64), 0, tensor(0.2002, dtype=torch.float64), tensor(0.1365, dtype=torch.float64), tensor(0.0406, dtype=torch.float64), tensor(0.4293, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.02994073813896644, 1.4800000190734885, 1]
normalized proposed parameters for next round by BO: [tensor(5.8561e-16, dtype=torch.float64), tensor(1.4210e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1933, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2002, dtype=torch.float64), tensor(0.1365, dtype=torch.float64), tensor(0.0406, dtype=torch.float64), tensor(0.4293, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2994, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.193
  triviaqa: 0
  truthfulqa_gen: 0.2
  wikitext: 0.137
  mmlu: 0.041
  arc_challenge: 0.429

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.02994073813896644,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.02994073813896644
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 285.97it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 478.69it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.31it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 607.43it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 641.57it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 666.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.88it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.1526, 'grad_norm': 0.17338526248931885, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.387953758239746, 'eval_runtime': 7.241, 'eval_samples_per_second': 137.964, 'eval_steps_per_second': 8.7, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 288.73it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.29it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.44it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 605.51it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 640.05it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 666.03it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.73it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.362, 'grad_norm': 0.07060570269823074, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.751603126525879, 'eval_runtime': 7.2626, 'eval_samples_per_second': 137.555, 'eval_steps_per_second': 8.675, 'epoch': 0.08}
{'loss': 1.5227, 'grad_norm': 0.09906253963708878, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4822505712509155, 'eval_runtime': 7.3001, 'eval_samples_per_second': 136.847, 'eval_steps_per_second': 8.63, 'epoch': 0.12}
{'loss': 1.3951, 'grad_norm': 0.0662461519241333, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.362106204032898, 'eval_runtime': 7.2974, 'eval_samples_per_second': 136.899, 'eval_steps_per_second': 8.633, 'epoch': 0.16}
{'loss': 1.297, 'grad_norm': 0.04503622651100159, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3172781467437744, 'eval_runtime': 7.3041, 'eval_samples_per_second': 136.772, 'eval_steps_per_second': 8.625, 'epoch': 0.2}
{'loss': 1.3052, 'grad_norm': 0.037509869784116745, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2934927940368652, 'eval_runtime': 7.3153, 'eval_samples_per_second': 136.563, 'eval_steps_per_second': 8.612, 'epoch': 0.24}
{'loss': 1.2914, 'grad_norm': 0.04601699486374855, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2720078229904175, 'eval_runtime': 7.3343, 'eval_samples_per_second': 136.21, 'eval_steps_per_second': 8.59, 'epoch': 0.28}
{'loss': 1.2165, 'grad_norm': 0.04469527304172516, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.253807544708252, 'eval_runtime': 7.3341, 'eval_samples_per_second': 136.212, 'eval_steps_per_second': 8.59, 'epoch': 0.32}
{'loss': 1.3179, 'grad_norm': 0.04043589159846306, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.236762285232544, 'eval_runtime': 7.3483, 'eval_samples_per_second': 135.95, 'eval_steps_per_second': 8.573, 'epoch': 0.36}
{'loss': 1.1834, 'grad_norm': 0.05455955117940903, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2207294702529907, 'eval_runtime': 7.3549, 'eval_samples_per_second': 135.828, 'eval_steps_per_second': 8.566, 'epoch': 0.4}
{'loss': 1.187, 'grad_norm': 0.039515674114227295, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2070661783218384, 'eval_runtime': 7.3468, 'eval_samples_per_second': 135.977, 'eval_steps_per_second': 8.575, 'epoch': 0.44}
{'loss': 1.1427, 'grad_norm': 0.05185253173112869, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1893407106399536, 'eval_runtime': 7.3436, 'eval_samples_per_second': 136.037, 'eval_steps_per_second': 8.579, 'epoch': 0.48}
{'loss': 1.2244, 'grad_norm': 0.04733920469880104, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1756705045700073, 'eval_runtime': 7.3416, 'eval_samples_per_second': 136.073, 'eval_steps_per_second': 8.581, 'epoch': 0.52}
{'loss': 1.1056, 'grad_norm': 0.06230118125677109, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1581281423568726, 'eval_runtime': 7.3392, 'eval_samples_per_second': 136.118, 'eval_steps_per_second': 8.584, 'epoch': 0.56}
{'loss': 1.1602, 'grad_norm': 0.0530330054461956, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1367167234420776, 'eval_runtime': 7.3706, 'eval_samples_per_second': 135.538, 'eval_steps_per_second': 8.547, 'epoch': 0.6}
{'loss': 1.1427, 'grad_norm': 0.06683231145143509, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1117825508117676, 'eval_runtime': 7.3474, 'eval_samples_per_second': 135.967, 'eval_steps_per_second': 8.575, 'epoch': 0.64}
{'loss': 1.0471, 'grad_norm': 0.07394281774759293, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0799373388290405, 'eval_runtime': 7.3462, 'eval_samples_per_second': 135.989, 'eval_steps_per_second': 8.576, 'epoch': 0.68}
{'loss': 1.0372, 'grad_norm': 0.058593135327100754, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.061683177947998, 'eval_runtime': 7.3495, 'eval_samples_per_second': 135.927, 'eval_steps_per_second': 8.572, 'epoch': 0.72}
{'loss': 1.0697, 'grad_norm': 0.051767632365226746, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0513176918029785, 'eval_runtime': 7.341, 'eval_samples_per_second': 136.084, 'eval_steps_per_second': 8.582, 'epoch': 0.76}
{'loss': 1.0275, 'grad_norm': 0.05811265856027603, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0438882112503052, 'eval_runtime': 7.3482, 'eval_samples_per_second': 135.952, 'eval_steps_per_second': 8.574, 'epoch': 0.8}
{'loss': 1.0785, 'grad_norm': 0.06237929314374924, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0376044511795044, 'eval_runtime': 7.3439, 'eval_samples_per_second': 136.032, 'eval_steps_per_second': 8.579, 'epoch': 0.84}
{'loss': 1.0169, 'grad_norm': 0.06531517207622528, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0336607694625854, 'eval_runtime': 7.343, 'eval_samples_per_second': 136.049, 'eval_steps_per_second': 8.58, 'epoch': 0.88}
{'loss': 0.9816, 'grad_norm': 0.06266395002603531, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.030856728553772, 'eval_runtime': 7.3419, 'eval_samples_per_second': 136.067, 'eval_steps_per_second': 8.581, 'epoch': 0.92}
{'loss': 1.0545, 'grad_norm': 0.06442568451166153, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.028016209602356, 'eval_runtime': 7.3394, 'eval_samples_per_second': 136.114, 'eval_steps_per_second': 8.584, 'epoch': 0.96}
{'loss': 1.0356, 'grad_norm': 0.07499128580093384, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0272988080978394, 'eval_runtime': 7.3374, 'eval_samples_per_second': 136.151, 'eval_steps_per_second': 8.586, 'epoch': 1.0}
{'train_runtime': 384.2315, 'train_samples_per_second': 26.021, 'train_steps_per_second': 1.627, 'train_loss': 1.3342015106201173, 'epoch': 1.0}
train_results:  {'eval_loss': [3.387953758239746, 1.751603126525879, 1.4822505712509155, 1.362106204032898, 1.3172781467437744, 1.2934927940368652, 1.2720078229904175, 1.253807544708252, 1.236762285232544, 1.2207294702529907, 1.2070661783218384, 1.1893407106399536, 1.1756705045700073, 1.1581281423568726, 1.1367167234420776, 1.1117825508117676, 1.0799373388290405, 1.061683177947998, 1.0513176918029785, 1.0438882112503052, 1.0376044511795044, 1.0336607694625854, 1.030856728553772, 1.028016209602356, 1.0272988080978394], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:23,  1.54it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 98.55it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 183.38it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 255.96it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 316.00it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 367.81it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 291.61it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8482542634010315
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099, 0.8481737375259399, 0.8477031588554382, 0.8482542634010315]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5199 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.8109649347022618
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.818629211541101
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.7657894724475205
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.798133051332693
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.7940547957838215
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0566, dtype=torch.float64), 0, 0, tensor(0.4912, dtype=torch.float64), 0, tensor(0.3555, dtype=torch.float64), tensor(0.0453, dtype=torch.float64), 0, tensor(0.0515, dtype=torch.float64), 32, 1, 1, 1, 0, 0, 128, 0.020784737501264975, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0566, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4912, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3555, dtype=torch.float64), tensor(0.0453, dtype=torch.float64), tensor(2.8178e-15, dtype=torch.float64), tensor(0.0515, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2078, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.057
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.491
  triviaqa: 0
  truthfulqa_gen: 0.355
  wikitext: 0.045
  mmlu: 0
  arc_challenge: 0.051

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.020784737501264975,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.020784737501264975
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.92it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.90it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 542.21it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 589.33it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 621.94it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 646.53it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 633.34it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.8732, 'grad_norm': 0.2843995690345764, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.63204026222229, 'eval_runtime': 5.5184, 'eval_samples_per_second': 181.032, 'eval_steps_per_second': 11.416, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 278.06it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 463.57it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 540.21it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 587.16it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 619.67it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 644.86it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 631.45it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.4528, 'grad_norm': 0.10813738405704498, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.7520484924316406, 'eval_runtime': 5.5365, 'eval_samples_per_second': 180.438, 'eval_steps_per_second': 11.379, 'epoch': 0.08}
{'loss': 1.5574, 'grad_norm': 0.09534235298633575, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3760957717895508, 'eval_runtime': 5.5475, 'eval_samples_per_second': 180.08, 'eval_steps_per_second': 11.356, 'epoch': 0.12}
{'loss': 1.3145, 'grad_norm': 0.048269279301166534, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2732058763504028, 'eval_runtime': 5.5668, 'eval_samples_per_second': 179.457, 'eval_steps_per_second': 11.317, 'epoch': 0.16}
{'loss': 1.2706, 'grad_norm': 0.05186723545193672, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2332462072372437, 'eval_runtime': 5.5514, 'eval_samples_per_second': 179.954, 'eval_steps_per_second': 11.348, 'epoch': 0.2}
{'loss': 1.2517, 'grad_norm': 0.05266404151916504, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.203442931175232, 'eval_runtime': 5.5684, 'eval_samples_per_second': 179.406, 'eval_steps_per_second': 11.314, 'epoch': 0.24}
{'loss': 1.1903, 'grad_norm': 0.05561254918575287, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1752618551254272, 'eval_runtime': 5.5989, 'eval_samples_per_second': 178.427, 'eval_steps_per_second': 11.252, 'epoch': 0.28}
{'loss': 1.1717, 'grad_norm': 0.05448184162378311, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1484273672103882, 'eval_runtime': 5.5763, 'eval_samples_per_second': 179.15, 'eval_steps_per_second': 11.298, 'epoch': 0.32}
{'loss': 1.1179, 'grad_norm': 0.06379637122154236, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1132452487945557, 'eval_runtime': 5.6011, 'eval_samples_per_second': 178.358, 'eval_steps_per_second': 11.248, 'epoch': 0.36}
{'loss': 1.1183, 'grad_norm': 0.07770439237356186, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0760531425476074, 'eval_runtime': 5.5774, 'eval_samples_per_second': 179.116, 'eval_steps_per_second': 11.296, 'epoch': 0.4}
{'loss': 1.0892, 'grad_norm': 0.07996540516614914, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0143767595291138, 'eval_runtime': 5.5824, 'eval_samples_per_second': 178.955, 'eval_steps_per_second': 11.285, 'epoch': 0.44}
{'loss': 1.0052, 'grad_norm': 0.06545092165470123, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9728692173957825, 'eval_runtime': 5.5835, 'eval_samples_per_second': 178.92, 'eval_steps_per_second': 11.283, 'epoch': 0.48}
{'loss': 0.9525, 'grad_norm': 0.0652545690536499, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9456387758255005, 'eval_runtime': 5.59, 'eval_samples_per_second': 178.713, 'eval_steps_per_second': 11.27, 'epoch': 0.52}
{'loss': 0.8737, 'grad_norm': 0.0605894960463047, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9160929918289185, 'eval_runtime': 5.5946, 'eval_samples_per_second': 178.566, 'eval_steps_per_second': 11.261, 'epoch': 0.56}
{'loss': 0.8955, 'grad_norm': 0.07083088159561157, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8818833827972412, 'eval_runtime': 5.5874, 'eval_samples_per_second': 178.796, 'eval_steps_per_second': 11.275, 'epoch': 0.6}
{'loss': 0.9332, 'grad_norm': 0.07370399683713913, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8545493483543396, 'eval_runtime': 5.5792, 'eval_samples_per_second': 179.057, 'eval_steps_per_second': 11.292, 'epoch': 0.64}
{'loss': 0.7946, 'grad_norm': 0.07084956765174866, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8461284637451172, 'eval_runtime': 5.5777, 'eval_samples_per_second': 179.106, 'eval_steps_per_second': 11.295, 'epoch': 0.68}
{'loss': 0.8337, 'grad_norm': 0.08029469102621078, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8406252264976501, 'eval_runtime': 5.5813, 'eval_samples_per_second': 178.991, 'eval_steps_per_second': 11.288, 'epoch': 0.72}
{'loss': 0.8398, 'grad_norm': 0.0760180652141571, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8366957306861877, 'eval_runtime': 5.598, 'eval_samples_per_second': 178.456, 'eval_steps_per_second': 11.254, 'epoch': 0.76}
{'loss': 0.8229, 'grad_norm': 0.08632761240005493, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8314754962921143, 'eval_runtime': 5.5765, 'eval_samples_per_second': 179.144, 'eval_steps_per_second': 11.297, 'epoch': 0.8}
{'loss': 0.8693, 'grad_norm': 0.0796143040060997, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8281089663505554, 'eval_runtime': 5.5807, 'eval_samples_per_second': 179.009, 'eval_steps_per_second': 11.289, 'epoch': 0.84}
{'loss': 0.8268, 'grad_norm': 0.07486610114574432, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8257268071174622, 'eval_runtime': 5.5773, 'eval_samples_per_second': 179.119, 'eval_steps_per_second': 11.296, 'epoch': 0.88}
{'loss': 0.825, 'grad_norm': 0.07859484106302261, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8237409591674805, 'eval_runtime': 5.5817, 'eval_samples_per_second': 178.978, 'eval_steps_per_second': 11.287, 'epoch': 0.92}
{'loss': 0.8677, 'grad_norm': 0.08534493297338486, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8229694366455078, 'eval_runtime': 5.5809, 'eval_samples_per_second': 179.004, 'eval_steps_per_second': 11.289, 'epoch': 0.96}
{'loss': 0.8677, 'grad_norm': 0.09351842850446701, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8221576809883118, 'eval_runtime': 5.5872, 'eval_samples_per_second': 178.802, 'eval_steps_per_second': 11.276, 'epoch': 1.0}
{'train_runtime': 299.2084, 'train_samples_per_second': 33.411, 'train_steps_per_second': 2.089, 'train_loss': 1.2246093292236329, 'epoch': 1.0}
train_results:  {'eval_loss': [3.63204026222229, 1.7520484924316406, 1.3760957717895508, 1.2732058763504028, 1.2332462072372437, 1.203442931175232, 1.1752618551254272, 1.1484273672103882, 1.1132452487945557, 1.0760531425476074, 1.0143767595291138, 0.9728692173957825, 0.9456387758255005, 0.9160929918289185, 0.8818833827972412, 0.8545493483543396, 0.8461284637451172, 0.8406252264976501, 0.8366957306861877, 0.8314754962921143, 0.8281089663505554, 0.8257268071174622, 0.8237409591674805, 0.8229694366455078, 0.8221576809883118], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:19,  3.58it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 212.75it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 310.60it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 368.62it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 405.17it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 437.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 423.25it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8483649492263794
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099, 0.8481737375259399, 0.8477031588554382, 0.8482542634010315, 0.8483649492263794]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6418 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.8095901396794621
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.7618541570629954
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.7602881122049433
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.8051557034610641
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.8158361886004177
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0515, dtype=torch.float64), 0, 0, tensor(0.2439, dtype=torch.float64), 0, tensor(0.6412, dtype=torch.float64), 0, 0, tensor(0.0580, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.060294707117817536, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0515, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2439, dtype=torch.float64), tensor(4.8284e-17, dtype=torch.float64), tensor(0.6412, dtype=torch.float64), tensor(0.0054, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0580, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6029, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.051
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.244
  triviaqa: 0
  truthfulqa_gen: 0.641
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.058

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.060294707117817536,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.060294707117817536
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9944
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  994
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 286.83it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 478.64it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.44it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 606.93it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 640.98it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 666.92it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.99it/s]
Evaluation performance at step 25: 0.75
{'loss': 5.0419, 'grad_norm': 0.23279497027397156, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.7990541458129883, 'eval_runtime': 4.9466, 'eval_samples_per_second': 200.945, 'eval_steps_per_second': 12.736, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 288.05it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.68it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.70it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 607.22it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 640.58it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 665.85it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.88it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5741, 'grad_norm': 0.07264076918363571, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.7898364067077637, 'eval_runtime': 4.673, 'eval_samples_per_second': 212.713, 'eval_steps_per_second': 13.482, 'epoch': 0.08}
{'loss': 1.5408, 'grad_norm': 0.15216003358364105, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.3854119777679443, 'eval_runtime': 4.6926, 'eval_samples_per_second': 211.822, 'eval_steps_per_second': 13.425, 'epoch': 0.12}
{'loss': 1.3141, 'grad_norm': 0.09488195925951004, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.2179540395736694, 'eval_runtime': 4.6838, 'eval_samples_per_second': 212.22, 'eval_steps_per_second': 13.451, 'epoch': 0.16}
{'loss': 1.2117, 'grad_norm': 0.05688725411891937, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.163404107093811, 'eval_runtime': 4.7031, 'eval_samples_per_second': 211.352, 'eval_steps_per_second': 13.396, 'epoch': 0.2}
{'loss': 1.1623, 'grad_norm': 0.0510401614010334, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.1325336694717407, 'eval_runtime': 4.706, 'eval_samples_per_second': 211.219, 'eval_steps_per_second': 13.387, 'epoch': 0.24}
{'loss': 1.1416, 'grad_norm': 0.04654831439256668, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.1091082096099854, 'eval_runtime': 4.7181, 'eval_samples_per_second': 210.677, 'eval_steps_per_second': 13.353, 'epoch': 0.28}
{'loss': 1.0916, 'grad_norm': 0.04869621992111206, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.0840480327606201, 'eval_runtime': 4.7207, 'eval_samples_per_second': 210.563, 'eval_steps_per_second': 13.346, 'epoch': 0.32}
{'loss': 1.0831, 'grad_norm': 0.0600118450820446, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.0605896711349487, 'eval_runtime': 4.7212, 'eval_samples_per_second': 210.541, 'eval_steps_per_second': 13.344, 'epoch': 0.36}
{'loss': 1.0834, 'grad_norm': 0.05766398459672928, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.0381911993026733, 'eval_runtime': 4.7204, 'eval_samples_per_second': 210.574, 'eval_steps_per_second': 13.346, 'epoch': 0.4}
{'loss': 1.0268, 'grad_norm': 0.06287781149148941, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.0129226446151733, 'eval_runtime': 4.7199, 'eval_samples_per_second': 210.598, 'eval_steps_per_second': 13.348, 'epoch': 0.44}
{'loss': 1.0204, 'grad_norm': 0.07204202562570572, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.9839965105056763, 'eval_runtime': 4.7258, 'eval_samples_per_second': 210.333, 'eval_steps_per_second': 13.331, 'epoch': 0.48}
{'loss': 1.0009, 'grad_norm': 0.08627619594335556, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.9482691884040833, 'eval_runtime': 4.719, 'eval_samples_per_second': 210.637, 'eval_steps_per_second': 13.35, 'epoch': 0.52}
{'loss': 0.9561, 'grad_norm': 0.09878653287887573, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.8989809155464172, 'eval_runtime': 4.7249, 'eval_samples_per_second': 210.374, 'eval_steps_per_second': 13.334, 'epoch': 0.56}
{'loss': 0.8642, 'grad_norm': 0.08786672353744507, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.8324291706085205, 'eval_runtime': 4.7197, 'eval_samples_per_second': 210.605, 'eval_steps_per_second': 13.348, 'epoch': 0.6}
{'loss': 0.8163, 'grad_norm': 0.07448472827672958, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.8024531602859497, 'eval_runtime': 4.7182, 'eval_samples_per_second': 210.675, 'eval_steps_per_second': 13.353, 'epoch': 0.64}
{'loss': 0.8048, 'grad_norm': 0.08701183646917343, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.7837322950363159, 'eval_runtime': 4.7197, 'eval_samples_per_second': 210.609, 'eval_steps_per_second': 13.348, 'epoch': 0.68}
{'loss': 0.78, 'grad_norm': 0.09526755660772324, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.7586966156959534, 'eval_runtime': 4.7157, 'eval_samples_per_second': 210.784, 'eval_steps_per_second': 13.36, 'epoch': 0.72}
{'loss': 0.782, 'grad_norm': 0.08615124225616455, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.7406741380691528, 'eval_runtime': 4.7236, 'eval_samples_per_second': 210.432, 'eval_steps_per_second': 13.337, 'epoch': 0.76}
{'loss': 0.7518, 'grad_norm': 0.08954796195030212, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.7221993803977966, 'eval_runtime': 4.7194, 'eval_samples_per_second': 210.621, 'eval_steps_per_second': 13.349, 'epoch': 0.8}
{'loss': 0.748, 'grad_norm': 0.0959492102265358, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.7063345909118652, 'eval_runtime': 4.7232, 'eval_samples_per_second': 210.45, 'eval_steps_per_second': 13.338, 'epoch': 0.84}
{'loss': 0.7322, 'grad_norm': 0.10773561149835587, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.6910473108291626, 'eval_runtime': 4.72, 'eval_samples_per_second': 210.595, 'eval_steps_per_second': 13.348, 'epoch': 0.88}
{'loss': 0.7129, 'grad_norm': 0.10207002609968185, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.6793984174728394, 'eval_runtime': 4.7221, 'eval_samples_per_second': 210.498, 'eval_steps_per_second': 13.341, 'epoch': 0.92}
{'loss': 0.7096, 'grad_norm': 0.10286030918359756, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.6730484366416931, 'eval_runtime': 4.7228, 'eval_samples_per_second': 210.47, 'eval_steps_per_second': 13.34, 'epoch': 0.96}
{'train_runtime': 262.1836, 'train_samples_per_second': 37.928, 'train_steps_per_second': 2.372, 'train_loss': 1.1885696024756722, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7990541458129883, 1.7898364067077637, 1.3854119777679443, 1.2179540395736694, 1.163404107093811, 1.1325336694717407, 1.1091082096099854, 1.0840480327606201, 1.0605896711349487, 1.0381911993026733, 1.0129226446151733, 0.9839965105056763, 0.9482691884040833, 0.8989809155464172, 0.8324291706085205, 0.8024531602859497, 0.7837322950363159, 0.7586966156959534, 0.7406741380691528, 0.7221993803977966, 0.7063345909118652, 0.6910473108291626, 0.6793984174728394, 0.6730484366416931], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:42,  4.89it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 164.42it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 263.97it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 333.46it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 382.53it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 422.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 398.43it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8484199047088623
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099, 0.8481737375259399, 0.8477031588554382, 0.8482542634010315, 0.8483649492263794, 0.8484199047088623]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0553 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.8211032794513999
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.8105261432694796
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.7883549683894698
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.8196354618216304
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.8204791523745383
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0496, dtype=torch.float64), 0, 0, tensor(0.3308, dtype=torch.float64), 0, tensor(0.1226, dtype=torch.float64), tensor(0.3062, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), tensor(0.1000, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 128, 0.04637037653855247, 1.480000019073489, 1]
normalized proposed parameters for next round by BO: [tensor(0.0496, dtype=torch.float64), tensor(3.3891e-17, dtype=torch.float64), tensor(3.3122e-16, dtype=torch.float64), tensor(0.3308, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1226, dtype=torch.float64), tensor(0.3062, dtype=torch.float64), tensor(0.0908, dtype=torch.float64), tensor(0.1000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4637, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.05
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.331
  triviaqa: 0
  truthfulqa_gen: 0.123
  wikitext: 0.306
  mmlu: 0.091
  arc_challenge: 0.1

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04637037653855247,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.480000019073489,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.04637037653855247
lora alpha:  1.480000019073489
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.30it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 431.32it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 502.41it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 545.86it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 576.04it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 598.98it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 586.70it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.179, 'grad_norm': 0.2680811882019043, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.173940658569336, 'eval_runtime': 8.4226, 'eval_samples_per_second': 118.61, 'eval_steps_per_second': 7.48, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.41it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.08it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 500.93it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 544.16it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 574.75it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 596.99it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 585.25it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.3031, 'grad_norm': 0.22704508900642395, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.7938504219055176, 'eval_runtime': 8.427, 'eval_samples_per_second': 118.547, 'eval_steps_per_second': 7.476, 'epoch': 0.08}
{'loss': 1.5946, 'grad_norm': 0.12861810624599457, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5202603340148926, 'eval_runtime': 8.4313, 'eval_samples_per_second': 118.488, 'eval_steps_per_second': 7.472, 'epoch': 0.12}
{'loss': 1.4721, 'grad_norm': 0.05704604834318161, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4185237884521484, 'eval_runtime': 8.4533, 'eval_samples_per_second': 118.178, 'eval_steps_per_second': 7.453, 'epoch': 0.16}
{'loss': 1.3187, 'grad_norm': 0.07860894501209259, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3307958841323853, 'eval_runtime': 8.4777, 'eval_samples_per_second': 117.838, 'eval_steps_per_second': 7.431, 'epoch': 0.2}
{'loss': 1.4011, 'grad_norm': 0.09343485534191132, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3022207021713257, 'eval_runtime': 8.48, 'eval_samples_per_second': 117.806, 'eval_steps_per_second': 7.429, 'epoch': 0.24}
{'loss': 1.3559, 'grad_norm': 0.12221429497003555, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2835007905960083, 'eval_runtime': 8.4963, 'eval_samples_per_second': 117.58, 'eval_steps_per_second': 7.415, 'epoch': 0.28}
{'loss': 1.2726, 'grad_norm': 0.06800032407045364, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2710955142974854, 'eval_runtime': 8.4867, 'eval_samples_per_second': 117.714, 'eval_steps_per_second': 7.423, 'epoch': 0.32}
{'loss': 1.293, 'grad_norm': 0.07446469366550446, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2596211433410645, 'eval_runtime': 8.488, 'eval_samples_per_second': 117.695, 'eval_steps_per_second': 7.422, 'epoch': 0.36}
{'loss': 1.2651, 'grad_norm': 0.06739874184131622, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2487331628799438, 'eval_runtime': 8.4985, 'eval_samples_per_second': 117.551, 'eval_steps_per_second': 7.413, 'epoch': 0.4}
{'loss': 1.2675, 'grad_norm': 0.07097528129816055, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2421178817749023, 'eval_runtime': 8.4855, 'eval_samples_per_second': 117.73, 'eval_steps_per_second': 7.424, 'epoch': 0.44}
{'loss': 1.2818, 'grad_norm': 0.05415702238678932, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2353298664093018, 'eval_runtime': 8.4842, 'eval_samples_per_second': 117.749, 'eval_steps_per_second': 7.426, 'epoch': 0.48}
{'loss': 1.2674, 'grad_norm': 0.07900286465883255, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.226383090019226, 'eval_runtime': 8.4816, 'eval_samples_per_second': 117.784, 'eval_steps_per_second': 7.428, 'epoch': 0.52}
{'loss': 1.3125, 'grad_norm': 0.06807659566402435, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2206954956054688, 'eval_runtime': 8.4839, 'eval_samples_per_second': 117.753, 'eval_steps_per_second': 7.426, 'epoch': 0.56}
{'loss': 1.2864, 'grad_norm': 0.05643165484070778, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2134898900985718, 'eval_runtime': 8.4838, 'eval_samples_per_second': 117.754, 'eval_steps_per_second': 7.426, 'epoch': 0.6}
{'loss': 1.156, 'grad_norm': 0.07134508341550827, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2099814414978027, 'eval_runtime': 8.4862, 'eval_samples_per_second': 117.72, 'eval_steps_per_second': 7.424, 'epoch': 0.64}
{'loss': 1.2496, 'grad_norm': 0.08038776367902756, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2055810689926147, 'eval_runtime': 8.4922, 'eval_samples_per_second': 117.638, 'eval_steps_per_second': 7.419, 'epoch': 0.68}
{'loss': 1.2291, 'grad_norm': 0.08456917852163315, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.20236074924469, 'eval_runtime': 8.4932, 'eval_samples_per_second': 117.623, 'eval_steps_per_second': 7.418, 'epoch': 0.72}
{'loss': 1.1901, 'grad_norm': 0.07937715202569962, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2001632452011108, 'eval_runtime': 8.489, 'eval_samples_per_second': 117.681, 'eval_steps_per_second': 7.421, 'epoch': 0.76}
{'loss': 1.1663, 'grad_norm': 0.0665641501545906, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1971744298934937, 'eval_runtime': 8.4882, 'eval_samples_per_second': 117.693, 'eval_steps_per_second': 7.422, 'epoch': 0.8}
{'loss': 1.2594, 'grad_norm': 0.07653874158859253, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1951169967651367, 'eval_runtime': 8.4846, 'eval_samples_per_second': 117.743, 'eval_steps_per_second': 7.425, 'epoch': 0.84}
{'loss': 1.2197, 'grad_norm': 0.07243639975786209, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1927968263626099, 'eval_runtime': 8.4886, 'eval_samples_per_second': 117.687, 'eval_steps_per_second': 7.422, 'epoch': 0.88}
{'loss': 1.2413, 'grad_norm': 0.062334686517715454, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1919147968292236, 'eval_runtime': 8.4938, 'eval_samples_per_second': 117.616, 'eval_steps_per_second': 7.417, 'epoch': 0.92}
{'loss': 1.2121, 'grad_norm': 0.06677644699811935, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1908550262451172, 'eval_runtime': 8.4868, 'eval_samples_per_second': 117.713, 'eval_steps_per_second': 7.423, 'epoch': 0.96}
{'loss': 1.2404, 'grad_norm': 0.0674699991941452, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.190530776977539, 'eval_runtime': 8.4879, 'eval_samples_per_second': 117.697, 'eval_steps_per_second': 7.422, 'epoch': 1.0}
{'train_runtime': 451.6538, 'train_samples_per_second': 22.136, 'train_steps_per_second': 1.384, 'train_loss': 1.4414020263671874, 'epoch': 1.0}
train_results:  {'eval_loss': [3.173940658569336, 1.7938504219055176, 1.5202603340148926, 1.4185237884521484, 1.3307958841323853, 1.3022207021713257, 1.2835007905960083, 1.2710955142974854, 1.2596211433410645, 1.2487331628799438, 1.2421178817749023, 1.2353298664093018, 1.226383090019226, 1.2206954956054688, 1.2134898900985718, 1.2099814414978027, 1.2055810689926147, 1.20236074924469, 1.2001632452011108, 1.1971744298934937, 1.1951169967651367, 1.1927968263626099, 1.1919147968292236, 1.1908550262451172, 1.190530776977539], 'performance': [0.76, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:03,  2.05it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 145.03it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 237.76it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 301.32it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 346.38it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 383.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 339.92it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8480094075202942
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099, 0.8481737375259399, 0.8477031588554382, 0.8482542634010315, 0.8483649492263794, 0.8484199047088623, 0.8480094075202942]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4157 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.7812687643007568
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.8221450245836331
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.8145120656007327
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.7728648419745693
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.7799724622356298
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0541, dtype=torch.float64), 0, 0, tensor(0.4813, dtype=torch.float64), 0, tensor(0.1883, dtype=torch.float64), tensor(0.1718, dtype=torch.float64), 0, tensor(0.1045, dtype=torch.float64), 32, 1, 1, 1, 0, 0, 128, 0.012037809516556952, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0541, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3592e-16, dtype=torch.float64), tensor(0.4813, dtype=torch.float64), tensor(1.3420e-16, dtype=torch.float64), tensor(0.1883, dtype=torch.float64), tensor(0.1718, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1045, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1204, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.054
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.481
  triviaqa: 0
  truthfulqa_gen: 0.188
  wikitext: 0.172
  mmlu: 0
  arc_challenge: 0.105

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.012037809516556952,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.012037809516556952
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.69it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 466.12it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 542.28it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 587.39it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 620.86it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 646.23it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 633.23it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.6553, 'grad_norm': 0.21604309976100922, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.532254219055176, 'eval_runtime': 6.6052, 'eval_samples_per_second': 151.246, 'eval_steps_per_second': 9.538, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 278.52it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.44it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.27it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 587.86it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 620.17it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 644.71it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 632.01it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5421, 'grad_norm': 0.09086182713508606, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8870364427566528, 'eval_runtime': 6.6173, 'eval_samples_per_second': 150.967, 'eval_steps_per_second': 9.52, 'epoch': 0.08}
{'loss': 1.5878, 'grad_norm': 0.09258070588111877, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5341895818710327, 'eval_runtime': 6.6471, 'eval_samples_per_second': 150.291, 'eval_steps_per_second': 9.478, 'epoch': 0.12}
{'loss': 1.4418, 'grad_norm': 0.04934540390968323, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4360615015029907, 'eval_runtime': 6.6401, 'eval_samples_per_second': 150.45, 'eval_steps_per_second': 9.488, 'epoch': 0.16}
{'loss': 1.388, 'grad_norm': 0.05281301587820053, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3942458629608154, 'eval_runtime': 6.6551, 'eval_samples_per_second': 150.111, 'eval_steps_per_second': 9.466, 'epoch': 0.2}
{'loss': 1.3656, 'grad_norm': 0.04733932390809059, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3627785444259644, 'eval_runtime': 6.6682, 'eval_samples_per_second': 149.816, 'eval_steps_per_second': 9.448, 'epoch': 0.24}
{'loss': 1.3792, 'grad_norm': 0.06881475448608398, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3347607851028442, 'eval_runtime': 6.6585, 'eval_samples_per_second': 150.033, 'eval_steps_per_second': 9.462, 'epoch': 0.28}
{'loss': 1.3069, 'grad_norm': 0.05543213710188866, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3110055923461914, 'eval_runtime': 6.6525, 'eval_samples_per_second': 150.168, 'eval_steps_per_second': 9.47, 'epoch': 0.32}
{'loss': 1.271, 'grad_norm': 0.05148710310459137, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2850233316421509, 'eval_runtime': 6.65, 'eval_samples_per_second': 150.226, 'eval_steps_per_second': 9.474, 'epoch': 0.36}
{'loss': 1.2395, 'grad_norm': 0.06617338955402374, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2607593536376953, 'eval_runtime': 6.6621, 'eval_samples_per_second': 149.952, 'eval_steps_per_second': 9.456, 'epoch': 0.4}
{'loss': 1.2368, 'grad_norm': 0.05939890816807747, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2360458374023438, 'eval_runtime': 6.6582, 'eval_samples_per_second': 150.04, 'eval_steps_per_second': 9.462, 'epoch': 0.44}
{'loss': 1.2769, 'grad_norm': 0.07450295239686966, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2084695100784302, 'eval_runtime': 6.6544, 'eval_samples_per_second': 150.127, 'eval_steps_per_second': 9.467, 'epoch': 0.48}
{'loss': 1.1283, 'grad_norm': 0.08868638426065445, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.174031376838684, 'eval_runtime': 6.6647, 'eval_samples_per_second': 149.893, 'eval_steps_per_second': 9.453, 'epoch': 0.52}
{'loss': 1.1786, 'grad_norm': 0.05721183866262436, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1448194980621338, 'eval_runtime': 6.6622, 'eval_samples_per_second': 149.951, 'eval_steps_per_second': 9.456, 'epoch': 0.56}
{'loss': 1.1498, 'grad_norm': 0.06960444152355194, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1311341524124146, 'eval_runtime': 6.6548, 'eval_samples_per_second': 150.118, 'eval_steps_per_second': 9.467, 'epoch': 0.6}
{'loss': 1.0995, 'grad_norm': 0.05870501324534416, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1185940504074097, 'eval_runtime': 6.6622, 'eval_samples_per_second': 149.951, 'eval_steps_per_second': 9.456, 'epoch': 0.64}
{'loss': 1.0954, 'grad_norm': 0.07414622604846954, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.110101342201233, 'eval_runtime': 6.6629, 'eval_samples_per_second': 149.934, 'eval_steps_per_second': 9.455, 'epoch': 0.68}
{'loss': 1.115, 'grad_norm': 0.06685170531272888, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.097856879234314, 'eval_runtime': 6.6614, 'eval_samples_per_second': 149.969, 'eval_steps_per_second': 9.458, 'epoch': 0.72}
{'loss': 1.1002, 'grad_norm': 0.07604683190584183, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0860487222671509, 'eval_runtime': 6.6609, 'eval_samples_per_second': 149.979, 'eval_steps_per_second': 9.458, 'epoch': 0.76}
{'loss': 1.0408, 'grad_norm': 0.06770118325948715, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0772532224655151, 'eval_runtime': 6.654, 'eval_samples_per_second': 150.136, 'eval_steps_per_second': 9.468, 'epoch': 0.8}
{'loss': 1.0655, 'grad_norm': 0.07206446677446365, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0676884651184082, 'eval_runtime': 6.6578, 'eval_samples_per_second': 150.049, 'eval_steps_per_second': 9.463, 'epoch': 0.84}
{'loss': 1.0691, 'grad_norm': 0.07583603262901306, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0594425201416016, 'eval_runtime': 6.6567, 'eval_samples_per_second': 150.074, 'eval_steps_per_second': 9.464, 'epoch': 0.88}
{'loss': 1.0694, 'grad_norm': 0.07141824811697006, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.052465796470642, 'eval_runtime': 6.6572, 'eval_samples_per_second': 150.064, 'eval_steps_per_second': 9.463, 'epoch': 0.92}
{'loss': 1.1012, 'grad_norm': 0.08064001053571701, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0484610795974731, 'eval_runtime': 6.6553, 'eval_samples_per_second': 150.107, 'eval_steps_per_second': 9.466, 'epoch': 0.96}
{'loss': 1.131, 'grad_norm': 0.08054710179567337, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0474830865859985, 'eval_runtime': 6.6526, 'eval_samples_per_second': 150.166, 'eval_steps_per_second': 9.47, 'epoch': 1.0}
{'train_runtime': 354.766, 'train_samples_per_second': 28.182, 'train_steps_per_second': 1.762, 'train_loss': 1.4013865051269532, 'epoch': 1.0}
train_results:  {'eval_loss': [3.532254219055176, 1.8870364427566528, 1.5341895818710327, 1.4360615015029907, 1.3942458629608154, 1.3627785444259644, 1.3347607851028442, 1.3110055923461914, 1.2850233316421509, 1.2607593536376953, 1.2360458374023438, 1.2084695100784302, 1.174031376838684, 1.1448194980621338, 1.1311341524124146, 1.1185940504074097, 1.110101342201233, 1.097856879234314, 1.0860487222671509, 1.0772532224655151, 1.0676884651184082, 1.0594425201416016, 1.052465796470642, 1.0484610795974731, 1.0474830865859985], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:21,  1.55it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:04, 104.34it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 189.89it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 261.09it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 318.11it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 366.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 295.42it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8484439253807068
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8190000000000001
BO observations:  [0.8227295875549316, 0.8317444920539856, 0.847478449344635, 0.8477747440338135, 0.8484109044075012, 0.8477435111999512, 0.837490439414978, 0.8483384847640991, 0.8343195915222168, 0.8145298957824707, 0.8141338229179382, 0.8473111391067505, 0.8472181558609009, 0.8321012854576111, 0.8389402031898499, 0.8474618792533875, 0.8474158644676208, 0.8458070755004883, 0.8477532863616943, 0.8479848504066467, 0.8482716083526611, 0.8485893607139587, 0.8483144044876099, 0.8481737375259399, 0.8477031588554382, 0.8482542634010315, 0.8483649492263794, 0.8484199047088623, 0.8480094075202942, 0.8484439253807068]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6688 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.8282583614979755
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.7806986336157403
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.8283111590259504
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.8212428884506959
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.7784324637964987
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0366, dtype=torch.float64), 0, 0, tensor(0.5132, dtype=torch.float64), 0, tensor(0.2301, dtype=torch.float64), tensor(0.0626, dtype=torch.float64), 0, tensor(0.1575, dtype=torch.float64), 32, 0, 0, 1, 0, 0, 128, 0.058751570487950405, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0366, dtype=torch.float64), tensor(1.4594e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5132, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2301, dtype=torch.float64), tensor(0.0626, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1575, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5875, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.798, 0.798, 0.798, 0.798, 0.798, 0.798, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/commonsense_qa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 17, 0, 1, 0, 1, 1, 74, 0.06944379333880846, 10, 0]
Checking history sample input_X_between_0_1:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 0.53125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.578125, 0.6944379333880846, 0.20833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 18, 0, 0, 1, 0, 0, 88, 0.07338847024538249, 48, 1]
Checking history sample input_X_between_0_1:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 0.5625, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6875, 0.7338847024538249, 1.0, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 27, 1, 1, 0, 1, 0, 35, 0.056205119903528195, 6, 0]
Checking history sample input_X_between_0_1:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 0.84375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2734375, 0.5620511990352819, 0.125, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 22, 0, 0, 0, 0, 1, 14, 0.05785794551060587, 9, 1]
Checking history sample input_X_between_0_1:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 0.6875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.109375, 0.5785794551060587, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 15, 1, 0, 0, 1, 1, 33, 0.046509797776235, 40, 0]
Checking history sample input_X_between_0_1:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 0.46875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2578125, 0.46509797776234996, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 11, 0, 0, 1, 1, 1, 70, 0.005359902393564798, 44, 1]
Checking history sample input_X_between_0_1:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 0.34375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.546875, 0.05359902393564797, 0.9166666666666666, 1.0]
Checking history sample performance at 625 steps:  0.73
Checking history sample input_X:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 3, 1, 1, 0, 0, 1, 89, 0.004230716614147934, 21, 1]
Checking history sample input_X_between_0_1:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 0.09375, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6953125, 0.042307166141479335, 0.4375, 1.0]
Checking history sample performance at 625 steps:  0.79
Checking history sample input_X:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 11, 0, 0, 1, 0, 1, 128, 0.02126981773222546, 43, 0]
Checking history sample input_X_between_0_1:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 0.34375, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2126981773222546, 0.8958333333333334, 0.0]
Checking history sample performance at 625 steps:  0.77
Checking history sample input_X:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 4, 0, 1, 0, 0, 1, 51, 0.048394033913014715, 30, 0]
Checking history sample input_X_between_0_1:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3984375, 0.4839403391301471, 0.625, 0.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 25, 1, 0, 1, 0, 0, 103, 0.021006146654874183, 41, 0]
Checking history sample input_X_between_0_1:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 0.78125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8046875, 0.2100614665487418, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.71
Checking history sample input_X:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 30, 0, 0, 0, 1, 0, 30, 0.06473585772462145, 22, 0]
Checking history sample input_X_between_0_1:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.234375, 0.6473585772462145, 0.4583333333333333, 0.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 10, 0, 0, 1, 0, 1, 33, 0.053839147652140054, 30, 1]
Checking history sample input_X_between_0_1:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 0.3125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2578125, 0.5383914765214005, 0.625, 1.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 27, 0, 1, 1, 1, 0, 91, 0.026257080995186557, 27, 0]
Checking history sample input_X_between_0_1:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 0.84375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7109375, 0.26257080995186555, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 28, 1, 1, 1, 0, 0, 23, 0.048743516472132736, 35, 1]
Checking history sample input_X_between_0_1:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 0.875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1796875, 0.48743516472132736, 0.7291666666666666, 1.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 14, 1, 0, 1, 0, 0, 32, 0.04806841934255027, 48, 0]
Checking history sample input_X_between_0_1:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 0.4375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.48068419342550267, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 19, 0, 0, 1, 1, 0, 22, 0.09470699670511286, 20, 1]
Checking history sample input_X_between_0_1:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 0.59375, 0.0, 0.0, 1.0, 1.0, 0.0, 0.171875, 0.9470699670511286, 0.4166666666666667, 1.0]
Checking history sample performance at 625 steps:  0.78
Checking history sample input_X:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 2, 0, 1, 1, 1, 0, 72, 0.07202092774167915, 33, 1]
Checking history sample input_X_between_0_1:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5625, 0.7202092774167914, 0.6875, 1.0]
Checking history sample performance at 625 steps:  0.75
Checking history sample input_X:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 15, 1, 0, 1, 0, 1, 69, 0.09577839904714924, 28, 0]
Checking history sample input_X_between_0_1:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 0.46875, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5390625, 0.9577839904714924, 0.5833333333333334, 0.0]
Checking history sample performance at 625 steps:  0.74
Checking history sample input_X:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 30, 1, 1, 1, 0, 1, 19, 0.07443761643578949, 27, 0]
Checking history sample input_X_between_0_1:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 0.9375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1484375, 0.7443761643578948, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.76
Checking history sample input_X:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 12, 1, 0, 0, 1, 1, 19, 0.0774188690692264, 1, 1]
Checking history sample input_X_between_0_1:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1484375, 0.774188690692264, 0.020833333333333332, 1.0]
Checking history sample performance at 625 steps:  0.75
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9294 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.814367276997614
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.814367276997614
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.814367276997614
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.814367276997614
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.814367276997614
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.0823, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), 0, tensor(0.4440, dtype=torch.float64), 0, tensor(0.3798, dtype=torch.float64), 0, tensor(0.0276, dtype=torch.float64), 0, 9, 1, 1, 0, 0, 1, 116, 0.0206892298889877, 9.7280745592472, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.0823, dtype=torch.float64), tensor(0.0662, dtype=torch.float64), tensor(2.0330e-19, dtype=torch.float64), tensor(0.4440, dtype=torch.float64), tensor(1.6197e-19, dtype=torch.float64), tensor(0.3798, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0276, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2816, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9026, dtype=torch.float64), tensor(0.2069, dtype=torch.float64), tensor(0.2027, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.082
  gsm8k: 0.066
  rowan_hellaswag: 0
  sciq: 0.444
  triviaqa: 0
  truthfulqa_gen: 0.38
  wikitext: 0
  mmlu: 0.028
  arc_challenge: 0

LoRA Parameters:
  lora_r: (116,)
  lora_dropout: (0.0206892298889877,)
  num_layers_to_apply: (9,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (9.7280745592472,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  9
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  116
lora dropout:  0.0206892298889877
lora alpha:  9.7280745592472
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 33,140,736 || all params: 8,063,401,984 || trainable%: 0.4110
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 306.33it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 510.04it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 595.74it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 645.47it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 682.56it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 710.25it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 695.68it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.6525, 'grad_norm': 0.37492918968200684, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.3427693843841553, 'eval_runtime': 6.1834, 'eval_samples_per_second': 161.562, 'eval_steps_per_second': 10.189, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 306.02it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 509.42it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 593.01it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 643.39it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 679.96it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 705.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 692.50it/s]
Evaluation performance at step 50: 0.76
{'loss': 2.3619, 'grad_norm': 0.2012067586183548, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 1.7087935209274292, 'eval_runtime': 6.1928, 'eval_samples_per_second': 161.317, 'eval_steps_per_second': 10.173, 'epoch': 0.08}
{'loss': 1.5215, 'grad_norm': 0.09196852147579193, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.372986078262329, 'eval_runtime': 6.2178, 'eval_samples_per_second': 160.667, 'eval_steps_per_second': 10.132, 'epoch': 0.12}
{'loss': 1.2937, 'grad_norm': 0.10326530784368515, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2791768312454224, 'eval_runtime': 6.219, 'eval_samples_per_second': 160.635, 'eval_steps_per_second': 10.13, 'epoch': 0.16}
{'loss': 1.2428, 'grad_norm': 0.13272380828857422, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2036484479904175, 'eval_runtime': 6.2495, 'eval_samples_per_second': 159.854, 'eval_steps_per_second': 10.081, 'epoch': 0.2}
{'loss': 1.1623, 'grad_norm': 0.11644600331783295, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0688152313232422, 'eval_runtime': 6.2684, 'eval_samples_per_second': 159.372, 'eval_steps_per_second': 10.05, 'epoch': 0.24}
{'loss': 1.0149, 'grad_norm': 0.12806998193264008, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9852923154830933, 'eval_runtime': 6.2654, 'eval_samples_per_second': 159.448, 'eval_steps_per_second': 10.055, 'epoch': 0.28}
{'loss': 0.9591, 'grad_norm': 0.11218644678592682, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9347693920135498, 'eval_runtime': 6.2737, 'eval_samples_per_second': 159.237, 'eval_steps_per_second': 10.042, 'epoch': 0.32}
{'loss': 0.9206, 'grad_norm': 0.13741391897201538, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9202865362167358, 'eval_runtime': 6.2726, 'eval_samples_per_second': 159.263, 'eval_steps_per_second': 10.044, 'epoch': 0.36}
{'loss': 0.9162, 'grad_norm': 0.11697418987751007, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9084912538528442, 'eval_runtime': 6.2721, 'eval_samples_per_second': 159.276, 'eval_steps_per_second': 10.044, 'epoch': 0.4}
{'loss': 0.8917, 'grad_norm': 0.10993584990501404, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9005564451217651, 'eval_runtime': 6.2695, 'eval_samples_per_second': 159.343, 'eval_steps_per_second': 10.049, 'epoch': 0.44}
{'loss': 0.9095, 'grad_norm': 0.11791843175888062, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8934106230735779, 'eval_runtime': 6.2772, 'eval_samples_per_second': 159.148, 'eval_steps_per_second': 10.036, 'epoch': 0.48}
{'loss': 0.8829, 'grad_norm': 0.11986308544874191, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8849459290504456, 'eval_runtime': 6.2728, 'eval_samples_per_second': 159.258, 'eval_steps_per_second': 10.043, 'epoch': 0.52}
{'loss': 0.8889, 'grad_norm': 0.12510153651237488, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8785417079925537, 'eval_runtime': 6.2692, 'eval_samples_per_second': 159.351, 'eval_steps_per_second': 10.049, 'epoch': 0.56}
{'loss': 0.8845, 'grad_norm': 0.13009613752365112, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8729634284973145, 'eval_runtime': 6.2856, 'eval_samples_per_second': 158.935, 'eval_steps_per_second': 10.023, 'epoch': 0.6}
{'loss': 0.8996, 'grad_norm': 0.14417263865470886, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.866928219795227, 'eval_runtime': 6.2816, 'eval_samples_per_second': 159.036, 'eval_steps_per_second': 10.029, 'epoch': 0.64}
{'loss': 0.8827, 'grad_norm': 0.12797985970973969, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8614108562469482, 'eval_runtime': 6.2784, 'eval_samples_per_second': 159.116, 'eval_steps_per_second': 10.034, 'epoch': 0.68}
{'loss': 0.8572, 'grad_norm': 0.1583506166934967, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.854363739490509, 'eval_runtime': 6.274, 'eval_samples_per_second': 159.228, 'eval_steps_per_second': 10.041, 'epoch': 0.72}
{'loss': 0.8673, 'grad_norm': 0.1071614995598793, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8506981134414673, 'eval_runtime': 6.2783, 'eval_samples_per_second': 159.121, 'eval_steps_per_second': 10.035, 'epoch': 0.76}
{'loss': 0.899, 'grad_norm': 0.11609962582588196, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8471219539642334, 'eval_runtime': 6.287, 'eval_samples_per_second': 158.9, 'eval_steps_per_second': 10.021, 'epoch': 0.8}
{'loss': 0.8778, 'grad_norm': 0.11747194826602936, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8430163264274597, 'eval_runtime': 6.2786, 'eval_samples_per_second': 159.111, 'eval_steps_per_second': 10.034, 'epoch': 0.84}
{'loss': 0.8428, 'grad_norm': 0.13448216021060944, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8391590118408203, 'eval_runtime': 6.2704, 'eval_samples_per_second': 159.32, 'eval_steps_per_second': 10.047, 'epoch': 0.88}
{'loss': 0.8568, 'grad_norm': 0.14252367615699768, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.836887001991272, 'eval_runtime': 6.2879, 'eval_samples_per_second': 158.877, 'eval_steps_per_second': 10.019, 'epoch': 0.92}
{'loss': 0.8764, 'grad_norm': 0.13289543986320496, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8351631760597229, 'eval_runtime': 6.2775, 'eval_samples_per_second': 159.141, 'eval_steps_per_second': 10.036, 'epoch': 0.96}
{'loss': 0.8823, 'grad_norm': 0.1343163549900055, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.834846556186676, 'eval_runtime': 6.2787, 'eval_samples_per_second': 159.11, 'eval_steps_per_second': 10.034, 'epoch': 1.0}
{'train_runtime': 282.857, 'train_samples_per_second': 35.346, 'train_steps_per_second': 2.21, 'train_loss': 1.1698022155761718, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3427693843841553, 1.7087935209274292, 1.372986078262329, 1.2791768312454224, 1.2036484479904175, 1.0688152313232422, 0.9852923154830933, 0.9347693920135498, 0.9202865362167358, 0.9084912538528442, 0.9005564451217651, 0.8934106230735779, 0.8849459290504456, 0.8785417079925537, 0.8729634284973145, 0.866928219795227, 0.8614108562469482, 0.854363739490509, 0.8506981134414673, 0.8471219539642334, 0.8430163264274597, 0.8391590118408203, 0.836887001991272, 0.8351631760597229, 0.834846556186676], 'performance': [0.75, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:36,  5.17it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 238.75it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 339.00it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 400.42it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 438.71it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 469.52it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 467.47it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.822745144367218
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.7875000000000001
BO observations:  [0.822745144367218]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4669 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 0.8277640043594666
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 0.8277640043594666
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 0.8277640043594666
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 0.8277640043594666
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 0.8277640043594666
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0810, dtype=torch.float64), tensor(0.0376, dtype=torch.float64), 0, tensor(0.7609, dtype=torch.float64), 0, tensor(0.1205, dtype=torch.float64), 0, 0, 0, 14, 1, 1, 0, 1, 1, 128, 0.0012476809557754617, 12.499865498914977, 1]
normalized proposed parameters for next round by BO: [tensor(0.0810, dtype=torch.float64), tensor(0.0376, dtype=torch.float64), tensor(4.5433e-19, dtype=torch.float64), tensor(0.7609, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1205, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4405, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0125, dtype=torch.float64), tensor(0.2604, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.081
  gsm8k: 0.038
  rowan_hellaswag: 0
  sciq: 0.761
  triviaqa: 0
  truthfulqa_gen: 0.121
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0012476809557754617,)
  num_layers_to_apply: (14,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (12.499865498914977,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  14
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.0012476809557754617
lora alpha:  12.499865498914977
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 89,915,392 || all params: 8,120,176,640 || trainable%: 1.1073
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 291.65it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 483.52it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 561.92it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 609.56it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 645.90it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 671.95it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 658.11it/s]
Evaluation performance at step 25: 0.72
{'loss': 4.1934, 'grad_norm': 2.118673324584961, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.72}
{'eval_loss': 2.23968505859375, 'eval_runtime': 5.5237, 'eval_samples_per_second': 180.856, 'eval_steps_per_second': 11.405, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 291.53it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 485.03it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 564.15it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 609.01it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 643.41it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 670.35it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 657.41it/s]
Evaluation performance at step 50: 0.76
{'loss': 1.5356, 'grad_norm': 0.4728621244430542, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 1.179203748703003, 'eval_runtime': 5.5299, 'eval_samples_per_second': 180.655, 'eval_steps_per_second': 11.393, 'epoch': 0.08}
{'loss': 1.0832, 'grad_norm': 0.17321109771728516, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9702296853065491, 'eval_runtime': 5.541, 'eval_samples_per_second': 180.293, 'eval_steps_per_second': 11.37, 'epoch': 0.12}
{'loss': 0.946, 'grad_norm': 0.39008694887161255, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9176164865493774, 'eval_runtime': 5.5478, 'eval_samples_per_second': 180.072, 'eval_steps_per_second': 11.356, 'epoch': 0.16}
{'loss': 0.897, 'grad_norm': 0.22614291310310364, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8608189225196838, 'eval_runtime': 5.5369, 'eval_samples_per_second': 180.425, 'eval_steps_per_second': 11.378, 'epoch': 0.2}
{'loss': 0.8477, 'grad_norm': 0.22400040924549103, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8143813014030457, 'eval_runtime': 5.5494, 'eval_samples_per_second': 180.018, 'eval_steps_per_second': 11.352, 'epoch': 0.24}
{'loss': 0.8456, 'grad_norm': 0.16916097700595856, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7984439730644226, 'eval_runtime': 5.5647, 'eval_samples_per_second': 179.524, 'eval_steps_per_second': 11.321, 'epoch': 0.28}
{'loss': 0.8101, 'grad_norm': 0.18762342631816864, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.790283203125, 'eval_runtime': 5.5559, 'eval_samples_per_second': 179.81, 'eval_steps_per_second': 11.339, 'epoch': 0.32}
{'loss': 0.8086, 'grad_norm': 0.23787716031074524, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7837433218955994, 'eval_runtime': 5.573, 'eval_samples_per_second': 179.258, 'eval_steps_per_second': 11.305, 'epoch': 0.36}
{'loss': 0.772, 'grad_norm': 0.16759294271469116, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7714778780937195, 'eval_runtime': 5.5661, 'eval_samples_per_second': 179.48, 'eval_steps_per_second': 11.319, 'epoch': 0.4}
{'loss': 0.82, 'grad_norm': 0.16513794660568237, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7701201438903809, 'eval_runtime': 5.5655, 'eval_samples_per_second': 179.5, 'eval_steps_per_second': 11.32, 'epoch': 0.44}
{'loss': 0.7882, 'grad_norm': 0.16609463095664978, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7651734948158264, 'eval_runtime': 5.5818, 'eval_samples_per_second': 178.973, 'eval_steps_per_second': 11.287, 'epoch': 0.48}
{'loss': 0.7821, 'grad_norm': 0.2080918848514557, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7622641921043396, 'eval_runtime': 5.5795, 'eval_samples_per_second': 179.048, 'eval_steps_per_second': 11.291, 'epoch': 0.52}
{'loss': 0.7743, 'grad_norm': 0.21575617790222168, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.755744993686676, 'eval_runtime': 5.5695, 'eval_samples_per_second': 179.371, 'eval_steps_per_second': 11.312, 'epoch': 0.56}
{'loss': 0.7983, 'grad_norm': 0.17421211302280426, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7528539299964905, 'eval_runtime': 5.5821, 'eval_samples_per_second': 178.964, 'eval_steps_per_second': 11.286, 'epoch': 0.6}
{'loss': 0.7587, 'grad_norm': 0.21163584291934967, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7479212284088135, 'eval_runtime': 5.574, 'eval_samples_per_second': 179.224, 'eval_steps_per_second': 11.302, 'epoch': 0.64}
{'loss': 0.7636, 'grad_norm': 0.20090973377227783, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7440924644470215, 'eval_runtime': 5.5691, 'eval_samples_per_second': 179.384, 'eval_steps_per_second': 11.313, 'epoch': 0.68}
{'loss': 0.7447, 'grad_norm': 0.17772865295410156, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7400903701782227, 'eval_runtime': 5.5747, 'eval_samples_per_second': 179.204, 'eval_steps_per_second': 11.301, 'epoch': 0.72}
{'loss': 0.757, 'grad_norm': 0.18689754605293274, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7350234389305115, 'eval_runtime': 5.5749, 'eval_samples_per_second': 179.195, 'eval_steps_per_second': 11.301, 'epoch': 0.76}
{'loss': 0.7679, 'grad_norm': 0.18685409426689148, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7323567271232605, 'eval_runtime': 5.5893, 'eval_samples_per_second': 178.734, 'eval_steps_per_second': 11.271, 'epoch': 0.8}
{'loss': 0.7461, 'grad_norm': 0.22767534852027893, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7290917634963989, 'eval_runtime': 5.5718, 'eval_samples_per_second': 179.297, 'eval_steps_per_second': 11.307, 'epoch': 0.84}
{'loss': 0.775, 'grad_norm': 0.1980845332145691, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.726485013961792, 'eval_runtime': 5.5755, 'eval_samples_per_second': 179.178, 'eval_steps_per_second': 11.3, 'epoch': 0.88}
{'loss': 0.758, 'grad_norm': 0.23774386942386627, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7240337133407593, 'eval_runtime': 5.5811, 'eval_samples_per_second': 178.998, 'eval_steps_per_second': 11.288, 'epoch': 0.92}
{'loss': 0.7694, 'grad_norm': 0.16833961009979248, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7239603400230408, 'eval_runtime': 5.5743, 'eval_samples_per_second': 179.216, 'eval_steps_per_second': 11.302, 'epoch': 0.96}
{'loss': 0.7825, 'grad_norm': 0.20551595091819763, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7234627604484558, 'eval_runtime': 5.5722, 'eval_samples_per_second': 179.283, 'eval_steps_per_second': 11.306, 'epoch': 1.0}
{'train_runtime': 301.2465, 'train_samples_per_second': 33.189, 'train_steps_per_second': 2.075, 'train_loss': 0.9729965972900391, 'epoch': 1.0}
train_results:  {'eval_loss': [2.23968505859375, 1.179203748703003, 0.9702296853065491, 0.9176164865493774, 0.8608189225196838, 0.8143813014030457, 0.7984439730644226, 0.790283203125, 0.7837433218955994, 0.7714778780937195, 0.7701201438903809, 0.7651734948158264, 0.7622641921043396, 0.755744993686676, 0.7528539299964905, 0.7479212284088135, 0.7440924644470215, 0.7400903701782227, 0.7350234389305115, 0.7323567271232605, 0.7290917634963989, 0.726485013961792, 0.7240337133407593, 0.7239603400230408, 0.7234627604484558], 'performance': [0.72, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:38,  2.28it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 166.99it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 268.90it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 338.65it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 387.44it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 426.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 381.70it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.72, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.831519365310669
current iteration best possible performance (full train run):  0.798
max performance so far:  0.798
BO observations:  [0.822745144367218, 0.831519365310669]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7502 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 0.8289136059402245
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.8289136059402245
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 0.8289136059402245
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 0.8289136059402245
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.8289136059402245
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.9307, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.0693, dtype=torch.float64), 0, 0, 1, 0, 1, 1, 1, 0, 19, 0.08954039476023562, 10.620718971032126, 1]
normalized proposed parameters for next round by BO: [tensor(0.9307, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.0502e-18, dtype=torch.float64), tensor(1.6327e-18, dtype=torch.float64), tensor(3.3674e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0693, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1468, dtype=torch.float64), tensor(0.8954, dtype=torch.float64), tensor(0.2213, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.931
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.069
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (19,)
  lora_dropout: (0.08954039476023562,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (10.620718971032126,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  19
lora dropout:  0.08954039476023562
lora alpha:  10.620718971032126
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 797,696 || all params: 8,031,058,944 || trainable%: 0.0099
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 318.83it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 530.40it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 619.92it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 672.28it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 711.46it/s]Running loglikelihood requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 446/500 [00:00<00:00, 756.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 725.03it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.5353, 'grad_norm': 1.9045833349227905, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 4.035903453826904, 'eval_runtime': 5.0001, 'eval_samples_per_second': 199.798, 'eval_steps_per_second': 12.6, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 318.01it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 529.66it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 617.90it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 666.91it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 706.92it/s]Running loglikelihood requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 443/500 [00:00<00:00, 743.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 721.60it/s]
Evaluation performance at step 50: 0.75
{'loss': 3.3063, 'grad_norm': 1.5457895994186401, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.6561813354492188, 'eval_runtime': 5.0072, 'eval_samples_per_second': 199.515, 'eval_steps_per_second': 12.582, 'epoch': 0.08}
{'loss': 2.1124, 'grad_norm': 2.1084611415863037, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.647162914276123, 'eval_runtime': 5.0219, 'eval_samples_per_second': 198.928, 'eval_steps_per_second': 12.545, 'epoch': 0.12}
{'loss': 1.5087, 'grad_norm': 1.2882546186447144, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4646812677383423, 'eval_runtime': 5.0131, 'eval_samples_per_second': 199.279, 'eval_steps_per_second': 12.567, 'epoch': 0.16}
{'loss': 1.4284, 'grad_norm': 1.074305772781372, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3956851959228516, 'eval_runtime': 5.0322, 'eval_samples_per_second': 198.523, 'eval_steps_per_second': 12.519, 'epoch': 0.2}
{'loss': 1.4034, 'grad_norm': 0.7134606838226318, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3353207111358643, 'eval_runtime': 5.0582, 'eval_samples_per_second': 197.503, 'eval_steps_per_second': 12.455, 'epoch': 0.24}
{'loss': 1.3304, 'grad_norm': 0.7456125617027283, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3011116981506348, 'eval_runtime': 5.0589, 'eval_samples_per_second': 197.474, 'eval_steps_per_second': 12.453, 'epoch': 0.28}
{'loss': 1.29, 'grad_norm': 0.814027726650238, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2229082584381104, 'eval_runtime': 5.0866, 'eval_samples_per_second': 196.398, 'eval_steps_per_second': 12.385, 'epoch': 0.32}
{'loss': 1.2495, 'grad_norm': 0.7502351999282837, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.200473427772522, 'eval_runtime': 5.0721, 'eval_samples_per_second': 196.958, 'eval_steps_per_second': 12.421, 'epoch': 0.36}
{'loss': 1.2042, 'grad_norm': 0.8311672210693359, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1904182434082031, 'eval_runtime': 5.0661, 'eval_samples_per_second': 197.194, 'eval_steps_per_second': 12.436, 'epoch': 0.4}
{'loss': 1.2135, 'grad_norm': 0.7835754752159119, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1765809059143066, 'eval_runtime': 5.0814, 'eval_samples_per_second': 196.6, 'eval_steps_per_second': 12.398, 'epoch': 0.44}
{'loss': 1.1599, 'grad_norm': 1.0123977661132812, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1568074226379395, 'eval_runtime': 5.0599, 'eval_samples_per_second': 197.436, 'eval_steps_per_second': 12.451, 'epoch': 0.48}
{'loss': 1.1678, 'grad_norm': 0.9793840646743774, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.119154930114746, 'eval_runtime': 5.0729, 'eval_samples_per_second': 196.929, 'eval_steps_per_second': 12.419, 'epoch': 0.52}
{'loss': 1.1117, 'grad_norm': 0.9923966526985168, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1076494455337524, 'eval_runtime': 5.0607, 'eval_samples_per_second': 197.404, 'eval_steps_per_second': 12.449, 'epoch': 0.56}
{'loss': 1.1483, 'grad_norm': 0.7089542746543884, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.101524829864502, 'eval_runtime': 5.0735, 'eval_samples_per_second': 196.905, 'eval_steps_per_second': 12.417, 'epoch': 0.6}
{'loss': 1.1407, 'grad_norm': 1.142553687095642, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0971508026123047, 'eval_runtime': 5.0671, 'eval_samples_per_second': 197.154, 'eval_steps_per_second': 12.433, 'epoch': 0.64}
{'loss': 1.1102, 'grad_norm': 0.8633915185928345, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.092796802520752, 'eval_runtime': 5.0649, 'eval_samples_per_second': 197.239, 'eval_steps_per_second': 12.439, 'epoch': 0.68}
{'loss': 1.0939, 'grad_norm': 1.1698939800262451, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0892066955566406, 'eval_runtime': 5.0682, 'eval_samples_per_second': 197.113, 'eval_steps_per_second': 12.431, 'epoch': 0.72}
{'loss': 1.0976, 'grad_norm': 1.1549285650253296, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0860310792922974, 'eval_runtime': 5.0653, 'eval_samples_per_second': 197.222, 'eval_steps_per_second': 12.437, 'epoch': 0.76}
{'loss': 1.09, 'grad_norm': 1.2367547750473022, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0833073854446411, 'eval_runtime': 5.0732, 'eval_samples_per_second': 196.916, 'eval_steps_per_second': 12.418, 'epoch': 0.8}
{'loss': 1.1077, 'grad_norm': 0.868920624256134, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0811878442764282, 'eval_runtime': 5.0636, 'eval_samples_per_second': 197.292, 'eval_steps_per_second': 12.442, 'epoch': 0.84}
{'loss': 1.0891, 'grad_norm': 1.3449251651763916, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0803413391113281, 'eval_runtime': 5.0641, 'eval_samples_per_second': 197.272, 'eval_steps_per_second': 12.441, 'epoch': 0.88}
{'loss': 1.0674, 'grad_norm': 1.323925256729126, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0774468183517456, 'eval_runtime': 5.0675, 'eval_samples_per_second': 197.14, 'eval_steps_per_second': 12.432, 'epoch': 0.92}
{'loss': 1.0967, 'grad_norm': 1.169568657875061, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0759384632110596, 'eval_runtime': 5.0713, 'eval_samples_per_second': 196.99, 'eval_steps_per_second': 12.423, 'epoch': 0.96}
{'loss': 1.1414, 'grad_norm': 0.8818039894104004, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0759705305099487, 'eval_runtime': 5.0821, 'eval_samples_per_second': 196.572, 'eval_steps_per_second': 12.396, 'epoch': 1.0}
{'train_runtime': 272.58, 'train_samples_per_second': 36.683, 'train_steps_per_second': 2.293, 'train_loss': 1.4481846221923829, 'epoch': 1.0}
train_results:  {'eval_loss': [4.035903453826904, 2.6561813354492188, 1.647162914276123, 1.4646812677383423, 1.3956851959228516, 1.3353207111358643, 1.3011116981506348, 1.2229082584381104, 1.200473427772522, 1.1904182434082031, 1.1765809059143066, 1.1568074226379395, 1.119154930114746, 1.1076494455337524, 1.101524829864502, 1.0971508026123047, 1.092796802520752, 1.0892066955566406, 1.0860310792922974, 1.0833073854446411, 1.0811878442764282, 1.0803413391113281, 1.0774468183517456, 1.0759384632110596, 1.0759705305099487], 'performance': [0.75, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:09,  2.63it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 187.22it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 296.61it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 371.10it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 422.08it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 464.61it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 420.17it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.7566924095153809
current iteration best possible performance (full train run):  0.8295000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3825 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.8140000373332074
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 0.8185730256203699
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.8186078577791656
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 0.818909968803233
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 0.8185730874503117
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7451, dtype=torch.float64), 0, tensor(0.2549, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 0, 1, 128, 0.1, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.6589e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7451, dtype=torch.float64), tensor(5.9370e-17, dtype=torch.float64), tensor(0.2549, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.745
  triviaqa: 0
  truthfulqa_gen: 0.255
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:43,  1.45it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 129.98it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 243.02it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 341.70it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 423.80it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 491.94it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 356.22it/s]
Evaluation performance at step 25: 0.74
{'loss': 5.3009, 'grad_norm': 0.33297428488731384, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.9179465770721436, 'eval_runtime': 4.3762, 'eval_samples_per_second': 228.279, 'eval_steps_per_second': 14.396, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.40it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.73it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 540.94it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 587.69it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 620.55it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 644.99it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 632.05it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5682, 'grad_norm': 0.09580183774232864, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.6613824367523193, 'eval_runtime': 3.4288, 'eval_samples_per_second': 291.355, 'eval_steps_per_second': 18.374, 'epoch': 0.08}
{'loss': 1.4643, 'grad_norm': 0.07745440304279327, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3474509716033936, 'eval_runtime': 3.4188, 'eval_samples_per_second': 292.209, 'eval_steps_per_second': 18.428, 'epoch': 0.12}
{'loss': 1.2552, 'grad_norm': 0.04395391047000885, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1835649013519287, 'eval_runtime': 3.4242, 'eval_samples_per_second': 291.745, 'eval_steps_per_second': 18.398, 'epoch': 0.16}
{'loss': 1.1543, 'grad_norm': 0.04071841016411781, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1375926733016968, 'eval_runtime': 3.4272, 'eval_samples_per_second': 291.492, 'eval_steps_per_second': 18.382, 'epoch': 0.2}
{'loss': 1.123, 'grad_norm': 0.051523368805646896, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1027553081512451, 'eval_runtime': 3.4203, 'eval_samples_per_second': 292.076, 'eval_steps_per_second': 18.419, 'epoch': 0.24}
{'loss': 1.0761, 'grad_norm': 0.06648313999176025, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0660933256149292, 'eval_runtime': 3.4261, 'eval_samples_per_second': 291.587, 'eval_steps_per_second': 18.388, 'epoch': 0.28}
{'loss': 1.053, 'grad_norm': 0.06430207192897797, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.017230749130249, 'eval_runtime': 3.4237, 'eval_samples_per_second': 291.793, 'eval_steps_per_second': 18.401, 'epoch': 0.32}
{'loss': 0.9907, 'grad_norm': 0.06356783956289291, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9120776057243347, 'eval_runtime': 3.4292, 'eval_samples_per_second': 291.323, 'eval_steps_per_second': 18.372, 'epoch': 0.36}
{'loss': 0.8928, 'grad_norm': 0.05523568391799927, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8445836901664734, 'eval_runtime': 3.4289, 'eval_samples_per_second': 291.344, 'eval_steps_per_second': 18.373, 'epoch': 0.4}
{'loss': 0.813, 'grad_norm': 0.06361827999353409, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7841262221336365, 'eval_runtime': 3.4349, 'eval_samples_per_second': 290.838, 'eval_steps_per_second': 18.341, 'epoch': 0.44}
{'loss': 0.7814, 'grad_norm': 0.07087847590446472, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.755499005317688, 'eval_runtime': 3.4362, 'eval_samples_per_second': 290.727, 'eval_steps_per_second': 18.334, 'epoch': 0.48}
{'loss': 0.7606, 'grad_norm': 0.05341542512178421, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7475650310516357, 'eval_runtime': 3.4351, 'eval_samples_per_second': 290.825, 'eval_steps_per_second': 18.34, 'epoch': 0.52}
{'loss': 0.7546, 'grad_norm': 0.06110155209898949, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7434674501419067, 'eval_runtime': 3.4413, 'eval_samples_per_second': 290.299, 'eval_steps_per_second': 18.307, 'epoch': 0.56}
{'loss': 0.7481, 'grad_norm': 0.056288573890924454, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7394418120384216, 'eval_runtime': 3.4373, 'eval_samples_per_second': 290.638, 'eval_steps_per_second': 18.329, 'epoch': 0.6}
{'loss': 0.7535, 'grad_norm': 0.049828000366687775, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7352527379989624, 'eval_runtime': 3.4456, 'eval_samples_per_second': 289.931, 'eval_steps_per_second': 18.284, 'epoch': 0.64}
{'loss': 0.7491, 'grad_norm': 0.07433115690946579, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7326300740242004, 'eval_runtime': 3.4364, 'eval_samples_per_second': 290.711, 'eval_steps_per_second': 18.333, 'epoch': 0.68}
{'loss': 0.7398, 'grad_norm': 0.06231413036584854, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7291865944862366, 'eval_runtime': 3.4372, 'eval_samples_per_second': 290.647, 'eval_steps_per_second': 18.329, 'epoch': 0.72}
{'loss': 0.7202, 'grad_norm': 0.06743226945400238, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7275407910346985, 'eval_runtime': 3.4555, 'eval_samples_per_second': 289.107, 'eval_steps_per_second': 18.232, 'epoch': 0.76}
{'loss': 0.7201, 'grad_norm': 0.05434846132993698, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7234036922454834, 'eval_runtime': 3.4364, 'eval_samples_per_second': 290.708, 'eval_steps_per_second': 18.333, 'epoch': 0.8}
{'loss': 0.7304, 'grad_norm': 0.0548301637172699, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7215925455093384, 'eval_runtime': 3.4451, 'eval_samples_per_second': 289.98, 'eval_steps_per_second': 18.287, 'epoch': 0.84}
{'loss': 0.745, 'grad_norm': 0.06036612018942833, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.720209002494812, 'eval_runtime': 3.4355, 'eval_samples_per_second': 290.787, 'eval_steps_per_second': 18.338, 'epoch': 0.88}
{'loss': 0.7232, 'grad_norm': 0.07251733541488647, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7184449434280396, 'eval_runtime': 3.4429, 'eval_samples_per_second': 290.161, 'eval_steps_per_second': 18.298, 'epoch': 0.92}
{'loss': 0.7196, 'grad_norm': 0.07477058470249176, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7172278761863708, 'eval_runtime': 3.439, 'eval_samples_per_second': 290.49, 'eval_steps_per_second': 18.319, 'epoch': 0.96}
{'loss': 0.7102, 'grad_norm': 0.07304911315441132, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.717098593711853, 'eval_runtime': 3.4389, 'eval_samples_per_second': 290.501, 'eval_steps_per_second': 18.32, 'epoch': 1.0}
{'train_runtime': 203.0549, 'train_samples_per_second': 49.243, 'train_steps_per_second': 3.078, 'train_loss': 1.121887548828125, 'epoch': 1.0}
train_results:  {'eval_loss': [3.9179465770721436, 1.6613824367523193, 1.3474509716033936, 1.1835649013519287, 1.1375926733016968, 1.1027553081512451, 1.0660933256149292, 1.017230749130249, 0.9120776057243347, 0.8445836901664734, 0.7841262221336365, 0.755499005317688, 0.7475650310516357, 0.7434674501419067, 0.7394418120384216, 0.7352527379989624, 0.7326300740242004, 0.7291865944862366, 0.7275407910346985, 0.7234036922454834, 0.7215925455093384, 0.720209002494812, 0.7184449434280396, 0.7172278761863708, 0.717098593711853], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:44,  4.79it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 246.64it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 339.00it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 389.33it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 421.31it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 446.30it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 450.64it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8474709987640381
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8697 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 0.8255765890225782
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.8182598084280253
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 0.8274731700640157
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.824267814309884
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.8167444073570157
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7594, dtype=torch.float64), 0, tensor(0.2406, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(8.0260e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7594, dtype=torch.float64), tensor(1.4927e-16, dtype=torch.float64), tensor(0.2406, dtype=torch.float64), tensor(7.0987e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.759
  triviaqa: 0
  truthfulqa_gen: 0.241
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:30,  5.51it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 308.97it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 444.46it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 527.25it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 580.42it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:00<00:00, 622.56it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 570.60it/s]
Evaluation performance at step 25: 0.75
{'loss': 5.2359, 'grad_norm': 0.5529072880744934, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.7125256061553955, 'eval_runtime': 4.7141, 'eval_samples_per_second': 211.918, 'eval_steps_per_second': 13.364, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 285.74it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 476.91it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 557.10it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 605.48it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 639.43it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 665.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 650.62it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.3001, 'grad_norm': 0.4421873688697815, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.3336933851242065, 'eval_runtime': 3.2356, 'eval_samples_per_second': 308.752, 'eval_steps_per_second': 19.471, 'epoch': 0.08}
{'loss': 1.0949, 'grad_norm': 0.1317358762025833, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9631445407867432, 'eval_runtime': 3.2303, 'eval_samples_per_second': 309.262, 'eval_steps_per_second': 19.503, 'epoch': 0.12}
{'loss': 0.9191, 'grad_norm': 0.11241377890110016, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8042482137680054, 'eval_runtime': 3.2371, 'eval_samples_per_second': 308.613, 'eval_steps_per_second': 19.462, 'epoch': 0.16}
{'loss': 0.8247, 'grad_norm': 0.057241301983594894, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7841458320617676, 'eval_runtime': 3.245, 'eval_samples_per_second': 307.855, 'eval_steps_per_second': 19.414, 'epoch': 0.2}
{'loss': 0.7848, 'grad_norm': 0.06875764578580856, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7735997438430786, 'eval_runtime': 3.2304, 'eval_samples_per_second': 309.251, 'eval_steps_per_second': 19.502, 'epoch': 0.24}
{'loss': 0.7918, 'grad_norm': 0.06791392713785172, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.763804018497467, 'eval_runtime': 3.2365, 'eval_samples_per_second': 308.667, 'eval_steps_per_second': 19.466, 'epoch': 0.28}
{'loss': 0.7848, 'grad_norm': 0.06835080683231354, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7588717937469482, 'eval_runtime': 3.2399, 'eval_samples_per_second': 308.345, 'eval_steps_per_second': 19.445, 'epoch': 0.32}
{'loss': 0.7549, 'grad_norm': 0.10826682299375534, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.754522979259491, 'eval_runtime': 3.2444, 'eval_samples_per_second': 307.915, 'eval_steps_per_second': 19.418, 'epoch': 0.36}
{'loss': 0.76, 'grad_norm': 0.05441540852189064, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7489628791809082, 'eval_runtime': 3.239, 'eval_samples_per_second': 308.431, 'eval_steps_per_second': 19.451, 'epoch': 0.4}
{'loss': 0.7788, 'grad_norm': 0.05904988572001457, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7497608661651611, 'eval_runtime': 3.2425, 'eval_samples_per_second': 308.098, 'eval_steps_per_second': 19.43, 'epoch': 0.44}
{'loss': 0.7746, 'grad_norm': 0.05492367967963219, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7426540851593018, 'eval_runtime': 3.243, 'eval_samples_per_second': 308.045, 'eval_steps_per_second': 19.426, 'epoch': 0.48}
{'loss': 0.7505, 'grad_norm': 0.05865366384387016, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7407560348510742, 'eval_runtime': 3.2379, 'eval_samples_per_second': 308.53, 'eval_steps_per_second': 19.457, 'epoch': 0.52}
{'loss': 0.7638, 'grad_norm': 0.05930681899189949, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.737853467464447, 'eval_runtime': 3.2467, 'eval_samples_per_second': 307.7, 'eval_steps_per_second': 19.404, 'epoch': 0.56}
{'loss': 0.7525, 'grad_norm': 0.07244697213172913, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7348775267601013, 'eval_runtime': 3.2441, 'eval_samples_per_second': 307.943, 'eval_steps_per_second': 19.42, 'epoch': 0.6}
{'loss': 0.7651, 'grad_norm': 0.07992903143167496, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7325931787490845, 'eval_runtime': 3.2475, 'eval_samples_per_second': 307.622, 'eval_steps_per_second': 19.4, 'epoch': 0.64}
{'loss': 0.7402, 'grad_norm': 0.07572606205940247, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7320334911346436, 'eval_runtime': 3.2475, 'eval_samples_per_second': 307.623, 'eval_steps_per_second': 19.4, 'epoch': 0.68}
{'loss': 0.7293, 'grad_norm': 0.0695960521697998, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7288224697113037, 'eval_runtime': 3.251, 'eval_samples_per_second': 307.294, 'eval_steps_per_second': 19.379, 'epoch': 0.72}
{'loss': 0.7459, 'grad_norm': 0.06263886392116547, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7274736762046814, 'eval_runtime': 3.2474, 'eval_samples_per_second': 307.627, 'eval_steps_per_second': 19.4, 'epoch': 0.76}
{'loss': 0.7479, 'grad_norm': 0.07445495575666428, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7254526615142822, 'eval_runtime': 3.2507, 'eval_samples_per_second': 307.322, 'eval_steps_per_second': 19.381, 'epoch': 0.8}
{'loss': 0.7279, 'grad_norm': 0.07521188259124756, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7239727973937988, 'eval_runtime': 3.2462, 'eval_samples_per_second': 307.744, 'eval_steps_per_second': 19.407, 'epoch': 0.84}
{'loss': 0.7531, 'grad_norm': 0.06826161593198776, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7234857082366943, 'eval_runtime': 3.2455, 'eval_samples_per_second': 307.809, 'eval_steps_per_second': 19.411, 'epoch': 0.88}
{'loss': 0.7285, 'grad_norm': 0.06600185483694077, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7218406200408936, 'eval_runtime': 3.2482, 'eval_samples_per_second': 307.559, 'eval_steps_per_second': 19.396, 'epoch': 0.92}
{'loss': 0.7632, 'grad_norm': 0.07173974066972733, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7211146950721741, 'eval_runtime': 3.2507, 'eval_samples_per_second': 307.319, 'eval_steps_per_second': 19.38, 'epoch': 0.96}
{'loss': 0.7378, 'grad_norm': 0.07745834439992905, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7210263013839722, 'eval_runtime': 3.2526, 'eval_samples_per_second': 307.141, 'eval_steps_per_second': 19.369, 'epoch': 1.0}
{'train_runtime': 194.3472, 'train_samples_per_second': 51.449, 'train_steps_per_second': 3.216, 'train_loss': 1.0204105010986328, 'epoch': 1.0}
train_results:  {'eval_loss': [3.7125256061553955, 1.3336933851242065, 0.9631445407867432, 0.8042482137680054, 0.7841458320617676, 0.7735997438430786, 0.763804018497467, 0.7588717937469482, 0.754522979259491, 0.7489628791809082, 0.7497608661651611, 0.7426540851593018, 0.7407560348510742, 0.737853467464447, 0.7348775267601013, 0.7325931787490845, 0.7320334911346436, 0.7288224697113037, 0.7274736762046814, 0.7254526615142822, 0.7239727973937988, 0.7234857082366943, 0.7218406200408936, 0.7211146950721741, 0.7210263013839722], 'performance': [0.75, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:01<09:56,  1.20s/it]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:05, 70.08it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 139.25it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 206.16it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:02<00:00, 267.29it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:02<00:00, 322.76it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 225.38it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8477509617805481
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7162 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 0.8380049395142518
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.8372393605459365
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 0.8374829437239085
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.8377507457188381
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 0.8360231656006505
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.6518, dtype=torch.float64), 0, 0, tensor(0.3482, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(5.8270e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6518, dtype=torch.float64), tensor(7.5201e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3482, dtype=torch.float64), tensor(1.1866e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.652
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.348
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.67it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.49it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 501.53it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 545.19it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 575.57it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 598.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 586.26it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.5427, 'grad_norm': 0.32134580612182617, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.341526985168457, 'eval_runtime': 7.9332, 'eval_samples_per_second': 125.926, 'eval_steps_per_second': 7.941, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 257.38it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.06it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 500.95it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 544.47it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 575.03it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 597.88it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 585.60it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.3501, 'grad_norm': 0.2606918513774872, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.7931678295135498, 'eval_runtime': 7.9466, 'eval_samples_per_second': 125.714, 'eval_steps_per_second': 7.928, 'epoch': 0.08}
{'loss': 1.7222, 'grad_norm': 0.12393978238105774, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.546791911125183, 'eval_runtime': 7.9639, 'eval_samples_per_second': 125.441, 'eval_steps_per_second': 7.911, 'epoch': 0.12}
{'loss': 1.4833, 'grad_norm': 0.08702775835990906, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.433050274848938, 'eval_runtime': 7.9698, 'eval_samples_per_second': 125.349, 'eval_steps_per_second': 7.905, 'epoch': 0.16}
{'loss': 1.3593, 'grad_norm': 0.08132162690162659, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3258661031723022, 'eval_runtime': 7.9878, 'eval_samples_per_second': 125.065, 'eval_steps_per_second': 7.887, 'epoch': 0.2}
{'loss': 1.3114, 'grad_norm': 0.07475192099809647, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3010163307189941, 'eval_runtime': 8.0019, 'eval_samples_per_second': 124.845, 'eval_steps_per_second': 7.873, 'epoch': 0.24}
{'loss': 1.3387, 'grad_norm': 0.08585081994533539, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2857401371002197, 'eval_runtime': 8.0016, 'eval_samples_per_second': 124.85, 'eval_steps_per_second': 7.873, 'epoch': 0.28}
{'loss': 1.2826, 'grad_norm': 0.08812825381755829, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2746655941009521, 'eval_runtime': 8.0044, 'eval_samples_per_second': 124.806, 'eval_steps_per_second': 7.871, 'epoch': 0.32}
{'loss': 1.2927, 'grad_norm': 0.06928353011608124, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2657909393310547, 'eval_runtime': 8.0117, 'eval_samples_per_second': 124.692, 'eval_steps_per_second': 7.863, 'epoch': 0.36}
{'loss': 1.2561, 'grad_norm': 0.09435715526342392, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2577306032180786, 'eval_runtime': 8.0088, 'eval_samples_per_second': 124.738, 'eval_steps_per_second': 7.866, 'epoch': 0.4}
{'loss': 1.1179, 'grad_norm': 0.07608597725629807, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.252309799194336, 'eval_runtime': 8.0039, 'eval_samples_per_second': 124.814, 'eval_steps_per_second': 7.871, 'epoch': 0.44}
{'loss': 1.273, 'grad_norm': 0.06380932778120041, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2452470064163208, 'eval_runtime': 8.0066, 'eval_samples_per_second': 124.772, 'eval_steps_per_second': 7.869, 'epoch': 0.48}
{'loss': 1.1744, 'grad_norm': 0.08322568237781525, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2413228750228882, 'eval_runtime': 8.0079, 'eval_samples_per_second': 124.751, 'eval_steps_per_second': 7.867, 'epoch': 0.52}
{'loss': 1.2611, 'grad_norm': 0.08591825515031815, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2373621463775635, 'eval_runtime': 8.0056, 'eval_samples_per_second': 124.788, 'eval_steps_per_second': 7.87, 'epoch': 0.56}
{'loss': 1.2983, 'grad_norm': 0.07908111810684204, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.23227059841156, 'eval_runtime': 8.0131, 'eval_samples_per_second': 124.672, 'eval_steps_per_second': 7.862, 'epoch': 0.6}
{'loss': 1.2362, 'grad_norm': 0.08427663892507553, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.228824257850647, 'eval_runtime': 7.9999, 'eval_samples_per_second': 124.877, 'eval_steps_per_second': 7.875, 'epoch': 0.64}
{'loss': 1.2918, 'grad_norm': 0.07276586443185806, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2256853580474854, 'eval_runtime': 7.9957, 'eval_samples_per_second': 124.942, 'eval_steps_per_second': 7.879, 'epoch': 0.68}
{'loss': 1.2404, 'grad_norm': 0.06682368367910385, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2221381664276123, 'eval_runtime': 7.9864, 'eval_samples_per_second': 125.088, 'eval_steps_per_second': 7.888, 'epoch': 0.72}
{'loss': 1.1675, 'grad_norm': 0.09896840900182724, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2213326692581177, 'eval_runtime': 7.9961, 'eval_samples_per_second': 124.936, 'eval_steps_per_second': 7.879, 'epoch': 0.76}
{'loss': 1.166, 'grad_norm': 0.09302449971437454, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2169967889785767, 'eval_runtime': 7.9961, 'eval_samples_per_second': 124.937, 'eval_steps_per_second': 7.879, 'epoch': 0.8}
{'loss': 1.2022, 'grad_norm': 0.08154237270355225, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2155861854553223, 'eval_runtime': 7.9964, 'eval_samples_per_second': 124.931, 'eval_steps_per_second': 7.879, 'epoch': 0.84}
{'loss': 1.2322, 'grad_norm': 0.11523392051458359, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2136924266815186, 'eval_runtime': 8.0016, 'eval_samples_per_second': 124.851, 'eval_steps_per_second': 7.873, 'epoch': 0.88}
{'loss': 1.2222, 'grad_norm': 0.1015317514538765, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.212148904800415, 'eval_runtime': 7.9992, 'eval_samples_per_second': 124.887, 'eval_steps_per_second': 7.876, 'epoch': 0.92}
{'loss': 1.2026, 'grad_norm': 0.07183833420276642, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2115473747253418, 'eval_runtime': 7.9994, 'eval_samples_per_second': 124.885, 'eval_steps_per_second': 7.876, 'epoch': 0.96}
{'loss': 1.2382, 'grad_norm': 0.0798640325665474, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.211327075958252, 'eval_runtime': 7.9975, 'eval_samples_per_second': 124.913, 'eval_steps_per_second': 7.877, 'epoch': 1.0}
{'train_runtime': 417.3966, 'train_samples_per_second': 23.956, 'train_steps_per_second': 1.497, 'train_loss': 1.4505259033203124, 'epoch': 1.0}
train_results:  {'eval_loss': [3.341526985168457, 1.7931678295135498, 1.546791911125183, 1.433050274848938, 1.3258661031723022, 1.3010163307189941, 1.2857401371002197, 1.2746655941009521, 1.2657909393310547, 1.2577306032180786, 1.252309799194336, 1.2452470064163208, 1.2413228750228882, 1.2373621463775635, 1.23227059841156, 1.228824257850647, 1.2256853580474854, 1.2221381664276123, 1.2213326692581177, 1.2169967889785767, 1.2155861854553223, 1.2136924266815186, 1.212148904800415, 1.2115473747253418, 1.211327075958252], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:52,  4.45it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 229.90it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 315.93it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 363.07it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 393.55it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 418.12it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 421.12it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8482861518859863
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6173 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 0.8412912460357864
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.8348316189330214
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 0.840025662804353
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.8333090470789724
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 0.8281594790439406
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4223, dtype=torch.float64), tensor(0.5777, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.3270e-17, dtype=torch.float64), tensor(0.4223, dtype=torch.float64), tensor(0.5777, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.422
  sciq: 0.578
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.44it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 465.99it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 542.15it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 587.10it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 619.70it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 643.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 631.74it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.4698, 'grad_norm': 0.21617411077022552, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.6293349266052246, 'eval_runtime': 9.8712, 'eval_samples_per_second': 101.204, 'eval_steps_per_second': 6.382, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.34it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 465.50it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.43it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 586.18it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 618.25it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 642.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 630.70it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.8108, 'grad_norm': 0.10921373218297958, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 2.2101941108703613, 'eval_runtime': 9.8775, 'eval_samples_per_second': 101.139, 'eval_steps_per_second': 6.378, 'epoch': 0.08}
{'loss': 1.9922, 'grad_norm': 0.08321080356836319, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8520395755767822, 'eval_runtime': 9.9268, 'eval_samples_per_second': 100.636, 'eval_steps_per_second': 6.346, 'epoch': 0.12}
{'loss': 1.765, 'grad_norm': 0.0652487650513649, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7442479133605957, 'eval_runtime': 9.9709, 'eval_samples_per_second': 100.191, 'eval_steps_per_second': 6.318, 'epoch': 0.16}
{'loss': 1.679, 'grad_norm': 0.05821939557790756, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7029173374176025, 'eval_runtime': 9.9757, 'eval_samples_per_second': 100.143, 'eval_steps_per_second': 6.315, 'epoch': 0.2}
{'loss': 1.6948, 'grad_norm': 0.05266949534416199, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6796212196350098, 'eval_runtime': 9.9762, 'eval_samples_per_second': 100.138, 'eval_steps_per_second': 6.315, 'epoch': 0.24}
{'loss': 1.6214, 'grad_norm': 0.04394441097974777, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.662196397781372, 'eval_runtime': 9.9689, 'eval_samples_per_second': 100.212, 'eval_steps_per_second': 6.32, 'epoch': 0.28}
{'loss': 1.6807, 'grad_norm': 0.043186005204916, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6481345891952515, 'eval_runtime': 9.9833, 'eval_samples_per_second': 100.067, 'eval_steps_per_second': 6.311, 'epoch': 0.32}
{'loss': 1.6175, 'grad_norm': 0.052894432097673416, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6369235515594482, 'eval_runtime': 9.9758, 'eval_samples_per_second': 100.142, 'eval_steps_per_second': 6.315, 'epoch': 0.36}
{'loss': 1.63, 'grad_norm': 0.06057482585310936, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6188952922821045, 'eval_runtime': 9.9759, 'eval_samples_per_second': 100.141, 'eval_steps_per_second': 6.315, 'epoch': 0.4}
{'loss': 1.6379, 'grad_norm': 0.05589761212468147, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6033146381378174, 'eval_runtime': 9.987, 'eval_samples_per_second': 100.03, 'eval_steps_per_second': 6.308, 'epoch': 0.44}
{'loss': 1.607, 'grad_norm': 0.0492507666349411, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.589676856994629, 'eval_runtime': 9.9684, 'eval_samples_per_second': 100.216, 'eval_steps_per_second': 6.32, 'epoch': 0.48}
{'loss': 1.607, 'grad_norm': 0.05924006924033165, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5702859163284302, 'eval_runtime': 9.978, 'eval_samples_per_second': 100.12, 'eval_steps_per_second': 6.314, 'epoch': 0.52}
{'loss': 1.5361, 'grad_norm': 0.05278089642524719, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.550706148147583, 'eval_runtime': 9.9788, 'eval_samples_per_second': 100.112, 'eval_steps_per_second': 6.313, 'epoch': 0.56}
{'loss': 1.5265, 'grad_norm': 0.057075273245573044, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5386608839035034, 'eval_runtime': 9.9742, 'eval_samples_per_second': 100.159, 'eval_steps_per_second': 6.316, 'epoch': 0.6}
{'loss': 1.5407, 'grad_norm': 0.05598478764295578, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5272510051727295, 'eval_runtime': 9.9856, 'eval_samples_per_second': 100.045, 'eval_steps_per_second': 6.309, 'epoch': 0.64}
{'loss': 1.481, 'grad_norm': 0.054191701114177704, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.5206376314163208, 'eval_runtime': 9.9774, 'eval_samples_per_second': 100.126, 'eval_steps_per_second': 6.314, 'epoch': 0.68}
{'loss': 1.547, 'grad_norm': 0.05275405943393707, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5131889581680298, 'eval_runtime': 9.9762, 'eval_samples_per_second': 100.139, 'eval_steps_per_second': 6.315, 'epoch': 0.72}
{'loss': 1.5274, 'grad_norm': 0.05448966845870018, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5068857669830322, 'eval_runtime': 9.9703, 'eval_samples_per_second': 100.198, 'eval_steps_per_second': 6.319, 'epoch': 0.76}
{'loss': 1.5004, 'grad_norm': 0.058146774768829346, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.5008704662322998, 'eval_runtime': 9.974, 'eval_samples_per_second': 100.16, 'eval_steps_per_second': 6.316, 'epoch': 0.8}
{'loss': 1.5212, 'grad_norm': 0.05239308997988701, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4964449405670166, 'eval_runtime': 9.9834, 'eval_samples_per_second': 100.066, 'eval_steps_per_second': 6.31, 'epoch': 0.84}
{'loss': 1.5252, 'grad_norm': 0.0616610012948513, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4927430152893066, 'eval_runtime': 9.973, 'eval_samples_per_second': 100.171, 'eval_steps_per_second': 6.317, 'epoch': 0.88}
{'loss': 1.469, 'grad_norm': 0.05426419898867607, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4889899492263794, 'eval_runtime': 9.9698, 'eval_samples_per_second': 100.203, 'eval_steps_per_second': 6.319, 'epoch': 0.92}
{'loss': 1.5197, 'grad_norm': 0.05378647893667221, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4865504503250122, 'eval_runtime': 9.9725, 'eval_samples_per_second': 100.176, 'eval_steps_per_second': 6.317, 'epoch': 0.96}
{'loss': 1.5418, 'grad_norm': 0.05332082137465477, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4863899946212769, 'eval_runtime': 9.9798, 'eval_samples_per_second': 100.102, 'eval_steps_per_second': 6.313, 'epoch': 1.0}
{'train_runtime': 510.4589, 'train_samples_per_second': 19.588, 'train_steps_per_second': 1.224, 'train_loss': 1.7619682067871094, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6293349266052246, 2.2101941108703613, 1.8520395755767822, 1.7442479133605957, 1.7029173374176025, 1.6796212196350098, 1.662196397781372, 1.6481345891952515, 1.6369235515594482, 1.6188952922821045, 1.6033146381378174, 1.589676856994629, 1.5702859163284302, 1.550706148147583, 1.5386608839035034, 1.5272510051727295, 1.5206376314163208, 1.5131889581680298, 1.5068857669830322, 1.5008704662322998, 1.4964449405670166, 1.4927430152893066, 1.4889899492263794, 1.4865504503250122, 1.4863899946212769], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:43,  4.81it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 171.46it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 269.31it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 335.58it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 381.58it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 418.81it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 399.22it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8479990363121033
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8489 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 0.8221736060879969
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.8101648562760038
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 0.8221827969741754
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.7986718583007756
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.8110239285655894
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3907, dtype=torch.float64), 0, 0, 0, tensor(0.6093, dtype=torch.float64), 0, 31, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(7.4004e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.0695e-18, dtype=torch.float64), tensor(0.3907, dtype=torch.float64), tensor(8.2909e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6093, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9658, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.391
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.609
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 199,098,368 || all params: 8,229,359,616 || trainable%: 2.4194
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 259.54it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 432.86it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 504.48it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 547.70it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 577.75it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 598.09it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 586.58it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.7621, 'grad_norm': 0.30693691968917847, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.7376856803894043, 'eval_runtime': 9.6652, 'eval_samples_per_second': 103.361, 'eval_steps_per_second': 6.518, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 259.99it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 433.35it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 505.10it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 546.11it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 576.24it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 598.20it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 587.33it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.0703, 'grad_norm': 0.32833218574523926, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.5442155599594116, 'eval_runtime': 9.6789, 'eval_samples_per_second': 103.214, 'eval_steps_per_second': 6.509, 'epoch': 0.08}
{'loss': 1.4603, 'grad_norm': 0.11693424731492996, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.418635368347168, 'eval_runtime': 9.7297, 'eval_samples_per_second': 102.675, 'eval_steps_per_second': 6.475, 'epoch': 0.12}
{'loss': 1.4084, 'grad_norm': 0.08723834902048111, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.357351303100586, 'eval_runtime': 9.7761, 'eval_samples_per_second': 102.188, 'eval_steps_per_second': 6.444, 'epoch': 0.16}
{'loss': 1.3536, 'grad_norm': 0.10359908640384674, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2884081602096558, 'eval_runtime': 9.7865, 'eval_samples_per_second': 102.08, 'eval_steps_per_second': 6.437, 'epoch': 0.2}
{'loss': 1.3063, 'grad_norm': 0.05596691742539406, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2275298833847046, 'eval_runtime': 9.7811, 'eval_samples_per_second': 102.136, 'eval_steps_per_second': 6.441, 'epoch': 0.24}
{'loss': 1.238, 'grad_norm': 0.054380036890506744, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.192821979522705, 'eval_runtime': 9.785, 'eval_samples_per_second': 102.095, 'eval_steps_per_second': 6.438, 'epoch': 0.28}
{'loss': 1.2193, 'grad_norm': 0.061055317521095276, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1374726295471191, 'eval_runtime': 9.7916, 'eval_samples_per_second': 102.026, 'eval_steps_per_second': 6.434, 'epoch': 0.32}
{'loss': 1.1502, 'grad_norm': 0.05567057430744171, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.126412034034729, 'eval_runtime': 9.7847, 'eval_samples_per_second': 102.098, 'eval_steps_per_second': 6.439, 'epoch': 0.36}
{'loss': 1.1286, 'grad_norm': 0.06175979599356651, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1220097541809082, 'eval_runtime': 9.7855, 'eval_samples_per_second': 102.09, 'eval_steps_per_second': 6.438, 'epoch': 0.4}
{'loss': 1.0901, 'grad_norm': 0.04700402542948723, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1166342496871948, 'eval_runtime': 9.7794, 'eval_samples_per_second': 102.153, 'eval_steps_per_second': 6.442, 'epoch': 0.44}
{'loss': 1.123, 'grad_norm': 0.04427073523402214, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.113294243812561, 'eval_runtime': 9.7864, 'eval_samples_per_second': 102.081, 'eval_steps_per_second': 6.438, 'epoch': 0.48}
{'loss': 1.185, 'grad_norm': 0.05066612362861633, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1098437309265137, 'eval_runtime': 9.784, 'eval_samples_per_second': 102.105, 'eval_steps_per_second': 6.439, 'epoch': 0.52}
{'loss': 1.0728, 'grad_norm': 0.04981387406587601, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1074496507644653, 'eval_runtime': 9.7755, 'eval_samples_per_second': 102.194, 'eval_steps_per_second': 6.445, 'epoch': 0.56}
{'loss': 1.1357, 'grad_norm': 0.06167911738157272, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.103517770767212, 'eval_runtime': 9.7796, 'eval_samples_per_second': 102.152, 'eval_steps_per_second': 6.442, 'epoch': 0.6}
{'loss': 1.1678, 'grad_norm': 0.0493231937289238, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1009039878845215, 'eval_runtime': 9.7877, 'eval_samples_per_second': 102.067, 'eval_steps_per_second': 6.437, 'epoch': 0.64}
{'loss': 1.0925, 'grad_norm': 0.04854567348957062, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0993150472640991, 'eval_runtime': 9.7973, 'eval_samples_per_second': 101.967, 'eval_steps_per_second': 6.43, 'epoch': 0.68}
{'loss': 1.1215, 'grad_norm': 0.04910798370838165, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0971403121948242, 'eval_runtime': 9.8165, 'eval_samples_per_second': 101.768, 'eval_steps_per_second': 6.418, 'epoch': 0.72}
{'loss': 1.1181, 'grad_norm': 0.05313156545162201, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0945887565612793, 'eval_runtime': 9.8119, 'eval_samples_per_second': 101.816, 'eval_steps_per_second': 6.421, 'epoch': 0.76}
{'loss': 1.1401, 'grad_norm': 0.05368902161717415, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0920480489730835, 'eval_runtime': 9.803, 'eval_samples_per_second': 101.907, 'eval_steps_per_second': 6.427, 'epoch': 0.8}
{'loss': 1.0742, 'grad_norm': 0.04949091374874115, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0903736352920532, 'eval_runtime': 9.7733, 'eval_samples_per_second': 102.217, 'eval_steps_per_second': 6.446, 'epoch': 0.84}
{'loss': 1.092, 'grad_norm': 0.05072325840592384, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0889979600906372, 'eval_runtime': 9.7638, 'eval_samples_per_second': 102.317, 'eval_steps_per_second': 6.452, 'epoch': 0.88}
{'loss': 1.1156, 'grad_norm': 0.06154608726501465, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0884170532226562, 'eval_runtime': 9.7617, 'eval_samples_per_second': 102.339, 'eval_steps_per_second': 6.454, 'epoch': 0.92}
{'loss': 1.1262, 'grad_norm': 0.052663933485746384, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0874794721603394, 'eval_runtime': 9.7662, 'eval_samples_per_second': 102.292, 'eval_steps_per_second': 6.451, 'epoch': 0.96}
{'loss': 1.048, 'grad_norm': 0.04986654594540596, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0873667001724243, 'eval_runtime': 9.7738, 'eval_samples_per_second': 102.212, 'eval_steps_per_second': 6.446, 'epoch': 1.0}
{'train_runtime': 507.9761, 'train_samples_per_second': 19.684, 'train_steps_per_second': 1.23, 'train_loss': 1.311992172241211, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7376856803894043, 1.5442155599594116, 1.418635368347168, 1.357351303100586, 1.2884081602096558, 1.2275298833847046, 1.192821979522705, 1.1374726295471191, 1.126412034034729, 1.1220097541809082, 1.1166342496871948, 1.113294243812561, 1.1098437309265137, 1.1074496507644653, 1.103517770767212, 1.1009039878845215, 1.0993150472640991, 1.0971403121948242, 1.0945887565612793, 1.0920480489730835, 1.0903736352920532, 1.0889979600906372, 1.0884170532226562, 1.0874794721603394, 1.0873667001724243], 'performance': [0.74, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:02,  1.65it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 113.40it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 198.51it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 264.40it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 316.47it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 359.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 300.29it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8468382358551025
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7316 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.7685308329360497
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.7800209980765124
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 0.8203425540280744
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 0.819425224012982
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 0.8203974458425921
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3103, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6897, dtype=torch.float64), 26, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(6.3940e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.4441e-17, dtype=torch.float64), tensor(0.3103, dtype=torch.float64), tensor(2.9384e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6897, dtype=torch.float64), tensor(0.8243, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.31
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.69

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 166,985,728 || all params: 8,197,246,976 || trainable%: 2.0371
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 268.89it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 447.35it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 521.57it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 566.45it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 597.80it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 622.13it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 609.52it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.0422, 'grad_norm': 0.29066330194473267, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.828477621078491, 'eval_runtime': 6.9307, 'eval_samples_per_second': 144.14, 'eval_steps_per_second': 9.09, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 268.17it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 447.11it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 519.97it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 565.17it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 596.16it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 620.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 608.02it/s]
Evaluation performance at step 50: 0.73
{'loss': 1.8918, 'grad_norm': 0.425163209438324, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.2373396158218384, 'eval_runtime': 6.9268, 'eval_samples_per_second': 144.222, 'eval_steps_per_second': 9.095, 'epoch': 0.08}
{'loss': 1.1091, 'grad_norm': 0.1343018114566803, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0688024759292603, 'eval_runtime': 6.9675, 'eval_samples_per_second': 143.38, 'eval_steps_per_second': 9.042, 'epoch': 0.12}
{'loss': 1.026, 'grad_norm': 0.10965117067098618, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0207339525222778, 'eval_runtime': 6.9673, 'eval_samples_per_second': 143.384, 'eval_steps_per_second': 9.042, 'epoch': 0.16}
{'loss': 0.9967, 'grad_norm': 0.13428504765033722, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9702156782150269, 'eval_runtime': 6.9912, 'eval_samples_per_second': 142.895, 'eval_steps_per_second': 9.011, 'epoch': 0.2}
{'loss': 0.9555, 'grad_norm': 0.04803507402539253, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9116827249526978, 'eval_runtime': 6.9832, 'eval_samples_per_second': 143.057, 'eval_steps_per_second': 9.022, 'epoch': 0.24}
{'loss': 0.9237, 'grad_norm': 0.049254707992076874, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8808050751686096, 'eval_runtime': 6.991, 'eval_samples_per_second': 142.897, 'eval_steps_per_second': 9.012, 'epoch': 0.28}
{'loss': 0.8492, 'grad_norm': 0.0755319893360138, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8361221551895142, 'eval_runtime': 6.9823, 'eval_samples_per_second': 143.077, 'eval_steps_per_second': 9.023, 'epoch': 0.32}
{'loss': 0.8504, 'grad_norm': 0.04580739140510559, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8051221370697021, 'eval_runtime': 6.9943, 'eval_samples_per_second': 142.83, 'eval_steps_per_second': 9.007, 'epoch': 0.36}
{'loss': 0.8158, 'grad_norm': 0.051147542893886566, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7947444319725037, 'eval_runtime': 6.99, 'eval_samples_per_second': 142.918, 'eval_steps_per_second': 9.013, 'epoch': 0.4}
{'loss': 0.7925, 'grad_norm': 0.06066453456878662, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7841976284980774, 'eval_runtime': 6.9993, 'eval_samples_per_second': 142.728, 'eval_steps_per_second': 9.001, 'epoch': 0.44}
{'loss': 0.7946, 'grad_norm': 0.06024765968322754, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7738636136054993, 'eval_runtime': 6.9897, 'eval_samples_per_second': 142.925, 'eval_steps_per_second': 9.013, 'epoch': 0.48}
{'loss': 0.7904, 'grad_norm': 0.060372088104486465, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7652194499969482, 'eval_runtime': 6.9893, 'eval_samples_per_second': 142.932, 'eval_steps_per_second': 9.014, 'epoch': 0.52}
{'loss': 0.7687, 'grad_norm': 0.057100746780633926, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7557262778282166, 'eval_runtime': 6.986, 'eval_samples_per_second': 143.0, 'eval_steps_per_second': 9.018, 'epoch': 0.56}
{'loss': 0.7369, 'grad_norm': 0.06569594889879227, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7442333698272705, 'eval_runtime': 6.9849, 'eval_samples_per_second': 143.023, 'eval_steps_per_second': 9.019, 'epoch': 0.6}
{'loss': 0.7473, 'grad_norm': 0.07596009224653244, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7357198596000671, 'eval_runtime': 6.9931, 'eval_samples_per_second': 142.856, 'eval_steps_per_second': 9.009, 'epoch': 0.64}
{'loss': 0.7254, 'grad_norm': 0.06715002655982971, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7240830063819885, 'eval_runtime': 6.9863, 'eval_samples_per_second': 142.995, 'eval_steps_per_second': 9.018, 'epoch': 0.68}
{'loss': 0.7416, 'grad_norm': 0.07330069690942764, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7154398560523987, 'eval_runtime': 6.9881, 'eval_samples_per_second': 142.957, 'eval_steps_per_second': 9.015, 'epoch': 0.72}
{'loss': 0.716, 'grad_norm': 0.07112246751785278, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7055121064186096, 'eval_runtime': 6.989, 'eval_samples_per_second': 142.94, 'eval_steps_per_second': 9.014, 'epoch': 0.76}
{'loss': 0.7287, 'grad_norm': 0.0854414850473404, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6959351897239685, 'eval_runtime': 6.995, 'eval_samples_per_second': 142.816, 'eval_steps_per_second': 9.006, 'epoch': 0.8}
{'loss': 0.7114, 'grad_norm': 0.08844582736492157, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6888882517814636, 'eval_runtime': 6.9924, 'eval_samples_per_second': 142.868, 'eval_steps_per_second': 9.01, 'epoch': 0.84}
{'loss': 0.7029, 'grad_norm': 0.08712032437324524, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.681601881980896, 'eval_runtime': 6.9921, 'eval_samples_per_second': 142.875, 'eval_steps_per_second': 9.01, 'epoch': 0.88}
{'loss': 0.6848, 'grad_norm': 0.0903182327747345, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6752727031707764, 'eval_runtime': 7.0029, 'eval_samples_per_second': 142.655, 'eval_steps_per_second': 8.996, 'epoch': 0.92}
{'loss': 0.7071, 'grad_norm': 0.09330074489116669, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6719319224357605, 'eval_runtime': 6.9983, 'eval_samples_per_second': 142.749, 'eval_steps_per_second': 9.002, 'epoch': 0.96}
{'loss': 0.6902, 'grad_norm': 0.10946298390626907, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6699645519256592, 'eval_runtime': 6.9853, 'eval_samples_per_second': 143.014, 'eval_steps_per_second': 9.019, 'epoch': 1.0}
{'train_runtime': 374.871, 'train_samples_per_second': 26.673, 'train_steps_per_second': 1.667, 'train_loss': 0.9799534240722656, 'epoch': 1.0}
train_results:  {'eval_loss': [2.828477621078491, 1.2373396158218384, 1.0688024759292603, 1.0207339525222778, 0.9702156782150269, 0.9116827249526978, 0.8808050751686096, 0.8361221551895142, 0.8051221370697021, 0.7947444319725037, 0.7841976284980774, 0.7738636136054993, 0.7652194499969482, 0.7557262778282166, 0.7442333698272705, 0.7357198596000671, 0.7240830063819885, 0.7154398560523987, 0.7055121064186096, 0.6959351897239685, 0.6888882517814636, 0.681601881980896, 0.6752727031707764, 0.6719319224357605, 0.6699645519256592], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<06:34,  1.27it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:05, 78.54it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 151.20it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 217.61it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 275.52it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 326.14it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 248.29it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8452725410461426
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1460 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.8126741927210742
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.8102621731843378
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 0.8184822344723957
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7669399290520812
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.8181211464263949
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0826, dtype=torch.float64), 0, 0, tensor(0.5549, dtype=torch.float64), tensor(0.3625, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 1, 0, 128, 3.4694469519536125e-19, 1.4800000190734894, 1]
normalized proposed parameters for next round by BO: [tensor(4.5240e-18, dtype=torch.float64), tensor(1.9653e-17, dtype=torch.float64), tensor(0.0826, dtype=torch.float64), tensor(3.3355e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5549, dtype=torch.float64), tensor(0.3625, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.4694e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.083
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.555
  wikitext: 0.362
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.4694469519536125e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734894,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  3.4694469519536125e-19
lora alpha:  1.4800000190734894
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.49it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.56it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 501.60it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 544.43it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 574.34it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 597.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 585.21it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.2141, 'grad_norm': 0.17645372450351715, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.336118459701538, 'eval_runtime': 9.4109, 'eval_samples_per_second': 106.153, 'eval_steps_per_second': 6.694, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.91it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 430.60it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 501.10it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 544.43it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 574.65it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 597.87it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 585.87it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.547, 'grad_norm': 0.2205808311700821, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.9639216661453247, 'eval_runtime': 9.4093, 'eval_samples_per_second': 106.172, 'eval_steps_per_second': 6.696, 'epoch': 0.08}
{'loss': 1.8734, 'grad_norm': 0.09717979282140732, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6672093868255615, 'eval_runtime': 9.4312, 'eval_samples_per_second': 105.925, 'eval_steps_per_second': 6.68, 'epoch': 0.12}
{'loss': 1.7096, 'grad_norm': 0.08153043687343597, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.538133978843689, 'eval_runtime': 9.469, 'eval_samples_per_second': 105.502, 'eval_steps_per_second': 6.653, 'epoch': 0.16}
{'loss': 1.5433, 'grad_norm': 0.07677856087684631, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4288430213928223, 'eval_runtime': 9.4859, 'eval_samples_per_second': 105.314, 'eval_steps_per_second': 6.641, 'epoch': 0.2}
{'loss': 1.4882, 'grad_norm': 0.06867813318967819, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3919137716293335, 'eval_runtime': 9.504, 'eval_samples_per_second': 105.114, 'eval_steps_per_second': 6.629, 'epoch': 0.24}
{'loss': 1.5005, 'grad_norm': 0.08443286269903183, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3687323331832886, 'eval_runtime': 9.4981, 'eval_samples_per_second': 105.179, 'eval_steps_per_second': 6.633, 'epoch': 0.28}
{'loss': 1.3517, 'grad_norm': 0.08217797428369522, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3499735593795776, 'eval_runtime': 9.495, 'eval_samples_per_second': 105.213, 'eval_steps_per_second': 6.635, 'epoch': 0.32}
{'loss': 1.3353, 'grad_norm': 0.08008867502212524, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3336920738220215, 'eval_runtime': 9.4987, 'eval_samples_per_second': 105.173, 'eval_steps_per_second': 6.633, 'epoch': 0.36}
{'loss': 1.3279, 'grad_norm': 0.08324426412582397, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3142585754394531, 'eval_runtime': 9.4958, 'eval_samples_per_second': 105.204, 'eval_steps_per_second': 6.634, 'epoch': 0.4}
{'loss': 1.3535, 'grad_norm': 0.07420818507671356, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2963032722473145, 'eval_runtime': 9.4917, 'eval_samples_per_second': 105.25, 'eval_steps_per_second': 6.637, 'epoch': 0.44}
{'loss': 1.4035, 'grad_norm': 0.09668469429016113, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2809569835662842, 'eval_runtime': 9.4955, 'eval_samples_per_second': 105.208, 'eval_steps_per_second': 6.635, 'epoch': 0.48}
{'loss': 1.3685, 'grad_norm': 0.09855220466852188, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.266344428062439, 'eval_runtime': 9.4963, 'eval_samples_per_second': 105.199, 'eval_steps_per_second': 6.634, 'epoch': 0.52}
{'loss': 1.3079, 'grad_norm': 0.11426272988319397, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2500680685043335, 'eval_runtime': 9.4934, 'eval_samples_per_second': 105.232, 'eval_steps_per_second': 6.636, 'epoch': 0.56}
{'loss': 1.2866, 'grad_norm': 0.08609538525342941, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2336492538452148, 'eval_runtime': 9.4885, 'eval_samples_per_second': 105.286, 'eval_steps_per_second': 6.64, 'epoch': 0.6}
{'loss': 1.3151, 'grad_norm': 0.09562918543815613, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.215720534324646, 'eval_runtime': 9.4863, 'eval_samples_per_second': 105.31, 'eval_steps_per_second': 6.641, 'epoch': 0.64}
{'loss': 1.3175, 'grad_norm': 0.19714631140232086, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2012724876403809, 'eval_runtime': 9.48, 'eval_samples_per_second': 105.38, 'eval_steps_per_second': 6.646, 'epoch': 0.68}
{'loss': 1.2346, 'grad_norm': 0.1777653992176056, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1832894086837769, 'eval_runtime': 9.4827, 'eval_samples_per_second': 105.35, 'eval_steps_per_second': 6.644, 'epoch': 0.72}
{'loss': 1.3044, 'grad_norm': 0.11133727431297302, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1684575080871582, 'eval_runtime': 9.4766, 'eval_samples_per_second': 105.417, 'eval_steps_per_second': 6.648, 'epoch': 0.76}
{'loss': 1.2961, 'grad_norm': 0.17925339937210083, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1545822620391846, 'eval_runtime': 9.4806, 'eval_samples_per_second': 105.373, 'eval_steps_per_second': 6.645, 'epoch': 0.8}
{'loss': 1.2683, 'grad_norm': 0.14288532733917236, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1461197137832642, 'eval_runtime': 9.4815, 'eval_samples_per_second': 105.364, 'eval_steps_per_second': 6.645, 'epoch': 0.84}
{'loss': 1.2304, 'grad_norm': 0.10560683906078339, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1382845640182495, 'eval_runtime': 9.4842, 'eval_samples_per_second': 105.333, 'eval_steps_per_second': 6.643, 'epoch': 0.88}
{'loss': 1.1999, 'grad_norm': 0.11195977032184601, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.132188081741333, 'eval_runtime': 9.4785, 'eval_samples_per_second': 105.396, 'eval_steps_per_second': 6.647, 'epoch': 0.92}
{'loss': 1.1779, 'grad_norm': 0.11135093122720718, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1285899877548218, 'eval_runtime': 9.484, 'eval_samples_per_second': 105.335, 'eval_steps_per_second': 6.643, 'epoch': 0.96}
{'loss': 1.1719, 'grad_norm': 0.13085420429706573, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1267024278640747, 'eval_runtime': 9.4707, 'eval_samples_per_second': 105.484, 'eval_steps_per_second': 6.652, 'epoch': 1.0}
{'train_runtime': 499.0108, 'train_samples_per_second': 20.038, 'train_steps_per_second': 1.252, 'train_loss': 1.5250821411132813, 'epoch': 1.0}
train_results:  {'eval_loss': [3.336118459701538, 1.9639216661453247, 1.6672093868255615, 1.538133978843689, 1.4288430213928223, 1.3919137716293335, 1.3687323331832886, 1.3499735593795776, 1.3336920738220215, 1.3142585754394531, 1.2963032722473145, 1.2809569835662842, 1.266344428062439, 1.2500680685043335, 1.2336492538452148, 1.215720534324646, 1.2012724876403809, 1.1832894086837769, 1.1684575080871582, 1.1545822620391846, 1.1461197137832642, 1.1382845640182495, 1.132188081741333, 1.1285899877548218, 1.1267024278640747], 'performance': [0.76, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:51,  4.48it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 231.06it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 317.17it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 363.86it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 393.56it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 418.03it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 421.76it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8481103181838989
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0998 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.7876722403555494
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.7615340076274955
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.8099736507028454
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.7791797627793745
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.7924567863568812
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2463, dtype=torch.float64), 0, tensor(0.2404, dtype=torch.float64), 0, 0, tensor(0.0936, dtype=torch.float64), tensor(0.4196, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.2463, dtype=torch.float64), tensor(2.0665e-16, dtype=torch.float64), tensor(0.2404, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9803e-17, dtype=torch.float64), tensor(0.0936, dtype=torch.float64), tensor(0.4196, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.246
  gsm8k: 0
  rowan_hellaswag: 0.24
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.094
  wikitext: 0.42
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.56it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 465.21it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 540.48it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 584.77it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 616.24it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 641.24it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 629.37it/s]
Evaluation performance at step 25: 0.76
{'loss': 4.0767, 'grad_norm': 0.19291093945503235, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.76}
{'eval_loss': 3.4991235733032227, 'eval_runtime': 9.7288, 'eval_samples_per_second': 102.685, 'eval_steps_per_second': 6.476, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.17it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 464.00it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 541.07it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 587.54it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 620.14it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 645.15it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 632.12it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.8942, 'grad_norm': 0.07035043835639954, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 2.339224338531494, 'eval_runtime': 9.7363, 'eval_samples_per_second': 102.605, 'eval_steps_per_second': 6.471, 'epoch': 0.08}
{'loss': 2.1219, 'grad_norm': 0.06466230750083923, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.9676610231399536, 'eval_runtime': 9.7577, 'eval_samples_per_second': 102.381, 'eval_steps_per_second': 6.456, 'epoch': 0.12}
{'loss': 1.9601, 'grad_norm': 0.050090957432985306, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.8537101745605469, 'eval_runtime': 9.7979, 'eval_samples_per_second': 101.961, 'eval_steps_per_second': 6.43, 'epoch': 0.16}
{'loss': 1.8597, 'grad_norm': 0.06044350191950798, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7991993427276611, 'eval_runtime': 9.817, 'eval_samples_per_second': 101.762, 'eval_steps_per_second': 6.417, 'epoch': 0.2}
{'loss': 1.8377, 'grad_norm': 0.05506496876478195, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.772334098815918, 'eval_runtime': 9.847, 'eval_samples_per_second': 101.452, 'eval_steps_per_second': 6.398, 'epoch': 0.24}
{'loss': 1.7761, 'grad_norm': 0.06808741390705109, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7542136907577515, 'eval_runtime': 9.8286, 'eval_samples_per_second': 101.642, 'eval_steps_per_second': 6.41, 'epoch': 0.28}
{'loss': 1.7793, 'grad_norm': 0.05923961475491524, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.7403151988983154, 'eval_runtime': 9.8155, 'eval_samples_per_second': 101.778, 'eval_steps_per_second': 6.418, 'epoch': 0.32}
{'loss': 1.7605, 'grad_norm': 0.05390319600701332, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.728355050086975, 'eval_runtime': 9.8077, 'eval_samples_per_second': 101.858, 'eval_steps_per_second': 6.423, 'epoch': 0.36}
{'loss': 1.7581, 'grad_norm': 0.05029582977294922, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7192485332489014, 'eval_runtime': 9.8172, 'eval_samples_per_second': 101.76, 'eval_steps_per_second': 6.417, 'epoch': 0.4}
{'loss': 1.7663, 'grad_norm': 0.052966926246881485, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7103559970855713, 'eval_runtime': 9.8164, 'eval_samples_per_second': 101.768, 'eval_steps_per_second': 6.418, 'epoch': 0.44}
{'loss': 1.6999, 'grad_norm': 0.04999664053320885, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.700915813446045, 'eval_runtime': 9.8141, 'eval_samples_per_second': 101.792, 'eval_steps_per_second': 6.419, 'epoch': 0.48}
{'loss': 1.6994, 'grad_norm': 0.060890380293130875, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.6925582885742188, 'eval_runtime': 9.8177, 'eval_samples_per_second': 101.755, 'eval_steps_per_second': 6.417, 'epoch': 0.52}
{'loss': 1.7386, 'grad_norm': 0.0540478453040123, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.68410062789917, 'eval_runtime': 9.817, 'eval_samples_per_second': 101.762, 'eval_steps_per_second': 6.417, 'epoch': 0.56}
{'loss': 1.684, 'grad_norm': 0.06747526675462723, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.6728368997573853, 'eval_runtime': 9.8403, 'eval_samples_per_second': 101.521, 'eval_steps_per_second': 6.402, 'epoch': 0.6}
{'loss': 1.7587, 'grad_norm': 0.1013486385345459, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.657180905342102, 'eval_runtime': 9.8295, 'eval_samples_per_second': 101.633, 'eval_steps_per_second': 6.409, 'epoch': 0.64}
{'loss': 1.6657, 'grad_norm': 0.04799516871571541, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.637673258781433, 'eval_runtime': 9.8182, 'eval_samples_per_second': 101.75, 'eval_steps_per_second': 6.417, 'epoch': 0.68}
{'loss': 1.6905, 'grad_norm': 0.06233352795243263, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6304240226745605, 'eval_runtime': 9.8117, 'eval_samples_per_second': 101.818, 'eval_steps_per_second': 6.421, 'epoch': 0.72}
{'loss': 1.6385, 'grad_norm': 0.06475962698459625, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6243443489074707, 'eval_runtime': 9.8196, 'eval_samples_per_second': 101.735, 'eval_steps_per_second': 6.416, 'epoch': 0.76}
{'loss': 1.7174, 'grad_norm': 0.0584137998521328, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.619372844696045, 'eval_runtime': 9.8086, 'eval_samples_per_second': 101.849, 'eval_steps_per_second': 6.423, 'epoch': 0.8}
{'loss': 1.678, 'grad_norm': 0.04659875109791756, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6147223711013794, 'eval_runtime': 9.8021, 'eval_samples_per_second': 101.917, 'eval_steps_per_second': 6.427, 'epoch': 0.84}
{'loss': 1.6911, 'grad_norm': 0.0610509030520916, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6107176542282104, 'eval_runtime': 9.7872, 'eval_samples_per_second': 102.073, 'eval_steps_per_second': 6.437, 'epoch': 0.88}
{'loss': 1.6755, 'grad_norm': 0.06281718611717224, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6075165271759033, 'eval_runtime': 9.7899, 'eval_samples_per_second': 102.044, 'eval_steps_per_second': 6.435, 'epoch': 0.92}
{'loss': 1.6541, 'grad_norm': 0.06527508795261383, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.60548734664917, 'eval_runtime': 9.7913, 'eval_samples_per_second': 102.029, 'eval_steps_per_second': 6.434, 'epoch': 0.96}
{'loss': 1.6148, 'grad_norm': 0.05890670046210289, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6046855449676514, 'eval_runtime': 9.8041, 'eval_samples_per_second': 101.896, 'eval_steps_per_second': 6.426, 'epoch': 1.0}
{'train_runtime': 496.4527, 'train_samples_per_second': 20.139, 'train_steps_per_second': 1.259, 'train_loss': 1.8878644104003905, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4991235733032227, 2.339224338531494, 1.9676610231399536, 1.8537101745605469, 1.7991993427276611, 1.772334098815918, 1.7542136907577515, 1.7403151988983154, 1.728355050086975, 1.7192485332489014, 1.7103559970855713, 1.700915813446045, 1.6925582885742188, 1.68410062789917, 1.6728368997573853, 1.657180905342102, 1.637673258781433, 1.6304240226745605, 1.6243443489074707, 1.619372844696045, 1.6147223711013794, 1.6107176542282104, 1.6075165271759033, 1.60548734664917, 1.6046855449676514], 'performance': [0.76, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:44,  4.79it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 247.40it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 339.04it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 389.37it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 421.85it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 447.27it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 451.44it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.76, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8476434946060181
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7630 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.813484948597711
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.814450217469363
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.7823880422777153
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.8159547526699003
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.8159477059662568
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1667, dtype=torch.float64), 0, 0, tensor(0.5083, dtype=torch.float64), tensor(0.3251, dtype=torch.float64), 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.1, 21.006703622464574, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(8.7031e-17, dtype=torch.float64), tensor(0.1667, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.8851e-16, dtype=torch.float64), tensor(0.5083, dtype=torch.float64), tensor(0.3251, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4376, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.167
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.508
  wikitext: 0.325
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (21.006703622464574,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  21.006703622464574
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 268.11it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 445.97it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 520.59it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 565.41it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 596.75it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 620.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 608.03it/s]
Evaluation performance at step 25: 0.72
{'loss': 3.4816, 'grad_norm': 0.5026772022247314, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.72}
{'eval_loss': 2.1770429611206055, 'eval_runtime': 9.7139, 'eval_samples_per_second': 102.843, 'eval_steps_per_second': 6.486, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 267.97it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 446.08it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 519.93it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 564.47it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 592.52it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 616.34it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 605.44it/s]
Evaluation performance at step 50: 0.75
{'loss': 1.9252, 'grad_norm': 0.39459678530693054, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 1.653378963470459, 'eval_runtime': 9.7221, 'eval_samples_per_second': 102.755, 'eval_steps_per_second': 6.48, 'epoch': 0.08}
{'loss': 1.576, 'grad_norm': 0.2987726330757141, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4950449466705322, 'eval_runtime': 9.7464, 'eval_samples_per_second': 102.499, 'eval_steps_per_second': 6.464, 'epoch': 0.12}
{'loss': 1.5213, 'grad_norm': 0.23975969851016998, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4538625478744507, 'eval_runtime': 9.7847, 'eval_samples_per_second': 102.098, 'eval_steps_per_second': 6.439, 'epoch': 0.16}
{'loss': 1.4826, 'grad_norm': 0.26229608058929443, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4247446060180664, 'eval_runtime': 9.7978, 'eval_samples_per_second': 101.962, 'eval_steps_per_second': 6.43, 'epoch': 0.2}
{'loss': 1.4021, 'grad_norm': 0.2064112424850464, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3909929990768433, 'eval_runtime': 9.8002, 'eval_samples_per_second': 101.937, 'eval_steps_per_second': 6.428, 'epoch': 0.24}
{'loss': 1.4743, 'grad_norm': 0.19524350762367249, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3555039167404175, 'eval_runtime': 9.7905, 'eval_samples_per_second': 102.037, 'eval_steps_per_second': 6.435, 'epoch': 0.28}
{'loss': 1.4167, 'grad_norm': 0.21678657829761505, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3336371183395386, 'eval_runtime': 9.7926, 'eval_samples_per_second': 102.016, 'eval_steps_per_second': 6.433, 'epoch': 0.32}
{'loss': 1.362, 'grad_norm': 0.3382900357246399, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.310208797454834, 'eval_runtime': 9.8003, 'eval_samples_per_second': 101.935, 'eval_steps_per_second': 6.428, 'epoch': 0.36}
{'loss': 1.3447, 'grad_norm': 0.19742771983146667, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.292856216430664, 'eval_runtime': 9.797, 'eval_samples_per_second': 101.97, 'eval_steps_per_second': 6.431, 'epoch': 0.4}
{'loss': 1.3504, 'grad_norm': 0.2690734267234802, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2752223014831543, 'eval_runtime': 9.7997, 'eval_samples_per_second': 101.942, 'eval_steps_per_second': 6.429, 'epoch': 0.44}
{'loss': 1.318, 'grad_norm': 0.2150735855102539, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2547481060028076, 'eval_runtime': 9.8056, 'eval_samples_per_second': 101.881, 'eval_steps_per_second': 6.425, 'epoch': 0.48}
{'loss': 1.2821, 'grad_norm': 0.21542802453041077, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2424945831298828, 'eval_runtime': 9.8051, 'eval_samples_per_second': 101.886, 'eval_steps_per_second': 6.425, 'epoch': 0.52}
{'loss': 1.26, 'grad_norm': 0.18066047132015228, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2310408353805542, 'eval_runtime': 9.8002, 'eval_samples_per_second': 101.936, 'eval_steps_per_second': 6.428, 'epoch': 0.56}
{'loss': 1.3363, 'grad_norm': 0.2514684498310089, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2134840488433838, 'eval_runtime': 9.8022, 'eval_samples_per_second': 101.916, 'eval_steps_per_second': 6.427, 'epoch': 0.6}
{'loss': 1.2836, 'grad_norm': 0.26005616784095764, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.203823208808899, 'eval_runtime': 9.7963, 'eval_samples_per_second': 101.977, 'eval_steps_per_second': 6.431, 'epoch': 0.64}
{'loss': 1.4066, 'grad_norm': 0.21042725443840027, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.193476915359497, 'eval_runtime': 9.7921, 'eval_samples_per_second': 102.021, 'eval_steps_per_second': 6.434, 'epoch': 0.68}
{'loss': 1.2504, 'grad_norm': 0.23148062825202942, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1864224672317505, 'eval_runtime': 9.7926, 'eval_samples_per_second': 102.016, 'eval_steps_per_second': 6.433, 'epoch': 0.72}
{'loss': 1.1884, 'grad_norm': 0.19123338162899017, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1761564016342163, 'eval_runtime': 9.7955, 'eval_samples_per_second': 101.986, 'eval_steps_per_second': 6.432, 'epoch': 0.76}
{'loss': 1.2586, 'grad_norm': 0.2101377248764038, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1689097881317139, 'eval_runtime': 9.7952, 'eval_samples_per_second': 101.989, 'eval_steps_per_second': 6.432, 'epoch': 0.8}
{'loss': 1.2177, 'grad_norm': 0.2338104546070099, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.162302017211914, 'eval_runtime': 9.7997, 'eval_samples_per_second': 101.941, 'eval_steps_per_second': 6.429, 'epoch': 0.84}
{'loss': 1.2304, 'grad_norm': 0.2511570453643799, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1558722257614136, 'eval_runtime': 9.797, 'eval_samples_per_second': 101.97, 'eval_steps_per_second': 6.431, 'epoch': 0.88}
{'loss': 1.2996, 'grad_norm': 0.1850052773952484, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.151843786239624, 'eval_runtime': 9.803, 'eval_samples_per_second': 101.907, 'eval_steps_per_second': 6.427, 'epoch': 0.92}
{'loss': 1.2572, 'grad_norm': 0.1826847344636917, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1499767303466797, 'eval_runtime': 9.8001, 'eval_samples_per_second': 101.938, 'eval_steps_per_second': 6.429, 'epoch': 0.96}
{'loss': 1.1835, 'grad_norm': 0.20759807527065277, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1487895250320435, 'eval_runtime': 9.8064, 'eval_samples_per_second': 101.873, 'eval_steps_per_second': 6.424, 'epoch': 1.0}
{'train_runtime': 506.7605, 'train_samples_per_second': 19.729, 'train_steps_per_second': 1.233, 'train_loss': 1.444367138671875, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1770429611206055, 1.653378963470459, 1.4950449466705322, 1.4538625478744507, 1.4247446060180664, 1.3909929990768433, 1.3555039167404175, 1.3336371183395386, 1.310208797454834, 1.292856216430664, 1.2752223014831543, 1.2547481060028076, 1.2424945831298828, 1.2310408353805542, 1.2134840488433838, 1.203823208808899, 1.193476915359497, 1.1864224672317505, 1.1761564016342163, 1.1689097881317139, 1.162302017211914, 1.1558722257614136, 1.151843786239624, 1.1499767303466797, 1.1487895250320435], 'performance': [0.72, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:05,  1.63it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 109.73it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 195.94it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 265.19it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 319.25it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 363.50it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 300.16it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.72, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8387138843536377
current iteration best possible performance (full train run):  0.756
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6793 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.8145414217971744
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.8145495474421479
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.814550221566138
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.7876190888194248
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.8145487038468255
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5033, dtype=torch.float64), 0, 0, 0, tensor(0.4967, dtype=torch.float64), 0, 0, 22, 0, 0, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(2.4951e-17, dtype=torch.float64), tensor(1.3395e-17, dtype=torch.float64), tensor(0.5033, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2826e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4967, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7022, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.503
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.497
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 103,809,024 || all params: 8,134,070,272 || trainable%: 1.2762
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 287.11it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.49it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.52it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 606.74it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 641.69it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 664.85it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.42it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.7611, 'grad_norm': 0.1563161164522171, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.3265907764434814, 'eval_runtime': 9.6439, 'eval_samples_per_second': 103.588, 'eval_steps_per_second': 6.533, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 287.65it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.62it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.70it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 603.84it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 637.35it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 664.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 651.33it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.8282, 'grad_norm': 0.23642517626285553, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.3745033740997314, 'eval_runtime': 9.6617, 'eval_samples_per_second': 103.398, 'eval_steps_per_second': 6.521, 'epoch': 0.08}
{'loss': 2.2991, 'grad_norm': 0.08604943752288818, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1295909881591797, 'eval_runtime': 9.6776, 'eval_samples_per_second': 103.229, 'eval_steps_per_second': 6.51, 'epoch': 0.12}
{'loss': 2.1177, 'grad_norm': 0.05192385986447334, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.025987386703491, 'eval_runtime': 9.6943, 'eval_samples_per_second': 103.05, 'eval_steps_per_second': 6.499, 'epoch': 0.16}
{'loss': 2.0299, 'grad_norm': 0.05921904742717743, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.9757709503173828, 'eval_runtime': 9.7106, 'eval_samples_per_second': 102.878, 'eval_steps_per_second': 6.488, 'epoch': 0.2}
{'loss': 1.9722, 'grad_norm': 0.03967924043536186, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.9355095624923706, 'eval_runtime': 9.7255, 'eval_samples_per_second': 102.72, 'eval_steps_per_second': 6.478, 'epoch': 0.24}
{'loss': 1.9594, 'grad_norm': 0.06089307740330696, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.9068992137908936, 'eval_runtime': 9.7242, 'eval_samples_per_second': 102.734, 'eval_steps_per_second': 6.479, 'epoch': 0.28}
{'loss': 1.9102, 'grad_norm': 0.04731690138578415, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.8889167308807373, 'eval_runtime': 9.7227, 'eval_samples_per_second': 102.749, 'eval_steps_per_second': 6.48, 'epoch': 0.32}
{'loss': 1.9271, 'grad_norm': 0.06000418961048126, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.8680012226104736, 'eval_runtime': 9.7203, 'eval_samples_per_second': 102.775, 'eval_steps_per_second': 6.481, 'epoch': 0.36}
{'loss': 1.9124, 'grad_norm': 0.05989433825016022, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.8549480438232422, 'eval_runtime': 9.7319, 'eval_samples_per_second': 102.652, 'eval_steps_per_second': 6.474, 'epoch': 0.4}
{'loss': 1.9027, 'grad_norm': 0.07279232144355774, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.8498425483703613, 'eval_runtime': 9.7102, 'eval_samples_per_second': 102.881, 'eval_steps_per_second': 6.488, 'epoch': 0.44}
{'loss': 1.8816, 'grad_norm': 0.04877949133515358, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.8443632125854492, 'eval_runtime': 9.698, 'eval_samples_per_second': 103.011, 'eval_steps_per_second': 6.496, 'epoch': 0.48}
{'loss': 1.8647, 'grad_norm': 0.050527457147836685, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.8407179117202759, 'eval_runtime': 9.6988, 'eval_samples_per_second': 103.002, 'eval_steps_per_second': 6.496, 'epoch': 0.52}
{'loss': 1.868, 'grad_norm': 0.047764357179403305, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.8375555276870728, 'eval_runtime': 9.6966, 'eval_samples_per_second': 103.026, 'eval_steps_per_second': 6.497, 'epoch': 0.56}
{'loss': 1.8585, 'grad_norm': 0.06216208264231682, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.833817720413208, 'eval_runtime': 9.6947, 'eval_samples_per_second': 103.046, 'eval_steps_per_second': 6.498, 'epoch': 0.6}
{'loss': 1.8876, 'grad_norm': 0.049300502985715866, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.8311539888381958, 'eval_runtime': 9.6972, 'eval_samples_per_second': 103.019, 'eval_steps_per_second': 6.497, 'epoch': 0.64}
{'loss': 1.849, 'grad_norm': 0.06577636301517487, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.8293159008026123, 'eval_runtime': 9.7055, 'eval_samples_per_second': 102.931, 'eval_steps_per_second': 6.491, 'epoch': 0.68}
{'loss': 1.8305, 'grad_norm': 0.06618951261043549, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.8259172439575195, 'eval_runtime': 9.7046, 'eval_samples_per_second': 102.941, 'eval_steps_per_second': 6.492, 'epoch': 0.72}
{'loss': 1.8638, 'grad_norm': 0.05423152819275856, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.8240262269973755, 'eval_runtime': 9.7001, 'eval_samples_per_second': 102.989, 'eval_steps_per_second': 6.495, 'epoch': 0.76}
{'loss': 1.8874, 'grad_norm': 0.07270336896181107, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.821574330329895, 'eval_runtime': 9.7097, 'eval_samples_per_second': 102.887, 'eval_steps_per_second': 6.488, 'epoch': 0.8}
{'loss': 1.8532, 'grad_norm': 0.04767478257417679, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.8204684257507324, 'eval_runtime': 9.7097, 'eval_samples_per_second': 102.887, 'eval_steps_per_second': 6.488, 'epoch': 0.84}
{'loss': 1.8414, 'grad_norm': 0.05779485031962395, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.8186836242675781, 'eval_runtime': 9.7016, 'eval_samples_per_second': 102.973, 'eval_steps_per_second': 6.494, 'epoch': 0.88}
{'loss': 1.8863, 'grad_norm': 0.047724444419145584, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.8183319568634033, 'eval_runtime': 9.6973, 'eval_samples_per_second': 103.018, 'eval_steps_per_second': 6.497, 'epoch': 0.92}
{'loss': 1.8621, 'grad_norm': 0.07959701120853424, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.8174171447753906, 'eval_runtime': 9.6986, 'eval_samples_per_second': 103.004, 'eval_steps_per_second': 6.496, 'epoch': 0.96}
{'loss': 1.8268, 'grad_norm': 0.05237247794866562, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.8169329166412354, 'eval_runtime': 9.7024, 'eval_samples_per_second': 102.964, 'eval_steps_per_second': 6.493, 'epoch': 1.0}
{'train_runtime': 489.841, 'train_samples_per_second': 20.413, 'train_steps_per_second': 1.276, 'train_loss': 2.027241131591797, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3265907764434814, 2.3745033740997314, 2.1295909881591797, 2.025987386703491, 1.9757709503173828, 1.9355095624923706, 1.9068992137908936, 1.8889167308807373, 1.8680012226104736, 1.8549480438232422, 1.8498425483703613, 1.8443632125854492, 1.8407179117202759, 1.8375555276870728, 1.833817720413208, 1.8311539888381958, 1.8293159008026123, 1.8259172439575195, 1.8240262269973755, 1.821574330329895, 1.8204684257507324, 1.8186836242675781, 1.8183319568634033, 1.8174171447753906, 1.8169329166412354], 'performance': [0.75, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<07:43,  1.08it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:05, 77.20it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 150.88it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 220.22it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 281.64it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 337.14it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 246.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8424214124679565
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0925 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.8070551599429328
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.8121292771301327
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.7707306542363168
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.8107908009376188
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.8127173916942255
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6429, dtype=torch.float64), 0, 0, tensor(0.3571, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 0, 128, 1.170938346284339e-18, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.6429, dtype=torch.float64), tensor(8.8100e-18, dtype=torch.float64), tensor(4.1535e-17, dtype=torch.float64), tensor(0.3571, dtype=torch.float64), tensor(8.9495e-17, dtype=torch.float64), tensor(1.1083e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.2742e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.1709e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.643
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.357
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.170938346284339e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  1.170938346284339e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 275.24it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 459.11it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 535.23it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 581.56it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 614.77it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 639.14it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 626.37it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.6826, 'grad_norm': 0.3565434217453003, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2925288677215576, 'eval_runtime': 5.736, 'eval_samples_per_second': 174.164, 'eval_steps_per_second': 10.983, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 276.56it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 459.77it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 535.95it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 582.39it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 614.67it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 639.76it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 627.06it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.0873, 'grad_norm': 0.3209373354911804, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.2504284381866455, 'eval_runtime': 4.98, 'eval_samples_per_second': 200.603, 'eval_steps_per_second': 12.651, 'epoch': 0.08}
{'loss': 1.1008, 'grad_norm': 0.06539243459701538, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.010587453842163, 'eval_runtime': 4.9838, 'eval_samples_per_second': 200.448, 'eval_steps_per_second': 12.641, 'epoch': 0.12}
{'loss': 0.9702, 'grad_norm': 0.08438470214605331, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8991180062294006, 'eval_runtime': 4.9847, 'eval_samples_per_second': 200.413, 'eval_steps_per_second': 12.639, 'epoch': 0.16}
{'loss': 0.8866, 'grad_norm': 0.04449161887168884, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8805000185966492, 'eval_runtime': 4.9896, 'eval_samples_per_second': 200.218, 'eval_steps_per_second': 12.626, 'epoch': 0.2}
{'loss': 0.874, 'grad_norm': 0.052931882441043854, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8734355568885803, 'eval_runtime': 5.0006, 'eval_samples_per_second': 199.777, 'eval_steps_per_second': 12.599, 'epoch': 0.24}
{'loss': 0.8637, 'grad_norm': 0.04537162557244301, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8640835881233215, 'eval_runtime': 5.0052, 'eval_samples_per_second': 199.593, 'eval_steps_per_second': 12.587, 'epoch': 0.28}
{'loss': 0.877, 'grad_norm': 0.044941358268260956, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8572801947593689, 'eval_runtime': 5.0032, 'eval_samples_per_second': 199.674, 'eval_steps_per_second': 12.592, 'epoch': 0.32}
{'loss': 0.8582, 'grad_norm': 0.04365596920251846, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8523978590965271, 'eval_runtime': 5.0055, 'eval_samples_per_second': 199.582, 'eval_steps_per_second': 12.586, 'epoch': 0.36}
{'loss': 0.8486, 'grad_norm': 0.044709306210279465, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8453018069267273, 'eval_runtime': 5.0103, 'eval_samples_per_second': 199.39, 'eval_steps_per_second': 12.574, 'epoch': 0.4}
{'loss': 0.8678, 'grad_norm': 0.04423283785581589, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8406758904457092, 'eval_runtime': 5.0083, 'eval_samples_per_second': 199.469, 'eval_steps_per_second': 12.579, 'epoch': 0.44}
{'loss': 0.8419, 'grad_norm': 0.060930877923965454, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8377441763877869, 'eval_runtime': 5.0147, 'eval_samples_per_second': 199.214, 'eval_steps_per_second': 12.563, 'epoch': 0.48}
{'loss': 0.8546, 'grad_norm': 0.04525110498070717, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8330661058425903, 'eval_runtime': 5.0228, 'eval_samples_per_second': 198.892, 'eval_steps_per_second': 12.543, 'epoch': 0.52}
{'loss': 0.8514, 'grad_norm': 0.0398724302649498, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8304764628410339, 'eval_runtime': 5.0196, 'eval_samples_per_second': 199.02, 'eval_steps_per_second': 12.551, 'epoch': 0.56}
{'loss': 0.8386, 'grad_norm': 0.04979747533798218, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8261961936950684, 'eval_runtime': 5.0223, 'eval_samples_per_second': 198.911, 'eval_steps_per_second': 12.544, 'epoch': 0.6}
{'loss': 0.8275, 'grad_norm': 0.055972225964069366, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8235562443733215, 'eval_runtime': 5.0138, 'eval_samples_per_second': 199.25, 'eval_steps_per_second': 12.565, 'epoch': 0.64}
{'loss': 0.8362, 'grad_norm': 0.050653453916311264, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8213663697242737, 'eval_runtime': 5.0153, 'eval_samples_per_second': 199.192, 'eval_steps_per_second': 12.562, 'epoch': 0.68}
{'loss': 0.8388, 'grad_norm': 0.051291774958372116, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.818530797958374, 'eval_runtime': 5.0132, 'eval_samples_per_second': 199.274, 'eval_steps_per_second': 12.567, 'epoch': 0.72}
{'loss': 0.8367, 'grad_norm': 0.05251512676477432, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.815843403339386, 'eval_runtime': 5.0239, 'eval_samples_per_second': 198.851, 'eval_steps_per_second': 12.54, 'epoch': 0.76}
{'loss': 0.8368, 'grad_norm': 0.05384776368737221, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8146956562995911, 'eval_runtime': 5.0181, 'eval_samples_per_second': 199.08, 'eval_steps_per_second': 12.555, 'epoch': 0.8}
{'loss': 0.8325, 'grad_norm': 0.050603821873664856, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8125530481338501, 'eval_runtime': 5.0109, 'eval_samples_per_second': 199.367, 'eval_steps_per_second': 12.573, 'epoch': 0.84}
{'loss': 0.8037, 'grad_norm': 0.04953514412045479, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8109051585197449, 'eval_runtime': 5.0242, 'eval_samples_per_second': 198.837, 'eval_steps_per_second': 12.539, 'epoch': 0.88}
{'loss': 0.8347, 'grad_norm': 0.048425301909446716, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8099086880683899, 'eval_runtime': 5.0406, 'eval_samples_per_second': 198.191, 'eval_steps_per_second': 12.499, 'epoch': 0.92}
{'loss': 0.8583, 'grad_norm': 0.05272999778389931, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8091599941253662, 'eval_runtime': 5.0407, 'eval_samples_per_second': 198.186, 'eval_steps_per_second': 12.498, 'epoch': 0.96}
{'loss': 0.8044, 'grad_norm': 0.05843420699238777, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8088317513465881, 'eval_runtime': 5.0154, 'eval_samples_per_second': 199.188, 'eval_steps_per_second': 12.561, 'epoch': 1.0}
{'train_runtime': 275.6497, 'train_samples_per_second': 36.274, 'train_steps_per_second': 2.267, 'train_loss': 1.0645083099365233, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2925288677215576, 1.2504284381866455, 1.010587453842163, 0.8991180062294006, 0.8805000185966492, 0.8734355568885803, 0.8640835881233215, 0.8572801947593689, 0.8523978590965271, 0.8453018069267273, 0.8406758904457092, 0.8377441763877869, 0.8330661058425903, 0.8304764628410339, 0.8261961936950684, 0.8235562443733215, 0.8213663697242737, 0.818530797958374, 0.815843403339386, 0.8146956562995911, 0.8125530481338501, 0.8109051585197449, 0.8099086880683899, 0.8091599941253662, 0.8088317513465881], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:51,  1.42it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 118.69it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 209.38it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 279.76it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 334.03it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 378.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 308.89it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8473409414291382
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6959 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7735926753563858
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.8123809863883116
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.8094158547513418
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.7652524028546449
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.8100520364263394
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 0, 0, 128, 0.09999999999999998, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(2.3383e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.5637e-17, dtype=torch.float64), tensor(3.4128e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 1.0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999998,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.09999999999999998
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:00,  4.16it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 257.42it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 398.05it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 491.20it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 554.77it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:00<00:00, 603.48it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 529.73it/s]
Evaluation performance at step 25: 0.74
{'loss': 5.0941, 'grad_norm': 0.24104289710521698, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.8159968852996826, 'eval_runtime': 3.3361, 'eval_samples_per_second': 299.751, 'eval_steps_per_second': 18.884, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 286.76it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.20it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 558.23it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 605.40it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 638.85it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 665.26it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.01it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.4824, 'grad_norm': 0.08041682839393616, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.642435073852539, 'eval_runtime': 3.3461, 'eval_samples_per_second': 298.859, 'eval_steps_per_second': 18.828, 'epoch': 0.08}
{'loss': 1.4498, 'grad_norm': 0.16553716361522675, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3078687191009521, 'eval_runtime': 3.3429, 'eval_samples_per_second': 299.14, 'eval_steps_per_second': 18.846, 'epoch': 0.12}
{'loss': 1.2412, 'grad_norm': 0.08323246985673904, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1441890001296997, 'eval_runtime': 3.351, 'eval_samples_per_second': 298.419, 'eval_steps_per_second': 18.8, 'epoch': 0.16}
{'loss': 1.1133, 'grad_norm': 0.05671126767992973, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.081264615058899, 'eval_runtime': 3.3612, 'eval_samples_per_second': 297.517, 'eval_steps_per_second': 18.744, 'epoch': 0.2}
{'loss': 1.0709, 'grad_norm': 0.06079794093966484, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.039849877357483, 'eval_runtime': 3.363, 'eval_samples_per_second': 297.355, 'eval_steps_per_second': 18.733, 'epoch': 0.24}
{'loss': 1.0156, 'grad_norm': 0.05291574075818062, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0018057823181152, 'eval_runtime': 3.3539, 'eval_samples_per_second': 298.161, 'eval_steps_per_second': 18.784, 'epoch': 0.28}
{'loss': 0.9896, 'grad_norm': 0.06332948803901672, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9598430395126343, 'eval_runtime': 3.3566, 'eval_samples_per_second': 297.924, 'eval_steps_per_second': 18.769, 'epoch': 0.32}
{'loss': 0.9783, 'grad_norm': 0.07579689472913742, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9152436852455139, 'eval_runtime': 3.3592, 'eval_samples_per_second': 297.693, 'eval_steps_per_second': 18.755, 'epoch': 0.36}
{'loss': 0.9063, 'grad_norm': 0.09238358587026596, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8660972714424133, 'eval_runtime': 3.3616, 'eval_samples_per_second': 297.475, 'eval_steps_per_second': 18.741, 'epoch': 0.4}
{'loss': 0.8537, 'grad_norm': 0.09906543046236038, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8143588900566101, 'eval_runtime': 3.3592, 'eval_samples_per_second': 297.686, 'eval_steps_per_second': 18.754, 'epoch': 0.44}
{'loss': 0.8094, 'grad_norm': 0.10523463785648346, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7577347159385681, 'eval_runtime': 3.3613, 'eval_samples_per_second': 297.507, 'eval_steps_per_second': 18.743, 'epoch': 0.48}
{'loss': 0.766, 'grad_norm': 0.12191931158304214, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.699708878993988, 'eval_runtime': 3.3624, 'eval_samples_per_second': 297.403, 'eval_steps_per_second': 18.736, 'epoch': 0.52}
{'loss': 0.6952, 'grad_norm': 0.1504243165254593, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.611970841884613, 'eval_runtime': 3.3629, 'eval_samples_per_second': 297.367, 'eval_steps_per_second': 18.734, 'epoch': 0.56}
{'loss': 0.5832, 'grad_norm': 0.1596127152442932, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5194686651229858, 'eval_runtime': 3.3643, 'eval_samples_per_second': 297.24, 'eval_steps_per_second': 18.726, 'epoch': 0.6}
{'loss': 0.5068, 'grad_norm': 0.15192027390003204, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4648769199848175, 'eval_runtime': 3.3625, 'eval_samples_per_second': 297.401, 'eval_steps_per_second': 18.736, 'epoch': 0.64}
{'loss': 0.4896, 'grad_norm': 0.14359977841377258, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.42501184344291687, 'eval_runtime': 3.3677, 'eval_samples_per_second': 296.939, 'eval_steps_per_second': 18.707, 'epoch': 0.68}
{'loss': 0.4374, 'grad_norm': 0.126842200756073, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.38431650400161743, 'eval_runtime': 3.3776, 'eval_samples_per_second': 296.066, 'eval_steps_per_second': 18.652, 'epoch': 0.72}
{'loss': 0.3996, 'grad_norm': 0.21408914029598236, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.35257238149642944, 'eval_runtime': 3.3705, 'eval_samples_per_second': 296.695, 'eval_steps_per_second': 18.692, 'epoch': 0.76}
{'loss': 0.3748, 'grad_norm': 0.18120139837265015, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3263242840766907, 'eval_runtime': 3.3703, 'eval_samples_per_second': 296.712, 'eval_steps_per_second': 18.693, 'epoch': 0.8}
{'loss': 0.3329, 'grad_norm': 0.1519467830657959, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.30443987250328064, 'eval_runtime': 3.3797, 'eval_samples_per_second': 295.886, 'eval_steps_per_second': 18.641, 'epoch': 0.84}
{'loss': 0.3263, 'grad_norm': 0.17364919185638428, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2888832986354828, 'eval_runtime': 3.3712, 'eval_samples_per_second': 296.633, 'eval_steps_per_second': 18.688, 'epoch': 0.88}
{'loss': 0.3025, 'grad_norm': 0.1582382768392563, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.27522966265678406, 'eval_runtime': 3.3733, 'eval_samples_per_second': 296.449, 'eval_steps_per_second': 18.676, 'epoch': 0.92}
{'loss': 0.2978, 'grad_norm': 0.1646811068058014, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.266677588224411, 'eval_runtime': 3.3715, 'eval_samples_per_second': 296.605, 'eval_steps_per_second': 18.686, 'epoch': 0.96}
{'loss': 0.2812, 'grad_norm': 0.16537059843540192, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2628159821033478, 'eval_runtime': 3.3684, 'eval_samples_per_second': 296.874, 'eval_steps_per_second': 18.703, 'epoch': 1.0}
{'train_runtime': 194.6577, 'train_samples_per_second': 51.372, 'train_steps_per_second': 3.211, 'train_loss': 0.9519048759460449, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8159968852996826, 1.642435073852539, 1.3078687191009521, 1.1441890001296997, 1.081264615058899, 1.039849877357483, 1.0018057823181152, 0.9598430395126343, 0.9152436852455139, 0.8660972714424133, 0.8143588900566101, 0.7577347159385681, 0.699708878993988, 0.611970841884613, 0.5194686651229858, 0.4648769199848175, 0.42501184344291687, 0.38431650400161743, 0.35257238149642944, 0.3263242840766907, 0.30443987250328064, 0.2888832986354828, 0.27522966265678406, 0.266677588224411, 0.2628159821033478], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:58,  2.80it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 188.09it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 289.70it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 355.27it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 398.42it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 434.57it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 403.65it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8484682440757751
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3782 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.8057316313009549
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.7505895781651121
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.8078110349010237
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.8075708182715309
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.7830476454514312
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0720, dtype=torch.float64), tensor(0.1858, dtype=torch.float64), tensor(0.1225, dtype=torch.float64), 0, tensor(0.2398, dtype=torch.float64), tensor(0.1889, dtype=torch.float64), 0, tensor(0.1909, dtype=torch.float64), 32, 0, 0, 1, 1, 0, 128, 0.025506866739419206, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(5.8394e-17, dtype=torch.float64), tensor(0.0720, dtype=torch.float64), tensor(0.1858, dtype=torch.float64), tensor(0.1225, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2398, dtype=torch.float64), tensor(0.1889, dtype=torch.float64), tensor(3.2467e-17, dtype=torch.float64), tensor(0.1909, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2551, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.072
  rowan_hellaswag: 0.186
  sciq: 0.122
  triviaqa: 0
  truthfulqa_gen: 0.24
  wikitext: 0.189
  mmlu: 0
  arc_challenge: 0.191

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.025506866739419206,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.025506866739419206
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 275.78it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 458.55it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 531.17it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 578.41it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 611.21it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 636.44it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 623.77it/s]
Evaluation performance at step 25: 0.75
{'loss': 3.9693, 'grad_norm': 0.2773960530757904, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.1315557956695557, 'eval_runtime': 9.8012, 'eval_samples_per_second': 101.927, 'eval_steps_per_second': 6.428, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 275.64it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 458.76it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 534.53it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 580.65it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 611.83it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 637.45it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 624.99it/s]
Evaluation performance at step 50: 0.77
{'loss': 2.4301, 'grad_norm': 0.1921757012605667, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 1.9426841735839844, 'eval_runtime': 9.8089, 'eval_samples_per_second': 101.847, 'eval_steps_per_second': 6.423, 'epoch': 0.08}
{'loss': 1.7636, 'grad_norm': 0.08760659396648407, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5810067653656006, 'eval_runtime': 9.8553, 'eval_samples_per_second': 101.366, 'eval_steps_per_second': 6.392, 'epoch': 0.12}
{'loss': 1.5686, 'grad_norm': 0.058974143117666245, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4747586250305176, 'eval_runtime': 9.8985, 'eval_samples_per_second': 100.924, 'eval_steps_per_second': 6.365, 'epoch': 0.16}
{'loss': 1.4689, 'grad_norm': 0.062172550708055496, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4073600769042969, 'eval_runtime': 9.9109, 'eval_samples_per_second': 100.798, 'eval_steps_per_second': 6.357, 'epoch': 0.2}
{'loss': 1.407, 'grad_norm': 0.0594191700220108, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3769949674606323, 'eval_runtime': 9.9085, 'eval_samples_per_second': 100.823, 'eval_steps_per_second': 6.358, 'epoch': 0.24}
{'loss': 1.3471, 'grad_norm': 0.052140817046165466, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3515812158584595, 'eval_runtime': 9.9143, 'eval_samples_per_second': 100.764, 'eval_steps_per_second': 6.354, 'epoch': 0.28}
{'loss': 1.3972, 'grad_norm': 0.057122886180877686, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3359482288360596, 'eval_runtime': 9.9164, 'eval_samples_per_second': 100.742, 'eval_steps_per_second': 6.353, 'epoch': 0.32}
{'loss': 1.3777, 'grad_norm': 0.05341213941574097, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3245881795883179, 'eval_runtime': 9.9142, 'eval_samples_per_second': 100.764, 'eval_steps_per_second': 6.355, 'epoch': 0.36}
{'loss': 1.3021, 'grad_norm': 0.0478735975921154, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.318205714225769, 'eval_runtime': 9.9205, 'eval_samples_per_second': 100.7, 'eval_steps_per_second': 6.35, 'epoch': 0.4}
{'loss': 1.2986, 'grad_norm': 0.06288550049066544, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.310804009437561, 'eval_runtime': 9.921, 'eval_samples_per_second': 100.696, 'eval_steps_per_second': 6.35, 'epoch': 0.44}
{'loss': 1.3689, 'grad_norm': 0.06197550892829895, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3035030364990234, 'eval_runtime': 9.918, 'eval_samples_per_second': 100.726, 'eval_steps_per_second': 6.352, 'epoch': 0.48}
{'loss': 1.366, 'grad_norm': 0.060860633850097656, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3001304864883423, 'eval_runtime': 9.9329, 'eval_samples_per_second': 100.575, 'eval_steps_per_second': 6.343, 'epoch': 0.52}
{'loss': 1.3457, 'grad_norm': 0.05065355822443962, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2942075729370117, 'eval_runtime': 9.9066, 'eval_samples_per_second': 100.842, 'eval_steps_per_second': 6.359, 'epoch': 0.56}
{'loss': 1.3323, 'grad_norm': 0.06672517210245132, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2901716232299805, 'eval_runtime': 9.9111, 'eval_samples_per_second': 100.797, 'eval_steps_per_second': 6.357, 'epoch': 0.6}
{'loss': 1.4202, 'grad_norm': 0.05435436964035034, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2861765623092651, 'eval_runtime': 9.9116, 'eval_samples_per_second': 100.791, 'eval_steps_per_second': 6.356, 'epoch': 0.64}
{'loss': 1.2674, 'grad_norm': 0.06121683493256569, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2823193073272705, 'eval_runtime': 9.8903, 'eval_samples_per_second': 101.008, 'eval_steps_per_second': 6.37, 'epoch': 0.68}
{'loss': 1.3042, 'grad_norm': 0.05841879919171333, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2781994342803955, 'eval_runtime': 9.8934, 'eval_samples_per_second': 100.976, 'eval_steps_per_second': 6.368, 'epoch': 0.72}
{'loss': 1.3036, 'grad_norm': 0.0621439665555954, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2759485244750977, 'eval_runtime': 9.8851, 'eval_samples_per_second': 101.061, 'eval_steps_per_second': 6.373, 'epoch': 0.76}
{'loss': 1.2641, 'grad_norm': 0.059854790568351746, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.273032546043396, 'eval_runtime': 9.8944, 'eval_samples_per_second': 100.966, 'eval_steps_per_second': 6.367, 'epoch': 0.8}
{'loss': 1.3113, 'grad_norm': 0.06647256016731262, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2706537246704102, 'eval_runtime': 9.8978, 'eval_samples_per_second': 100.932, 'eval_steps_per_second': 6.365, 'epoch': 0.84}
{'loss': 1.3166, 'grad_norm': 0.056203752756118774, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2688664197921753, 'eval_runtime': 9.9017, 'eval_samples_per_second': 100.891, 'eval_steps_per_second': 6.363, 'epoch': 0.88}
{'loss': 1.2804, 'grad_norm': 0.05100227892398834, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2672439813613892, 'eval_runtime': 9.9002, 'eval_samples_per_second': 100.907, 'eval_steps_per_second': 6.364, 'epoch': 0.92}
{'loss': 1.2359, 'grad_norm': 0.06138085946440697, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2671027183532715, 'eval_runtime': 9.9008, 'eval_samples_per_second': 100.901, 'eval_steps_per_second': 6.363, 'epoch': 0.96}
{'loss': 1.3494, 'grad_norm': 0.059285301715135574, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2664541006088257, 'eval_runtime': 9.9082, 'eval_samples_per_second': 100.825, 'eval_steps_per_second': 6.358, 'epoch': 1.0}
{'train_runtime': 503.0176, 'train_samples_per_second': 19.874, 'train_steps_per_second': 1.243, 'train_loss': 1.5118493255615235, 'epoch': 1.0}
train_results:  {'eval_loss': [3.1315557956695557, 1.9426841735839844, 1.5810067653656006, 1.4747586250305176, 1.4073600769042969, 1.3769949674606323, 1.3515812158584595, 1.3359482288360596, 1.3245881795883179, 1.318205714225769, 1.310804009437561, 1.3035030364990234, 1.3001304864883423, 1.2942075729370117, 1.2901716232299805, 1.2861765623092651, 1.2823193073272705, 1.2781994342803955, 1.2759485244750977, 1.273032546043396, 1.2706537246704102, 1.2688664197921753, 1.2672439813613892, 1.2671027183532715, 1.2664541006088257], 'performance': [0.75, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:43,  3.06it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 194.14it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 291.55it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 351.28it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 391.54it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 423.90it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 402.23it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.8478282690048218
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7286 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.8066786640403499
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.8067916628933304
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.8084043310817636
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.8049633609533045
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.8067978310981972
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0340, dtype=torch.float64), 0, tensor(0.6947, dtype=torch.float64), 0, tensor(0.2712, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 1, 0, 0, 128, 0.0, 1.4800000190734899, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0340, dtype=torch.float64), tensor(4.8084e-17, dtype=torch.float64), tensor(0.6947, dtype=torch.float64), tensor(1.8118e-16, dtype=torch.float64), tensor(0.2712, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9777e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.034
  rowan_hellaswag: 0
  sciq: 0.695
  triviaqa: 0
  truthfulqa_gen: 0.271
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734899,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734899
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 300.09it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 500.24it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 582.65it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 632.90it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 668.18it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 695.22it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 681.29it/s]
Evaluation performance at step 25: 0.74
{'loss': 5.0709, 'grad_norm': 0.23794065415859222, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.835857391357422, 'eval_runtime': 6.5838, 'eval_samples_per_second': 151.736, 'eval_steps_per_second': 9.569, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 299.95it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 500.00it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 582.56it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 632.93it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 666.92it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 693.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 680.66it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.5086, 'grad_norm': 0.10242795944213867, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.7076102495193481, 'eval_runtime': 4.9657, 'eval_samples_per_second': 201.182, 'eval_steps_per_second': 12.687, 'epoch': 0.08}
{'loss': 1.5353, 'grad_norm': 0.16771626472473145, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.405447244644165, 'eval_runtime': 4.9832, 'eval_samples_per_second': 200.473, 'eval_steps_per_second': 12.642, 'epoch': 0.12}
{'loss': 1.3043, 'grad_norm': 0.08356615900993347, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2509428262710571, 'eval_runtime': 4.9863, 'eval_samples_per_second': 200.348, 'eval_steps_per_second': 12.635, 'epoch': 0.16}
{'loss': 1.2177, 'grad_norm': 0.06173913553357124, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1989229917526245, 'eval_runtime': 4.9965, 'eval_samples_per_second': 199.94, 'eval_steps_per_second': 12.609, 'epoch': 0.2}
{'loss': 1.1843, 'grad_norm': 0.05487858131527901, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1699625253677368, 'eval_runtime': 5.0039, 'eval_samples_per_second': 199.644, 'eval_steps_per_second': 12.59, 'epoch': 0.24}
{'loss': 1.1541, 'grad_norm': 0.047937121242284775, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.142643928527832, 'eval_runtime': 5.0075, 'eval_samples_per_second': 199.501, 'eval_steps_per_second': 12.581, 'epoch': 0.28}
{'loss': 1.1307, 'grad_norm': 0.046628437936306, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1205133199691772, 'eval_runtime': 5.016, 'eval_samples_per_second': 199.165, 'eval_steps_per_second': 12.56, 'epoch': 0.32}
{'loss': 1.1196, 'grad_norm': 0.047372739762067795, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1003224849700928, 'eval_runtime': 5.0171, 'eval_samples_per_second': 199.12, 'eval_steps_per_second': 12.557, 'epoch': 0.36}
{'loss': 1.1016, 'grad_norm': 0.05151832848787308, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0815280675888062, 'eval_runtime': 5.0234, 'eval_samples_per_second': 198.87, 'eval_steps_per_second': 12.541, 'epoch': 0.4}
{'loss': 1.0686, 'grad_norm': 0.04826251044869423, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0575740337371826, 'eval_runtime': 5.0176, 'eval_samples_per_second': 199.099, 'eval_steps_per_second': 12.556, 'epoch': 0.44}
{'loss': 1.0425, 'grad_norm': 0.05808570608496666, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0312429666519165, 'eval_runtime': 5.0236, 'eval_samples_per_second': 198.86, 'eval_steps_per_second': 12.541, 'epoch': 0.48}
{'loss': 1.0133, 'grad_norm': 0.0784897580742836, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9946045875549316, 'eval_runtime': 5.0283, 'eval_samples_per_second': 198.674, 'eval_steps_per_second': 12.529, 'epoch': 0.52}
{'loss': 0.9764, 'grad_norm': 0.1102631464600563, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9361094236373901, 'eval_runtime': 5.0316, 'eval_samples_per_second': 198.544, 'eval_steps_per_second': 12.521, 'epoch': 0.56}
{'loss': 0.9304, 'grad_norm': 0.049927663058042526, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8847872614860535, 'eval_runtime': 5.0301, 'eval_samples_per_second': 198.605, 'eval_steps_per_second': 12.525, 'epoch': 0.6}
{'loss': 0.8826, 'grad_norm': 0.05294639244675636, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8685614466667175, 'eval_runtime': 5.0361, 'eval_samples_per_second': 198.367, 'eval_steps_per_second': 12.51, 'epoch': 0.64}
{'loss': 0.8772, 'grad_norm': 0.05088130384683609, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.852961540222168, 'eval_runtime': 5.033, 'eval_samples_per_second': 198.489, 'eval_steps_per_second': 12.517, 'epoch': 0.68}
{'loss': 0.8433, 'grad_norm': 0.06076274439692497, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8344681262969971, 'eval_runtime': 5.0304, 'eval_samples_per_second': 198.591, 'eval_steps_per_second': 12.524, 'epoch': 0.72}
{'loss': 0.859, 'grad_norm': 0.05741320922970772, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8182505965232849, 'eval_runtime': 5.0409, 'eval_samples_per_second': 198.179, 'eval_steps_per_second': 12.498, 'epoch': 0.76}
{'loss': 0.8118, 'grad_norm': 0.06673955917358398, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8026801347732544, 'eval_runtime': 5.0339, 'eval_samples_per_second': 198.456, 'eval_steps_per_second': 12.515, 'epoch': 0.8}
{'loss': 0.7909, 'grad_norm': 0.06621398776769638, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7869705557823181, 'eval_runtime': 5.0378, 'eval_samples_per_second': 198.3, 'eval_steps_per_second': 12.505, 'epoch': 0.84}
{'loss': 0.797, 'grad_norm': 0.061314865946769714, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7746359705924988, 'eval_runtime': 5.0304, 'eval_samples_per_second': 198.593, 'eval_steps_per_second': 12.524, 'epoch': 0.88}
{'loss': 0.7855, 'grad_norm': 0.0636228546500206, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7652976512908936, 'eval_runtime': 5.0317, 'eval_samples_per_second': 198.542, 'eval_steps_per_second': 12.521, 'epoch': 0.92}
{'loss': 0.749, 'grad_norm': 0.06955009698867798, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7604605555534363, 'eval_runtime': 5.0286, 'eval_samples_per_second': 198.662, 'eval_steps_per_second': 12.528, 'epoch': 0.96}
{'loss': 0.7785, 'grad_norm': 0.05969469994306564, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.758302628993988, 'eval_runtime': 5.0384, 'eval_samples_per_second': 198.279, 'eval_steps_per_second': 12.504, 'epoch': 1.0}
{'train_runtime': 264.4713, 'train_samples_per_second': 37.808, 'train_steps_per_second': 2.363, 'train_loss': 1.2213173706054687, 'epoch': 1.0}
train_results:  {'eval_loss': [3.835857391357422, 1.7076102495193481, 1.405447244644165, 1.2509428262710571, 1.1989229917526245, 1.1699625253677368, 1.142643928527832, 1.1205133199691772, 1.1003224849700928, 1.0815280675888062, 1.0575740337371826, 1.0312429666519165, 0.9946045875549316, 0.9361094236373901, 0.8847872614860535, 0.8685614466667175, 0.852961540222168, 0.8344681262969971, 0.8182505965232849, 0.8026801347732544, 0.7869705557823181, 0.7746359705924988, 0.7652976512908936, 0.7604605555534363, 0.758302628993988], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<02:23,  3.48it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 215.94it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 321.17it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 384.20it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 425.63it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 459.43it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 439.63it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8484796285629272
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9776 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.796580975941487
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.7965443324430527
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.8026276235943814
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.8042236145813679
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.7956761243499667
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0387, dtype=torch.float64), tensor(0.2054, dtype=torch.float64), tensor(0.2323, dtype=torch.float64), 0, tensor(0.2578, dtype=torch.float64), tensor(0.2468, dtype=torch.float64), 0, tensor(0.0190, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 115, 0.058430151292666745, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.7116e-17, dtype=torch.float64), tensor(0.0387, dtype=torch.float64), tensor(0.2054, dtype=torch.float64), tensor(0.2323, dtype=torch.float64), tensor(1.9017e-16, dtype=torch.float64), tensor(0.2578, dtype=torch.float64), tensor(0.2468, dtype=torch.float64), tensor(1.8042e-16, dtype=torch.float64), tensor(0.0190, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9013, dtype=torch.float64), tensor(0.5843, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.039
  rowan_hellaswag: 0.205
  sciq: 0.232
  triviaqa: 0
  truthfulqa_gen: 0.258
  wikitext: 0.247
  mmlu: 0
  arc_challenge: 0.019

LoRA Parameters:
  lora_r: (115,)
  lora_dropout: (0.058430151292666745,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  115
lora dropout:  0.058430151292666745
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 165,806,080 || all params: 8,196,067,328 || trainable%: 2.0230
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 243.04it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 404.98it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 471.44it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 512.23it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 540.53it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 561.72it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 551.25it/s]
Evaluation performance at step 25: 0.73
{'loss': 4.1896, 'grad_norm': 0.21123622357845306, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.73}
{'eval_loss': 3.2824370861053467, 'eval_runtime': 10.9297, 'eval_samples_per_second': 91.403, 'eval_steps_per_second': 5.764, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 241.68it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 402.14it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 469.79it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 510.92it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 539.49it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 560.83it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 549.78it/s]
Evaluation performance at step 50: 0.76
{'loss': 2.5848, 'grad_norm': 0.2517700791358948, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 2.052189826965332, 'eval_runtime': 10.9731, 'eval_samples_per_second': 91.041, 'eval_steps_per_second': 5.741, 'epoch': 0.08}
{'loss': 1.9109, 'grad_norm': 0.10930001735687256, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.730147361755371, 'eval_runtime': 10.9791, 'eval_samples_per_second': 90.991, 'eval_steps_per_second': 5.738, 'epoch': 0.12}
{'loss': 1.7212, 'grad_norm': 0.05654001608490944, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6146094799041748, 'eval_runtime': 11.0102, 'eval_samples_per_second': 90.734, 'eval_steps_per_second': 5.722, 'epoch': 0.16}
{'loss': 1.6287, 'grad_norm': 0.07968434691429138, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5358163118362427, 'eval_runtime': 11.0123, 'eval_samples_per_second': 90.717, 'eval_steps_per_second': 5.721, 'epoch': 0.2}
{'loss': 1.5444, 'grad_norm': 0.06301060318946838, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.49739408493042, 'eval_runtime': 10.9961, 'eval_samples_per_second': 90.85, 'eval_steps_per_second': 5.729, 'epoch': 0.24}
{'loss': 1.4983, 'grad_norm': 0.054991934448480606, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4759440422058105, 'eval_runtime': 11.0206, 'eval_samples_per_second': 90.649, 'eval_steps_per_second': 5.717, 'epoch': 0.28}
{'loss': 1.4518, 'grad_norm': 0.06387181580066681, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4615455865859985, 'eval_runtime': 11.0026, 'eval_samples_per_second': 90.797, 'eval_steps_per_second': 5.726, 'epoch': 0.32}
{'loss': 1.5539, 'grad_norm': 0.05218551680445671, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4498097896575928, 'eval_runtime': 11.0056, 'eval_samples_per_second': 90.772, 'eval_steps_per_second': 5.724, 'epoch': 0.36}
{'loss': 1.4823, 'grad_norm': 0.054698605090379715, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4406946897506714, 'eval_runtime': 10.9946, 'eval_samples_per_second': 90.863, 'eval_steps_per_second': 5.73, 'epoch': 0.4}
{'loss': 1.4485, 'grad_norm': 0.10932574421167374, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4353679418563843, 'eval_runtime': 10.9832, 'eval_samples_per_second': 90.957, 'eval_steps_per_second': 5.736, 'epoch': 0.44}
{'loss': 1.5048, 'grad_norm': 0.05542200803756714, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.426922082901001, 'eval_runtime': 11.0, 'eval_samples_per_second': 90.818, 'eval_steps_per_second': 5.727, 'epoch': 0.48}
{'loss': 1.5026, 'grad_norm': 0.06227840483188629, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4212613105773926, 'eval_runtime': 10.9978, 'eval_samples_per_second': 90.836, 'eval_steps_per_second': 5.728, 'epoch': 0.52}
{'loss': 1.4128, 'grad_norm': 0.07955779135227203, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.419559359550476, 'eval_runtime': 10.9972, 'eval_samples_per_second': 90.841, 'eval_steps_per_second': 5.729, 'epoch': 0.56}
{'loss': 1.4531, 'grad_norm': 0.06752220541238785, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.414207935333252, 'eval_runtime': 11.0013, 'eval_samples_per_second': 90.807, 'eval_steps_per_second': 5.727, 'epoch': 0.6}
{'loss': 1.4278, 'grad_norm': 0.053729988634586334, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.409936547279358, 'eval_runtime': 10.9871, 'eval_samples_per_second': 90.925, 'eval_steps_per_second': 5.734, 'epoch': 0.64}
{'loss': 1.4405, 'grad_norm': 0.058742716908454895, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4059081077575684, 'eval_runtime': 10.997, 'eval_samples_per_second': 90.843, 'eval_steps_per_second': 5.729, 'epoch': 0.68}
{'loss': 1.4533, 'grad_norm': 0.056138087064027786, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4037076234817505, 'eval_runtime': 10.9976, 'eval_samples_per_second': 90.838, 'eval_steps_per_second': 5.729, 'epoch': 0.72}
{'loss': 1.4034, 'grad_norm': 0.07051947712898254, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.400498390197754, 'eval_runtime': 10.9948, 'eval_samples_per_second': 90.861, 'eval_steps_per_second': 5.73, 'epoch': 0.76}
{'loss': 1.3987, 'grad_norm': 0.06311879307031631, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3975547552108765, 'eval_runtime': 10.989, 'eval_samples_per_second': 90.909, 'eval_steps_per_second': 5.733, 'epoch': 0.8}
{'loss': 1.4235, 'grad_norm': 0.07293086498975754, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.393867015838623, 'eval_runtime': 10.9867, 'eval_samples_per_second': 90.928, 'eval_steps_per_second': 5.734, 'epoch': 0.84}
{'loss': 1.4031, 'grad_norm': 0.10058806091547012, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.393004059791565, 'eval_runtime': 11.0029, 'eval_samples_per_second': 90.794, 'eval_steps_per_second': 5.726, 'epoch': 0.88}
{'loss': 1.3948, 'grad_norm': 0.05889247730374336, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3913027048110962, 'eval_runtime': 11.0048, 'eval_samples_per_second': 90.779, 'eval_steps_per_second': 5.725, 'epoch': 0.92}
{'loss': 1.4012, 'grad_norm': 0.06408017873764038, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3909969329833984, 'eval_runtime': 10.9773, 'eval_samples_per_second': 91.006, 'eval_steps_per_second': 5.739, 'epoch': 0.96}
{'loss': 1.4715, 'grad_norm': 0.062574103474617, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3901968002319336, 'eval_runtime': 11.0181, 'eval_samples_per_second': 90.669, 'eval_steps_per_second': 5.718, 'epoch': 1.0}
{'train_runtime': 565.5413, 'train_samples_per_second': 17.677, 'train_steps_per_second': 1.105, 'train_loss': 1.644220751953125, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2824370861053467, 2.052189826965332, 1.730147361755371, 1.6146094799041748, 1.5358163118362427, 1.49739408493042, 1.4759440422058105, 1.4615455865859985, 1.4498097896575928, 1.4406946897506714, 1.4353679418563843, 1.426922082901001, 1.4212613105773926, 1.419559359550476, 1.414207935333252, 1.409936547279358, 1.4059081077575684, 1.4037076234817505, 1.400498390197754, 1.3975547552108765, 1.393867015838623, 1.393004059791565, 1.3913027048110962, 1.3909969329833984, 1.3901968002319336], 'performance': [0.73, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:58,  4.22it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 218.33it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 299.42it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 343.46it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 371.50it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 395.73it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 398.93it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.73, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.8390657901763916
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6925 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.8067620519755675
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.8077818056836207
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.7898882512932722
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.8082885726751975
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.8082065392999495
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.6622, dtype=torch.float64), tensor(0.0487, dtype=torch.float64), 0, 0, 0, tensor(0.2891, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 1.480000019073488, 1]
normalized proposed parameters for next round by BO: [tensor(0.6622, dtype=torch.float64), tensor(0.0487, dtype=torch.float64), tensor(6.2000e-17, dtype=torch.float64), tensor(4.1590e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2891, dtype=torch.float64), tensor(1.2692e-17, dtype=torch.float64), tensor(1.0107e-17, dtype=torch.float64), tensor(3.0924e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.662
  gsm8k: 0.049
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.289
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.05it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 431.03it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 502.12it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 545.66it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 576.02it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 598.67it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 586.62it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.3857, 'grad_norm': 0.28233617544174194, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.0460329055786133, 'eval_runtime': 7.0043, 'eval_samples_per_second': 142.627, 'eval_steps_per_second': 8.994, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.73it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 429.58it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 500.70it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 543.84it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 573.43it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 596.24it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 584.62it/s]
Evaluation performance at step 50: 0.73
{'loss': 1.9395, 'grad_norm': 0.23634862899780273, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.2656855583190918, 'eval_runtime': 7.0123, 'eval_samples_per_second': 142.464, 'eval_steps_per_second': 8.984, 'epoch': 0.08}
{'loss': 1.1567, 'grad_norm': 0.06532923132181168, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0331144332885742, 'eval_runtime': 7.0257, 'eval_samples_per_second': 142.192, 'eval_steps_per_second': 8.967, 'epoch': 0.12}
{'loss': 0.9732, 'grad_norm': 0.07898802310228348, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9276670217514038, 'eval_runtime': 7.0301, 'eval_samples_per_second': 142.103, 'eval_steps_per_second': 8.961, 'epoch': 0.16}
{'loss': 0.9303, 'grad_norm': 0.03896615281701088, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9008628726005554, 'eval_runtime': 7.0486, 'eval_samples_per_second': 141.731, 'eval_steps_per_second': 8.938, 'epoch': 0.2}
{'loss': 0.8873, 'grad_norm': 0.04735128581523895, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.885485827922821, 'eval_runtime': 7.0556, 'eval_samples_per_second': 141.589, 'eval_steps_per_second': 8.929, 'epoch': 0.24}
{'loss': 0.883, 'grad_norm': 0.049085188657045364, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8736670017242432, 'eval_runtime': 7.0555, 'eval_samples_per_second': 141.592, 'eval_steps_per_second': 8.929, 'epoch': 0.28}
{'loss': 0.8608, 'grad_norm': 0.05090399831533432, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8662458658218384, 'eval_runtime': 7.0633, 'eval_samples_per_second': 141.436, 'eval_steps_per_second': 8.919, 'epoch': 0.32}
{'loss': 0.875, 'grad_norm': 0.04750628024339676, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8567020893096924, 'eval_runtime': 7.0644, 'eval_samples_per_second': 141.413, 'eval_steps_per_second': 8.918, 'epoch': 0.36}
{'loss': 0.8751, 'grad_norm': 0.04587544873356819, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8502093553543091, 'eval_runtime': 7.0737, 'eval_samples_per_second': 141.226, 'eval_steps_per_second': 8.906, 'epoch': 0.4}
{'loss': 0.8661, 'grad_norm': 0.05282650142908096, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8444770574569702, 'eval_runtime': 7.0671, 'eval_samples_per_second': 141.359, 'eval_steps_per_second': 8.915, 'epoch': 0.44}
{'loss': 0.8595, 'grad_norm': 0.05176571011543274, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8385599255561829, 'eval_runtime': 7.0619, 'eval_samples_per_second': 141.463, 'eval_steps_per_second': 8.921, 'epoch': 0.48}
{'loss': 0.8631, 'grad_norm': 0.05012093111872673, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8337474465370178, 'eval_runtime': 7.0709, 'eval_samples_per_second': 141.283, 'eval_steps_per_second': 8.91, 'epoch': 0.52}
{'loss': 0.832, 'grad_norm': 0.049964215606451035, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8299756050109863, 'eval_runtime': 7.0676, 'eval_samples_per_second': 141.348, 'eval_steps_per_second': 8.914, 'epoch': 0.56}
{'loss': 0.8507, 'grad_norm': 0.0622587613761425, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.825782835483551, 'eval_runtime': 7.064, 'eval_samples_per_second': 141.421, 'eval_steps_per_second': 8.918, 'epoch': 0.6}
{'loss': 0.8185, 'grad_norm': 0.05581294000148773, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.821739912033081, 'eval_runtime': 7.0687, 'eval_samples_per_second': 141.328, 'eval_steps_per_second': 8.913, 'epoch': 0.64}
{'loss': 0.836, 'grad_norm': 0.055152833461761475, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8175308108329773, 'eval_runtime': 7.0625, 'eval_samples_per_second': 141.451, 'eval_steps_per_second': 8.92, 'epoch': 0.68}
{'loss': 0.8235, 'grad_norm': 0.05886996537446976, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8134300708770752, 'eval_runtime': 7.057, 'eval_samples_per_second': 141.562, 'eval_steps_per_second': 8.927, 'epoch': 0.72}
{'loss': 0.8332, 'grad_norm': 0.05871307849884033, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8100538849830627, 'eval_runtime': 7.0808, 'eval_samples_per_second': 141.085, 'eval_steps_per_second': 8.897, 'epoch': 0.76}
{'loss': 0.8318, 'grad_norm': 0.060716692358255386, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8077338933944702, 'eval_runtime': 7.0503, 'eval_samples_per_second': 141.696, 'eval_steps_per_second': 8.936, 'epoch': 0.8}
{'loss': 0.8192, 'grad_norm': 0.0561615489423275, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8050552606582642, 'eval_runtime': 7.051, 'eval_samples_per_second': 141.682, 'eval_steps_per_second': 8.935, 'epoch': 0.84}
{'loss': 0.8103, 'grad_norm': 0.06340789049863815, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8026652336120605, 'eval_runtime': 7.0569, 'eval_samples_per_second': 141.564, 'eval_steps_per_second': 8.927, 'epoch': 0.88}
{'loss': 0.8112, 'grad_norm': 0.0700613483786583, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8010024428367615, 'eval_runtime': 7.0599, 'eval_samples_per_second': 141.504, 'eval_steps_per_second': 8.924, 'epoch': 0.92}
{'loss': 0.8144, 'grad_norm': 0.059050075709819794, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7998215556144714, 'eval_runtime': 7.0574, 'eval_samples_per_second': 141.554, 'eval_steps_per_second': 8.927, 'epoch': 0.96}
{'loss': 0.8246, 'grad_norm': 0.06786854565143585, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7992215156555176, 'eval_runtime': 7.0668, 'eval_samples_per_second': 141.366, 'eval_steps_per_second': 8.915, 'epoch': 1.0}
{'train_runtime': 381.4835, 'train_samples_per_second': 26.208, 'train_steps_per_second': 1.638, 'train_loss': 1.0504260467529296, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0460329055786133, 1.2656855583190918, 1.0331144332885742, 0.9276670217514038, 0.9008628726005554, 0.885485827922821, 0.8736670017242432, 0.8662458658218384, 0.8567020893096924, 0.8502093553543091, 0.8444770574569702, 0.8385599255561829, 0.8337474465370178, 0.8299756050109863, 0.825782835483551, 0.821739912033081, 0.8175308108329773, 0.8134300708770752, 0.8100538849830627, 0.8077338933944702, 0.8050552606582642, 0.8026652336120605, 0.8010024428367615, 0.7998215556144714, 0.7992215156555176], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:51,  4.47it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 231.73it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 318.12it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 365.45it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 395.04it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 420.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 423.15it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8473272919654846
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5312 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.800327366347491
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.8043197100784171
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.7650290283580099
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.8038985188046659
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.7980082797033214
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0231, dtype=torch.float64), tensor(0.4259, dtype=torch.float64), 0, 0, tensor(0.0593, dtype=torch.float64), 0, 0, tensor(0.4917, dtype=torch.float64), 32, 0, 0, 1, 0, 0, 128, 0.023802118985806153, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0231, dtype=torch.float64), tensor(0.4259, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3275e-16, dtype=torch.float64), tensor(0.0593, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4917, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2380, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.023
  rowan_hellaswag: 0.426
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.059
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.492

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.023802118985806153,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.023802118985806153
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 299.35it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 498.78it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 580.25it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 629.40it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 665.90it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 692.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 678.40it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.0272, 'grad_norm': 0.0974169671535492, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 3.4876110553741455, 'eval_runtime': 9.3944, 'eval_samples_per_second': 106.34, 'eval_steps_per_second': 6.706, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 299.93it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 497.40it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 577.66it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 628.70it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 663.89it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 690.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 677.00it/s]
Evaluation performance at step 50: 0.75
{'loss': 2.6909, 'grad_norm': 0.08926901966333389, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.75}
{'eval_loss': 2.1283280849456787, 'eval_runtime': 9.3985, 'eval_samples_per_second': 106.294, 'eval_steps_per_second': 6.703, 'epoch': 0.08}
{'loss': 1.8894, 'grad_norm': 0.0728742703795433, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7606730461120605, 'eval_runtime': 9.4417, 'eval_samples_per_second': 105.808, 'eval_steps_per_second': 6.673, 'epoch': 0.12}
{'loss': 1.5929, 'grad_norm': 0.06350161135196686, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6545982360839844, 'eval_runtime': 9.4969, 'eval_samples_per_second': 105.192, 'eval_steps_per_second': 6.634, 'epoch': 0.16}
{'loss': 1.5903, 'grad_norm': 0.04808662459254265, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6023365259170532, 'eval_runtime': 9.5054, 'eval_samples_per_second': 105.098, 'eval_steps_per_second': 6.628, 'epoch': 0.2}
{'loss': 1.5549, 'grad_norm': 0.04698367044329643, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5779187679290771, 'eval_runtime': 9.4999, 'eval_samples_per_second': 105.159, 'eval_steps_per_second': 6.632, 'epoch': 0.24}
{'loss': 1.5285, 'grad_norm': 0.03949058800935745, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.5594213008880615, 'eval_runtime': 9.5039, 'eval_samples_per_second': 105.115, 'eval_steps_per_second': 6.629, 'epoch': 0.28}
{'loss': 1.5728, 'grad_norm': 0.04023858532309532, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5461602210998535, 'eval_runtime': 9.5108, 'eval_samples_per_second': 105.039, 'eval_steps_per_second': 6.624, 'epoch': 0.32}
{'loss': 1.4989, 'grad_norm': 0.03542289510369301, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5332456827163696, 'eval_runtime': 9.5071, 'eval_samples_per_second': 105.079, 'eval_steps_per_second': 6.627, 'epoch': 0.36}
{'loss': 1.5352, 'grad_norm': 0.04229547083377838, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5244245529174805, 'eval_runtime': 9.5058, 'eval_samples_per_second': 105.094, 'eval_steps_per_second': 6.628, 'epoch': 0.4}
{'loss': 1.4804, 'grad_norm': 0.038779135793447495, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5154513120651245, 'eval_runtime': 9.5035, 'eval_samples_per_second': 105.119, 'eval_steps_per_second': 6.629, 'epoch': 0.44}
{'loss': 1.5175, 'grad_norm': 0.039634477347135544, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5072484016418457, 'eval_runtime': 9.5082, 'eval_samples_per_second': 105.068, 'eval_steps_per_second': 6.626, 'epoch': 0.48}
{'loss': 1.4884, 'grad_norm': 0.042946383357048035, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.498245358467102, 'eval_runtime': 9.524, 'eval_samples_per_second': 104.893, 'eval_steps_per_second': 6.615, 'epoch': 0.52}
{'loss': 1.4336, 'grad_norm': 0.04442772641777992, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4892610311508179, 'eval_runtime': 9.5181, 'eval_samples_per_second': 104.958, 'eval_steps_per_second': 6.619, 'epoch': 0.56}
{'loss': 1.5132, 'grad_norm': 0.0427863672375679, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4802192449569702, 'eval_runtime': 9.5147, 'eval_samples_per_second': 104.995, 'eval_steps_per_second': 6.621, 'epoch': 0.6}
{'loss': 1.461, 'grad_norm': 0.04906439781188965, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.470636010169983, 'eval_runtime': 9.5081, 'eval_samples_per_second': 105.068, 'eval_steps_per_second': 6.626, 'epoch': 0.64}
{'loss': 1.4048, 'grad_norm': 0.049326181411743164, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4599144458770752, 'eval_runtime': 9.5101, 'eval_samples_per_second': 105.046, 'eval_steps_per_second': 6.625, 'epoch': 0.68}
{'loss': 1.4127, 'grad_norm': 0.05149475857615471, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.446565866470337, 'eval_runtime': 9.5093, 'eval_samples_per_second': 105.055, 'eval_steps_per_second': 6.625, 'epoch': 0.72}
{'loss': 1.4552, 'grad_norm': 0.05157956853508949, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4328070878982544, 'eval_runtime': 9.5039, 'eval_samples_per_second': 105.115, 'eval_steps_per_second': 6.629, 'epoch': 0.76}
{'loss': 1.4526, 'grad_norm': 0.05097685381770134, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4191102981567383, 'eval_runtime': 9.5057, 'eval_samples_per_second': 105.095, 'eval_steps_per_second': 6.628, 'epoch': 0.8}
{'loss': 1.384, 'grad_norm': 0.044368382543325424, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.412179946899414, 'eval_runtime': 9.5211, 'eval_samples_per_second': 104.925, 'eval_steps_per_second': 6.617, 'epoch': 0.84}
{'loss': 1.3842, 'grad_norm': 0.048631515353918076, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4079911708831787, 'eval_runtime': 9.5345, 'eval_samples_per_second': 104.778, 'eval_steps_per_second': 6.608, 'epoch': 0.88}
{'loss': 1.3655, 'grad_norm': 0.044361867010593414, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.405413031578064, 'eval_runtime': 9.5317, 'eval_samples_per_second': 104.808, 'eval_steps_per_second': 6.61, 'epoch': 0.92}
{'loss': 1.3834, 'grad_norm': 0.048257194459438324, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4044132232666016, 'eval_runtime': 9.4982, 'eval_samples_per_second': 105.177, 'eval_steps_per_second': 6.633, 'epoch': 0.96}
{'loss': 1.4277, 'grad_norm': 0.048978887498378754, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4034236669540405, 'eval_runtime': 9.5039, 'eval_samples_per_second': 105.114, 'eval_steps_per_second': 6.629, 'epoch': 1.0}
{'train_runtime': 475.9111, 'train_samples_per_second': 21.008, 'train_steps_per_second': 1.313, 'train_loss': 1.641812957763672, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4876110553741455, 2.1283280849456787, 1.7606730461120605, 1.6545982360839844, 1.6023365259170532, 1.5779187679290771, 1.5594213008880615, 1.5461602210998535, 1.5332456827163696, 1.5244245529174805, 1.5154513120651245, 1.5072484016418457, 1.498245358467102, 1.4892610311508179, 1.4802192449569702, 1.470636010169983, 1.4599144458770752, 1.446565866470337, 1.4328070878982544, 1.4191102981567383, 1.412179946899414, 1.4079911708831787, 1.405413031578064, 1.4044132232666016, 1.4034236669540405], 'performance': [0.75, 0.75]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:38,  5.08it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 261.76it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 359.64it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 413.35it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 446.97it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:00<00:00, 475.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 479.12it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.75]
current iteration observed (possibly low-fid or predicted) performance:  0.8476366400718689
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1311 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.7567395490180273
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.8054694952924366
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.7938627052527809
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.8002130193377613
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.7921854709627664
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0401, dtype=torch.float64), 0, tensor(0.1633, dtype=torch.float64), 0, tensor(0.1258, dtype=torch.float64), tensor(0.6397, dtype=torch.float64), 0, tensor(0.0309, dtype=torch.float64), 32, 0, 0, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.0591e-16, dtype=torch.float64), tensor(0.0401, dtype=torch.float64), tensor(1.0441e-17, dtype=torch.float64), tensor(0.1633, dtype=torch.float64), tensor(1.8080e-16, dtype=torch.float64), tensor(0.1258, dtype=torch.float64), tensor(0.6397, dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(0.0309, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.04
  rowan_hellaswag: 0
  sciq: 0.163
  triviaqa: 0
  truthfulqa_gen: 0.126
  wikitext: 0.64
  mmlu: 0
  arc_challenge: 0.031

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9994
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 299.13it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 497.64it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 580.26it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 631.34it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 666.54it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 693.24it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 679.47it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.6613, 'grad_norm': 0.028094109147787094, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.514495372772217, 'eval_runtime': 7.8303, 'eval_samples_per_second': 127.581, 'eval_steps_per_second': 8.046, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 298.26it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 495.97it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 576.87it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 627.57it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 663.07it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 689.44it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 675.91it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.9152, 'grad_norm': 0.08083046227693558, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 2.457458972930908, 'eval_runtime': 7.8429, 'eval_samples_per_second': 127.376, 'eval_steps_per_second': 8.033, 'epoch': 0.08}
{'loss': 2.1752, 'grad_norm': 0.0941452756524086, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.126931667327881, 'eval_runtime': 7.8535, 'eval_samples_per_second': 127.204, 'eval_steps_per_second': 8.022, 'epoch': 0.12}
{'loss': 2.0397, 'grad_norm': 0.06693466007709503, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9313725233078003, 'eval_runtime': 7.8928, 'eval_samples_per_second': 126.571, 'eval_steps_per_second': 7.982, 'epoch': 0.16}
{'loss': 1.8817, 'grad_norm': 0.06395194679498672, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.858128309249878, 'eval_runtime': 7.9045, 'eval_samples_per_second': 126.384, 'eval_steps_per_second': 7.97, 'epoch': 0.2}
{'loss': 1.8769, 'grad_norm': 0.04634295031428337, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.827065110206604, 'eval_runtime': 7.8928, 'eval_samples_per_second': 126.571, 'eval_steps_per_second': 7.982, 'epoch': 0.24}
{'loss': 1.8806, 'grad_norm': 0.0730474516749382, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.8092013597488403, 'eval_runtime': 7.9017, 'eval_samples_per_second': 126.429, 'eval_steps_per_second': 7.973, 'epoch': 0.28}
{'loss': 1.8114, 'grad_norm': 0.08405037224292755, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.793498158454895, 'eval_runtime': 7.8951, 'eval_samples_per_second': 126.535, 'eval_steps_per_second': 7.98, 'epoch': 0.32}
{'loss': 1.8159, 'grad_norm': 0.0697992593050003, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7812104225158691, 'eval_runtime': 7.8947, 'eval_samples_per_second': 126.541, 'eval_steps_per_second': 7.98, 'epoch': 0.36}
{'loss': 1.7509, 'grad_norm': 0.05839809402823448, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7685952186584473, 'eval_runtime': 7.8978, 'eval_samples_per_second': 126.491, 'eval_steps_per_second': 7.977, 'epoch': 0.4}
{'loss': 1.6987, 'grad_norm': 0.05863315984606743, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7536259889602661, 'eval_runtime': 7.9149, 'eval_samples_per_second': 126.217, 'eval_steps_per_second': 7.96, 'epoch': 0.44}
{'loss': 1.747, 'grad_norm': 0.05339318513870239, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7398631572723389, 'eval_runtime': 7.9082, 'eval_samples_per_second': 126.324, 'eval_steps_per_second': 7.966, 'epoch': 0.48}
{'loss': 1.6934, 'grad_norm': 0.07226800918579102, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7260311841964722, 'eval_runtime': 7.8941, 'eval_samples_per_second': 126.55, 'eval_steps_per_second': 7.981, 'epoch': 0.52}
{'loss': 1.797, 'grad_norm': 0.049233611673116684, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.7091089487075806, 'eval_runtime': 7.8991, 'eval_samples_per_second': 126.469, 'eval_steps_per_second': 7.976, 'epoch': 0.56}
{'loss': 1.6591, 'grad_norm': 0.06412533670663834, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.688642144203186, 'eval_runtime': 7.8984, 'eval_samples_per_second': 126.482, 'eval_steps_per_second': 7.976, 'epoch': 0.6}
{'loss': 1.7824, 'grad_norm': 0.0754532441496849, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6688172817230225, 'eval_runtime': 7.9083, 'eval_samples_per_second': 126.323, 'eval_steps_per_second': 7.966, 'epoch': 0.64}
{'loss': 1.7934, 'grad_norm': 0.06925610452890396, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6510709524154663, 'eval_runtime': 7.8885, 'eval_samples_per_second': 126.64, 'eval_steps_per_second': 7.986, 'epoch': 0.68}
{'loss': 1.598, 'grad_norm': 0.057669173926115036, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6442540884017944, 'eval_runtime': 7.8981, 'eval_samples_per_second': 126.486, 'eval_steps_per_second': 7.977, 'epoch': 0.72}
{'loss': 1.6153, 'grad_norm': 0.10444366186857224, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6381901502609253, 'eval_runtime': 7.8868, 'eval_samples_per_second': 126.667, 'eval_steps_per_second': 7.988, 'epoch': 0.76}
{'loss': 1.6315, 'grad_norm': 0.1465192288160324, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6330852508544922, 'eval_runtime': 7.8861, 'eval_samples_per_second': 126.679, 'eval_steps_per_second': 7.989, 'epoch': 0.8}
{'loss': 1.7341, 'grad_norm': 0.06358180940151215, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.6281622648239136, 'eval_runtime': 7.8972, 'eval_samples_per_second': 126.5, 'eval_steps_per_second': 7.977, 'epoch': 0.84}
{'loss': 1.7401, 'grad_norm': 0.07134319841861725, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6234307289123535, 'eval_runtime': 7.9055, 'eval_samples_per_second': 126.367, 'eval_steps_per_second': 7.969, 'epoch': 0.88}
{'loss': 1.6931, 'grad_norm': 0.08705022186040878, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.62019681930542, 'eval_runtime': 7.8875, 'eval_samples_per_second': 126.656, 'eval_steps_per_second': 7.987, 'epoch': 0.92}
{'loss': 1.7126, 'grad_norm': 0.07124753296375275, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.618955373764038, 'eval_runtime': 7.8883, 'eval_samples_per_second': 126.644, 'eval_steps_per_second': 7.987, 'epoch': 0.96}
{'loss': 1.7517, 'grad_norm': 0.10042048990726471, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.6181142330169678, 'eval_runtime': 7.9022, 'eval_samples_per_second': 126.421, 'eval_steps_per_second': 7.972, 'epoch': 1.0}
{'train_runtime': 409.3839, 'train_samples_per_second': 24.412, 'train_steps_per_second': 1.527, 'train_loss': 1.898256219482422, 'epoch': 1.0}
train_results:  {'eval_loss': [3.514495372772217, 2.457458972930908, 2.126931667327881, 1.9313725233078003, 1.858128309249878, 1.827065110206604, 1.8092013597488403, 1.793498158454895, 1.7812104225158691, 1.7685952186584473, 1.7536259889602661, 1.7398631572723389, 1.7260311841964722, 1.7091089487075806, 1.688642144203186, 1.6688172817230225, 1.6510709524154663, 1.6442540884017944, 1.6381901502609253, 1.6330852508544922, 1.6281622648239136, 1.6234307289123535, 1.62019681930542, 1.618955373764038, 1.6181142330169678], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:01<10:11,  1.23s/it]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:05, 78.28it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:02, 153.55it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 224.62it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 288.43it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:02<00:00, 346.16it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 240.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.848089873790741
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0603 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.8043916481656987
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.800258267100352
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.8050394843955374
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.7838945315569276
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.7270300668410544
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0323, dtype=torch.float64), 0, 0, 0, tensor(0.4674, dtype=torch.float64), tensor(0.0684, dtype=torch.float64), tensor(0.4319, dtype=torch.float64), 0, 32, 1, 1, 1, 0, 0, 128, 2.0318461833433301e-19, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0323, dtype=torch.float64), tensor(7.5488e-18, dtype=torch.float64), tensor(7.6138e-18, dtype=torch.float64), tensor(9.8899e-18, dtype=torch.float64), tensor(0.4674, dtype=torch.float64), tensor(0.0684, dtype=torch.float64), tensor(0.4319, dtype=torch.float64), tensor(5.8091e-05, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.0318e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.032
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.467
  wikitext: 0.068
  mmlu: 0.432
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.0318461833433301e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  2.0318461833433301e-19
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 279.26it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 466.28it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 544.07it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 590.18it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 622.41it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 646.89it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 634.18it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.9593, 'grad_norm': 0.19798806309700012, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.016462802886963, 'eval_runtime': 9.0091, 'eval_samples_per_second': 110.888, 'eval_steps_per_second': 6.993, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 277.55it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 463.66it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 540.29it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 586.97it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 620.03it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 643.75it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 630.75it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.2502, 'grad_norm': 0.07674812525510788, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.7855796813964844, 'eval_runtime': 9.0234, 'eval_samples_per_second': 110.712, 'eval_steps_per_second': 6.982, 'epoch': 0.08}
{'loss': 1.626, 'grad_norm': 0.07545778155326843, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5305657386779785, 'eval_runtime': 9.0555, 'eval_samples_per_second': 110.319, 'eval_steps_per_second': 6.957, 'epoch': 0.12}
{'loss': 1.5147, 'grad_norm': 0.0464460551738739, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4439473152160645, 'eval_runtime': 9.118, 'eval_samples_per_second': 109.564, 'eval_steps_per_second': 6.909, 'epoch': 0.16}
{'loss': 1.454, 'grad_norm': 0.04738554358482361, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4031894207000732, 'eval_runtime': 9.1261, 'eval_samples_per_second': 109.466, 'eval_steps_per_second': 6.903, 'epoch': 0.2}
{'loss': 1.3906, 'grad_norm': 0.043031033128499985, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3680853843688965, 'eval_runtime': 9.1174, 'eval_samples_per_second': 109.571, 'eval_steps_per_second': 6.91, 'epoch': 0.24}
{'loss': 1.3827, 'grad_norm': 0.05402171611785889, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3411961793899536, 'eval_runtime': 9.1269, 'eval_samples_per_second': 109.456, 'eval_steps_per_second': 6.903, 'epoch': 0.28}
{'loss': 1.3392, 'grad_norm': 0.0625564232468605, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.317315936088562, 'eval_runtime': 9.1304, 'eval_samples_per_second': 109.414, 'eval_steps_per_second': 6.9, 'epoch': 0.32}
{'loss': 1.3571, 'grad_norm': 0.05911022052168846, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2961755990982056, 'eval_runtime': 9.1309, 'eval_samples_per_second': 109.408, 'eval_steps_per_second': 6.9, 'epoch': 0.36}
{'loss': 1.2739, 'grad_norm': 0.050005145370960236, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2768635749816895, 'eval_runtime': 9.135, 'eval_samples_per_second': 109.359, 'eval_steps_per_second': 6.897, 'epoch': 0.4}
{'loss': 1.2661, 'grad_norm': 0.05507948622107506, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2536770105361938, 'eval_runtime': 9.1419, 'eval_samples_per_second': 109.277, 'eval_steps_per_second': 6.891, 'epoch': 0.44}
{'loss': 1.278, 'grad_norm': 0.07186467945575714, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2235827445983887, 'eval_runtime': 9.1232, 'eval_samples_per_second': 109.501, 'eval_steps_per_second': 6.905, 'epoch': 0.48}
{'loss': 1.2452, 'grad_norm': 0.06081264466047287, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1856664419174194, 'eval_runtime': 9.1275, 'eval_samples_per_second': 109.45, 'eval_steps_per_second': 6.902, 'epoch': 0.52}
{'loss': 1.1787, 'grad_norm': 0.06704293191432953, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1671574115753174, 'eval_runtime': 9.124, 'eval_samples_per_second': 109.491, 'eval_steps_per_second': 6.905, 'epoch': 0.56}
{'loss': 1.1909, 'grad_norm': 0.0632481798529625, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.152289628982544, 'eval_runtime': 9.1086, 'eval_samples_per_second': 109.676, 'eval_steps_per_second': 6.917, 'epoch': 0.6}
{'loss': 1.2004, 'grad_norm': 0.07248390465974808, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1372945308685303, 'eval_runtime': 9.1198, 'eval_samples_per_second': 109.541, 'eval_steps_per_second': 6.908, 'epoch': 0.64}
{'loss': 1.1079, 'grad_norm': 0.06338771432638168, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1229346990585327, 'eval_runtime': 9.1465, 'eval_samples_per_second': 109.222, 'eval_steps_per_second': 6.888, 'epoch': 0.68}
{'loss': 1.0893, 'grad_norm': 0.07372923940420151, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1073613166809082, 'eval_runtime': 9.1235, 'eval_samples_per_second': 109.498, 'eval_steps_per_second': 6.905, 'epoch': 0.72}
{'loss': 1.0489, 'grad_norm': 0.08412279933691025, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0901769399642944, 'eval_runtime': 9.1306, 'eval_samples_per_second': 109.412, 'eval_steps_per_second': 6.9, 'epoch': 0.76}
{'loss': 1.1553, 'grad_norm': 0.06347483396530151, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0755016803741455, 'eval_runtime': 9.1292, 'eval_samples_per_second': 109.429, 'eval_steps_per_second': 6.901, 'epoch': 0.8}
{'loss': 1.1702, 'grad_norm': 0.06395424902439117, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0667202472686768, 'eval_runtime': 9.1397, 'eval_samples_per_second': 109.303, 'eval_steps_per_second': 6.893, 'epoch': 0.84}
{'loss': 1.063, 'grad_norm': 0.07717934995889664, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0609444379806519, 'eval_runtime': 9.1344, 'eval_samples_per_second': 109.367, 'eval_steps_per_second': 6.897, 'epoch': 0.88}
{'loss': 0.9777, 'grad_norm': 0.07613954693078995, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0573687553405762, 'eval_runtime': 9.1268, 'eval_samples_per_second': 109.458, 'eval_steps_per_second': 6.903, 'epoch': 0.92}
{'loss': 1.1618, 'grad_norm': 0.09817461669445038, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.055080771446228, 'eval_runtime': 9.1334, 'eval_samples_per_second': 109.379, 'eval_steps_per_second': 6.898, 'epoch': 0.96}
{'loss': 1.1173, 'grad_norm': 0.09156675636768341, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0539932250976562, 'eval_runtime': 9.1399, 'eval_samples_per_second': 109.301, 'eval_steps_per_second': 6.893, 'epoch': 1.0}
{'train_runtime': 470.2818, 'train_samples_per_second': 21.257, 'train_steps_per_second': 1.329, 'train_loss': 1.3919300415039062, 'epoch': 1.0}
train_results:  {'eval_loss': [3.016462802886963, 1.7855796813964844, 1.5305657386779785, 1.4439473152160645, 1.4031894207000732, 1.3680853843688965, 1.3411961793899536, 1.317315936088562, 1.2961755990982056, 1.2768635749816895, 1.2536770105361938, 1.2235827445983887, 1.1856664419174194, 1.1671574115753174, 1.152289628982544, 1.1372945308685303, 1.1229346990585327, 1.1073613166809082, 1.0901769399642944, 1.0755016803741455, 1.0667202472686768, 1.0609444379806519, 1.0573687553405762, 1.055080771446228, 1.0539932250976562], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<06:16,  1.32it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:01<00:04, 93.52it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 174.82it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:01, 245.55it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 304.21it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 355.35it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 276.28it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.847747266292572
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6898 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.7900239071818892
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.8050974536290639
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.8050308431114587
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.8050966957213866
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.792244231439167
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0373, dtype=torch.float64), tensor(0.2231, dtype=torch.float64), tensor(0.1482, dtype=torch.float64), 0, tensor(0.5914, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.1, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(6.1893e-17, dtype=torch.float64), tensor(0.0373, dtype=torch.float64), tensor(0.2231, dtype=torch.float64), tensor(0.1482, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5914, dtype=torch.float64), tensor(4.9276e-17, dtype=torch.float64), tensor(2.8450e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.037
  rowan_hellaswag: 0.223
  sciq: 0.148
  triviaqa: 0
  truthfulqa_gen: 0.591
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 265.02it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 440.49it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 513.49it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 558.94it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 593.13it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 618.40it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 603.99it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.404, 'grad_norm': 0.2283998280763626, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2890708446502686, 'eval_runtime': 9.9105, 'eval_samples_per_second': 100.802, 'eval_steps_per_second': 6.357, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 267.61it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 446.34it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 519.38it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 564.03it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 595.01it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 618.62it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 606.70it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.4565, 'grad_norm': 0.23927123844623566, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8217535018920898, 'eval_runtime': 9.9131, 'eval_samples_per_second': 100.776, 'eval_steps_per_second': 6.355, 'epoch': 0.08}
{'loss': 1.6917, 'grad_norm': 0.09118182212114334, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4753552675247192, 'eval_runtime': 9.9885, 'eval_samples_per_second': 100.015, 'eval_steps_per_second': 6.307, 'epoch': 0.12}
{'loss': 1.3224, 'grad_norm': 0.053229525685310364, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3428964614868164, 'eval_runtime': 10.0326, 'eval_samples_per_second': 99.575, 'eval_steps_per_second': 6.28, 'epoch': 0.16}
{'loss': 1.317, 'grad_norm': 0.06086602061986923, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3043394088745117, 'eval_runtime': 10.0448, 'eval_samples_per_second': 99.455, 'eval_steps_per_second': 6.272, 'epoch': 0.2}
{'loss': 1.2546, 'grad_norm': 0.06515538692474365, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.28130304813385, 'eval_runtime': 10.0512, 'eval_samples_per_second': 99.391, 'eval_steps_per_second': 6.268, 'epoch': 0.24}
{'loss': 1.2835, 'grad_norm': 0.07334765046834946, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.262296199798584, 'eval_runtime': 10.0585, 'eval_samples_per_second': 99.319, 'eval_steps_per_second': 6.263, 'epoch': 0.28}
{'loss': 1.2962, 'grad_norm': 0.05400848388671875, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2488863468170166, 'eval_runtime': 10.0487, 'eval_samples_per_second': 99.416, 'eval_steps_per_second': 6.269, 'epoch': 0.32}
{'loss': 1.2415, 'grad_norm': 0.06135646626353264, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2353527545928955, 'eval_runtime': 10.0478, 'eval_samples_per_second': 99.425, 'eval_steps_per_second': 6.27, 'epoch': 0.36}
{'loss': 1.276, 'grad_norm': 0.0642017275094986, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2273240089416504, 'eval_runtime': 10.0494, 'eval_samples_per_second': 99.409, 'eval_steps_per_second': 6.269, 'epoch': 0.4}
{'loss': 1.2403, 'grad_norm': 0.06923364102840424, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.217516303062439, 'eval_runtime': 10.0449, 'eval_samples_per_second': 99.453, 'eval_steps_per_second': 6.272, 'epoch': 0.44}
{'loss': 1.1905, 'grad_norm': 0.06085571274161339, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2082762718200684, 'eval_runtime': 10.0415, 'eval_samples_per_second': 99.487, 'eval_steps_per_second': 6.274, 'epoch': 0.48}
{'loss': 1.1728, 'grad_norm': 0.07130356132984161, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1982799768447876, 'eval_runtime': 10.0458, 'eval_samples_per_second': 99.444, 'eval_steps_per_second': 6.271, 'epoch': 0.52}
{'loss': 1.1777, 'grad_norm': 0.06425248086452484, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1896620988845825, 'eval_runtime': 10.0503, 'eval_samples_per_second': 99.4, 'eval_steps_per_second': 6.268, 'epoch': 0.56}
{'loss': 1.1919, 'grad_norm': 0.07420052587985992, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1803312301635742, 'eval_runtime': 10.0562, 'eval_samples_per_second': 99.342, 'eval_steps_per_second': 6.265, 'epoch': 0.6}
{'loss': 1.2165, 'grad_norm': 0.07204393297433853, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.174201488494873, 'eval_runtime': 10.0508, 'eval_samples_per_second': 99.395, 'eval_steps_per_second': 6.268, 'epoch': 0.64}
{'loss': 1.1483, 'grad_norm': 0.07216541469097137, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1670821905136108, 'eval_runtime': 10.0408, 'eval_samples_per_second': 99.494, 'eval_steps_per_second': 6.274, 'epoch': 0.68}
{'loss': 1.135, 'grad_norm': 0.06983371824026108, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1581865549087524, 'eval_runtime': 10.0481, 'eval_samples_per_second': 99.421, 'eval_steps_per_second': 6.27, 'epoch': 0.72}
{'loss': 1.1771, 'grad_norm': 0.08126243203878403, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1496320962905884, 'eval_runtime': 10.0655, 'eval_samples_per_second': 99.25, 'eval_steps_per_second': 6.259, 'epoch': 0.76}
{'loss': 1.1575, 'grad_norm': 0.06740223616361618, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1412591934204102, 'eval_runtime': 10.0481, 'eval_samples_per_second': 99.421, 'eval_steps_per_second': 6.27, 'epoch': 0.8}
{'loss': 1.154, 'grad_norm': 0.07719142735004425, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1343960762023926, 'eval_runtime': 10.0422, 'eval_samples_per_second': 99.48, 'eval_steps_per_second': 6.274, 'epoch': 0.84}
{'loss': 1.1303, 'grad_norm': 0.08783704042434692, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.129351258277893, 'eval_runtime': 10.0638, 'eval_samples_per_second': 99.267, 'eval_steps_per_second': 6.26, 'epoch': 0.88}
{'loss': 1.1025, 'grad_norm': 0.08931303769350052, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1258810758590698, 'eval_runtime': 10.0383, 'eval_samples_per_second': 99.519, 'eval_steps_per_second': 6.276, 'epoch': 0.92}
{'loss': 1.1433, 'grad_norm': 0.07927658408880234, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1226165294647217, 'eval_runtime': 10.0379, 'eval_samples_per_second': 99.523, 'eval_steps_per_second': 6.276, 'epoch': 0.96}
{'loss': 1.1733, 'grad_norm': 0.0963597521185875, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1215429306030273, 'eval_runtime': 10.0318, 'eval_samples_per_second': 99.583, 'eval_steps_per_second': 6.28, 'epoch': 1.0}
{'train_runtime': 517.0871, 'train_samples_per_second': 19.335, 'train_steps_per_second': 1.209, 'train_loss': 1.4021685760498046, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2890708446502686, 1.8217535018920898, 1.4753552675247192, 1.3428964614868164, 1.3043394088745117, 1.28130304813385, 1.262296199798584, 1.2488863468170166, 1.2353527545928955, 1.2273240089416504, 1.217516303062439, 1.2082762718200684, 1.1982799768447876, 1.1896620988845825, 1.1803312301635742, 1.174201488494873, 1.1670821905136108, 1.1581865549087524, 1.1496320962905884, 1.1412591934204102, 1.1343960762023926, 1.129351258277893, 1.1258810758590698, 1.1226165294647217, 1.1215429306030273], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:47,  4.63it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 139.86it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 232.23it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 299.59it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 348.02it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 387.79it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 359.46it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8479082584381104
current iteration best possible performance (full train run):  0.777
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7096 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.8008578943190973
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.7696304482260319
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.8028667041608389
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.7923818803054165
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.8019900529585936
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0410, dtype=torch.float64), tensor(0.0249, dtype=torch.float64), 0, 0, 0, tensor(0.2266, dtype=torch.float64), tensor(0.2993, dtype=torch.float64), 0, tensor(0.4082, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.00646839821670096, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0410, dtype=torch.float64), tensor(0.0249, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2266, dtype=torch.float64), tensor(0.2993, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4082, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0647, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.041
  gsm8k: 0.025
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.227
  wikitext: 0.299
  mmlu: 0
  arc_challenge: 0.408

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.00646839821670096,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.00646839821670096
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 269.57it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 449.72it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 523.52it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 568.52it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 599.42it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 618.42it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 609.11it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.7725, 'grad_norm': 0.12147820740938187, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.023399829864502, 'eval_runtime': 8.2949, 'eval_samples_per_second': 120.436, 'eval_steps_per_second': 7.595, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 269.89it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 450.07it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 522.70it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 565.93it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 597.84it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 623.39it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 610.91it/s]
Evaluation performance at step 50: 0.73
{'loss': 2.2111, 'grad_norm': 0.05745687708258629, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.73}
{'eval_loss': 1.7132112979888916, 'eval_runtime': 8.3121, 'eval_samples_per_second': 120.186, 'eval_steps_per_second': 7.579, 'epoch': 0.08}
{'loss': 1.632, 'grad_norm': 0.07899202406406403, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5173749923706055, 'eval_runtime': 8.33, 'eval_samples_per_second': 119.928, 'eval_steps_per_second': 7.563, 'epoch': 0.12}
{'loss': 1.3957, 'grad_norm': 0.03916187584400177, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.42213773727417, 'eval_runtime': 8.3577, 'eval_samples_per_second': 119.53, 'eval_steps_per_second': 7.538, 'epoch': 0.16}
{'loss': 1.4252, 'grad_norm': 0.035345371812582016, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3745955228805542, 'eval_runtime': 8.3745, 'eval_samples_per_second': 119.291, 'eval_steps_per_second': 7.523, 'epoch': 0.2}
{'loss': 1.3022, 'grad_norm': 0.04645104706287384, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3354098796844482, 'eval_runtime': 8.3892, 'eval_samples_per_second': 119.082, 'eval_steps_per_second': 7.51, 'epoch': 0.24}
{'loss': 1.277, 'grad_norm': 0.04763057455420494, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3069138526916504, 'eval_runtime': 8.3853, 'eval_samples_per_second': 119.138, 'eval_steps_per_second': 7.513, 'epoch': 0.28}
{'loss': 1.3177, 'grad_norm': 0.05412786453962326, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2820310592651367, 'eval_runtime': 8.3928, 'eval_samples_per_second': 119.031, 'eval_steps_per_second': 7.506, 'epoch': 0.32}
{'loss': 1.2783, 'grad_norm': 0.0417838953435421, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2618294954299927, 'eval_runtime': 8.379, 'eval_samples_per_second': 119.227, 'eval_steps_per_second': 7.519, 'epoch': 0.36}
{'loss': 1.2549, 'grad_norm': 0.055152133107185364, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2389219999313354, 'eval_runtime': 8.3874, 'eval_samples_per_second': 119.107, 'eval_steps_per_second': 7.511, 'epoch': 0.4}
{'loss': 1.2299, 'grad_norm': 0.05687607824802399, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2091269493103027, 'eval_runtime': 8.3821, 'eval_samples_per_second': 119.182, 'eval_steps_per_second': 7.516, 'epoch': 0.44}
{'loss': 1.2415, 'grad_norm': 0.05983387678861618, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1604117155075073, 'eval_runtime': 8.3885, 'eval_samples_per_second': 119.092, 'eval_steps_per_second': 7.51, 'epoch': 0.48}
{'loss': 1.17, 'grad_norm': 0.0543069988489151, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.142354130744934, 'eval_runtime': 8.3879, 'eval_samples_per_second': 119.1, 'eval_steps_per_second': 7.511, 'epoch': 0.52}
{'loss': 1.0925, 'grad_norm': 0.0654594898223877, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.12379789352417, 'eval_runtime': 8.379, 'eval_samples_per_second': 119.226, 'eval_steps_per_second': 7.519, 'epoch': 0.56}
{'loss': 1.1584, 'grad_norm': 0.05129658803343773, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1041240692138672, 'eval_runtime': 8.3855, 'eval_samples_per_second': 119.134, 'eval_steps_per_second': 7.513, 'epoch': 0.6}
{'loss': 1.0755, 'grad_norm': 0.06064162775874138, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0842598676681519, 'eval_runtime': 8.3867, 'eval_samples_per_second': 119.117, 'eval_steps_per_second': 7.512, 'epoch': 0.64}
{'loss': 1.123, 'grad_norm': 0.07421469688415527, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0626904964447021, 'eval_runtime': 8.3903, 'eval_samples_per_second': 119.066, 'eval_steps_per_second': 7.509, 'epoch': 0.68}
{'loss': 1.1101, 'grad_norm': 0.07159662991762161, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0423247814178467, 'eval_runtime': 8.3891, 'eval_samples_per_second': 119.083, 'eval_steps_per_second': 7.51, 'epoch': 0.72}
{'loss': 1.1128, 'grad_norm': 0.062297187745571136, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0297939777374268, 'eval_runtime': 8.3941, 'eval_samples_per_second': 119.012, 'eval_steps_per_second': 7.505, 'epoch': 0.76}
{'loss': 1.015, 'grad_norm': 0.06800585985183716, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0206754207611084, 'eval_runtime': 8.4053, 'eval_samples_per_second': 118.854, 'eval_steps_per_second': 7.495, 'epoch': 0.8}
{'loss': 0.982, 'grad_norm': 0.08956792950630188, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0098695755004883, 'eval_runtime': 8.4271, 'eval_samples_per_second': 118.546, 'eval_steps_per_second': 7.476, 'epoch': 0.84}
{'loss': 1.07, 'grad_norm': 0.07198812812566757, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0011013746261597, 'eval_runtime': 8.4577, 'eval_samples_per_second': 118.117, 'eval_steps_per_second': 7.449, 'epoch': 0.88}
{'loss': 1.0091, 'grad_norm': 0.08261077105998993, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9965611100196838, 'eval_runtime': 8.4133, 'eval_samples_per_second': 118.74, 'eval_steps_per_second': 7.488, 'epoch': 0.92}
{'loss': 0.9427, 'grad_norm': 0.10568880289793015, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.99288010597229, 'eval_runtime': 8.4049, 'eval_samples_per_second': 118.859, 'eval_steps_per_second': 7.496, 'epoch': 0.96}
{'loss': 1.0785, 'grad_norm': 0.08080954849720001, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9912799596786499, 'eval_runtime': 8.3972, 'eval_samples_per_second': 118.968, 'eval_steps_per_second': 7.502, 'epoch': 1.0}
{'train_runtime': 428.0544, 'train_samples_per_second': 23.357, 'train_steps_per_second': 1.46, 'train_loss': 1.331097848510742, 'epoch': 1.0}
train_results:  {'eval_loss': [3.023399829864502, 1.7132112979888916, 1.5173749923706055, 1.42213773727417, 1.3745955228805542, 1.3354098796844482, 1.3069138526916504, 1.2820310592651367, 1.2618294954299927, 1.2389219999313354, 1.2091269493103027, 1.1604117155075073, 1.142354130744934, 1.12379789352417, 1.1041240692138672, 1.0842598676681519, 1.0626904964447021, 1.0423247814178467, 1.0297939777374268, 1.0206754207611084, 1.0098695755004883, 1.0011013746261597, 0.9965611100196838, 0.99288010597229, 0.9912799596786499], 'performance': [0.74, 0.73]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:57,  1.68it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 106.40it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 191.12it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 260.32it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 315.53it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 361.29it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 297.19it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.73]
current iteration observed (possibly low-fid or predicted) performance:  0.8477389812469482
current iteration best possible performance (full train run):  0.8085000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104, 0.8477389812469482]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2034 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.7880468227067908
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.7875421758325081
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.7903049773327159
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.7903049773327159
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.790301211024747
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0132, dtype=torch.float64), 0, tensor(0.0982, dtype=torch.float64), tensor(0.1802, dtype=torch.float64), tensor(0.0119, dtype=torch.float64), tensor(0.3787, dtype=torch.float64), tensor(0.2729, dtype=torch.float64), 0, tensor(0.0449, dtype=torch.float64), 32, 0, 1, 1, 0, 0, 128, 0.061241185208156794, 5.676478480807267, 1]
normalized proposed parameters for next round by BO: [tensor(0.0132, dtype=torch.float64), tensor(2.6021e-18, dtype=torch.float64), tensor(0.0982, dtype=torch.float64), tensor(0.1802, dtype=torch.float64), tensor(0.0119, dtype=torch.float64), tensor(0.3787, dtype=torch.float64), tensor(0.2729, dtype=torch.float64), tensor(8.8861e-19, dtype=torch.float64), tensor(0.0449, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6124, dtype=torch.float64), tensor(0.1183, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.013
  gsm8k: 0
  rowan_hellaswag: 0.098
  sciq: 0.18
  triviaqa: 0.012
  truthfulqa_gen: 0.379
  wikitext: 0.273
  mmlu: 0
  arc_challenge: 0.045

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.061241185208156794,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (5.676478480807267,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.061241185208156794
lora alpha:  5.676478480807267
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 289.98it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 483.70it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 563.45it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 611.62it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 646.58it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 671.87it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 658.47it/s]
Evaluation performance at step 25: 0.75
{'loss': 4.039, 'grad_norm': 0.5255060195922852, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 2.899724245071411, 'eval_runtime': 8.3365, 'eval_samples_per_second': 119.834, 'eval_steps_per_second': 7.557, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 290.69it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 484.01it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 563.06it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 611.70it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 645.90it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 671.58it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 658.34it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.3255, 'grad_norm': 0.1677396148443222, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8936238288879395, 'eval_runtime': 8.3523, 'eval_samples_per_second': 119.608, 'eval_steps_per_second': 7.543, 'epoch': 0.08}
{'loss': 1.7672, 'grad_norm': 0.11124754697084427, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6883124113082886, 'eval_runtime': 8.3515, 'eval_samples_per_second': 119.619, 'eval_steps_per_second': 7.544, 'epoch': 0.12}
{'loss': 1.6411, 'grad_norm': 0.1037328764796257, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6117329597473145, 'eval_runtime': 8.3753, 'eval_samples_per_second': 119.28, 'eval_steps_per_second': 7.522, 'epoch': 0.16}
{'loss': 1.6022, 'grad_norm': 0.13674308359622955, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5628397464752197, 'eval_runtime': 8.4034, 'eval_samples_per_second': 118.881, 'eval_steps_per_second': 7.497, 'epoch': 0.2}
{'loss': 1.5054, 'grad_norm': 0.11120612919330597, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5118045806884766, 'eval_runtime': 8.4194, 'eval_samples_per_second': 118.655, 'eval_steps_per_second': 7.483, 'epoch': 0.24}
{'loss': 1.4655, 'grad_norm': 0.13573351502418518, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4506947994232178, 'eval_runtime': 8.4295, 'eval_samples_per_second': 118.512, 'eval_steps_per_second': 7.474, 'epoch': 0.28}
{'loss': 1.4702, 'grad_norm': 0.15399415791034698, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4293169975280762, 'eval_runtime': 8.4247, 'eval_samples_per_second': 118.58, 'eval_steps_per_second': 7.478, 'epoch': 0.32}
{'loss': 1.4523, 'grad_norm': 0.1455496996641159, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4138400554656982, 'eval_runtime': 8.4257, 'eval_samples_per_second': 118.566, 'eval_steps_per_second': 7.477, 'epoch': 0.36}
{'loss': 1.4705, 'grad_norm': 0.15118208527565002, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3882194757461548, 'eval_runtime': 8.4268, 'eval_samples_per_second': 118.551, 'eval_steps_per_second': 7.476, 'epoch': 0.4}
{'loss': 1.416, 'grad_norm': 0.14422264695167542, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3662761449813843, 'eval_runtime': 8.4414, 'eval_samples_per_second': 118.345, 'eval_steps_per_second': 7.463, 'epoch': 0.44}
{'loss': 1.3876, 'grad_norm': 0.14334924519062042, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3478436470031738, 'eval_runtime': 8.4371, 'eval_samples_per_second': 118.406, 'eval_steps_per_second': 7.467, 'epoch': 0.48}
{'loss': 1.3826, 'grad_norm': 0.12714354693889618, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3229682445526123, 'eval_runtime': 8.4419, 'eval_samples_per_second': 118.339, 'eval_steps_per_second': 7.463, 'epoch': 0.52}
{'loss': 1.3599, 'grad_norm': 0.15067073702812195, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2952131032943726, 'eval_runtime': 8.4389, 'eval_samples_per_second': 118.38, 'eval_steps_per_second': 7.465, 'epoch': 0.56}
{'loss': 1.2875, 'grad_norm': 0.1658545285463333, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2735685110092163, 'eval_runtime': 8.4294, 'eval_samples_per_second': 118.514, 'eval_steps_per_second': 7.474, 'epoch': 0.6}
{'loss': 1.3145, 'grad_norm': 0.1439964324235916, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2595943212509155, 'eval_runtime': 8.4227, 'eval_samples_per_second': 118.608, 'eval_steps_per_second': 7.48, 'epoch': 0.64}
{'loss': 1.2551, 'grad_norm': 0.15888239443302155, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2488738298416138, 'eval_runtime': 8.4293, 'eval_samples_per_second': 118.515, 'eval_steps_per_second': 7.474, 'epoch': 0.68}
{'loss': 1.3214, 'grad_norm': 0.15596364438533783, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2371946573257446, 'eval_runtime': 8.4217, 'eval_samples_per_second': 118.623, 'eval_steps_per_second': 7.481, 'epoch': 0.72}
{'loss': 1.2864, 'grad_norm': 0.25556543469429016, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2278279066085815, 'eval_runtime': 8.4227, 'eval_samples_per_second': 118.608, 'eval_steps_per_second': 7.48, 'epoch': 0.76}
{'loss': 1.3158, 'grad_norm': 0.15335796773433685, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2169090509414673, 'eval_runtime': 8.4121, 'eval_samples_per_second': 118.758, 'eval_steps_per_second': 7.489, 'epoch': 0.8}
{'loss': 1.2321, 'grad_norm': 0.17019030451774597, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2078335285186768, 'eval_runtime': 8.4074, 'eval_samples_per_second': 118.824, 'eval_steps_per_second': 7.493, 'epoch': 0.84}
{'loss': 1.3674, 'grad_norm': 0.1771579086780548, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2015694379806519, 'eval_runtime': 8.4165, 'eval_samples_per_second': 118.696, 'eval_steps_per_second': 7.485, 'epoch': 0.88}
{'loss': 1.3062, 'grad_norm': 0.13592980802059174, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1964707374572754, 'eval_runtime': 8.422, 'eval_samples_per_second': 118.618, 'eval_steps_per_second': 7.48, 'epoch': 0.92}
{'loss': 1.1949, 'grad_norm': 0.18877223134040833, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.192913293838501, 'eval_runtime': 8.4184, 'eval_samples_per_second': 118.668, 'eval_steps_per_second': 7.484, 'epoch': 0.96}
{'loss': 1.2264, 'grad_norm': 0.19453679025173187, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1915388107299805, 'eval_runtime': 8.4155, 'eval_samples_per_second': 118.71, 'eval_steps_per_second': 7.486, 'epoch': 1.0}
{'train_runtime': 438.2472, 'train_samples_per_second': 22.811, 'train_steps_per_second': 1.426, 'train_loss': 1.535712890625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.899724245071411, 1.8936238288879395, 1.6883124113082886, 1.6117329597473145, 1.5628397464752197, 1.5118045806884766, 1.4506947994232178, 1.4293169975280762, 1.4138400554656982, 1.3882194757461548, 1.3662761449813843, 1.3478436470031738, 1.3229682445526123, 1.2952131032943726, 1.2735685110092163, 1.2595943212509155, 1.2488738298416138, 1.2371946573257446, 1.2278279066085815, 1.2169090509414673, 1.2078335285186768, 1.2015694379806519, 1.1964707374572754, 1.192913293838501, 1.1915388107299805], 'performance': [0.75, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<05:12,  1.60it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:04, 101.52it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:01<00:01, 187.36it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 260.81it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 321.21it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 372.93it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 297.56it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8454647064208984
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104, 0.8477389812469482, 0.8454647064208984]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4941 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.794348496280434
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.7957613676372143
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.7994936667958126
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.7957613753073252
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.7681990458467555
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1968, dtype=torch.float64), 0, tensor(0.0316, dtype=torch.float64), tensor(0.1120, dtype=torch.float64), 0, tensor(0.1716, dtype=torch.float64), tensor(0.0818, dtype=torch.float64), tensor(0.0581, dtype=torch.float64), tensor(0.3481, dtype=torch.float64), 32, 0, 0, 1, 0, 0, 128, 0.058227360196271644, 8.257585355461549, 1]
normalized proposed parameters for next round by BO: [tensor(0.1968, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0316, dtype=torch.float64), tensor(0.1120, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1716, dtype=torch.float64), tensor(0.0818, dtype=torch.float64), tensor(0.0581, dtype=torch.float64), tensor(0.3481, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5823, dtype=torch.float64), tensor(0.1720, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.197
  gsm8k: 0
  rowan_hellaswag: 0.032
  sciq: 0.112
  triviaqa: 0
  truthfulqa_gen: 0.172
  wikitext: 0.082
  mmlu: 0.058
  arc_challenge: 0.348

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.058227360196271644,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (8.257585355461549,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.058227360196271644
lora alpha:  8.257585355461549
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 299.03it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 499.35it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 582.85it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 632.88it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 667.72it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 694.54it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 680.92it/s]
Evaluation performance at step 25: 0.74
{'loss': 3.7913, 'grad_norm': 0.3196565508842468, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 2.4524850845336914, 'eval_runtime': 7.4369, 'eval_samples_per_second': 134.33, 'eval_steps_per_second': 8.471, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 299.06it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 497.58it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 579.60it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 630.77it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 665.71it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 692.79it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 678.78it/s]
Evaluation performance at step 50: 0.74
{'loss': 1.8021, 'grad_norm': 0.24383871257305145, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.4898433685302734, 'eval_runtime': 7.4549, 'eval_samples_per_second': 134.006, 'eval_steps_per_second': 8.451, 'epoch': 0.08}
{'loss': 1.4024, 'grad_norm': 0.10382462292909622, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3316781520843506, 'eval_runtime': 7.4698, 'eval_samples_per_second': 133.739, 'eval_steps_per_second': 8.434, 'epoch': 0.12}
{'loss': 1.3059, 'grad_norm': 0.09755171835422516, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2811559438705444, 'eval_runtime': 7.4903, 'eval_samples_per_second': 133.372, 'eval_steps_per_second': 8.411, 'epoch': 0.16}
{'loss': 1.2808, 'grad_norm': 0.11258326470851898, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2467353343963623, 'eval_runtime': 7.4973, 'eval_samples_per_second': 133.248, 'eval_steps_per_second': 8.403, 'epoch': 0.2}
{'loss': 1.2589, 'grad_norm': 0.09472553431987762, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2106614112854004, 'eval_runtime': 7.5025, 'eval_samples_per_second': 133.155, 'eval_steps_per_second': 8.397, 'epoch': 0.24}
{'loss': 1.2813, 'grad_norm': 0.11434296518564224, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1664382219314575, 'eval_runtime': 7.5326, 'eval_samples_per_second': 132.623, 'eval_steps_per_second': 8.364, 'epoch': 0.28}
{'loss': 1.1056, 'grad_norm': 0.09332405030727386, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1086339950561523, 'eval_runtime': 7.5444, 'eval_samples_per_second': 132.417, 'eval_steps_per_second': 8.351, 'epoch': 0.32}
{'loss': 1.1241, 'grad_norm': 0.09987778961658478, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0872926712036133, 'eval_runtime': 7.5155, 'eval_samples_per_second': 132.925, 'eval_steps_per_second': 8.383, 'epoch': 0.36}
{'loss': 1.0845, 'grad_norm': 0.12876717746257782, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0615532398223877, 'eval_runtime': 7.5069, 'eval_samples_per_second': 133.078, 'eval_steps_per_second': 8.392, 'epoch': 0.4}
{'loss': 1.057, 'grad_norm': 0.11529590934515, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0363192558288574, 'eval_runtime': 7.517, 'eval_samples_per_second': 132.898, 'eval_steps_per_second': 8.381, 'epoch': 0.44}
{'loss': 1.0606, 'grad_norm': 0.11859001219272614, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0029627084732056, 'eval_runtime': 7.5134, 'eval_samples_per_second': 132.962, 'eval_steps_per_second': 8.385, 'epoch': 0.48}
{'loss': 1.0288, 'grad_norm': 0.11517316848039627, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9840443134307861, 'eval_runtime': 7.5147, 'eval_samples_per_second': 132.939, 'eval_steps_per_second': 8.384, 'epoch': 0.52}
{'loss': 1.0865, 'grad_norm': 0.1337561309337616, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.975651204586029, 'eval_runtime': 7.5154, 'eval_samples_per_second': 132.927, 'eval_steps_per_second': 8.383, 'epoch': 0.56}
{'loss': 1.056, 'grad_norm': 0.12164377421140671, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9663121104240417, 'eval_runtime': 7.5095, 'eval_samples_per_second': 133.031, 'eval_steps_per_second': 8.389, 'epoch': 0.6}
{'loss': 0.9713, 'grad_norm': 0.12525522708892822, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9573115706443787, 'eval_runtime': 7.5333, 'eval_samples_per_second': 132.611, 'eval_steps_per_second': 8.363, 'epoch': 0.64}
{'loss': 0.982, 'grad_norm': 0.12846417725086212, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.948628306388855, 'eval_runtime': 7.5167, 'eval_samples_per_second': 132.903, 'eval_steps_per_second': 8.381, 'epoch': 0.68}
{'loss': 0.946, 'grad_norm': 0.14169229567050934, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9430221319198608, 'eval_runtime': 7.5356, 'eval_samples_per_second': 132.571, 'eval_steps_per_second': 8.36, 'epoch': 0.72}
{'loss': 0.9837, 'grad_norm': 0.13487817347049713, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9351991415023804, 'eval_runtime': 7.5196, 'eval_samples_per_second': 132.852, 'eval_steps_per_second': 8.378, 'epoch': 0.76}
{'loss': 0.9245, 'grad_norm': 0.15427419543266296, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9289785623550415, 'eval_runtime': 7.5161, 'eval_samples_per_second': 132.915, 'eval_steps_per_second': 8.382, 'epoch': 0.8}
{'loss': 0.9561, 'grad_norm': 0.1561921238899231, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.926028311252594, 'eval_runtime': 7.5068, 'eval_samples_per_second': 133.08, 'eval_steps_per_second': 8.392, 'epoch': 0.84}
{'loss': 0.9414, 'grad_norm': 0.1553245335817337, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.919557511806488, 'eval_runtime': 7.5169, 'eval_samples_per_second': 132.9, 'eval_steps_per_second': 8.381, 'epoch': 0.88}
{'loss': 0.9671, 'grad_norm': 0.15833978354930878, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9171727895736694, 'eval_runtime': 7.5179, 'eval_samples_per_second': 132.883, 'eval_steps_per_second': 8.38, 'epoch': 0.92}
{'loss': 0.9624, 'grad_norm': 0.14799004793167114, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.914562463760376, 'eval_runtime': 7.5167, 'eval_samples_per_second': 132.903, 'eval_steps_per_second': 8.381, 'epoch': 0.96}
{'loss': 0.9721, 'grad_norm': 0.1732090711593628, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9136114120483398, 'eval_runtime': 7.525, 'eval_samples_per_second': 132.757, 'eval_steps_per_second': 8.372, 'epoch': 1.0}
{'train_runtime': 385.537, 'train_samples_per_second': 25.927, 'train_steps_per_second': 1.621, 'train_loss': 1.213293881225586, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4524850845336914, 1.4898433685302734, 1.3316781520843506, 1.2811559438705444, 1.2467353343963623, 1.2106614112854004, 1.1664382219314575, 1.1086339950561523, 1.0872926712036133, 1.0615532398223877, 1.0363192558288574, 1.0029627084732056, 0.9840443134307861, 0.975651204586029, 0.9663121104240417, 0.9573115706443787, 0.948628306388855, 0.9430221319198608, 0.9351991415023804, 0.9289785623550415, 0.926028311252594, 0.919557511806488, 0.9171727895736694, 0.914562463760376, 0.9136114120483398], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<01:38,  5.07it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:01, 260.24it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:00, 358.00it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 411.59it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:00<00:00, 445.32it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:00<00:00, 474.25it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 476.91it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8436519503593445
current iteration best possible performance (full train run):  0.8190000000000001
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104, 0.8477389812469482, 0.8454647064208984, 0.8436519503593445]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1093 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.8001005956010043
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.8013689948356251
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.7607302056554084
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.7959944133260557
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.7738395438243629
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0319, dtype=torch.float64), tensor(0.0290, dtype=torch.float64), tensor(0.3408, dtype=torch.float64), 0, tensor(0.1869, dtype=torch.float64), tensor(0.4114, dtype=torch.float64), 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.025326775069970914, 1.480000019073487, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0319, dtype=torch.float64), tensor(0.0290, dtype=torch.float64), tensor(0.3408, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1869, dtype=torch.float64), tensor(0.4114, dtype=torch.float64), tensor(2.2424e-17, dtype=torch.float64), tensor(3.5181e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2533, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.032
  rowan_hellaswag: 0.029
  sciq: 0.341
  triviaqa: 0
  truthfulqa_gen: 0.187
  wikitext: 0.411
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.025326775069970914,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.025326775069970914
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 258.81it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 431.31it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 502.48it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 545.88it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 575.89it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 598.37it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 586.57it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.1726, 'grad_norm': 0.3264887034893036, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.2204861640930176, 'eval_runtime': 8.8278, 'eval_samples_per_second': 113.166, 'eval_steps_per_second': 7.137, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 257.12it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 428.20it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 498.80it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 542.59it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 573.35it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 596.84it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 584.11it/s]
Evaluation performance at step 50: 0.74
{'loss': 2.5354, 'grad_norm': 0.3299735486507416, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.74}
{'eval_loss': 1.8999253511428833, 'eval_runtime': 8.8308, 'eval_samples_per_second': 113.126, 'eval_steps_per_second': 7.134, 'epoch': 0.08}
{'loss': 1.7855, 'grad_norm': 0.16148319840431213, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6575486660003662, 'eval_runtime': 8.8469, 'eval_samples_per_second': 112.921, 'eval_steps_per_second': 7.121, 'epoch': 0.12}
{'loss': 1.7247, 'grad_norm': 0.06200920790433884, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5247236490249634, 'eval_runtime': 8.8778, 'eval_samples_per_second': 112.527, 'eval_steps_per_second': 7.096, 'epoch': 0.16}
{'loss': 1.4734, 'grad_norm': 0.08351767063140869, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4230883121490479, 'eval_runtime': 8.8974, 'eval_samples_per_second': 112.28, 'eval_steps_per_second': 7.081, 'epoch': 0.2}
{'loss': 1.4372, 'grad_norm': 0.06700736284255981, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3936996459960938, 'eval_runtime': 8.9067, 'eval_samples_per_second': 112.163, 'eval_steps_per_second': 7.073, 'epoch': 0.24}
{'loss': 1.4766, 'grad_norm': 0.12037286162376404, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3761112689971924, 'eval_runtime': 8.8994, 'eval_samples_per_second': 112.254, 'eval_steps_per_second': 7.079, 'epoch': 0.28}
{'loss': 1.4341, 'grad_norm': 0.08947217464447021, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3622928857803345, 'eval_runtime': 8.8999, 'eval_samples_per_second': 112.248, 'eval_steps_per_second': 7.079, 'epoch': 0.32}
{'loss': 1.4764, 'grad_norm': 0.07621736079454422, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.349692940711975, 'eval_runtime': 8.8937, 'eval_samples_per_second': 112.326, 'eval_steps_per_second': 7.084, 'epoch': 0.36}
{'loss': 1.4209, 'grad_norm': 0.0651264488697052, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.341794729232788, 'eval_runtime': 8.8821, 'eval_samples_per_second': 112.473, 'eval_steps_per_second': 7.093, 'epoch': 0.4}
{'loss': 1.4119, 'grad_norm': 0.07321616262197495, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3356772661209106, 'eval_runtime': 8.8827, 'eval_samples_per_second': 112.466, 'eval_steps_per_second': 7.092, 'epoch': 0.44}
{'loss': 1.3044, 'grad_norm': 0.0667325034737587, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.328569769859314, 'eval_runtime': 8.886, 'eval_samples_per_second': 112.425, 'eval_steps_per_second': 7.09, 'epoch': 0.48}
{'loss': 1.4364, 'grad_norm': 0.08712472766637802, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3222687244415283, 'eval_runtime': 8.8831, 'eval_samples_per_second': 112.461, 'eval_steps_per_second': 7.092, 'epoch': 0.52}
{'loss': 1.3663, 'grad_norm': 0.09113381803035736, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3188540935516357, 'eval_runtime': 8.8851, 'eval_samples_per_second': 112.435, 'eval_steps_per_second': 7.091, 'epoch': 0.56}
{'loss': 1.3757, 'grad_norm': 0.07122069597244263, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3115073442459106, 'eval_runtime': 8.8766, 'eval_samples_per_second': 112.544, 'eval_steps_per_second': 7.097, 'epoch': 0.6}
{'loss': 1.4079, 'grad_norm': 0.09623979777097702, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3066149950027466, 'eval_runtime': 8.8759, 'eval_samples_per_second': 112.551, 'eval_steps_per_second': 7.098, 'epoch': 0.64}
{'loss': 1.4762, 'grad_norm': 0.07538539916276932, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.301984190940857, 'eval_runtime': 8.8881, 'eval_samples_per_second': 112.398, 'eval_steps_per_second': 7.088, 'epoch': 0.68}
{'loss': 1.361, 'grad_norm': 0.0765051618218422, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2989130020141602, 'eval_runtime': 8.8894, 'eval_samples_per_second': 112.382, 'eval_steps_per_second': 7.087, 'epoch': 0.72}
{'loss': 1.3088, 'grad_norm': 0.08043273538351059, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2957144975662231, 'eval_runtime': 8.8926, 'eval_samples_per_second': 112.34, 'eval_steps_per_second': 7.085, 'epoch': 0.76}
{'loss': 1.3894, 'grad_norm': 0.091044582426548, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.292924404144287, 'eval_runtime': 8.9002, 'eval_samples_per_second': 112.245, 'eval_steps_per_second': 7.078, 'epoch': 0.8}
{'loss': 1.3544, 'grad_norm': 0.07690361887216568, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2907766103744507, 'eval_runtime': 8.8775, 'eval_samples_per_second': 112.532, 'eval_steps_per_second': 7.097, 'epoch': 0.84}
{'loss': 1.3146, 'grad_norm': 0.14951688051223755, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2875615358352661, 'eval_runtime': 8.8832, 'eval_samples_per_second': 112.459, 'eval_steps_per_second': 7.092, 'epoch': 0.88}
{'loss': 1.3821, 'grad_norm': 0.08028892427682877, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2860428094863892, 'eval_runtime': 8.8969, 'eval_samples_per_second': 112.286, 'eval_steps_per_second': 7.081, 'epoch': 0.92}
{'loss': 1.4504, 'grad_norm': 0.07579874992370605, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2851165533065796, 'eval_runtime': 8.8887, 'eval_samples_per_second': 112.39, 'eval_steps_per_second': 7.088, 'epoch': 0.96}
{'loss': 1.2117, 'grad_norm': 0.09464550763368607, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2846473455429077, 'eval_runtime': 8.8905, 'eval_samples_per_second': 112.368, 'eval_steps_per_second': 7.086, 'epoch': 1.0}
{'train_runtime': 467.9807, 'train_samples_per_second': 21.364, 'train_steps_per_second': 1.336, 'train_loss': 1.5795215515136718, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2204861640930176, 1.8999253511428833, 1.6575486660003662, 1.5247236490249634, 1.4230883121490479, 1.3936996459960938, 1.3761112689971924, 1.3622928857803345, 1.349692940711975, 1.341794729232788, 1.3356772661209106, 1.328569769859314, 1.3222687244415283, 1.3188540935516357, 1.3115073442459106, 1.3066149950027466, 1.301984190940857, 1.2989130020141602, 1.2957144975662231, 1.292924404144287, 1.2907766103744507, 1.2875615358352661, 1.2860428094863892, 1.2851165533065796, 1.2846473455429077], 'performance': [0.74, 0.74]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:16,  2.54it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 170.77it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 264.06it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 322.20it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 362.94it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 395.32it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 367.09it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.74]
current iteration observed (possibly low-fid or predicted) performance:  0.8481879830360413
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104, 0.8477389812469482, 0.8454647064208984, 0.8436519503593445, 0.8481879830360413]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.9896 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.8067547741544757
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.7631851827264104
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.7440019888886871
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.8085264470615336
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.8057086185995779
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.2328, dtype=torch.float64), tensor(0.0132, dtype=torch.float64), tensor(0.3001, dtype=torch.float64), tensor(0.3345, dtype=torch.float64), 0, tensor(0.1194, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.05337391759710075, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(3.5463e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5262e-17, dtype=torch.float64), tensor(0.2328, dtype=torch.float64), tensor(0.0132, dtype=torch.float64), tensor(0.3001, dtype=torch.float64), tensor(0.3345, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1194, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5337, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.233
  triviaqa: 0.013
  truthfulqa_gen: 0.3
  wikitext: 0.335
  mmlu: 0
  arc_challenge: 0.119

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05337391759710075,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.05337391759710075
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 287.43it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 479.13it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 559.23it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 607.42it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 639.59it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 663.21it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 651.31it/s]
Evaluation performance at step 25: 0.74
{'loss': 4.4718, 'grad_norm': 0.16211017966270447, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.74}
{'eval_loss': 3.6074469089508057, 'eval_runtime': 7.3773, 'eval_samples_per_second': 135.415, 'eval_steps_per_second': 8.54, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 288.82it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 480.37it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 556.74it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 606.10it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 639.65it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 665.64it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 652.37it/s]
Evaluation performance at step 50: 0.76
{'loss': 2.7503, 'grad_norm': 0.0695384070277214, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.76}
{'eval_loss': 2.1255874633789062, 'eval_runtime': 7.4043, 'eval_samples_per_second': 134.921, 'eval_steps_per_second': 8.509, 'epoch': 0.08}
{'loss': 1.9795, 'grad_norm': 0.12679751217365265, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7661505937576294, 'eval_runtime': 7.441, 'eval_samples_per_second': 134.256, 'eval_steps_per_second': 8.467, 'epoch': 0.12}
{'loss': 1.7164, 'grad_norm': 0.06721657514572144, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6202332973480225, 'eval_runtime': 7.4301, 'eval_samples_per_second': 134.452, 'eval_steps_per_second': 8.479, 'epoch': 0.16}
{'loss': 1.6497, 'grad_norm': 0.04521828144788742, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5589426755905151, 'eval_runtime': 7.4479, 'eval_samples_per_second': 134.131, 'eval_steps_per_second': 8.459, 'epoch': 0.2}
{'loss': 1.5089, 'grad_norm': 0.05335327610373497, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.519562005996704, 'eval_runtime': 7.4608, 'eval_samples_per_second': 133.9, 'eval_steps_per_second': 8.444, 'epoch': 0.24}
{'loss': 1.4522, 'grad_norm': 0.053891491144895554, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4871702194213867, 'eval_runtime': 7.4582, 'eval_samples_per_second': 133.946, 'eval_steps_per_second': 8.447, 'epoch': 0.28}
{'loss': 1.4561, 'grad_norm': 0.0559736005961895, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4590349197387695, 'eval_runtime': 7.4554, 'eval_samples_per_second': 133.997, 'eval_steps_per_second': 8.45, 'epoch': 0.32}
{'loss': 1.5412, 'grad_norm': 0.04670063033699989, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4408209323883057, 'eval_runtime': 7.4771, 'eval_samples_per_second': 133.608, 'eval_steps_per_second': 8.426, 'epoch': 0.36}
{'loss': 1.4631, 'grad_norm': 0.05057786777615547, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4271289110183716, 'eval_runtime': 7.4618, 'eval_samples_per_second': 133.881, 'eval_steps_per_second': 8.443, 'epoch': 0.4}
{'loss': 1.4484, 'grad_norm': 0.058349840342998505, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4115110635757446, 'eval_runtime': 7.4575, 'eval_samples_per_second': 133.959, 'eval_steps_per_second': 8.448, 'epoch': 0.44}
{'loss': 1.4239, 'grad_norm': 0.058686960488557816, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.398977518081665, 'eval_runtime': 7.4592, 'eval_samples_per_second': 133.929, 'eval_steps_per_second': 8.446, 'epoch': 0.48}
{'loss': 1.4601, 'grad_norm': 0.06410224735736847, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3876688480377197, 'eval_runtime': 7.4563, 'eval_samples_per_second': 133.98, 'eval_steps_per_second': 8.449, 'epoch': 0.52}
{'loss': 1.4111, 'grad_norm': 0.057342905551195145, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3738431930541992, 'eval_runtime': 7.4558, 'eval_samples_per_second': 133.99, 'eval_steps_per_second': 8.45, 'epoch': 0.56}
{'loss': 1.4412, 'grad_norm': 0.0670696496963501, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3633191585540771, 'eval_runtime': 7.4644, 'eval_samples_per_second': 133.835, 'eval_steps_per_second': 8.44, 'epoch': 0.6}
{'loss': 1.4061, 'grad_norm': 0.07760300487279892, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.350822925567627, 'eval_runtime': 7.461, 'eval_samples_per_second': 133.897, 'eval_steps_per_second': 8.444, 'epoch': 0.64}
{'loss': 1.3945, 'grad_norm': 0.07791014015674591, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3374898433685303, 'eval_runtime': 7.4649, 'eval_samples_per_second': 133.827, 'eval_steps_per_second': 8.44, 'epoch': 0.68}
{'loss': 1.399, 'grad_norm': 0.07040820270776749, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3225383758544922, 'eval_runtime': 7.4673, 'eval_samples_per_second': 133.783, 'eval_steps_per_second': 8.437, 'epoch': 0.72}
{'loss': 1.3255, 'grad_norm': 0.07671432942152023, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.301770567893982, 'eval_runtime': 7.4562, 'eval_samples_per_second': 133.982, 'eval_steps_per_second': 8.449, 'epoch': 0.76}
{'loss': 1.3696, 'grad_norm': 0.07071925699710846, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2813223600387573, 'eval_runtime': 7.4626, 'eval_samples_per_second': 133.867, 'eval_steps_per_second': 8.442, 'epoch': 0.8}
{'loss': 1.3227, 'grad_norm': 0.07743397355079651, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2669494152069092, 'eval_runtime': 7.4703, 'eval_samples_per_second': 133.73, 'eval_steps_per_second': 8.433, 'epoch': 0.84}
{'loss': 1.3412, 'grad_norm': 0.061362288892269135, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2592445611953735, 'eval_runtime': 7.4718, 'eval_samples_per_second': 133.702, 'eval_steps_per_second': 8.432, 'epoch': 0.88}
{'loss': 1.2908, 'grad_norm': 0.07534264028072357, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2555348873138428, 'eval_runtime': 7.462, 'eval_samples_per_second': 133.878, 'eval_steps_per_second': 8.443, 'epoch': 0.92}
{'loss': 1.3251, 'grad_norm': 0.0627649649977684, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2539470195770264, 'eval_runtime': 7.4757, 'eval_samples_per_second': 133.634, 'eval_steps_per_second': 8.427, 'epoch': 0.96}
{'loss': 1.2631, 'grad_norm': 0.08266312628984451, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2533159255981445, 'eval_runtime': 7.4729, 'eval_samples_per_second': 133.683, 'eval_steps_per_second': 8.43, 'epoch': 1.0}
{'train_runtime': 384.8407, 'train_samples_per_second': 25.977, 'train_steps_per_second': 1.624, 'train_loss': 1.6244530151367187, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6074469089508057, 2.1255874633789062, 1.7661505937576294, 1.6202332973480225, 1.5589426755905151, 1.519562005996704, 1.4871702194213867, 1.4590349197387695, 1.4408209323883057, 1.4271289110183716, 1.4115110635757446, 1.398977518081665, 1.3876688480377197, 1.3738431930541992, 1.3633191585540771, 1.350822925567627, 1.3374898433685303, 1.3225383758544922, 1.301770567893982, 1.2813223600387573, 1.2669494152069092, 1.2592445611953735, 1.2555348873138428, 1.2539470195770264, 1.2533159255981445], 'performance': [0.74, 0.76]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:26,  1.88it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:03, 138.18it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 234.61it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 306.74it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 360.00it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 404.29it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 344.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.74, 0.76]
current iteration observed (possibly low-fid or predicted) performance:  0.8483567237854004
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8295000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104, 0.8477389812469482, 0.8454647064208984, 0.8436519503593445, 0.8481879830360413, 0.8483567237854004]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8073 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.8066163476181428
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.8017754358586429
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.7798467441690674
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.8063606471043393
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.8064950593936695
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [0, tensor(0.2714, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7286, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 0, 1, 25, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.7647e-17, dtype=torch.float64), tensor(0.2714, dtype=torch.float64), tensor(2.1323e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3353e-16, dtype=torch.float64), tensor(2.9544e-17, dtype=torch.float64), tensor(0.7286, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1989, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.271
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.729
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (25,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  25
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 14,745,600 || all params: 8,045,006,848 || trainable%: 0.1833
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 294.10it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 488.49it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 568.55it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 618.59it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 656.03it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 683.39it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 668.27it/s]
Evaluation performance at step 25: 0.75
{'loss': 2.5625, 'grad_norm': 0.3982921540737152, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.75}
{'eval_loss': 2.1316816806793213, 'eval_runtime': 8.9705, 'eval_samples_per_second': 111.365, 'eval_steps_per_second': 7.023, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 294.17it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 489.77it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 570.76it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 619.07it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 654.27it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 681.31it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 667.32it/s]
Evaluation performance at step 50: 0.78
{'loss': 1.903, 'grad_norm': 0.5133058428764343, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.78}
{'eval_loss': 1.6722371578216553, 'eval_runtime': 8.9859, 'eval_samples_per_second': 111.174, 'eval_steps_per_second': 7.011, 'epoch': 0.08}
{'loss': 1.6707, 'grad_norm': 0.44343292713165283, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.59617280960083, 'eval_runtime': 9.0089, 'eval_samples_per_second': 110.891, 'eval_steps_per_second': 6.993, 'epoch': 0.12}
{'loss': 1.4925, 'grad_norm': 0.5019626617431641, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5213561058044434, 'eval_runtime': 9.0419, 'eval_samples_per_second': 110.485, 'eval_steps_per_second': 6.968, 'epoch': 0.16}
{'loss': 1.5656, 'grad_norm': 0.5901336669921875, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4928157329559326, 'eval_runtime': 9.0737, 'eval_samples_per_second': 110.098, 'eval_steps_per_second': 6.943, 'epoch': 0.2}
{'loss': 1.5768, 'grad_norm': 0.5211019515991211, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4775757789611816, 'eval_runtime': 9.0796, 'eval_samples_per_second': 110.027, 'eval_steps_per_second': 6.939, 'epoch': 0.24}
{'loss': 1.5055, 'grad_norm': 0.5597267150878906, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4614752531051636, 'eval_runtime': 9.059, 'eval_samples_per_second': 110.278, 'eval_steps_per_second': 6.954, 'epoch': 0.28}
{'loss': 1.5154, 'grad_norm': 0.4314822852611542, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4520076513290405, 'eval_runtime': 9.0602, 'eval_samples_per_second': 110.262, 'eval_steps_per_second': 6.953, 'epoch': 0.32}
{'loss': 1.5573, 'grad_norm': 0.3967576324939728, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.443387508392334, 'eval_runtime': 9.0674, 'eval_samples_per_second': 110.175, 'eval_steps_per_second': 6.948, 'epoch': 0.36}
{'loss': 1.5073, 'grad_norm': 0.376006156206131, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4279311895370483, 'eval_runtime': 9.0615, 'eval_samples_per_second': 110.247, 'eval_steps_per_second': 6.953, 'epoch': 0.4}
{'loss': 1.5116, 'grad_norm': 0.4612267315387726, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4151835441589355, 'eval_runtime': 9.0735, 'eval_samples_per_second': 110.101, 'eval_steps_per_second': 6.943, 'epoch': 0.44}
{'loss': 1.5693, 'grad_norm': 0.4390525817871094, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4054415225982666, 'eval_runtime': 9.0863, 'eval_samples_per_second': 109.946, 'eval_steps_per_second': 6.934, 'epoch': 0.48}
{'loss': 1.3991, 'grad_norm': 0.5240419507026672, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.397942066192627, 'eval_runtime': 9.0798, 'eval_samples_per_second': 110.024, 'eval_steps_per_second': 6.938, 'epoch': 0.52}
{'loss': 1.4704, 'grad_norm': 0.4569077491760254, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3902090787887573, 'eval_runtime': 9.0721, 'eval_samples_per_second': 110.118, 'eval_steps_per_second': 6.944, 'epoch': 0.56}
{'loss': 1.4955, 'grad_norm': 0.5003593564033508, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3813666105270386, 'eval_runtime': 9.0743, 'eval_samples_per_second': 110.091, 'eval_steps_per_second': 6.943, 'epoch': 0.6}
{'loss': 1.3632, 'grad_norm': 0.48756131529808044, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.371476650238037, 'eval_runtime': 9.0933, 'eval_samples_per_second': 109.861, 'eval_steps_per_second': 6.928, 'epoch': 0.64}
{'loss': 1.5197, 'grad_norm': 0.6283814311027527, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3666291236877441, 'eval_runtime': 9.0746, 'eval_samples_per_second': 110.087, 'eval_steps_per_second': 6.942, 'epoch': 0.68}
{'loss': 1.4068, 'grad_norm': 0.475896418094635, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3593226671218872, 'eval_runtime': 9.0831, 'eval_samples_per_second': 109.984, 'eval_steps_per_second': 6.936, 'epoch': 0.72}
{'loss': 1.3777, 'grad_norm': 0.3928309381008148, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3522683382034302, 'eval_runtime': 9.0875, 'eval_samples_per_second': 109.931, 'eval_steps_per_second': 6.933, 'epoch': 0.76}
{'loss': 1.4037, 'grad_norm': 0.6304898858070374, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3476393222808838, 'eval_runtime': 9.0705, 'eval_samples_per_second': 110.137, 'eval_steps_per_second': 6.946, 'epoch': 0.8}
{'loss': 1.4385, 'grad_norm': 0.6039858460426331, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.343571662902832, 'eval_runtime': 9.0672, 'eval_samples_per_second': 110.178, 'eval_steps_per_second': 6.948, 'epoch': 0.84}
{'loss': 1.4278, 'grad_norm': 0.48550844192504883, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3394962549209595, 'eval_runtime': 9.0593, 'eval_samples_per_second': 110.274, 'eval_steps_per_second': 6.954, 'epoch': 0.88}
{'loss': 1.4411, 'grad_norm': 0.5813817977905273, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3358404636383057, 'eval_runtime': 9.0632, 'eval_samples_per_second': 110.227, 'eval_steps_per_second': 6.951, 'epoch': 0.92}
{'loss': 1.385, 'grad_norm': 0.7799831032752991, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3341002464294434, 'eval_runtime': 9.047, 'eval_samples_per_second': 110.424, 'eval_steps_per_second': 6.964, 'epoch': 0.96}
{'loss': 1.4807, 'grad_norm': 0.5754064321517944, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3336771726608276, 'eval_runtime': 9.0581, 'eval_samples_per_second': 110.288, 'eval_steps_per_second': 6.955, 'epoch': 1.0}
{'train_runtime': 454.3756, 'train_samples_per_second': 22.006, 'train_steps_per_second': 1.376, 'train_loss': 1.5418717834472657, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1316816806793213, 1.6722371578216553, 1.59617280960083, 1.5213561058044434, 1.4928157329559326, 1.4775757789611816, 1.4614752531051636, 1.4520076513290405, 1.443387508392334, 1.4279311895370483, 1.4151835441589355, 1.4054415225982666, 1.397942066192627, 1.3902090787887573, 1.3813666105270386, 1.371476650238037, 1.3666291236877441, 1.3593226671218872, 1.3522683382034302, 1.3476393222808838, 1.343571662902832, 1.3394962549209595, 1.3358404636383057, 1.3341002464294434, 1.3336771726608276], 'performance': [0.75, 0.78]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<03:35,  2.31it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 160.29it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 261.17it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:00<00:00, 332.88it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 383.47it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 426.05it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 377.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.75, 0.78]
current iteration observed (possibly low-fid or predicted) performance:  0.8015010356903076
current iteration best possible performance (full train run):  0.8400000000000001
max performance so far:  0.8400000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104, 0.8477389812469482, 0.8454647064208984, 0.8436519503593445, 0.8481879830360413, 0.8483567237854004, 0.8015010356903076]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1855 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.8032131850422727
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.8132917368399778
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.8132145593371332
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.8112573612635311
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.7827129644224278
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0224, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(0.0573, dtype=torch.float64), tensor(0.1581, dtype=torch.float64), tensor(0.2655, dtype=torch.float64), tensor(0.0402, dtype=torch.float64), tensor(0.1441, dtype=torch.float64), tensor(0.1379, dtype=torch.float64), tensor(0.1448, dtype=torch.float64), 32, 0, 0, 0, 1, 1, 2, 0.022213642014250776, 18.45692836476574, 0]
normalized proposed parameters for next round by BO: [tensor(0.0224, dtype=torch.float64), tensor(0.0296, dtype=torch.float64), tensor(0.0573, dtype=torch.float64), tensor(0.1581, dtype=torch.float64), tensor(0.2655, dtype=torch.float64), tensor(0.0402, dtype=torch.float64), tensor(0.1441, dtype=torch.float64), tensor(0.1379, dtype=torch.float64), tensor(0.1448, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.2221, dtype=torch.float64), tensor(0.3845, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.022
  gsm8k: 0.03
  rowan_hellaswag: 0.057
  sciq: 0.158
  triviaqa: 0.265
  truthfulqa_gen: 0.04
  wikitext: 0.144
  mmlu: 0.138
  arc_challenge: 0.145

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.022213642014250776,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (18.45692836476574,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  2
lora dropout:  0.022213642014250776
lora alpha:  18.45692836476574
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,359,296 || all params: 8,032,620,544 || trainable%: 0.0294
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'commonsense_qa': (1.0, 'acc,none')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 272.29it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 454.18it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 529.74it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 575.10it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 606.98it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 631.04it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 618.99it/s]
Evaluation performance at step 25: 0.72
{'loss': 3.3239, 'grad_norm': 5.257900238037109, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.72}
{'eval_loss': 2.1394238471984863, 'eval_runtime': 8.8734, 'eval_samples_per_second': 112.584, 'eval_steps_per_second': 7.1, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 3
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   8%|‚ñä         | 41/500 [00:00<00:01, 270.30it/s]Running loglikelihood requests:  24%|‚ñà‚ñà‚ñç       | 121/500 [00:00<00:00, 452.53it/s]Running loglikelihood requests:  40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [00:00<00:00, 528.66it/s]Running loglikelihood requests:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [00:00<00:00, 574.68it/s]Running loglikelihood requests:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [00:00<00:00, 606.86it/s]Running loglikelihood requests:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [00:00<00:00, 631.49it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 618.59it/s]
Evaluation performance at step 50: 0.77
{'loss': 1.6603, 'grad_norm': 5.334163665771484, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.77}
{'eval_loss': 1.4272445440292358, 'eval_runtime': 8.9238, 'eval_samples_per_second': 111.948, 'eval_steps_per_second': 7.06, 'epoch': 0.08}
{'loss': 1.3715, 'grad_norm': 1.3539574146270752, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3048871755599976, 'eval_runtime': 8.959, 'eval_samples_per_second': 111.508, 'eval_steps_per_second': 7.032, 'epoch': 0.12}
{'loss': 1.293, 'grad_norm': 1.3802489042282104, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.257470965385437, 'eval_runtime': 8.9816, 'eval_samples_per_second': 111.227, 'eval_steps_per_second': 7.014, 'epoch': 0.16}
{'loss': 1.3083, 'grad_norm': 1.0759751796722412, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2297683954238892, 'eval_runtime': 8.9977, 'eval_samples_per_second': 111.029, 'eval_steps_per_second': 7.002, 'epoch': 0.2}
{'loss': 1.2235, 'grad_norm': 1.0517220497131348, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2137442827224731, 'eval_runtime': 8.9988, 'eval_samples_per_second': 111.015, 'eval_steps_per_second': 7.001, 'epoch': 0.24}
{'loss': 1.2764, 'grad_norm': 1.1105612516403198, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1978721618652344, 'eval_runtime': 8.9891, 'eval_samples_per_second': 111.135, 'eval_steps_per_second': 7.009, 'epoch': 0.28}
{'loss': 1.2146, 'grad_norm': 1.3085360527038574, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1887402534484863, 'eval_runtime': 8.9985, 'eval_samples_per_second': 111.018, 'eval_steps_per_second': 7.001, 'epoch': 0.32}
{'loss': 1.1925, 'grad_norm': 1.1347930431365967, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1797772645950317, 'eval_runtime': 9.0001, 'eval_samples_per_second': 110.998, 'eval_steps_per_second': 7.0, 'epoch': 0.36}
{'loss': 1.1832, 'grad_norm': 1.4652632474899292, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1709775924682617, 'eval_runtime': 8.992, 'eval_samples_per_second': 111.099, 'eval_steps_per_second': 7.006, 'epoch': 0.4}
{'loss': 1.2142, 'grad_norm': 2.4558868408203125, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1617510318756104, 'eval_runtime': 8.9918, 'eval_samples_per_second': 111.101, 'eval_steps_per_second': 7.006, 'epoch': 0.44}
{'loss': 1.2338, 'grad_norm': 1.1295045614242554, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1521964073181152, 'eval_runtime': 8.9967, 'eval_samples_per_second': 111.04, 'eval_steps_per_second': 7.003, 'epoch': 0.48}
{'loss': 1.1951, 'grad_norm': 1.0155402421951294, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1473973989486694, 'eval_runtime': 8.9868, 'eval_samples_per_second': 111.163, 'eval_steps_per_second': 7.01, 'epoch': 0.52}
{'loss': 1.18, 'grad_norm': 1.4476388692855835, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.140683650970459, 'eval_runtime': 8.9981, 'eval_samples_per_second': 111.024, 'eval_steps_per_second': 7.002, 'epoch': 0.56}
{'loss': 1.1959, 'grad_norm': 0.9943029880523682, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1329877376556396, 'eval_runtime': 8.9903, 'eval_samples_per_second': 111.12, 'eval_steps_per_second': 7.008, 'epoch': 0.6}
{'loss': 1.1959, 'grad_norm': 1.0640430450439453, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.125052809715271, 'eval_runtime': 8.9894, 'eval_samples_per_second': 111.131, 'eval_steps_per_second': 7.008, 'epoch': 0.64}
{'loss': 1.1618, 'grad_norm': 0.9089392423629761, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1179651021957397, 'eval_runtime': 9.0108, 'eval_samples_per_second': 110.867, 'eval_steps_per_second': 6.992, 'epoch': 0.68}
{'loss': 1.2202, 'grad_norm': 1.1180434226989746, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1138049364089966, 'eval_runtime': 9.0167, 'eval_samples_per_second': 110.794, 'eval_steps_per_second': 6.987, 'epoch': 0.72}
{'loss': 1.217, 'grad_norm': 0.9781703352928162, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1091283559799194, 'eval_runtime': 9.0126, 'eval_samples_per_second': 110.845, 'eval_steps_per_second': 6.99, 'epoch': 0.76}
{'loss': 1.1813, 'grad_norm': 1.2316560745239258, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.10377836227417, 'eval_runtime': 9.0177, 'eval_samples_per_second': 110.782, 'eval_steps_per_second': 6.986, 'epoch': 0.8}
{'loss': 1.1607, 'grad_norm': 1.9125268459320068, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1015607118606567, 'eval_runtime': 9.0078, 'eval_samples_per_second': 110.904, 'eval_steps_per_second': 6.994, 'epoch': 0.84}
{'loss': 1.1752, 'grad_norm': 1.1638169288635254, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0975103378295898, 'eval_runtime': 9.0133, 'eval_samples_per_second': 110.836, 'eval_steps_per_second': 6.99, 'epoch': 0.88}
{'loss': 1.1969, 'grad_norm': 1.209550380706787, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0941942930221558, 'eval_runtime': 9.0136, 'eval_samples_per_second': 110.833, 'eval_steps_per_second': 6.989, 'epoch': 0.92}
{'loss': 1.1851, 'grad_norm': 1.096130132675171, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0935114622116089, 'eval_runtime': 9.0059, 'eval_samples_per_second': 110.928, 'eval_steps_per_second': 6.995, 'epoch': 0.96}
{'loss': 1.1954, 'grad_norm': 1.3058236837387085, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0928401947021484, 'eval_runtime': 9.0029, 'eval_samples_per_second': 110.965, 'eval_steps_per_second': 6.998, 'epoch': 1.0}
{'train_runtime': 464.9091, 'train_samples_per_second': 21.501, 'train_steps_per_second': 1.344, 'train_loss': 1.318234814453125, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1394238471984863, 1.4272445440292358, 1.3048871755599976, 1.257470965385437, 1.2297683954238892, 1.2137442827224731, 1.1978721618652344, 1.1887402534484863, 1.1797772645950317, 1.1709775924682617, 1.1617510318756104, 1.1521964073181152, 1.1473973989486694, 1.140683650970459, 1.1329877376556396, 1.125052809715271, 1.1179651021957397, 1.1138049364089966, 1.1091283559799194, 1.10377836227417, 1.1015607118606567, 1.0975103378295898, 1.0941942930221558, 1.0935114622116089, 1.0928401947021484], 'performance': [0.72, 0.77]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['commonsense_qa']
Overwriting default num_fewshot of commonsense_qa from None to 5
Running loglikelihood requests:   0%|          | 0/500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/500 [00:00<04:13,  1.97it/s]Running loglikelihood requests:  16%|‚ñà‚ñå        | 81/500 [00:00<00:02, 148.73it/s]Running loglikelihood requests:  32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:00<00:01, 244.46it/s]Running loglikelihood requests:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [00:01<00:00, 311.57it/s]Running loglikelihood requests:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [00:01<00:00, 359.03it/s]Running loglikelihood requests:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [00:01<00:00, 398.70it/s]Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 349.99it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.72, 0.77]
current iteration observed (possibly low-fid or predicted) performance:  0.8182864785194397
current iteration best possible performance (full train run):  0.798
max performance so far:  0.8400000000000001
BO observations:  [0.822745144367218, 0.831519365310669, 0.7566924095153809, 0.8474709987640381, 0.8477509617805481, 0.8482861518859863, 0.8479990363121033, 0.8468382358551025, 0.8452725410461426, 0.8481103181838989, 0.8476434946060181, 0.8387138843536377, 0.8424214124679565, 0.8473409414291382, 0.8484682440757751, 0.8478282690048218, 0.8484796285629272, 0.8390657901763916, 0.8473272919654846, 0.8476366400718689, 0.848089873790741, 0.847747266292572, 0.8479082584381104, 0.8477389812469482, 0.8454647064208984, 0.8436519503593445, 0.8481879830360413, 0.8483567237854004, 0.8015010356903076, 0.8182864785194397]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8550 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.8118050147236776
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.8000195028815625
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.8118102189205666
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.811176216647739
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.7696919548137816
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0317, dtype=torch.float64), 0, tensor(0.3979, dtype=torch.float64), 0, tensor(0.3047, dtype=torch.float64), tensor(0.2572, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 0, 128, 0.06099240207997519, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(3.3891e-18, dtype=torch.float64), tensor(0.0317, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3979, dtype=torch.float64), tensor(0.0085, dtype=torch.float64), tensor(0.3047, dtype=torch.float64), tensor(0.2572, dtype=torch.float64), tensor(4.1028e-19, dtype=torch.float64), tensor(1.0722e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6099, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.7875000000000001, 0.798, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8400000000000001, 0.8400000000000001]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'commonsense_qa', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_commonsense_qa_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['commonsense_qa'], 'weight': [1.0], 'random': [[0.777, 0.8295000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001], [0.798, 0.798, 0.798, 0.798, 0.798, 0.798, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8085000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001, 0.8190000000000001], [0.7875000000000001, 0.798, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8295000000000001, 0.8400000000000001, 0.8400000000000001]], 'random_full_inputs': [[[0.062256267503948776, 0.0835099612598002, 0, 0.4623022673299022, 0, 0.35388755055565707, 0, 0.03804395335069178, 0, 22, 1, 1, 0, 0, 1, 114, 0.03870725114564932, 11.316541150265632, 0], [0, 0, 0, 0.7660328930775708, 0, 0.23235724710327635, 0, 0, 0, 15, 1, 1, 0, 1, 1, 128, 0.011031599296155284, 9.01917275306552, 0], [0, 0, 0, 0.8535679815491385, 0, 0.14643201845086193, 0, 0, 0, 32, 0, 0, 1, 0, 1, 122, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0.757467096351203, 0, 0.24253290364879704, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0.740395318714578, 0, 0, 0.25960468128542197, 0, 0, 32, 1, 1, 1, 0, 0, 113, 0.0, 1.4800000190734868, 1], [0, 0, 0, 1.0, 0, 0, 0, 0, 0, 32, 1, 1, 1, 0, 0, 109, 0.1, 36.48962017173048, 1], [0, 0, 0.40005491228415807, 0.5999450877158421, 0, 0, 0, 0, 0, 27, 1, 1, 1, 0, 0, 125, 0.1, 1.4800000190734868, 1], [0.21926410287196055, 0, 0, 0.7807358971280395, 0, 0, 0, 0, 0, 26, 1, 0, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 1.0, 0, 0, 0, 0, 0, 19, 1, 1, 1, 0, 0, 108, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0.6988623532891259, 0, 0, 0, 0.3011376467108741, 0, 32, 1, 0, 1, 0, 0, 128, 0.04977896194447279, 1.4800000190734879, 1], [0, 0, 0, 0.7161276621416737, 0, 0, 0, 0, 0.2838723378583262, 32, 1, 0, 1, 0, 0, 128, 0.027575143222777057, 1.4800000190734866, 1], [0, 0, 0, 0.5028913484262586, 0, 0, 0, 0, 0.49710865157374146, 32, 0, 0, 1, 1, 0, 128, 0.1, 1.480000019073488, 1], [0.17978389129485395, 0, 0, 0, 0, 0.7458108099989832, 0, 0.07440529870616268, 0, 32, 1, 0, 1, 0, 0, 128, 0.0004933716822149742, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0.05669745361948857, 0.32091266048667877, 0.6223898858938327, 0, 32, 0, 0, 1, 1, 0, 128, 0.1, 1.480000019073489, 1], [0.1702811872016377, 0, 0, 0, 0, 0, 0, 0.8297188127983619, 0, 32, 0, 1, 1, 1, 0, 128, 1.2446640940133589e-17, 1.4800000190734874, 1], [0.13705457246968836, 0, 0, 0, 0, 0, 0.2518020892318744, 0, 0.6111433382984373, 32, 0, 1, 1, 0, 0, 128, 0.012103003850376465, 1.4800000190734863, 1], [0.1480681791594252, 0, 0.6313169135833562, 0.0654800817324226, 0, 0, 0.15513482552479613, 0, 0, 32, 0, 0, 1, 1, 0, 128, 0.010825838706351955, 1.4800000190734863, 1], [0.2916158826709873, 0, 0, 0.1784037717379272, 0, 0.07730889063695842, 0.4228287465488237, 0, 0.02984270840530321, 32, 1, 0, 1, 1, 0, 128, 0.09400558727506707, 1.4800000190734863, 1], [0, 0, 0, 0.14635644863315506, 0, 0.38409606249971767, 0.46954748886712744, 0, 0, 32, 1, 0, 1, 1, 0, 128, 2.573256931286264e-17, 1.4800000190734925, 1], [0, 0, 0, 0, 0, 0.5781797598377401, 0, 0, 0.4218202401622599, 32, 1, 0, 1, 1, 0, 128, 0.09386065074231442, 1.4800000190734872, 1], [0.1127804156318752, 0, 0, 0.3588542035607823, 0, 0.16800066813894543, 0.19789334051710789, 0, 0.16247137215128915, 32, 1, 0, 1, 1, 0, 128, 0.03989517198000765, 8.254002950226319, 1], [0, 0, 0, 0, 0, 0.18988263922653417, 0.17694937232943742, 0, 0.6330639805615026, 32, 1, 0, 0, 0, 0, 128, 0.004870508447921457, 1.4800000190734905, 1], [0.3903588227003088, 0, 0, 0.046749981273785726, 0, 0.2076018803000198, 0, 0, 0.3552893157258855, 32, 1, 0, 1, 1, 1, 128, 0.07503071888068505, 1.480000019073487, 0], [0, 0, 0, 0.32110136076715573, 0, 0.18535608694051522, 0.44436756709205794, 0, 0.04917498520027105, 32, 0, 1, 1, 0, 0, 128, 4.7342270802550844e-18, 1.4800000190734897, 0], [0, 0, 0, 0.08801772169257306, 0, 0.6475218723852919, 0.19896008317232003, 0, 0.06550032274981428, 32, 0, 1, 1, 0, 0, 128, 0.044193144309328365, 1.4800000190734868, 1], [0.06547966736233243, 0, 0, 0.21846651827182886, 0, 0.18000319052641298, 0.43195713148551673, 0, 0.10409349235390793, 32, 1, 1, 1, 1, 1, 128, 0.006788488747274855, 1.4800000190734863, 1], [0, 0, 0, 0.34368279774815746, 0, 0.4890200145387794, 0, 0, 0.16729718771306373, 32, 0, 0, 1, 1, 0, 128, 0.00029088842590773437, 1.4800000190734885, 1], [0, 0, 0, 0.3638824213501979, 0, 0.6182652203202115, 0, 0, 0.017852358329589146, 32, 1, 0, 1, 0, 0, 128, 0.052100361592288064, 1.4800000190734868, 1], [0, 0.21603344316795242, 0, 0.18208710863180316, 0, 0, 0.6018794482002452, 0, 0, 32, 1, 0, 1, 0, 0, 52, 0.028166568155987, 48.0, 1], [0, 0.35890395852701495, 0.2007198409504505, 0, 0.2674675844487279, 0.03087361732358228, 0.05848104209392822, 0.03449763279514292, 0.047689957227968455, 32, 1, 0, 1, 0, 0, 2, 0.0032466031964377786, 38.44121406207496, 1]], [[0.08234428848345592, 0.06620634519549658, 0, 0.4440290577561982, 0, 0.37982578798024985, 0, 0.027594520584599484, 0, 9, 1, 1, 0, 0, 1, 116, 0.0206892298889877, 9.7280745592472, 0], [0, 0.04332353899645505, 0, 0.771092069197991, 0, 0.18558439180555397, 0, 0, 0, 5, 1, 1, 0, 1, 1, 128, 0.005304672690025761, 7.2908735337414114, 1], [0, 0, 0, 0.7870580272792719, 0, 0.21294197272072823, 0, 0, 0, 32, 1, 1, 0, 0, 1, 128, 0.1, 1.4800000190734877, 0], [0, 0, 0, 0.8193573779006006, 0, 0.18064262209939952, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734928, 1], [0, 0, 0, 0.5574235929329808, 0, 0, 0.4425764070670193, 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.480000019073488, 1], [0, 0, 0.5820912770182555, 0.37553287019697806, 0, 0, 0.04237585278476634, 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0.23319845778137363, 0.347504689999015, 0, 0.41929685221961144, 8, 0, 1, 1, 1, 0, 128, 0.1, 1.4800000190734903, 1], [0, 0, 0, 0, 0, 0.9999999999999998, 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734868, 1], [0, 0, 0, 0, 0, 0.013819100536103694, 0.9861808994638964, 0, 0, 1, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0, 0.06984868483799259, 0, 0.9301513151620076, 32, 1, 1, 1, 1, 0, 14, 0.1, 48.0, 1], [0, 0, 0.044929950460424375, 0.1402645524857831, 0, 0.542964284633847, 0.2718412124199454, 0, 0, 32, 1, 1, 1, 1, 0, 78, 0.1, 1.4800000190734866, 1], [0, 0, 0.08315940237038441, 0.24295476916912476, 0, 0.16971632720025998, 0.11882373704752322, 0.38534576421270755, 0, 32, 1, 1, 1, 0, 1, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0, 0.8378139806514012, 0, 0.16218601934859928, 32, 0, 1, 0, 1, 0, 128, 5.204170427930422e-19, 1.4800000190734868, 0], [0, 0.2424908318560663, 0, 0, 0, 0, 0.088844095277468, 0, 0.6686650728664657, 32, 1, 1, 1, 1, 0, 2, 0.1, 48.0, 1], [0, 0, 0, 0, 0, 0.11359255530072883, 0.8864074446992712, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 21.96940284060174, 1], [0, 0, 0.6239797567759655, 0.06447196421917094, 0, 0.31154827900486326, 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 1.4796357832869638e-18, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0.9999999999999999, 32, 1, 1, 0, 1, 0, 128, 0.08518208392622047, 1.4800000190734908, 1], [0, 0, 0.857208106170862, 0, 0, 0, 0.14279189382913832, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 0], [0, 0, 0, 0.16202924575837177, 0, 0.3690247744000665, 0, 0.4689459798415616, 0, 32, 1, 1, 1, 0, 0, 128, 3.2200804522819496e-18, 1.4800000190734899, 1], [0, 0, 0, 0.2296113602372628, 0, 0.3800275752664294, 0.3903610644963077, 0, 0, 32, 1, 1, 1, 0, 1, 128, 0.03852099331783329, 1.4800000190734914, 1], [0, 0, 0, 0.6179565496806342, 0, 0.33803006452669104, 0.04401338579267475, 0, 0, 32, 0, 0, 1, 1, 0, 128, 0.027222360930444146, 1.4800000190734872, 1], [0, 0, 0, 0.37539104234831056, 0, 0.5975050321134003, 0, 0, 0.027103925538289157, 32, 1, 1, 1, 0, 0, 128, 0.0, 1.480000019073488, 1], [0.10782299662063353, 0, 0, 0.2482730524061876, 0, 0.39758889985937534, 0.13516183061898723, 0, 0.11115322049481549, 32, 1, 1, 1, 0, 0, 128, 0.08105546686148014, 1.4800000190734863, 1], [0.08271559336710083, 0, 0, 0.41164679715520525, 0, 0.16293306793440845, 0, 0, 0.34270454154328545, 32, 1, 1, 1, 1, 0, 128, 0.004062062420920336, 1.4800000190734863, 1], [0.12079823608851192, 0, 0, 0.05231207038593147, 0, 0.4862152616538258, 0.1312220179259976, 0.18560208088720812, 0.0238503330585252, 32, 0, 0, 1, 0, 0, 128, 0.047236545243694375, 1.4800000190734863, 0], [0, 0, 0, 0.19334078488077278, 0, 0.20021918465031074, 0.1365489789523553, 0.040571001094976564, 0.4293200504215844, 32, 1, 0, 1, 0, 0, 128, 0.02994073813896644, 1.4800000190734885, 1], [0.05655547638721996, 0, 0, 0.49115364940444034, 0, 0.3554984047538486, 0.045326309007029976, 0, 0.05146616044745857, 32, 1, 1, 1, 0, 0, 128, 0.020784737501264975, 1.4800000190734863, 0], [0.05146145953781297, 0, 0, 0.24387653832785786, 0, 0.6412099725619371, 0, 0, 0.0580410042535697, 32, 1, 0, 1, 0, 0, 128, 0.060294707117817536, 1.4800000190734863, 1], [0.049552999326658735, 0, 0, 0.3307588499333642, 0, 0.12262891365074782, 0.30624223382389, 0.09081370885806223, 0.10000329440727668, 32, 1, 1, 1, 1, 0, 128, 0.04637037653855247, 1.480000019073489, 1], [0.054084320743023176, 0, 0, 0.4812702273318426, 0, 0.18833315106778378, 0.17180023685165474, 0, 0.1045120640056955, 32, 1, 1, 1, 0, 0, 128, 0.012037809516556952, 1.4800000190734863, 1]], [[0.08234428848345592, 0.06620634519549658, 0, 0.4440290577561982, 0, 0.37982578798024985, 0, 0.027594520584599484, 0, 9, 1, 1, 0, 0, 1, 116, 0.0206892298889877, 9.7280745592472, 0], [0.08102729054219492, 0.037585161472293974, 0, 0.7608711201787106, 0, 0.12051642780680051, 0, 0, 0, 14, 1, 1, 0, 1, 1, 128, 0.0012476809557754617, 12.499865498914977, 1], [0.9307375558575265, 0, 0, 0, 0, 0, 0.06926244414247353, 0, 0, 1, 0, 1, 1, 1, 0, 19, 0.08954039476023562, 10.620718971032126, 1], [0, 0, 0, 0.7450519361142623, 0, 0.25494806388573765, 0, 0, 0, 32, 1, 1, 0, 0, 1, 128, 0.1, 1.4800000190734863, 0], [0, 0, 0, 0.7594143905356224, 0, 0.2405856094643774, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0.6518413332355679, 0, 0, 0.3481586667644321, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0.4222703756962758, 0.5777296243037245, 0, 0, 0, 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0.3907438020182852, 0, 0, 0, 0.6092561979817148, 0, 31, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0.3102679613712565, 0, 0, 0, 0, 0.6897320386287443, 26, 1, 1, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0.08260286909869363, 0, 0, 0.5549233217271802, 0.3624738091741262, 0, 0, 32, 1, 1, 1, 1, 0, 128, 3.4694469519536125e-19, 1.4800000190734894, 1], [0.24628853763812594, 0, 0.24043453497811634, 0, 0, 0.09364326388241784, 0.4196336635013397, 0, 0, 32, 1, 1, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0.16669419829654644, 0, 0, 0.508251859880283, 0.3250539418231704, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.1, 21.006703622464574, 1], [0, 0, 0.5033407949961509, 0, 0, 0, 0.4966592050038491, 0, 0, 22, 0, 0, 1, 1, 0, 128, 0.1, 1.4800000190734863, 1], [0.6429366173985852, 0, 0, 0.3570633826014146, 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 0, 128, 1.170938346284339e-18, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 1.0, 0, 0, 0, 32, 1, 0, 1, 0, 0, 128, 0.09999999999999998, 1.4800000190734866, 1], [0, 0.07204268724283022, 0.1858019211466473, 0.1224876478633428, 0, 0.23983619734178593, 0.18889147005810816, 0, 0.1909400763472858, 32, 0, 0, 1, 1, 0, 128, 0.025506866739419206, 1.4800000190734863, 1], [0, 0.03402925953844735, 0, 0.6947211442615118, 0, 0.2712495962000475, 0, 0, 0, 32, 0, 0, 1, 0, 0, 128, 0.0, 1.4800000190734899, 1], [0, 0.03867664742979292, 0.20538136166486204, 0.23231486427468873, 0, 0.2577773183080672, 0.24684040157065187, 0, 0.019009406751936766, 32, 1, 0, 1, 1, 0, 115, 0.058430151292666745, 1.4800000190734863, 1], [0.6622425844803324, 0.048673282856636815, 0, 0, 0, 0.28908413266303085, 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.1, 1.480000019073488, 1], [0, 0.023106827045888418, 0.42589008258941974, 0, 0, 0.059281192795120376, 0, 0, 0.4917218975695714, 32, 0, 0, 1, 0, 0, 128, 0.023802118985806153, 1.4800000190734863, 1], [0, 0.040098295141434245, 0, 0.16329160651455268, 0, 0.12581224780165576, 0.6396816712813753, 0, 0.030899650794401982, 32, 0, 0, 1, 0, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0.03229074266192222, 0, 0, 0, 0.4673534867318205, 0.0684002792593498, 0.43189740054980347, 0, 32, 1, 1, 1, 0, 0, 128, 2.0318461833433301e-19, 1.4800000190734863, 1], [0, 0.037266363765912726, 0.22311105943091508, 0.14822468083726073, 0, 0.5913978959659116, 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.1, 1.4800000190734868, 1], [0.04097579230490268, 0.024873921940891525, 0, 0, 0, 0.22662690816076148, 0.29931007911111074, 0, 0.40821329848233373, 32, 1, 0, 1, 0, 1, 128, 0.00646839821670096, 1.4800000190734863, 1], [0.013240773496754074, 0, 0.0981881624158401, 0.1801765674752825, 0.011926084797307803, 0.3786536717440119, 0.2729127073170789, 0, 0.04490203275372469, 32, 0, 1, 1, 0, 0, 128, 0.061241185208156794, 5.676478480807267, 1], [0.1967764330699779, 0, 0.03158076023569663, 0.11201654551314436, 0, 0.17157612076780082, 0.08176667930663786, 0.058139342911018264, 0.34814411819572444, 32, 0, 0, 1, 0, 0, 128, 0.058227360196271644, 8.257585355461549, 1], [0, 0.03192916049718165, 0.028962908668798082, 0.3408263202716787, 0, 0.18686863469562734, 0.4114129758667144, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.025326775069970914, 1.480000019073487, 1], [0, 0, 0, 0.2327883783965315, 0.013212472655525396, 0.3000638799050471, 0.3345496510111809, 0, 0.11938561803171495, 32, 1, 0, 1, 0, 0, 128, 0.05337391759710075, 1.4800000190734866, 1], [0, 0.271381011840641, 0, 0, 0, 0, 0.7286189881593587, 0, 0, 32, 0, 0, 0, 0, 1, 25, 0.1, 48.0, 0], [0.02237714581699508, 0.02963383496354784, 0.057333592090730565, 0.15811445333394358, 0.26547101690198893, 0.04018810002813901, 0.14414110310705258, 0.13792864118034237, 0.14481211257725998, 32, 0, 0, 0, 1, 1, 2, 0.022213642014250776, 18.45692836476574, 0]]], 'random_full_train_performance': [0.7875000000000001, 0.798, 0.8295000000000001, 0.7665, 0.7665, 0.7875000000000001, 0.7665, 0.8190000000000001, 0.7875000000000001, 0.7035000000000001, 0.7665, 0.756, 0.777, 0.8085000000000001, 0.8085000000000001, 0.7875000000000001, 0.7875000000000001, 0.7665, 0.8085000000000001, 0.7875000000000001, 0.7875000000000001, 0.8190000000000001, 0.777, 0.8085000000000001, 0.798, 0.8190000000000001, 0.7665, 0.798, 0.8400000000000001, 0.798]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 1.398 MB uploadedwandb: | 1.331 MB of 1.398 MB uploadedwandb: / 1.398 MB of 1.398 MB uploadedwandb: - 1.398 MB of 1.398 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÉ
wandb:            eval/runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÅ‚ñÜ‚ñÇ‚ñÖ‚ñà‚ñà‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñà‚ñá‚ñà‚ñÉ‚ñà‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñá
wandb: eval/samples_per_second ‚ñà‚ñá‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:   eval/steps_per_second ‚ñà‚ñá‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:             train/epoch ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÇ
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÇ
wandb:         train/grad_norm ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÖ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÖ‚ñà‚ñÑ‚ñá‚ñÉ‚ñà‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÖ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÉ
wandb:              train/loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:                eval/loss 1.09284
wandb:             eval/runtime 9.0029
wandb:  eval/samples_per_second 110.965
wandb:    eval/steps_per_second 6.998
wandb:               total_flos 1.0350394505743565e+17
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 1.30582
wandb:      train/learning_rate 0.0
wandb:               train/loss 1.1954
wandb:               train_loss 1.31823
wandb:            train_runtime 464.9091
wandb: train_samples_per_second 21.501
wandb:   train_steps_per_second 1.344
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/77z8f64m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260102_144621-77z8f64m/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

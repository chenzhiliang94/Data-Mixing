2026-01-01 22:41:05.601372: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 22:41:05.631435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 22:41:05.631488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 22:41:05.632332: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 22:41:05.637307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 22:41:06.713431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'truthfulqa_gen', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_truthfulqa_gen_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['truthfulqa_gen']
evaluation tasks and weights:  {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/truthfulqa_gen/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 20, 1, 1, 1, 1, 1, 18, 0.05137445104835325, 2, 1]
Checking history sample input_X_between_0_1:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.5137445104835324, 0.041666666666666664, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 22, 1, 1, 0, 0, 1, 9, 0.09530863992118319, 22, 1]
Checking history sample input_X_between_0_1:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 0.6875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0703125, 0.9530863992118318, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 7, 0, 0, 0, 0, 1, 45, 0.012049704078718804, 22, 1]
Checking history sample input_X_between_0_1:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 0.21875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3515625, 0.12049704078718804, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 2, 1, 1, 1, 0, 0, 32, 0.03789572912213354, 26, 1]
Checking history sample input_X_between_0_1:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.3789572912213354, 0.5416666666666666, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 8, 0, 0, 1, 1, 1, 57, 0.05639372568359048, 47, 0]
Checking history sample input_X_between_0_1:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4453125, 0.5639372568359048, 0.9791666666666666, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 18, 1, 1, 0, 0, 1, 112, 0.0011300351648876107, 2, 1]
Checking history sample input_X_between_0_1:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 0.5625, 1.0, 1.0, 0.0, 0.0, 1.0, 0.875, 0.011300351648876106, 0.041666666666666664, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 11, 0, 1, 1, 0, 1, 121, 0.04409228366491266, 38, 0]
Checking history sample input_X_between_0_1:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 0.34375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9453125, 0.4409228366491266, 0.7916666666666666, 0.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 19, 1, 0, 0, 1, 0, 20, 0.057419890903339765, 17, 1]
Checking history sample input_X_between_0_1:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 0.59375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15625, 0.5741989090333977, 0.3541666666666667, 1.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 13, 0, 1, 0, 0, 1, 54, 0.07044921211215552, 48, 0]
Checking history sample input_X_between_0_1:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 0.40625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.421875, 0.7044921211215551, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 9, 1, 0, 1, 1, 0, 17, 0.07776680881547844, 40, 0]
Checking history sample input_X_between_0_1:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 0.28125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1328125, 0.7776680881547844, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 1, 0, 0, 0, 1, 0, 60, 0.001514616808966751, 14, 1]
Checking history sample input_X_between_0_1:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.46875, 0.015146168089667511, 0.2916666666666667, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 18, 1, 1, 1, 0, 1, 45, 0.001287895623877422, 34, 1]
Checking history sample input_X_between_0_1:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 0.5625, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3515625, 0.01287895623877422, 0.7083333333333334, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 8, 0, 0, 0, 1, 1, 51, 0.08835721159033366, 35, 0]
Checking history sample input_X_between_0_1:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3984375, 0.8835721159033366, 0.7291666666666666, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 2, 0, 0, 0, 1, 1, 28, 0.09450434861769766, 16, 1]
Checking history sample input_X_between_0_1:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21875, 0.9450434861769765, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 2, 0, 0, 0, 1, 0, 27, 0.08742966606550949, 14, 0]
Checking history sample input_X_between_0_1:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2109375, 0.8742966606550948, 0.2916666666666667, 0.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 30, 0, 0, 0, 1, 0, 18, 0.07914275308569024, 23, 0]
Checking history sample input_X_between_0_1:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.140625, 0.7914275308569024, 0.4791666666666667, 0.0]
Checking history sample performance at 625 steps:  0.66
Checking history sample input_X:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 4, 1, 1, 1, 1, 1, 126, 0.005789639303569194, 25, 1]
Checking history sample input_X_between_0_1:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.05789639303569194, 0.5208333333333334, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 9, 1, 1, 1, 1, 1, 31, 0.03322680456132531, 22, 1]
Checking history sample input_X_between_0_1:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2421875, 0.33226804561325307, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 15, 0, 1, 0, 1, 1, 49, 0.0008783405064032635, 29, 1]
Checking history sample input_X_between_0_1:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 0.46875, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3828125, 0.008783405064032634, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.69
Checking history sample input_X:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 1, 0, 1, 1, 0, 1, 106, 0.08696702158391928, 5, 0]
Checking history sample input_X_between_0_1:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 0.03125, 0.0, 1.0, 1.0, 0.0, 1.0, 0.828125, 0.8696702158391928, 0.10416666666666667, 0.0]
Checking history sample performance at 625 steps:  0.46
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1209 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.44494378566741943, 0.870684027671814, 0.6196607947349548, 0.25824791193008423, 0.7217321991920471, 0.27950453758239746, 0.1285102367401123, 0.5691673159599304, 0.6038083434104919, 0.3287566006183624, 0.9936530590057373, 0.9333697557449341, 0.7635009288787842, 0.7091029286384583, 0.3918544054031372, 0.8287849426269531, 0.22841346263885498, 0.43761107325553894, 0.6035021543502808]  ‚Üí  acq = 0.7807377311255721
X = [0.6099547147750854, 0.6162807941436768, 0.05181962251663208, 0.8556089997291565, 0.9393987655639648, 0.8337303996086121, 0.06413918733596802, 0.6154499650001526, 0.4129674434661865, 0.9772079586982727, 0.7303782105445862, 0.836664617061615, 0.8008795380592346, 0.677672266960144, 0.8165919780731201, 0.09528008103370667, 0.13021016120910645, 0.9233152866363525, 0.134593665599823]  ‚Üí  acq = 0.7850678718181774
X = [0.28480076789855957, 0.5154266357421875, 0.4081253409385681, 0.492275595664978, 0.9278949499130249, 0.5173035264015198, 0.3260180354118347, 0.08579903841018677, 0.03100752830505371, 0.8581081032752991, 0.7083061337471008, 0.9962025880813599, 0.48452287912368774, 0.19220125675201416, 0.8536391854286194, 0.8297621011734009, 0.5841216444969177, 0.22646741569042206, 0.03983980417251587]  ‚Üí  acq = 0.7847473940907519
X = [0.7510704398155212, 0.31102919578552246, 0.33490312099456787, 0.24376773834228516, 0.6119269132614136, 0.8870519399642944, 0.2915107011795044, 0.5663825869560242, 0.7175367474555969, 0.7543261647224426, 0.46818745136260986, 0.7646597623825073, 0.7208578586578369, 0.8616017699241638, 0.5292365550994873, 0.7050721049308777, 0.30200058221817017, 0.6794230937957764, 0.034432053565979004]  ‚Üí  acq = 0.7850018445930615
X = [0.08823943138122559, 0.5164797902107239, 0.3000202178955078, 0.15515464544296265, 0.27109211683273315, 0.507201075553894, 0.27235281467437744, 0.47632163763046265, 0.41925638914108276, 0.9017006754875183, 0.6759482622146606, 0.8859538435935974, 0.998011589050293, 0.79975825548172, 0.7751549482345581, 0.3066976070404053, 0.41240036487579346, 0.9937276840209961, 0.546432614326477]  ‚Üí  acq = 0.7983633151746173
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.1122, dtype=torch.float64), tensor(0.0512, dtype=torch.float64), tensor(0.0743, dtype=torch.float64), 0, tensor(0.4313, dtype=torch.float64), tensor(0.0896, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.2130, dtype=torch.float64), 32, 0, 0, 0, 1, 0, 108, 1.7347234759768072e-19, 44.7393382712138, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(1.5541e-17, dtype=torch.float64), tensor(0.1122, dtype=torch.float64), tensor(0.0512, dtype=torch.float64), tensor(0.0743, dtype=torch.float64), tensor(1.3372e-17, dtype=torch.float64), tensor(0.4313, dtype=torch.float64), tensor(0.0896, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.2130, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8476, dtype=torch.float64), tensor(1.7347e-18, dtype=torch.float64), tensor(0.9321, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.112
  rowan_hellaswag: 0.051
  sciq: 0.074
  triviaqa: 0
  truthfulqa_gen: 0.431
  wikitext: 0.09
  mmlu: 0.028
  arc_challenge: 0.213

LoRA Parameters:
  lora_r: (108,)
  lora_dropout: (1.7347234759768072e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (44.7393382712138,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  108
lora dropout:  1.7347234759768072e-19
lora alpha:  44.7393382712138
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 63,700,992 || all params: 8,093,962,240 || trainable%: 0.7870
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_224259-s084mrnh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/s084mrnh
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:58,  1.70it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:07, 10.00it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.78it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 14.95it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 15.44it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 17.52it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 16.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 16.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 17.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 16.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.11it/s]
Evaluation performance at step 25: 0.59
{'loss': 3.0671, 'grad_norm': 0.8926138877868652, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.59}
{'eval_loss': 1.6506366729736328, 'eval_runtime': 8.541, 'eval_samples_per_second': 116.966, 'eval_steps_per_second': 7.376, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:42,  2.33it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:06, 13.16it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.42it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 17.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 20.18it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 21.14it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 23.37it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 17.21it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 19.63it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:03<00:01, 20.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 21.76it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 21.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 23.63it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.70it/s]
Evaluation performance at step 50: 0.67
{'loss': 1.3218, 'grad_norm': 0.4266377389431, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.67}
{'eval_loss': 1.1870081424713135, 'eval_runtime': 8.5451, 'eval_samples_per_second': 116.909, 'eval_steps_per_second': 7.373, 'epoch': 0.08}
{'loss': 1.086, 'grad_norm': 0.26415807008743286, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1096667051315308, 'eval_runtime': 8.5986, 'eval_samples_per_second': 116.182, 'eval_steps_per_second': 7.327, 'epoch': 0.12}
{'loss': 1.0856, 'grad_norm': 0.4111718535423279, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0774319171905518, 'eval_runtime': 8.5539, 'eval_samples_per_second': 116.789, 'eval_steps_per_second': 7.365, 'epoch': 0.16}
{'loss': 1.1202, 'grad_norm': 0.29516932368278503, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0475002527236938, 'eval_runtime': 8.5861, 'eval_samples_per_second': 116.351, 'eval_steps_per_second': 7.337, 'epoch': 0.2}
{'loss': 1.0427, 'grad_norm': 0.2669556438922882, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.026183009147644, 'eval_runtime': 8.6529, 'eval_samples_per_second': 115.453, 'eval_steps_per_second': 7.281, 'epoch': 0.24}
{'loss': 1.0357, 'grad_norm': 0.27006658911705017, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0070757865905762, 'eval_runtime': 8.6578, 'eval_samples_per_second': 115.387, 'eval_steps_per_second': 7.277, 'epoch': 0.28}
{'loss': 1.0121, 'grad_norm': 0.2585667371749878, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9874435067176819, 'eval_runtime': 8.6575, 'eval_samples_per_second': 115.391, 'eval_steps_per_second': 7.277, 'epoch': 0.32}
{'loss': 0.9891, 'grad_norm': 0.2538461685180664, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9762259721755981, 'eval_runtime': 8.6696, 'eval_samples_per_second': 115.23, 'eval_steps_per_second': 7.267, 'epoch': 0.36}
{'loss': 0.9598, 'grad_norm': 0.2653023600578308, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9614413976669312, 'eval_runtime': 8.6565, 'eval_samples_per_second': 115.404, 'eval_steps_per_second': 7.278, 'epoch': 0.4}
{'loss': 0.9833, 'grad_norm': 0.3183627426624298, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9488080739974976, 'eval_runtime': 8.6696, 'eval_samples_per_second': 115.231, 'eval_steps_per_second': 7.267, 'epoch': 0.44}
{'loss': 0.9315, 'grad_norm': 0.3018208146095276, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9320181012153625, 'eval_runtime': 8.6632, 'eval_samples_per_second': 115.315, 'eval_steps_per_second': 7.272, 'epoch': 0.48}
{'loss': 0.9812, 'grad_norm': 0.3410952389240265, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.915295422077179, 'eval_runtime': 8.6751, 'eval_samples_per_second': 115.157, 'eval_steps_per_second': 7.262, 'epoch': 0.52}
{'loss': 0.9551, 'grad_norm': 0.28108155727386475, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9019210934638977, 'eval_runtime': 8.6675, 'eval_samples_per_second': 115.258, 'eval_steps_per_second': 7.268, 'epoch': 0.56}
{'loss': 0.9666, 'grad_norm': 0.3940008282661438, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8913269639015198, 'eval_runtime': 8.6595, 'eval_samples_per_second': 115.364, 'eval_steps_per_second': 7.275, 'epoch': 0.6}
{'loss': 0.9109, 'grad_norm': 0.29005905985832214, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8767178058624268, 'eval_runtime': 8.66, 'eval_samples_per_second': 115.358, 'eval_steps_per_second': 7.275, 'epoch': 0.64}
{'loss': 0.8927, 'grad_norm': 0.43326783180236816, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8652840256690979, 'eval_runtime': 8.6294, 'eval_samples_per_second': 115.767, 'eval_steps_per_second': 7.301, 'epoch': 0.68}
{'loss': 0.8722, 'grad_norm': 0.41279336810112, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8546149730682373, 'eval_runtime': 8.6513, 'eval_samples_per_second': 115.473, 'eval_steps_per_second': 7.282, 'epoch': 0.72}
{'loss': 0.8503, 'grad_norm': 0.33871349692344666, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8459119200706482, 'eval_runtime': 8.6571, 'eval_samples_per_second': 115.397, 'eval_steps_per_second': 7.277, 'epoch': 0.76}
{'loss': 0.9022, 'grad_norm': 0.3531864881515503, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8346785306930542, 'eval_runtime': 8.6419, 'eval_samples_per_second': 115.599, 'eval_steps_per_second': 7.29, 'epoch': 0.8}
{'loss': 0.9443, 'grad_norm': 0.2700853645801544, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8255148530006409, 'eval_runtime': 8.6359, 'eval_samples_per_second': 115.679, 'eval_steps_per_second': 7.295, 'epoch': 0.84}
{'loss': 0.9301, 'grad_norm': 0.3640083372592926, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8179970383644104, 'eval_runtime': 8.656, 'eval_samples_per_second': 115.411, 'eval_steps_per_second': 7.278, 'epoch': 0.88}
{'loss': 0.8642, 'grad_norm': 0.27773037552833557, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.812812089920044, 'eval_runtime': 8.6504, 'eval_samples_per_second': 115.486, 'eval_steps_per_second': 7.283, 'epoch': 0.92}
{'loss': 0.802, 'grad_norm': 0.390921026468277, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8075600862503052, 'eval_runtime': 8.6522, 'eval_samples_per_second': 115.461, 'eval_steps_per_second': 7.281, 'epoch': 0.96}
{'loss': 0.8378, 'grad_norm': 0.37686029076576233, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8063215613365173, 'eval_runtime': 8.6491, 'eval_samples_per_second': 115.504, 'eval_steps_per_second': 7.284, 'epoch': 1.0}
{'train_runtime': 526.5093, 'train_samples_per_second': 18.987, 'train_steps_per_second': 1.187, 'train_loss': 1.0537826629638671, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6506366729736328, 1.1870081424713135, 1.1096667051315308, 1.0774319171905518, 1.0475002527236938, 1.026183009147644, 1.0070757865905762, 0.9874435067176819, 0.9762259721755981, 0.9614413976669312, 0.9488080739974976, 0.9320181012153625, 0.915295422077179, 0.9019210934638977, 0.8913269639015198, 0.8767178058624268, 0.8652840256690979, 0.8546149730682373, 0.8459119200706482, 0.8346785306930542, 0.8255148530006409, 0.8179970383644104, 0.812812089920044, 0.8075600862503052, 0.8063215613365173], 'performance': [0.59, 0.67]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:10,  1.41it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 25.37it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:01, 29.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 32.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 30.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 34.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 29.50it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.59, 0.67]
current iteration observed (possibly low-fid or predicted) performance:  1.4170141220092773
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.7140000000000001
BO observations:  [1.4170141220092773]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4192 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.3653041113344777
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 1.3738549696445737
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.369373446098459
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.3880442850711976
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.358943764213747
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2572, dtype=torch.float64), 0, 0, tensor(0.5387, dtype=torch.float64), tensor(0.2041, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2572, dtype=torch.float64), tensor(3.3583e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5387, dtype=torch.float64), tensor(0.2041, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.257
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.539
  wikitext: 0.204
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:01,  1.61it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.64it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 15.13it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 17.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 18.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 19.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:08,  4.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:05,  6.62it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:03,  8.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01,  9.81it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 11.80it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 13.88it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.95it/s]
Evaluation performance at step 25: 0.52
{'loss': 3.4869, 'grad_norm': 0.6249255537986755, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.52}
{'eval_loss': 2.145261287689209, 'eval_runtime': 9.282, 'eval_samples_per_second': 107.628, 'eval_steps_per_second': 6.787, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:43,  2.27it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 12.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 17.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 18.98it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 20.02it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 18.31it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 16.40it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 18.56it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 19.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:01, 18.94it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 19.38it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 19.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.50it/s]
Evaluation performance at step 50: 0.59
{'loss': 1.8401, 'grad_norm': 0.44771432876586914, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 1.6387051343917847, 'eval_runtime': 9.2819, 'eval_samples_per_second': 107.629, 'eval_steps_per_second': 6.787, 'epoch': 0.08}
{'loss': 1.6096, 'grad_norm': 0.2914676070213318, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5261023044586182, 'eval_runtime': 9.3604, 'eval_samples_per_second': 106.726, 'eval_steps_per_second': 6.73, 'epoch': 0.12}
{'loss': 1.5392, 'grad_norm': 0.2860746681690216, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4875837564468384, 'eval_runtime': 9.3521, 'eval_samples_per_second': 106.821, 'eval_steps_per_second': 6.736, 'epoch': 0.16}
{'loss': 1.4468, 'grad_norm': 0.27432072162628174, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4626723527908325, 'eval_runtime': 9.3471, 'eval_samples_per_second': 106.878, 'eval_steps_per_second': 6.74, 'epoch': 0.2}
{'loss': 1.3969, 'grad_norm': 0.3133249580860138, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4447271823883057, 'eval_runtime': 9.3492, 'eval_samples_per_second': 106.854, 'eval_steps_per_second': 6.739, 'epoch': 0.24}
{'loss': 1.4379, 'grad_norm': 0.25729483366012573, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4185431003570557, 'eval_runtime': 9.3459, 'eval_samples_per_second': 106.892, 'eval_steps_per_second': 6.741, 'epoch': 0.28}
{'loss': 1.4696, 'grad_norm': 0.27863359451293945, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3949452638626099, 'eval_runtime': 9.3571, 'eval_samples_per_second': 106.763, 'eval_steps_per_second': 6.733, 'epoch': 0.32}
{'loss': 1.4041, 'grad_norm': 0.35660040378570557, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3810402154922485, 'eval_runtime': 9.3716, 'eval_samples_per_second': 106.599, 'eval_steps_per_second': 6.722, 'epoch': 0.36}
{'loss': 1.431, 'grad_norm': 0.24396894872188568, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3612086772918701, 'eval_runtime': 9.3704, 'eval_samples_per_second': 106.612, 'eval_steps_per_second': 6.723, 'epoch': 0.4}
{'loss': 1.3202, 'grad_norm': 0.35669752955436707, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3439829349517822, 'eval_runtime': 9.3707, 'eval_samples_per_second': 106.609, 'eval_steps_per_second': 6.723, 'epoch': 0.44}
{'loss': 1.3707, 'grad_norm': 0.27799975872039795, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3278166055679321, 'eval_runtime': 9.4122, 'eval_samples_per_second': 106.139, 'eval_steps_per_second': 6.693, 'epoch': 0.48}
{'loss': 1.3497, 'grad_norm': 0.2833429276943207, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.311517596244812, 'eval_runtime': 9.4313, 'eval_samples_per_second': 105.924, 'eval_steps_per_second': 6.68, 'epoch': 0.52}
{'loss': 1.3712, 'grad_norm': 0.5034223198890686, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3057701587677002, 'eval_runtime': 9.4894, 'eval_samples_per_second': 105.275, 'eval_steps_per_second': 6.639, 'epoch': 0.56}
{'loss': 1.3634, 'grad_norm': 0.32887405157089233, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2898505926132202, 'eval_runtime': 9.4597, 'eval_samples_per_second': 105.606, 'eval_steps_per_second': 6.66, 'epoch': 0.6}
{'loss': 1.3323, 'grad_norm': 0.4176633656024933, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2763009071350098, 'eval_runtime': 9.4794, 'eval_samples_per_second': 105.386, 'eval_steps_per_second': 6.646, 'epoch': 0.64}
{'loss': 1.3674, 'grad_norm': 0.3487592935562134, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.26358163356781, 'eval_runtime': 9.4506, 'eval_samples_per_second': 105.708, 'eval_steps_per_second': 6.666, 'epoch': 0.68}
{'loss': 1.2655, 'grad_norm': 0.35092929005622864, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.255700945854187, 'eval_runtime': 9.4317, 'eval_samples_per_second': 105.919, 'eval_steps_per_second': 6.68, 'epoch': 0.72}
{'loss': 1.1991, 'grad_norm': 0.3253041207790375, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2468717098236084, 'eval_runtime': 9.4378, 'eval_samples_per_second': 105.851, 'eval_steps_per_second': 6.675, 'epoch': 0.76}
{'loss': 1.2796, 'grad_norm': 0.28288400173187256, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2410392761230469, 'eval_runtime': 9.4348, 'eval_samples_per_second': 105.884, 'eval_steps_per_second': 6.677, 'epoch': 0.8}
{'loss': 1.277, 'grad_norm': 0.25781089067459106, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2346749305725098, 'eval_runtime': 9.433, 'eval_samples_per_second': 105.905, 'eval_steps_per_second': 6.679, 'epoch': 0.84}
{'loss': 1.2579, 'grad_norm': 0.3448334038257599, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2279787063598633, 'eval_runtime': 9.4334, 'eval_samples_per_second': 105.9, 'eval_steps_per_second': 6.678, 'epoch': 0.88}
{'loss': 1.2858, 'grad_norm': 0.2445686310529709, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2232391834259033, 'eval_runtime': 9.4338, 'eval_samples_per_second': 105.896, 'eval_steps_per_second': 6.678, 'epoch': 0.92}
{'loss': 1.2337, 'grad_norm': 0.22021794319152832, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2208412885665894, 'eval_runtime': 9.3936, 'eval_samples_per_second': 106.349, 'eval_steps_per_second': 6.707, 'epoch': 0.96}
{'loss': 1.301, 'grad_norm': 0.309583455324173, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2200279235839844, 'eval_runtime': 9.371, 'eval_samples_per_second': 106.605, 'eval_steps_per_second': 6.723, 'epoch': 1.0}
{'train_runtime': 539.3276, 'train_samples_per_second': 18.538, 'train_steps_per_second': 1.159, 'train_loss': 1.4654603515625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.145261287689209, 1.6387051343917847, 1.5261023044586182, 1.4875837564468384, 1.4626723527908325, 1.4447271823883057, 1.4185431003570557, 1.3949452638626099, 1.3810402154922485, 1.3612086772918701, 1.3439829349517822, 1.3278166055679321, 1.311517596244812, 1.3057701587677002, 1.2898505926132202, 1.2763009071350098, 1.26358163356781, 1.255700945854187, 1.2468717098236084, 1.2410392761230469, 1.2346749305725098, 1.2279787063598633, 1.2232391834259033, 1.2208412885665894, 1.2200279235839844], 'performance': [0.52, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:53,  1.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 16.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 25.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 30.10it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 33.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 33.12it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 37.85it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 31.54it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.52, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.4707750082015991
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.7875000000000001
BO observations:  [1.4170141220092773, 1.4707750082015991]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9520 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.3824014616271425
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 1.2247502042037355
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.3308760047145185
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 1.2879694211158064
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 1.2357827519509823
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.9636, dtype=torch.float64), 0, 0, tensor(0.0364, dtype=torch.float64), 32, 1, 0, 0, 1, 0, 128, 0.0, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(5.6837e-17, dtype=torch.float64), tensor(2.8760e-17, dtype=torch.float64), tensor(2.0986e-17, dtype=torch.float64), tensor(7.3163e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9636, dtype=torch.float64), tensor(1.2311e-17, dtype=torch.float64), tensor(1.5494e-17, dtype=torch.float64), tensor(0.0364, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.964
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.036

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 1, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:11,  1.38it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:13,  6.95it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 10.90it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:10,  7.03it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.31it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 13.96it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:02, 15.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:01, 18.35it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.43it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:00, 19.32it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 14.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 15.85it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.40it/s]
Evaluation performance at step 25: 0.51
{'loss': 3.3844, 'grad_norm': 0.8435041904449463, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.51}
{'eval_loss': 1.1749430894851685, 'eval_runtime': 4.5548, 'eval_samples_per_second': 219.331, 'eval_steps_per_second': 13.832, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:52,  1.90it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.70it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.14it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 15.32it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 16.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 17.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 19.32it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 17.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 20.09it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 19.39it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 19.88it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 19.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 18.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.82it/s]
Evaluation performance at step 50: 0.57
{'loss': 0.9087, 'grad_norm': 0.43199649453163147, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 0.7433139681816101, 'eval_runtime': 4.0876, 'eval_samples_per_second': 244.395, 'eval_steps_per_second': 15.412, 'epoch': 0.08}
{'loss': 0.7288, 'grad_norm': 0.3225124478340149, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.6440771818161011, 'eval_runtime': 4.1083, 'eval_samples_per_second': 243.167, 'eval_steps_per_second': 15.335, 'epoch': 0.12}
{'loss': 0.638, 'grad_norm': 0.3307749629020691, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.5646206736564636, 'eval_runtime': 4.1264, 'eval_samples_per_second': 242.102, 'eval_steps_per_second': 15.268, 'epoch': 0.16}
{'loss': 0.5778, 'grad_norm': 0.3357512652873993, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.49992623925209045, 'eval_runtime': 4.1094, 'eval_samples_per_second': 243.099, 'eval_steps_per_second': 15.331, 'epoch': 0.2}
{'loss': 0.4973, 'grad_norm': 0.3793448507785797, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.42282649874687195, 'eval_runtime': 4.1169, 'eval_samples_per_second': 242.658, 'eval_steps_per_second': 15.303, 'epoch': 0.24}
{'loss': 0.4113, 'grad_norm': 0.36012303829193115, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.37491604685783386, 'eval_runtime': 4.1167, 'eval_samples_per_second': 242.67, 'eval_steps_per_second': 15.304, 'epoch': 0.28}
{'loss': 0.3961, 'grad_norm': 0.4186455309391022, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.34041377902030945, 'eval_runtime': 4.1178, 'eval_samples_per_second': 242.608, 'eval_steps_per_second': 15.3, 'epoch': 0.32}
{'loss': 0.3337, 'grad_norm': 0.3529609739780426, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.3047441840171814, 'eval_runtime': 4.1375, 'eval_samples_per_second': 241.452, 'eval_steps_per_second': 15.227, 'epoch': 0.36}
{'loss': 0.3249, 'grad_norm': 0.341238409280777, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.27487626671791077, 'eval_runtime': 4.1483, 'eval_samples_per_second': 240.822, 'eval_steps_per_second': 15.187, 'epoch': 0.4}
{'loss': 0.2867, 'grad_norm': 0.24807965755462646, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.24830502271652222, 'eval_runtime': 4.1187, 'eval_samples_per_second': 242.55, 'eval_steps_per_second': 15.296, 'epoch': 0.44}
{'loss': 0.2613, 'grad_norm': 0.30415838956832886, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.23231449723243713, 'eval_runtime': 4.1286, 'eval_samples_per_second': 241.971, 'eval_steps_per_second': 15.259, 'epoch': 0.48}
{'loss': 0.2389, 'grad_norm': 0.37545445561408997, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.21511834859848022, 'eval_runtime': 4.1126, 'eval_samples_per_second': 242.911, 'eval_steps_per_second': 15.319, 'epoch': 0.52}
{'loss': 0.2208, 'grad_norm': 0.18030454218387604, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.20573025941848755, 'eval_runtime': 4.0926, 'eval_samples_per_second': 244.098, 'eval_steps_per_second': 15.394, 'epoch': 0.56}
{'loss': 0.232, 'grad_norm': 0.291032999753952, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.19546028971672058, 'eval_runtime': 4.0882, 'eval_samples_per_second': 244.359, 'eval_steps_per_second': 15.41, 'epoch': 0.6}
{'loss': 0.2319, 'grad_norm': 0.2076805979013443, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.18954584002494812, 'eval_runtime': 4.0854, 'eval_samples_per_second': 244.53, 'eval_steps_per_second': 15.421, 'epoch': 0.64}
{'loss': 0.1821, 'grad_norm': 0.19558368623256683, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.1846638172864914, 'eval_runtime': 4.0803, 'eval_samples_per_second': 244.836, 'eval_steps_per_second': 15.44, 'epoch': 0.68}
{'loss': 0.2249, 'grad_norm': 0.18316131830215454, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.18098856508731842, 'eval_runtime': 4.1096, 'eval_samples_per_second': 243.088, 'eval_steps_per_second': 15.33, 'epoch': 0.72}
{'loss': 0.2071, 'grad_norm': 0.16590365767478943, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.17717406153678894, 'eval_runtime': 4.0898, 'eval_samples_per_second': 244.268, 'eval_steps_per_second': 15.404, 'epoch': 0.76}
{'loss': 0.1765, 'grad_norm': 0.15952946245670319, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.1734909862279892, 'eval_runtime': 4.111, 'eval_samples_per_second': 243.009, 'eval_steps_per_second': 15.325, 'epoch': 0.8}
{'loss': 0.1933, 'grad_norm': 0.16992835700511932, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.1720699965953827, 'eval_runtime': 4.1129, 'eval_samples_per_second': 242.895, 'eval_steps_per_second': 15.318, 'epoch': 0.84}
{'loss': 0.1945, 'grad_norm': 0.13166950643062592, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.17045187950134277, 'eval_runtime': 4.1007, 'eval_samples_per_second': 243.615, 'eval_steps_per_second': 15.363, 'epoch': 0.88}
{'loss': 0.1907, 'grad_norm': 0.15737828612327576, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.16934695839881897, 'eval_runtime': 4.0911, 'eval_samples_per_second': 244.189, 'eval_steps_per_second': 15.399, 'epoch': 0.92}
{'loss': 0.1708, 'grad_norm': 0.17193716764450073, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.16782920062541962, 'eval_runtime': 4.0869, 'eval_samples_per_second': 244.442, 'eval_steps_per_second': 15.415, 'epoch': 0.96}
{'loss': 0.1929, 'grad_norm': 0.1802310198545456, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.16720446944236755, 'eval_runtime': 4.0932, 'eval_samples_per_second': 244.062, 'eval_steps_per_second': 15.391, 'epoch': 1.0}
{'train_runtime': 296.4305, 'train_samples_per_second': 33.731, 'train_steps_per_second': 2.108, 'train_loss': 0.45621146850585936, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1749430894851685, 0.7433139681816101, 0.6440771818161011, 0.5646206736564636, 0.49992623925209045, 0.42282649874687195, 0.37491604685783386, 0.34041377902030945, 0.3047441840171814, 0.27487626671791077, 0.24830502271652222, 0.23231449723243713, 0.21511834859848022, 0.20573025941848755, 0.19546028971672058, 0.18954584002494812, 0.1846638172864914, 0.18098856508731842, 0.17717406153678894, 0.1734909862279892, 0.1720699965953827, 0.17045187950134277, 0.16934695839881897, 0.16782920062541962, 0.16720446944236755], 'performance': [0.51, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:57,  1.71it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 19.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 26.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 30.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 31.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 32.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 37.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 31.94it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.51, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.4716897010803223
current iteration best possible performance (full train run):  0.8925
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8295 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 1.0661961877300108
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.198303434954528
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 1.0969025112670852
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.1509255096324393
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.2053750829340868
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.4754, dtype=torch.float64), 0, tensor(0.5246, dtype=torch.float64), 0, 32, 0, 0, 0, 1, 0, 128, 4.163336342344338e-18, 47.999999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(1.5939e-16, dtype=torch.float64), tensor(7.3005e-17, dtype=torch.float64), tensor(2.4358e-17, dtype=torch.float64), tensor(6.2657e-17, dtype=torch.float64), tensor(7.1044e-17, dtype=torch.float64), tensor(0.4754, dtype=torch.float64), tensor(7.3911e-18, dtype=torch.float64), tensor(0.5246, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(4.1633e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.475
  wikitext: 0
  mmlu: 0.525
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.163336342344338e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (47.999999999999986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  4.163336342344338e-18
lora alpha:  47.999999999999986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:36,  4.01s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:07<01:10,  1.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:29,  2.78it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:12<00:31,  2.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:30,  2.23it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:19<00:27,  2.18it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:23<00:23,  2.13it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:27<00:20,  2.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:32<00:17,  2.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:32<00:09,  2.78it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:36<00:07,  2.47it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:37<00:03,  3.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:41<00:01,  2.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.40it/s]
Evaluation performance at step 25: 0.57
{'loss': 2.9681, 'grad_norm': 0.9262825846672058, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 1.5118660926818848, 'eval_runtime': 8.2037, 'eval_samples_per_second': 121.774, 'eval_steps_per_second': 7.679, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:46,  2.13it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 12.39it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 17.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 20.61it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 20.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 23.19it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:07,  5.44it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:04,  7.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02,  9.07it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 11.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 13.38it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 15.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 12.34it/s]
Evaluation performance at step 50: 0.72
{'loss': 1.3377, 'grad_norm': 0.4494137167930603, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 1.190522313117981, 'eval_runtime': 8.2308, 'eval_samples_per_second': 121.373, 'eval_steps_per_second': 7.654, 'epoch': 0.08}
{'loss': 1.1709, 'grad_norm': 0.33891579508781433, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1141663789749146, 'eval_runtime': 8.2999, 'eval_samples_per_second': 120.363, 'eval_steps_per_second': 7.59, 'epoch': 0.12}
{'loss': 1.1207, 'grad_norm': 0.22002893686294556, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0876542329788208, 'eval_runtime': 8.3099, 'eval_samples_per_second': 120.218, 'eval_steps_per_second': 7.581, 'epoch': 0.16}
{'loss': 1.1202, 'grad_norm': 0.23539313673973083, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0684278011322021, 'eval_runtime': 8.3286, 'eval_samples_per_second': 119.948, 'eval_steps_per_second': 7.564, 'epoch': 0.2}
{'loss': 1.0943, 'grad_norm': 0.2202187478542328, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.047614336013794, 'eval_runtime': 8.3476, 'eval_samples_per_second': 119.675, 'eval_steps_per_second': 7.547, 'epoch': 0.24}
{'loss': 1.1122, 'grad_norm': 0.2339043766260147, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0346124172210693, 'eval_runtime': 8.3757, 'eval_samples_per_second': 119.273, 'eval_steps_per_second': 7.522, 'epoch': 0.28}
{'loss': 1.0322, 'grad_norm': 0.2693241536617279, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.017984390258789, 'eval_runtime': 8.3829, 'eval_samples_per_second': 119.171, 'eval_steps_per_second': 7.515, 'epoch': 0.32}
{'loss': 1.0357, 'grad_norm': 0.6630126237869263, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9967036247253418, 'eval_runtime': 8.3595, 'eval_samples_per_second': 119.504, 'eval_steps_per_second': 7.536, 'epoch': 0.36}
{'loss': 1.0011, 'grad_norm': 0.3303130269050598, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9797444939613342, 'eval_runtime': 8.3636, 'eval_samples_per_second': 119.446, 'eval_steps_per_second': 7.533, 'epoch': 0.4}
{'loss': 0.9946, 'grad_norm': 0.25327521562576294, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9622739553451538, 'eval_runtime': 8.3572, 'eval_samples_per_second': 119.538, 'eval_steps_per_second': 7.538, 'epoch': 0.44}
{'loss': 0.9941, 'grad_norm': 0.2562449872493744, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9501873850822449, 'eval_runtime': 8.3556, 'eval_samples_per_second': 119.56, 'eval_steps_per_second': 7.54, 'epoch': 0.48}
{'loss': 0.9621, 'grad_norm': 0.2797465920448303, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9337724447250366, 'eval_runtime': 8.3612, 'eval_samples_per_second': 119.48, 'eval_steps_per_second': 7.535, 'epoch': 0.52}
{'loss': 0.9796, 'grad_norm': 0.24956771731376648, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9202536940574646, 'eval_runtime': 8.371, 'eval_samples_per_second': 119.34, 'eval_steps_per_second': 7.526, 'epoch': 0.56}
{'loss': 1.0064, 'grad_norm': 0.3432169556617737, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9038338661193848, 'eval_runtime': 8.3721, 'eval_samples_per_second': 119.326, 'eval_steps_per_second': 7.525, 'epoch': 0.6}
{'loss': 0.9307, 'grad_norm': 0.3200779855251312, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8911430835723877, 'eval_runtime': 8.4023, 'eval_samples_per_second': 118.896, 'eval_steps_per_second': 7.498, 'epoch': 0.64}
{'loss': 0.9912, 'grad_norm': 0.2794930338859558, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8785605430603027, 'eval_runtime': 8.388, 'eval_samples_per_second': 119.099, 'eval_steps_per_second': 7.511, 'epoch': 0.68}
{'loss': 0.9263, 'grad_norm': 0.31801456212997437, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8665772080421448, 'eval_runtime': 8.3977, 'eval_samples_per_second': 118.961, 'eval_steps_per_second': 7.502, 'epoch': 0.72}
{'loss': 0.8894, 'grad_norm': 0.3430764377117157, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.855341911315918, 'eval_runtime': 8.3963, 'eval_samples_per_second': 118.981, 'eval_steps_per_second': 7.503, 'epoch': 0.76}
{'loss': 0.9145, 'grad_norm': 0.25948554277420044, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8463209867477417, 'eval_runtime': 8.3272, 'eval_samples_per_second': 119.969, 'eval_steps_per_second': 7.566, 'epoch': 0.8}
{'loss': 0.8875, 'grad_norm': 0.34943464398384094, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8380169868469238, 'eval_runtime': 8.3256, 'eval_samples_per_second': 119.991, 'eval_steps_per_second': 7.567, 'epoch': 0.84}
{'loss': 0.927, 'grad_norm': 0.28582146763801575, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.829653263092041, 'eval_runtime': 8.3422, 'eval_samples_per_second': 119.752, 'eval_steps_per_second': 7.552, 'epoch': 0.88}
{'loss': 0.8637, 'grad_norm': 0.3511943817138672, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.82652348279953, 'eval_runtime': 8.3479, 'eval_samples_per_second': 119.67, 'eval_steps_per_second': 7.547, 'epoch': 0.92}
{'loss': 0.8528, 'grad_norm': 0.37487509846687317, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8228696584701538, 'eval_runtime': 8.3572, 'eval_samples_per_second': 119.537, 'eval_steps_per_second': 7.538, 'epoch': 0.96}
{'loss': 0.8951, 'grad_norm': 0.2636256814002991, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.821298360824585, 'eval_runtime': 8.3155, 'eval_samples_per_second': 120.137, 'eval_steps_per_second': 7.576, 'epoch': 1.0}
{'train_runtime': 531.5261, 'train_samples_per_second': 18.812, 'train_steps_per_second': 1.176, 'train_loss': 1.0803277404785157, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5118660926818848, 1.190522313117981, 1.1141663789749146, 1.0876542329788208, 1.0684278011322021, 1.047614336013794, 1.0346124172210693, 1.017984390258789, 0.9967036247253418, 0.9797444939613342, 0.9622739553451538, 0.9501873850822449, 0.9337724447250366, 0.9202536940574646, 0.9038338661193848, 0.8911430835723877, 0.8785605430603027, 0.8665772080421448, 0.855341911315918, 0.8463209867477417, 0.8380169868469238, 0.829653263092041, 0.82652348279953, 0.8228696584701538, 0.821298360824585], 'performance': [0.57, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<08:01,  4.87s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:22,  3.68it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:10<00:19,  3.48it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:13<00:11,  4.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:17<00:08,  3.95it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:18<00:03,  5.68it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:18<00:00,  8.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18<00:00,  5.30it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  1.4683763980865479
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8021 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.2876306366039951
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 1.2629533855284483
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.2278259109169114
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.9111948968300383
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 1.1566706270415499
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.7133, dtype=torch.float64), 0, tensor(0.2867, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(7.5615e-17, dtype=torch.float64), tensor(7.3232e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7133, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2867, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1176e-18, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.713
  triviaqa: 0
  truthfulqa_gen: 0.287
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:35,  1.03it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.61it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 10.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.53it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.25it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.48it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.78it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 13.77it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 15.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 12.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 11.16it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 12.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 12.12it/s]
Evaluation performance at step 25: 0.55
{'loss': 5.2859, 'grad_norm': 0.6262845396995544, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.978616714477539, 'eval_runtime': 4.9013, 'eval_samples_per_second': 203.822, 'eval_steps_per_second': 12.854, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:36,  1.03it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.43it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.35it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 13.85it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.12it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.85it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 14.73it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.02it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.54it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.56it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:02,  5.29it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00,  6.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.13it/s]
Evaluation performance at step 50: 0.44
{'loss': 2.4825, 'grad_norm': 0.47626736760139465, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.44}
{'eval_loss': 1.450766682624817, 'eval_runtime': 3.182, 'eval_samples_per_second': 313.957, 'eval_steps_per_second': 19.799, 'epoch': 0.08}
{'loss': 1.1592, 'grad_norm': 0.15456630289554596, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0079295635223389, 'eval_runtime': 3.1724, 'eval_samples_per_second': 314.904, 'eval_steps_per_second': 19.859, 'epoch': 0.12}
{'loss': 0.922, 'grad_norm': 0.13076402246952057, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8502452373504639, 'eval_runtime': 3.1897, 'eval_samples_per_second': 313.193, 'eval_steps_per_second': 19.751, 'epoch': 0.16}
{'loss': 0.8429, 'grad_norm': 0.0603177510201931, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8169183731079102, 'eval_runtime': 3.2057, 'eval_samples_per_second': 311.628, 'eval_steps_per_second': 19.652, 'epoch': 0.2}
{'loss': 0.7862, 'grad_norm': 0.0589713491499424, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8090471625328064, 'eval_runtime': 3.2251, 'eval_samples_per_second': 309.758, 'eval_steps_per_second': 19.534, 'epoch': 0.24}
{'loss': 0.7929, 'grad_norm': 0.06472016125917435, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7964035868644714, 'eval_runtime': 3.1802, 'eval_samples_per_second': 314.131, 'eval_steps_per_second': 19.81, 'epoch': 0.28}
{'loss': 0.7992, 'grad_norm': 0.06082930043339729, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7903868556022644, 'eval_runtime': 3.1551, 'eval_samples_per_second': 316.627, 'eval_steps_per_second': 19.967, 'epoch': 0.32}
{'loss': 0.7877, 'grad_norm': 0.0599607452750206, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.783515453338623, 'eval_runtime': 3.2095, 'eval_samples_per_second': 311.259, 'eval_steps_per_second': 19.629, 'epoch': 0.36}
{'loss': 0.7954, 'grad_norm': 0.06284579634666443, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7786939740180969, 'eval_runtime': 3.1787, 'eval_samples_per_second': 314.284, 'eval_steps_per_second': 19.82, 'epoch': 0.4}
{'loss': 0.7681, 'grad_norm': 0.05412603169679642, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7737410664558411, 'eval_runtime': 3.2076, 'eval_samples_per_second': 311.446, 'eval_steps_per_second': 19.641, 'epoch': 0.44}
{'loss': 0.7537, 'grad_norm': 0.07306522876024246, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7687140107154846, 'eval_runtime': 3.1805, 'eval_samples_per_second': 314.106, 'eval_steps_per_second': 19.808, 'epoch': 0.48}
{'loss': 0.7397, 'grad_norm': 0.06931371241807938, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7663424015045166, 'eval_runtime': 3.1812, 'eval_samples_per_second': 314.033, 'eval_steps_per_second': 19.804, 'epoch': 0.52}
{'loss': 0.781, 'grad_norm': 0.05771823599934578, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7634064555168152, 'eval_runtime': 3.1785, 'eval_samples_per_second': 314.3, 'eval_steps_per_second': 19.821, 'epoch': 0.56}
{'loss': 0.7598, 'grad_norm': 0.06930258870124817, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7608423233032227, 'eval_runtime': 3.1802, 'eval_samples_per_second': 314.136, 'eval_steps_per_second': 19.81, 'epoch': 0.6}
{'loss': 0.7698, 'grad_norm': 0.06340482085943222, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7573539614677429, 'eval_runtime': 3.1787, 'eval_samples_per_second': 314.275, 'eval_steps_per_second': 19.819, 'epoch': 0.64}
{'loss': 0.7598, 'grad_norm': 0.06723088026046753, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7545588612556458, 'eval_runtime': 3.2177, 'eval_samples_per_second': 310.473, 'eval_steps_per_second': 19.579, 'epoch': 0.68}
{'loss': 0.7493, 'grad_norm': 0.06866616010665894, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7531002759933472, 'eval_runtime': 3.1768, 'eval_samples_per_second': 314.469, 'eval_steps_per_second': 19.831, 'epoch': 0.72}
{'loss': 0.753, 'grad_norm': 0.06759756803512573, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7508888244628906, 'eval_runtime': 3.1712, 'eval_samples_per_second': 315.018, 'eval_steps_per_second': 19.866, 'epoch': 0.76}
{'loss': 0.7549, 'grad_norm': 0.061537232249975204, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7491941452026367, 'eval_runtime': 3.172, 'eval_samples_per_second': 314.94, 'eval_steps_per_second': 19.861, 'epoch': 0.8}
{'loss': 0.7395, 'grad_norm': 0.06827197223901749, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7475891709327698, 'eval_runtime': 3.1887, 'eval_samples_per_second': 313.29, 'eval_steps_per_second': 19.757, 'epoch': 0.84}
{'loss': 0.7505, 'grad_norm': 0.06895226240158081, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7461900115013123, 'eval_runtime': 3.1744, 'eval_samples_per_second': 314.703, 'eval_steps_per_second': 19.846, 'epoch': 0.88}
{'loss': 0.7431, 'grad_norm': 0.06762063503265381, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7454835176467896, 'eval_runtime': 3.1694, 'eval_samples_per_second': 315.201, 'eval_steps_per_second': 19.878, 'epoch': 0.92}
{'loss': 0.7395, 'grad_norm': 0.07284052670001984, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7447590231895447, 'eval_runtime': 3.1688, 'eval_samples_per_second': 315.266, 'eval_steps_per_second': 19.882, 'epoch': 0.96}
{'loss': 0.748, 'grad_norm': 0.09702222049236298, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7446443438529968, 'eval_runtime': 3.1647, 'eval_samples_per_second': 315.671, 'eval_steps_per_second': 19.907, 'epoch': 1.0}
{'train_runtime': 255.7789, 'train_samples_per_second': 39.092, 'train_steps_per_second': 2.444, 'train_loss': 1.0385540466308594, 'epoch': 1.0}
train_results:  {'eval_loss': [3.978616714477539, 1.450766682624817, 1.0079295635223389, 0.8502452373504639, 0.8169183731079102, 0.8090471625328064, 0.7964035868644714, 0.7903868556022644, 0.783515453338623, 0.7786939740180969, 0.7737410664558411, 0.7687140107154846, 0.7663424015045166, 0.7634064555168152, 0.7608423233032227, 0.7573539614677429, 0.7545588612556458, 0.7531002759933472, 0.7508888244628906, 0.7491941452026367, 0.7475891709327698, 0.7461900115013123, 0.7454835176467896, 0.7447590231895447, 0.7446443438529968], 'performance': [0.55, 0.44]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:39,  1.62s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08, 10.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 16.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 13.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 16.87it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 20.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.40it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.44]
current iteration observed (possibly low-fid or predicted) performance:  1.4805963039398193
current iteration best possible performance (full train run):  0.6825000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2116 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.2998923916733602
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 1.0696586315353052
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 1.2888005542328806
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.8430078157248637
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.092450722560311
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 2.2204460492503132e-17, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.5255e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2691e-16, dtype=torch.float64), tensor(1.0854e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(7.8948e-17, dtype=torch.float64), tensor(2.0909e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(2.2204e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 1.0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.2204460492503132e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  2.2204460492503132e-17
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:47,  1.09s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:13,  6.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.54it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.97it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 12.35it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.55it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.31it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 12.91it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 13.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 10.95it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 12.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 13.95it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 12.26it/s]
Evaluation performance at step 25: 0.55
{'loss': 3.3095, 'grad_norm': 0.18074382841587067, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.045900583267212, 'eval_runtime': 8.9285, 'eval_samples_per_second': 112.001, 'eval_steps_per_second': 7.056, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:06,  1.28s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:15,  5.76it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.72it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:10,  7.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  8.88it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.36it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.25it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.36it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:01, 13.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 13.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 15.74it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 16.04it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.57it/s]
Evaluation performance at step 50: 0.56
{'loss': 2.6755, 'grad_norm': 0.32531553506851196, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 2.3754332065582275, 'eval_runtime': 8.8597, 'eval_samples_per_second': 112.87, 'eval_steps_per_second': 7.111, 'epoch': 0.08}
{'loss': 2.2601, 'grad_norm': 0.1409408301115036, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.2504193782806396, 'eval_runtime': 8.9095, 'eval_samples_per_second': 112.24, 'eval_steps_per_second': 7.071, 'epoch': 0.12}
{'loss': 2.1936, 'grad_norm': 0.12088453769683838, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.2196872234344482, 'eval_runtime': 8.9363, 'eval_samples_per_second': 111.903, 'eval_steps_per_second': 7.05, 'epoch': 0.16}
{'loss': 2.1926, 'grad_norm': 0.17434626817703247, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.201765298843384, 'eval_runtime': 8.9338, 'eval_samples_per_second': 111.934, 'eval_steps_per_second': 7.052, 'epoch': 0.2}
{'loss': 2.1621, 'grad_norm': 0.11574456095695496, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.1956214904785156, 'eval_runtime': 8.9209, 'eval_samples_per_second': 112.096, 'eval_steps_per_second': 7.062, 'epoch': 0.24}
{'loss': 2.1623, 'grad_norm': 0.10267740488052368, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1800341606140137, 'eval_runtime': 8.9323, 'eval_samples_per_second': 111.953, 'eval_steps_per_second': 7.053, 'epoch': 0.28}
{'loss': 2.1828, 'grad_norm': 0.12163260579109192, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.1712939739227295, 'eval_runtime': 8.9327, 'eval_samples_per_second': 111.948, 'eval_steps_per_second': 7.053, 'epoch': 0.32}
{'loss': 2.1105, 'grad_norm': 0.22573181986808777, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.163771629333496, 'eval_runtime': 8.9341, 'eval_samples_per_second': 111.931, 'eval_steps_per_second': 7.052, 'epoch': 0.36}
{'loss': 2.1835, 'grad_norm': 0.2289901226758957, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.1583027839660645, 'eval_runtime': 8.9344, 'eval_samples_per_second': 111.926, 'eval_steps_per_second': 7.051, 'epoch': 0.4}
{'loss': 2.2054, 'grad_norm': 0.09688396751880646, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.1527493000030518, 'eval_runtime': 8.9371, 'eval_samples_per_second': 111.893, 'eval_steps_per_second': 7.049, 'epoch': 0.44}
{'loss': 2.1585, 'grad_norm': 0.18245074152946472, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.1482203006744385, 'eval_runtime': 8.9743, 'eval_samples_per_second': 111.43, 'eval_steps_per_second': 7.02, 'epoch': 0.48}
{'loss': 2.1413, 'grad_norm': 0.09116136282682419, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.143587350845337, 'eval_runtime': 8.9729, 'eval_samples_per_second': 111.446, 'eval_steps_per_second': 7.021, 'epoch': 0.52}
{'loss': 2.1825, 'grad_norm': 0.136726975440979, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.140249252319336, 'eval_runtime': 8.9893, 'eval_samples_per_second': 111.243, 'eval_steps_per_second': 7.008, 'epoch': 0.56}
{'loss': 2.1981, 'grad_norm': 0.13998791575431824, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.137775421142578, 'eval_runtime': 8.9827, 'eval_samples_per_second': 111.325, 'eval_steps_per_second': 7.013, 'epoch': 0.6}
{'loss': 2.1228, 'grad_norm': 0.308040976524353, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.134533166885376, 'eval_runtime': 8.9722, 'eval_samples_per_second': 111.456, 'eval_steps_per_second': 7.022, 'epoch': 0.64}
{'loss': 2.1682, 'grad_norm': 0.11588132381439209, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.1311709880828857, 'eval_runtime': 8.9577, 'eval_samples_per_second': 111.635, 'eval_steps_per_second': 7.033, 'epoch': 0.68}
{'loss': 2.1345, 'grad_norm': 0.07911603152751923, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.1295359134674072, 'eval_runtime': 8.958, 'eval_samples_per_second': 111.632, 'eval_steps_per_second': 7.033, 'epoch': 0.72}
{'loss': 2.1549, 'grad_norm': 0.12416724860668182, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.127150774002075, 'eval_runtime': 9.0053, 'eval_samples_per_second': 111.045, 'eval_steps_per_second': 6.996, 'epoch': 0.76}
{'loss': 2.1372, 'grad_norm': 0.27755534648895264, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.125261068344116, 'eval_runtime': 8.9981, 'eval_samples_per_second': 111.135, 'eval_steps_per_second': 7.002, 'epoch': 0.8}
{'loss': 2.0673, 'grad_norm': 0.08820081502199173, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.1241934299468994, 'eval_runtime': 8.9464, 'eval_samples_per_second': 111.776, 'eval_steps_per_second': 7.042, 'epoch': 0.84}
{'loss': 2.102, 'grad_norm': 0.11852904409170151, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.122398853302002, 'eval_runtime': 8.951, 'eval_samples_per_second': 111.72, 'eval_steps_per_second': 7.038, 'epoch': 0.88}
{'loss': 2.1119, 'grad_norm': 0.18718045949935913, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.1208882331848145, 'eval_runtime': 8.9376, 'eval_samples_per_second': 111.886, 'eval_steps_per_second': 7.049, 'epoch': 0.92}
{'loss': 2.1549, 'grad_norm': 0.11935392767190933, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.120296001434326, 'eval_runtime': 8.9542, 'eval_samples_per_second': 111.679, 'eval_steps_per_second': 7.036, 'epoch': 0.96}
{'loss': 2.0921, 'grad_norm': 0.08304894715547562, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.1201226711273193, 'eval_runtime': 8.9623, 'eval_samples_per_second': 111.578, 'eval_steps_per_second': 7.029, 'epoch': 1.0}
{'train_runtime': 526.3899, 'train_samples_per_second': 18.997, 'train_steps_per_second': 1.187, 'train_loss': 2.22256904296875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.045900583267212, 2.3754332065582275, 2.2504193782806396, 2.2196872234344482, 2.201765298843384, 2.1956214904785156, 2.1800341606140137, 2.1712939739227295, 2.163771629333496, 2.1583027839660645, 2.1527493000030518, 2.1482203006744385, 2.143587350845337, 2.140249252319336, 2.137775421142578, 2.134533166885376, 2.1311709880828857, 2.1295359134674072, 2.127150774002075, 2.125261068344116, 2.1241934299468994, 2.122398853302002, 2.1208882331848145, 2.120296001434326, 2.1201226711273193], 'performance': [0.55, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:32,  5.18s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.92it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:10<00:16,  4.15it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:15<00:14,  3.63it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:20<00:10,  3.43it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:26<00:05,  3.33it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:26<00:00,  4.77it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:26<00:00,  3.77it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.486405849456787
current iteration best possible performance (full train run):  0.5355000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6496 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.2039717975129265
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.8640770322473447
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.0487419042199881
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.907779426938471
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.0640687986939499
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8722, dtype=torch.float64), 0, 0, 0, tensor(0.1278, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 1.544370808988263e-17, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(2.4024e-16, dtype=torch.float64), tensor(0.8722, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.4919e-17, dtype=torch.float64), tensor(6.0934e-17, dtype=torch.float64), tensor(0.1278, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3291e-16, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.5444e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.872
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.128
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.544370808988263e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  1.544370808988263e-17
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:46,  1.07s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:16,  5.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.60it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:07,  9.75it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.21it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 11.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.15it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.26it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:02, 13.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 10.75it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01,  9.52it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 10.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.61it/s]
Evaluation performance at step 25: 0.54
{'loss': 2.6699, 'grad_norm': 0.2220619022846222, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 2.0953071117401123, 'eval_runtime': 9.687, 'eval_samples_per_second': 103.128, 'eval_steps_per_second': 6.504, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:56,  1.18s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.34it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 12.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.34it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.15it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 15.13it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 11.41it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01, 10.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 12.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.35it/s]
Evaluation performance at step 50: 0.55
{'loss': 1.5371, 'grad_norm': 0.18892985582351685, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 1.222150206565857, 'eval_runtime': 9.6763, 'eval_samples_per_second': 103.242, 'eval_steps_per_second': 6.511, 'epoch': 0.08}
{'loss': 1.0519, 'grad_norm': 0.04795939475297928, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9704550504684448, 'eval_runtime': 9.7865, 'eval_samples_per_second': 102.08, 'eval_steps_per_second': 6.437, 'epoch': 0.12}
{'loss': 0.9518, 'grad_norm': 0.06443079560995102, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9076128602027893, 'eval_runtime': 9.7899, 'eval_samples_per_second': 102.044, 'eval_steps_per_second': 6.435, 'epoch': 0.16}
{'loss': 0.8887, 'grad_norm': 0.04959918186068535, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8848446011543274, 'eval_runtime': 9.7739, 'eval_samples_per_second': 102.211, 'eval_steps_per_second': 6.446, 'epoch': 0.2}
{'loss': 0.8636, 'grad_norm': 0.03525950387120247, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8750433921813965, 'eval_runtime': 9.7812, 'eval_samples_per_second': 102.135, 'eval_steps_per_second': 6.441, 'epoch': 0.24}
{'loss': 0.8577, 'grad_norm': 0.03550304099917412, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8696045875549316, 'eval_runtime': 9.7887, 'eval_samples_per_second': 102.057, 'eval_steps_per_second': 6.436, 'epoch': 0.28}
{'loss': 0.8837, 'grad_norm': 0.04071836918592453, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8644019961357117, 'eval_runtime': 9.7753, 'eval_samples_per_second': 102.196, 'eval_steps_per_second': 6.445, 'epoch': 0.32}
{'loss': 0.854, 'grad_norm': 0.040030162781476974, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.858494222164154, 'eval_runtime': 9.78, 'eval_samples_per_second': 102.147, 'eval_steps_per_second': 6.442, 'epoch': 0.36}
{'loss': 0.8603, 'grad_norm': 0.04643194004893303, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8545377850532532, 'eval_runtime': 9.7872, 'eval_samples_per_second': 102.072, 'eval_steps_per_second': 6.437, 'epoch': 0.4}
{'loss': 0.8613, 'grad_norm': 0.04486130550503731, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8503377437591553, 'eval_runtime': 9.8489, 'eval_samples_per_second': 101.433, 'eval_steps_per_second': 6.397, 'epoch': 0.44}
{'loss': 0.8524, 'grad_norm': 0.040305495262145996, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8469839692115784, 'eval_runtime': 9.8664, 'eval_samples_per_second': 101.253, 'eval_steps_per_second': 6.385, 'epoch': 0.48}
{'loss': 0.8485, 'grad_norm': 0.055134035646915436, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8444582223892212, 'eval_runtime': 9.8556, 'eval_samples_per_second': 101.364, 'eval_steps_per_second': 6.392, 'epoch': 0.52}
{'loss': 0.8409, 'grad_norm': 0.04779583588242531, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8423663973808289, 'eval_runtime': 9.8677, 'eval_samples_per_second': 101.24, 'eval_steps_per_second': 6.384, 'epoch': 0.56}
{'loss': 0.8363, 'grad_norm': 0.05430333688855171, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8394054174423218, 'eval_runtime': 9.9096, 'eval_samples_per_second': 100.811, 'eval_steps_per_second': 6.357, 'epoch': 0.6}
{'loss': 0.8234, 'grad_norm': 0.04723947122693062, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8380577564239502, 'eval_runtime': 9.9166, 'eval_samples_per_second': 100.74, 'eval_steps_per_second': 6.353, 'epoch': 0.64}
{'loss': 0.8264, 'grad_norm': 0.04827600345015526, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8354547023773193, 'eval_runtime': 9.9105, 'eval_samples_per_second': 100.802, 'eval_steps_per_second': 6.357, 'epoch': 0.68}
{'loss': 0.8373, 'grad_norm': 0.04590539261698723, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8333779573440552, 'eval_runtime': 9.8824, 'eval_samples_per_second': 101.088, 'eval_steps_per_second': 6.375, 'epoch': 0.72}
{'loss': 0.8047, 'grad_norm': 0.04711899161338806, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8319446444511414, 'eval_runtime': 9.848, 'eval_samples_per_second': 101.442, 'eval_steps_per_second': 6.397, 'epoch': 0.76}
{'loss': 0.8425, 'grad_norm': 0.0502900667488575, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8305466175079346, 'eval_runtime': 9.8615, 'eval_samples_per_second': 101.303, 'eval_steps_per_second': 6.388, 'epoch': 0.8}
{'loss': 0.8149, 'grad_norm': 0.051193732768297195, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8294594883918762, 'eval_runtime': 9.8523, 'eval_samples_per_second': 101.397, 'eval_steps_per_second': 6.394, 'epoch': 0.84}
{'loss': 0.8276, 'grad_norm': 0.04651455953717232, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8285835981369019, 'eval_runtime': 9.8611, 'eval_samples_per_second': 101.308, 'eval_steps_per_second': 6.389, 'epoch': 0.88}
{'loss': 0.8505, 'grad_norm': 0.05291106179356575, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8279436826705933, 'eval_runtime': 9.8625, 'eval_samples_per_second': 101.293, 'eval_steps_per_second': 6.388, 'epoch': 0.92}
{'loss': 0.8294, 'grad_norm': 0.04519297927618027, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8273440003395081, 'eval_runtime': 9.845, 'eval_samples_per_second': 101.472, 'eval_steps_per_second': 6.399, 'epoch': 0.96}
{'loss': 0.8396, 'grad_norm': 0.04758765548467636, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8271836042404175, 'eval_runtime': 9.8486, 'eval_samples_per_second': 101.436, 'eval_steps_per_second': 6.397, 'epoch': 1.0}
{'train_runtime': 569.1289, 'train_samples_per_second': 17.569, 'train_steps_per_second': 1.098, 'train_loss': 0.9581763122558594, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0953071117401123, 1.222150206565857, 0.9704550504684448, 0.9076128602027893, 0.8848446011543274, 0.8750433921813965, 0.8696045875549316, 0.8644019961357117, 0.858494222164154, 0.8545377850532532, 0.8503377437591553, 0.8469839692115784, 0.8444582223892212, 0.8423663973808289, 0.8394054174423218, 0.8380577564239502, 0.8354547023773193, 0.8333779573440552, 0.8319446444511414, 0.8305466175079346, 0.8294594883918762, 0.8285835981369019, 0.8279436826705933, 0.8273440003395081, 0.8271836042404175], 'performance': [0.54, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:13,  1.35it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:27,  2.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:11,  5.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  7.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 12.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 16.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.4819879531860352
current iteration best possible performance (full train run):  0.47250000000000003
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9966 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.1937615177941991
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 1.0973087357598166
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.1876711299231049
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 1.0985502478145386
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.6751897387237817
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.3432, dtype=torch.float64), 0, 0, tensor(0.6568, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 4.823128858793852e-16, 1.4800000190734974, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(9.4683e-17, dtype=torch.float64), tensor(3.5083e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.4867e-17, dtype=torch.float64), tensor(0.3432, dtype=torch.float64), tensor(6.9620e-16, dtype=torch.float64), tensor(6.5332e-17, dtype=torch.float64), tensor(0.6568, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(4.8231e-15, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.343
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.657

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.823128858793852e-16,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734974,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  4.823128858793852e-16
lora alpha:  1.4800000190734974
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:15,  1.97s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:24,  3.74it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:12,  6.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.93it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.68it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:04, 10.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 10.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:02, 11.93it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  9.24it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  9.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.38it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.1533, 'grad_norm': 0.3897542357444763, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.0699923038482666, 'eval_runtime': 6.8987, 'eval_samples_per_second': 144.81, 'eval_steps_per_second': 9.132, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:45,  2.17it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 11.07it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:21,  3.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  5.09it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  6.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:10<00:14,  3.61it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:11<00:09,  4.35it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:12<00:06,  5.74it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:13<00:03,  6.94it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:13<00:02,  8.18it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:14<00:01,  9.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:14<00:00, 10.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.86it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.0417, 'grad_norm': 0.3084862530231476, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.281085729598999, 'eval_runtime': 6.4304, 'eval_samples_per_second': 155.357, 'eval_steps_per_second': 9.797, 'epoch': 0.08}
{'loss': 1.0818, 'grad_norm': 0.07868047803640366, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9700213074684143, 'eval_runtime': 6.4548, 'eval_samples_per_second': 154.769, 'eval_steps_per_second': 9.76, 'epoch': 0.12}
{'loss': 0.9532, 'grad_norm': 0.09720371663570404, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8756524920463562, 'eval_runtime': 6.5342, 'eval_samples_per_second': 152.887, 'eval_steps_per_second': 9.642, 'epoch': 0.16}
{'loss': 0.8916, 'grad_norm': 0.0542566180229187, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8415272831916809, 'eval_runtime': 6.492, 'eval_samples_per_second': 153.881, 'eval_steps_per_second': 9.704, 'epoch': 0.2}
{'loss': 0.8314, 'grad_norm': 0.04416443035006523, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8299060463905334, 'eval_runtime': 6.4783, 'eval_samples_per_second': 154.208, 'eval_steps_per_second': 9.725, 'epoch': 0.24}
{'loss': 0.841, 'grad_norm': 0.056334249675273895, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8206574320793152, 'eval_runtime': 6.4755, 'eval_samples_per_second': 154.275, 'eval_steps_per_second': 9.729, 'epoch': 0.28}
{'loss': 0.8226, 'grad_norm': 0.046485885977745056, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8120144605636597, 'eval_runtime': 6.4477, 'eval_samples_per_second': 154.938, 'eval_steps_per_second': 9.771, 'epoch': 0.32}
{'loss': 0.8064, 'grad_norm': 0.0459323525428772, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8053358793258667, 'eval_runtime': 6.4562, 'eval_samples_per_second': 154.734, 'eval_steps_per_second': 9.758, 'epoch': 0.36}
{'loss': 0.8026, 'grad_norm': 0.05051334202289581, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7992063164710999, 'eval_runtime': 6.4624, 'eval_samples_per_second': 154.587, 'eval_steps_per_second': 9.749, 'epoch': 0.4}
{'loss': 0.8051, 'grad_norm': 0.05494038760662079, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7926447987556458, 'eval_runtime': 6.4599, 'eval_samples_per_second': 154.647, 'eval_steps_per_second': 9.753, 'epoch': 0.44}
{'loss': 0.813, 'grad_norm': 0.06053639575839043, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7869870662689209, 'eval_runtime': 6.4579, 'eval_samples_per_second': 154.695, 'eval_steps_per_second': 9.756, 'epoch': 0.48}
{'loss': 0.8321, 'grad_norm': 0.05909043550491333, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7826514840126038, 'eval_runtime': 6.4538, 'eval_samples_per_second': 154.792, 'eval_steps_per_second': 9.762, 'epoch': 0.52}
{'loss': 0.7973, 'grad_norm': 0.05889063701033592, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7792559266090393, 'eval_runtime': 6.4537, 'eval_samples_per_second': 154.795, 'eval_steps_per_second': 9.762, 'epoch': 0.56}
{'loss': 0.776, 'grad_norm': 0.06434156000614166, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.774472713470459, 'eval_runtime': 6.4832, 'eval_samples_per_second': 154.091, 'eval_steps_per_second': 9.717, 'epoch': 0.6}
{'loss': 0.7817, 'grad_norm': 0.06214486435055733, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7705364227294922, 'eval_runtime': 6.4803, 'eval_samples_per_second': 154.16, 'eval_steps_per_second': 9.722, 'epoch': 0.64}
{'loss': 0.7789, 'grad_norm': 0.06550903618335724, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.767841100692749, 'eval_runtime': 6.468, 'eval_samples_per_second': 154.452, 'eval_steps_per_second': 9.74, 'epoch': 0.68}
{'loss': 0.7871, 'grad_norm': 0.06256777793169022, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7630243897438049, 'eval_runtime': 6.4629, 'eval_samples_per_second': 154.576, 'eval_steps_per_second': 9.748, 'epoch': 0.72}
{'loss': 0.7754, 'grad_norm': 0.07140619307756424, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7611107230186462, 'eval_runtime': 6.4575, 'eval_samples_per_second': 154.704, 'eval_steps_per_second': 9.756, 'epoch': 0.76}
{'loss': 0.7729, 'grad_norm': 0.06861864030361176, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7584373950958252, 'eval_runtime': 6.4689, 'eval_samples_per_second': 154.431, 'eval_steps_per_second': 9.739, 'epoch': 0.8}
{'loss': 0.7609, 'grad_norm': 0.06736837327480316, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7557826042175293, 'eval_runtime': 6.4747, 'eval_samples_per_second': 154.293, 'eval_steps_per_second': 9.73, 'epoch': 0.84}
{'loss': 0.7632, 'grad_norm': 0.07540105283260345, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7536606788635254, 'eval_runtime': 6.4614, 'eval_samples_per_second': 154.61, 'eval_steps_per_second': 9.75, 'epoch': 0.88}
{'loss': 0.7627, 'grad_norm': 0.07349725067615509, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7521502375602722, 'eval_runtime': 6.4587, 'eval_samples_per_second': 154.674, 'eval_steps_per_second': 9.754, 'epoch': 0.92}
{'loss': 0.7459, 'grad_norm': 0.0734875425696373, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7509136199951172, 'eval_runtime': 6.4552, 'eval_samples_per_second': 154.758, 'eval_steps_per_second': 9.76, 'epoch': 0.96}
{'loss': 0.7626, 'grad_norm': 0.07348036766052246, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7506119012832642, 'eval_runtime': 6.4572, 'eval_samples_per_second': 154.711, 'eval_steps_per_second': 9.757, 'epoch': 1.0}
{'train_runtime': 425.0935, 'train_samples_per_second': 23.522, 'train_steps_per_second': 1.47, 'train_loss': 0.9976066711425782, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0699923038482666, 1.281085729598999, 0.9700213074684143, 0.8756524920463562, 0.8415272831916809, 0.8299060463905334, 0.8206574320793152, 0.8120144605636597, 0.8053358793258667, 0.7992063164710999, 0.7926447987556458, 0.7869870662689209, 0.7826514840126038, 0.7792559266090393, 0.774472713470459, 0.7705364227294922, 0.767841100692749, 0.7630243897438049, 0.7611107230186462, 0.7584373950958252, 0.7557826042175293, 0.7536606788635254, 0.7521502375602722, 0.7509136199951172, 0.7506119012832642], 'performance': [0.56, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:32,  5.17s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:43,  1.92it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:15<00:27,  2.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:20<00:19,  2.63it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:12,  2.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:31<00:06,  2.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  4.19it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.14it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4871513843536377
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7470 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.7351201780986243
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.9930868150287534
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.204508568696124
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.277911531716921
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.2466066166460164
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.5520, dtype=torch.float64), 0, 0, tensor(0.4480, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 0, 1, 128, 6.938893903907229e-19, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(9.0078e-17, dtype=torch.float64), tensor(6.4453e-17, dtype=torch.float64), tensor(0.5520, dtype=torch.float64), tensor(9.5226e-17, dtype=torch.float64), tensor(1.3399e-16, dtype=torch.float64), tensor(0.4480, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(6.9389e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.552
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.448
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (6.938893903907229e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  6.938893903907229e-19
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:24,  2.07s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:25,  3.54it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:13,  6.08it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:11,  6.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  7.62it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:07,  8.39it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.23it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  9.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 10.11it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  7.14it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:12<00:01,  7.46it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00,  8.54it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.64it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.2776, 'grad_norm': 0.16608189046382904, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.3600687980651855, 'eval_runtime': 10.4291, 'eval_samples_per_second': 95.79, 'eval_steps_per_second': 6.041, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:54,  1.81it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  8.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 11.96it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:07<00:26,  2.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:16,  4.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:08<00:10,  5.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  6.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:09<00:06,  7.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:10<00:04,  8.56it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:11<00:02,  9.58it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:01, 10.45it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:12<00:01, 10.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00, 10.67it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.60it/s]
Evaluation performance at step 50: 0.47
{'loss': 2.6826, 'grad_norm': 0.05753859132528305, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.47}
{'eval_loss': 2.1265695095062256, 'eval_runtime': 10.4563, 'eval_samples_per_second': 95.54, 'eval_steps_per_second': 6.025, 'epoch': 0.08}
{'loss': 2.0003, 'grad_norm': 0.05321969464421272, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8326772451400757, 'eval_runtime': 10.5487, 'eval_samples_per_second': 94.703, 'eval_steps_per_second': 5.972, 'epoch': 0.12}
{'loss': 1.8137, 'grad_norm': 0.038159169256687164, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7470874786376953, 'eval_runtime': 10.5519, 'eval_samples_per_second': 94.675, 'eval_steps_per_second': 5.97, 'epoch': 0.16}
{'loss': 1.7233, 'grad_norm': 0.041750989854335785, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7105371952056885, 'eval_runtime': 10.592, 'eval_samples_per_second': 94.316, 'eval_steps_per_second': 5.948, 'epoch': 0.2}
{'loss': 1.7408, 'grad_norm': 0.04419749602675438, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6862595081329346, 'eval_runtime': 10.5841, 'eval_samples_per_second': 94.387, 'eval_steps_per_second': 5.952, 'epoch': 0.24}
{'loss': 1.7062, 'grad_norm': 0.05520298331975937, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6656895875930786, 'eval_runtime': 10.5952, 'eval_samples_per_second': 94.288, 'eval_steps_per_second': 5.946, 'epoch': 0.28}
{'loss': 1.6759, 'grad_norm': 0.046939603984355927, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.644517183303833, 'eval_runtime': 10.5947, 'eval_samples_per_second': 94.293, 'eval_steps_per_second': 5.946, 'epoch': 0.32}
{'loss': 1.6523, 'grad_norm': 0.043918803334236145, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6216553449630737, 'eval_runtime': 10.5896, 'eval_samples_per_second': 94.338, 'eval_steps_per_second': 5.949, 'epoch': 0.36}
{'loss': 1.6084, 'grad_norm': 0.05425630137324333, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.576236367225647, 'eval_runtime': 10.5605, 'eval_samples_per_second': 94.598, 'eval_steps_per_second': 5.966, 'epoch': 0.4}
{'loss': 1.6034, 'grad_norm': 0.04568254202604294, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5481138229370117, 'eval_runtime': 10.5704, 'eval_samples_per_second': 94.509, 'eval_steps_per_second': 5.96, 'epoch': 0.44}
{'loss': 1.5615, 'grad_norm': 0.04626205936074257, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5279160737991333, 'eval_runtime': 10.5673, 'eval_samples_per_second': 94.537, 'eval_steps_per_second': 5.962, 'epoch': 0.48}
{'loss': 1.5748, 'grad_norm': 0.05330739915370941, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.504571795463562, 'eval_runtime': 10.5795, 'eval_samples_per_second': 94.428, 'eval_steps_per_second': 5.955, 'epoch': 0.52}
{'loss': 1.5206, 'grad_norm': 0.04811437427997589, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.4902911186218262, 'eval_runtime': 10.5913, 'eval_samples_per_second': 94.323, 'eval_steps_per_second': 5.948, 'epoch': 0.56}
{'loss': 1.5111, 'grad_norm': 0.05556182563304901, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.4822505712509155, 'eval_runtime': 10.6042, 'eval_samples_per_second': 94.208, 'eval_steps_per_second': 5.941, 'epoch': 0.6}
{'loss': 1.5328, 'grad_norm': 0.05682983249425888, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4762049913406372, 'eval_runtime': 10.6146, 'eval_samples_per_second': 94.115, 'eval_steps_per_second': 5.935, 'epoch': 0.64}
{'loss': 1.4861, 'grad_norm': 0.05806209519505501, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4703500270843506, 'eval_runtime': 10.601, 'eval_samples_per_second': 94.237, 'eval_steps_per_second': 5.943, 'epoch': 0.68}
{'loss': 1.5142, 'grad_norm': 0.049285922199487686, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4672602415084839, 'eval_runtime': 10.5968, 'eval_samples_per_second': 94.274, 'eval_steps_per_second': 5.945, 'epoch': 0.72}
{'loss': 1.509, 'grad_norm': 0.05392027273774147, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4612237215042114, 'eval_runtime': 10.6048, 'eval_samples_per_second': 94.202, 'eval_steps_per_second': 5.941, 'epoch': 0.76}
{'loss': 1.5241, 'grad_norm': 0.06724093854427338, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.456221580505371, 'eval_runtime': 10.5847, 'eval_samples_per_second': 94.382, 'eval_steps_per_second': 5.952, 'epoch': 0.8}
{'loss': 1.4881, 'grad_norm': 0.06645448505878448, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4530237913131714, 'eval_runtime': 10.5911, 'eval_samples_per_second': 94.324, 'eval_steps_per_second': 5.948, 'epoch': 0.84}
{'loss': 1.4863, 'grad_norm': 0.06124384328722954, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.44870126247406, 'eval_runtime': 10.601, 'eval_samples_per_second': 94.236, 'eval_steps_per_second': 5.943, 'epoch': 0.88}
{'loss': 1.5009, 'grad_norm': 0.05388456955552101, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4458473920822144, 'eval_runtime': 10.5844, 'eval_samples_per_second': 94.384, 'eval_steps_per_second': 5.952, 'epoch': 0.92}
{'loss': 1.5213, 'grad_norm': 0.052904363721609116, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4439412355422974, 'eval_runtime': 10.5857, 'eval_samples_per_second': 94.372, 'eval_steps_per_second': 5.951, 'epoch': 0.96}
{'loss': 1.4667, 'grad_norm': 0.0631554126739502, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.443362832069397, 'eval_runtime': 10.6132, 'eval_samples_per_second': 94.128, 'eval_steps_per_second': 5.936, 'epoch': 1.0}
{'train_runtime': 611.9289, 'train_samples_per_second': 16.34, 'train_steps_per_second': 1.021, 'train_loss': 1.747289031982422, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3600687980651855, 2.1265695095062256, 1.8326772451400757, 1.7470874786376953, 1.7105371952056885, 1.6862595081329346, 1.6656895875930786, 1.644517183303833, 1.6216553449630737, 1.576236367225647, 1.5481138229370117, 1.5279160737991333, 1.504571795463562, 1.4902911186218262, 1.4822505712509155, 1.4762049913406372, 1.4703500270843506, 1.4672602415084839, 1.4612237215042114, 1.456221580505371, 1.4530237913131714, 1.44870126247406, 1.4458473920822144, 1.4439412355422974, 1.443362832069397], 'performance': [0.55, 0.47]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:13,  6.19s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:27,  3.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:08<00:12,  5.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:09<00:06,  7.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:10<00:03,  9.47it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:16<00:03,  5.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:17<00:00,  7.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:17<00:00,  5.86it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.47]
current iteration observed (possibly low-fid or predicted) performance:  1.4756464958190918
current iteration best possible performance (full train run):  0.5775000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4996 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 1.2309606091696879
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 1.029902107672357
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.180596188832952
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7742085410845782
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 1.1849403454689846
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.7736, dtype=torch.float64), tensor(0.2264, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(3.0942e-17, dtype=torch.float64), tensor(3.7867e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7736, dtype=torch.float64), tensor(0.2264, dtype=torch.float64), tensor(4.9343e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.774
  wikitext: 0.226
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:54,  1.16s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:17,  5.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.73it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.55it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.27it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 10.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 11.82it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.94it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.88it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  9.89it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.90it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.7183, 'grad_norm': 0.43884655833244324, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.561516523361206, 'eval_runtime': 5.6309, 'eval_samples_per_second': 177.413, 'eval_steps_per_second': 11.188, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:03,  1.57it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.94it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.52it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 13.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.81it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 17.95it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.84it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 13.54it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 14.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.03it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.4849, 'grad_norm': 0.33261823654174805, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.6800243854522705, 'eval_runtime': 5.6288, 'eval_samples_per_second': 177.479, 'eval_steps_per_second': 11.192, 'epoch': 0.08}
{'loss': 1.5697, 'grad_norm': 0.09193713963031769, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.310141921043396, 'eval_runtime': 5.6583, 'eval_samples_per_second': 176.554, 'eval_steps_per_second': 11.134, 'epoch': 0.12}
{'loss': 1.3324, 'grad_norm': 0.07001891732215881, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2001643180847168, 'eval_runtime': 5.6523, 'eval_samples_per_second': 176.741, 'eval_steps_per_second': 11.146, 'epoch': 0.16}
{'loss': 1.3021, 'grad_norm': 0.08925089240074158, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1085944175720215, 'eval_runtime': 5.6795, 'eval_samples_per_second': 175.895, 'eval_steps_per_second': 11.093, 'epoch': 0.2}
{'loss': 1.0995, 'grad_norm': 0.07631374895572662, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.063690185546875, 'eval_runtime': 5.6828, 'eval_samples_per_second': 175.794, 'eval_steps_per_second': 11.086, 'epoch': 0.24}
{'loss': 1.163, 'grad_norm': 0.1003708615899086, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0365303754806519, 'eval_runtime': 5.6935, 'eval_samples_per_second': 175.464, 'eval_steps_per_second': 11.065, 'epoch': 0.28}
{'loss': 0.9846, 'grad_norm': 0.09166901558637619, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0162650346755981, 'eval_runtime': 5.7091, 'eval_samples_per_second': 174.985, 'eval_steps_per_second': 11.035, 'epoch': 0.32}
{'loss': 1.0667, 'grad_norm': 0.10282877087593079, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9975751042366028, 'eval_runtime': 5.726, 'eval_samples_per_second': 174.468, 'eval_steps_per_second': 11.002, 'epoch': 0.36}
{'loss': 1.1359, 'grad_norm': 0.09685563296079636, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.980205774307251, 'eval_runtime': 5.7196, 'eval_samples_per_second': 174.663, 'eval_steps_per_second': 11.015, 'epoch': 0.4}
{'loss': 1.0029, 'grad_norm': 0.08169163763523102, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9690135717391968, 'eval_runtime': 5.7466, 'eval_samples_per_second': 173.842, 'eval_steps_per_second': 10.963, 'epoch': 0.44}
{'loss': 1.1032, 'grad_norm': 0.12262889742851257, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9544384479522705, 'eval_runtime': 5.7501, 'eval_samples_per_second': 173.736, 'eval_steps_per_second': 10.956, 'epoch': 0.48}
{'loss': 0.9355, 'grad_norm': 0.13210615515708923, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9418185353279114, 'eval_runtime': 5.7391, 'eval_samples_per_second': 174.069, 'eval_steps_per_second': 10.977, 'epoch': 0.52}
{'loss': 0.9527, 'grad_norm': 0.1045655682682991, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9310436248779297, 'eval_runtime': 5.7121, 'eval_samples_per_second': 174.893, 'eval_steps_per_second': 11.029, 'epoch': 0.56}
{'loss': 1.0688, 'grad_norm': 0.08868683129549026, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9194469451904297, 'eval_runtime': 5.7078, 'eval_samples_per_second': 175.024, 'eval_steps_per_second': 11.038, 'epoch': 0.6}
{'loss': 0.9608, 'grad_norm': 0.1251685917377472, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9093465805053711, 'eval_runtime': 5.7027, 'eval_samples_per_second': 175.179, 'eval_steps_per_second': 11.047, 'epoch': 0.64}
{'loss': 0.967, 'grad_norm': 0.1355390101671219, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9008278250694275, 'eval_runtime': 5.705, 'eval_samples_per_second': 175.11, 'eval_steps_per_second': 11.043, 'epoch': 0.68}
{'loss': 0.9839, 'grad_norm': 0.16065312922000885, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8912962079048157, 'eval_runtime': 5.7034, 'eval_samples_per_second': 175.159, 'eval_steps_per_second': 11.046, 'epoch': 0.72}
{'loss': 0.96, 'grad_norm': 0.11908616870641708, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8825950026512146, 'eval_runtime': 5.7104, 'eval_samples_per_second': 174.944, 'eval_steps_per_second': 11.032, 'epoch': 0.76}
{'loss': 0.9555, 'grad_norm': 0.13380756974220276, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8765708804130554, 'eval_runtime': 5.7038, 'eval_samples_per_second': 175.146, 'eval_steps_per_second': 11.045, 'epoch': 0.8}
{'loss': 0.8256, 'grad_norm': 0.1682068407535553, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8686053156852722, 'eval_runtime': 5.7073, 'eval_samples_per_second': 175.038, 'eval_steps_per_second': 11.038, 'epoch': 0.84}
{'loss': 0.9794, 'grad_norm': 0.15671949088573456, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8625864386558533, 'eval_runtime': 5.693, 'eval_samples_per_second': 175.48, 'eval_steps_per_second': 11.066, 'epoch': 0.88}
{'loss': 0.9062, 'grad_norm': 0.15354299545288086, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8590139746665955, 'eval_runtime': 5.6975, 'eval_samples_per_second': 175.34, 'eval_steps_per_second': 11.057, 'epoch': 0.92}
{'loss': 0.929, 'grad_norm': 0.14982615411281586, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8561080694198608, 'eval_runtime': 5.7023, 'eval_samples_per_second': 175.193, 'eval_steps_per_second': 11.048, 'epoch': 0.96}
{'loss': 0.9285, 'grad_norm': 0.17357854545116425, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8546893000602722, 'eval_runtime': 5.6946, 'eval_samples_per_second': 175.43, 'eval_steps_per_second': 11.063, 'epoch': 1.0}
{'train_runtime': 389.9734, 'train_samples_per_second': 25.64, 'train_steps_per_second': 1.603, 'train_loss': 1.2526458465576171, 'epoch': 1.0}
train_results:  {'eval_loss': [3.561516523361206, 1.6800243854522705, 1.310141921043396, 1.2001643180847168, 1.1085944175720215, 1.063690185546875, 1.0365303754806519, 1.0162650346755981, 0.9975751042366028, 0.980205774307251, 0.9690135717391968, 0.9544384479522705, 0.9418185353279114, 0.9310436248779297, 0.9194469451904297, 0.9093465805053711, 0.9008278250694275, 0.8912962079048157, 0.8825950026512146, 0.8765708804130554, 0.8686053156852722, 0.8625864386558533, 0.8590139746665955, 0.8561080694198608, 0.8546893000602722], 'performance': [0.56, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:33,  5.18s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:20,  4.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:08,  8.08it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  6.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  9.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 12.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 17.10it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.65it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4931979179382324
current iteration best possible performance (full train run):  0.651
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4856 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.9968232023603876
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.9030487734683115
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.9772837214444442
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.8324414637711604
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 1.104443702570599
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(1.0000, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.0, 1.4800000190734992, 1]
normalized proposed parameters for next round by BO: [tensor(1.2745e-17, dtype=torch.float64), tensor(1.0449e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.4202e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(4.6838e-17, dtype=torch.float64), tensor(3.2049e-16, dtype=torch.float64), tensor(2.2335e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 1.0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734992,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734992
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:02,  1.24s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.94it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:09,  8.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:05,  9.25it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04,  9.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.38it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 11.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.51it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.26it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.50it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.9533, 'grad_norm': 0.40338724851608276, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.3957605361938477, 'eval_runtime': 3.6728, 'eval_samples_per_second': 272.0, 'eval_steps_per_second': 17.153, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:04,  1.54it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.44it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.44it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 12.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:05, 13.25it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 13.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 14.42it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:08,  4.17it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:05,  5.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  6.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  6.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  8.54it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.38it/s]
Evaluation performance at step 50: 0.42
{'loss': 2.0647, 'grad_norm': 0.39836734533309937, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.42}
{'eval_loss': 1.2174898386001587, 'eval_runtime': 3.6789, 'eval_samples_per_second': 271.545, 'eval_steps_per_second': 17.124, 'epoch': 0.08}
{'loss': 1.0319, 'grad_norm': 0.1221172958612442, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9253382086753845, 'eval_runtime': 3.6769, 'eval_samples_per_second': 271.698, 'eval_steps_per_second': 17.134, 'epoch': 0.12}
{'loss': 0.8412, 'grad_norm': 0.07341860979795456, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7637249231338501, 'eval_runtime': 3.6891, 'eval_samples_per_second': 270.796, 'eval_steps_per_second': 17.077, 'epoch': 0.16}
{'loss': 0.7393, 'grad_norm': 0.07703156024217606, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7344062924385071, 'eval_runtime': 3.6907, 'eval_samples_per_second': 270.677, 'eval_steps_per_second': 17.07, 'epoch': 0.2}
{'loss': 0.6901, 'grad_norm': 0.05759726092219353, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7012384533882141, 'eval_runtime': 3.6677, 'eval_samples_per_second': 272.38, 'eval_steps_per_second': 17.177, 'epoch': 0.24}
{'loss': 0.7007, 'grad_norm': 0.06786392629146576, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6688622236251831, 'eval_runtime': 3.6666, 'eval_samples_per_second': 272.459, 'eval_steps_per_second': 17.182, 'epoch': 0.28}
{'loss': 0.6528, 'grad_norm': 0.07631932199001312, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6285783052444458, 'eval_runtime': 3.6699, 'eval_samples_per_second': 272.212, 'eval_steps_per_second': 17.167, 'epoch': 0.32}
{'loss': 0.6006, 'grad_norm': 0.09128426015377045, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5855387449264526, 'eval_runtime': 3.6771, 'eval_samples_per_second': 271.682, 'eval_steps_per_second': 17.133, 'epoch': 0.36}
{'loss': 0.5305, 'grad_norm': 0.10245414823293686, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5383741855621338, 'eval_runtime': 3.6842, 'eval_samples_per_second': 271.158, 'eval_steps_per_second': 17.1, 'epoch': 0.4}
{'loss': 0.5282, 'grad_norm': 0.11934757977724075, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.48944321274757385, 'eval_runtime': 3.6756, 'eval_samples_per_second': 271.792, 'eval_steps_per_second': 17.14, 'epoch': 0.44}
{'loss': 0.4936, 'grad_norm': 0.11790754646062851, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.44115856289863586, 'eval_runtime': 3.6851, 'eval_samples_per_second': 271.092, 'eval_steps_per_second': 17.096, 'epoch': 0.48}
{'loss': 0.4255, 'grad_norm': 0.1301913559436798, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3916206955909729, 'eval_runtime': 3.679, 'eval_samples_per_second': 271.538, 'eval_steps_per_second': 17.124, 'epoch': 0.52}
{'loss': 0.3995, 'grad_norm': 0.14593540132045746, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.34344473481178284, 'eval_runtime': 3.6789, 'eval_samples_per_second': 271.545, 'eval_steps_per_second': 17.124, 'epoch': 0.56}
{'loss': 0.3314, 'grad_norm': 0.13673590123653412, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3052370548248291, 'eval_runtime': 3.676, 'eval_samples_per_second': 271.763, 'eval_steps_per_second': 17.138, 'epoch': 0.6}
{'loss': 0.3078, 'grad_norm': 0.16573508083820343, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.2705569267272949, 'eval_runtime': 3.6833, 'eval_samples_per_second': 271.226, 'eval_steps_per_second': 17.104, 'epoch': 0.64}
{'loss': 0.2714, 'grad_norm': 0.1553286463022232, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.24832487106323242, 'eval_runtime': 3.6787, 'eval_samples_per_second': 271.566, 'eval_steps_per_second': 17.126, 'epoch': 0.68}
{'loss': 0.2404, 'grad_norm': 0.13120068609714508, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.22856619954109192, 'eval_runtime': 3.707, 'eval_samples_per_second': 269.49, 'eval_steps_per_second': 16.995, 'epoch': 0.72}
{'loss': 0.2391, 'grad_norm': 0.15556278824806213, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2097383588552475, 'eval_runtime': 3.6787, 'eval_samples_per_second': 271.561, 'eval_steps_per_second': 17.125, 'epoch': 0.76}
{'loss': 0.2161, 'grad_norm': 0.1353369504213333, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.19591860473155975, 'eval_runtime': 3.6834, 'eval_samples_per_second': 271.215, 'eval_steps_per_second': 17.104, 'epoch': 0.8}
{'loss': 0.1978, 'grad_norm': 0.17436449229717255, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.18531090021133423, 'eval_runtime': 3.6784, 'eval_samples_per_second': 271.585, 'eval_steps_per_second': 17.127, 'epoch': 0.84}
{'loss': 0.1861, 'grad_norm': 0.09834522753953934, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.17786525189876556, 'eval_runtime': 3.6857, 'eval_samples_per_second': 271.047, 'eval_steps_per_second': 17.093, 'epoch': 0.88}
{'loss': 0.1807, 'grad_norm': 0.09484237432479858, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.17223379015922546, 'eval_runtime': 3.6814, 'eval_samples_per_second': 271.367, 'eval_steps_per_second': 17.113, 'epoch': 0.92}
{'loss': 0.181, 'grad_norm': 0.1405669003725052, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.16796128451824188, 'eval_runtime': 3.6767, 'eval_samples_per_second': 271.712, 'eval_steps_per_second': 17.135, 'epoch': 0.96}
{'loss': 0.174, 'grad_norm': 0.13894696533679962, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.16617847979068756, 'eval_runtime': 3.6788, 'eval_samples_per_second': 271.555, 'eval_steps_per_second': 17.125, 'epoch': 1.0}
{'train_runtime': 284.7848, 'train_samples_per_second': 35.111, 'train_steps_per_second': 2.195, 'train_loss': 0.6871067337036133, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3957605361938477, 1.2174898386001587, 0.9253382086753845, 0.7637249231338501, 0.7344062924385071, 0.7012384533882141, 0.6688622236251831, 0.6285783052444458, 0.5855387449264526, 0.5383741855621338, 0.48944321274757385, 0.44115856289863586, 0.3916206955909729, 0.34344473481178284, 0.3052370548248291, 0.2705569267272949, 0.24832487106323242, 0.22856619954109192, 0.2097383588552475, 0.19591860473155975, 0.18531090021133423, 0.17786525189876556, 0.17223379015922546, 0.16796128451824188, 0.16617847979068756], 'performance': [0.55, 0.42]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:54,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 18.03it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 21.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:09,  5.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:04,  8.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.64it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 14.37it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.36it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.42]
current iteration observed (possibly low-fid or predicted) performance:  1.4928396940231323
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7615 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 1.1864766211310018
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.9243055735641919
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.9781238559467555
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 1.0899096323733304
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 1.155068065694561
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.2265, dtype=torch.float64), 0, tensor(0.7735, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.6774e-17, dtype=torch.float64), tensor(4.4471e-17, dtype=torch.float64), tensor(4.7100e-17, dtype=torch.float64), tensor(4.0627e-17, dtype=torch.float64), tensor(1.2384e-16, dtype=torch.float64), tensor(0.2265, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7735, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.227
  wikitext: 0
  mmlu: 0.773
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:54,  2.36s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:28,  3.19it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:04<00:16,  5.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:13,  5.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:09,  6.97it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:07,  7.70it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:06,  7.60it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:09<00:05,  7.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:04,  8.71it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:02,  9.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:12<00:02,  6.52it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:13<00:01,  6.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:14<00:00,  7.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.85it/s]
Evaluation performance at step 25: 0.56
{'loss': 3.6856, 'grad_norm': 0.33102211356163025, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 2.6608169078826904, 'eval_runtime': 9.735, 'eval_samples_per_second': 102.62, 'eval_steps_per_second': 6.472, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:49,  2.02it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  8.67it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.86it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:14,  4.49it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:11<00:20,  2.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:16<00:22,  2.26it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:21,  2.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:26<00:18,  1.88it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:27<00:10,  2.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:27<00:05,  3.36it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:28<00:02,  4.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:29<00:00,  5.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:29<00:00,  3.44it/s]
Evaluation performance at step 50: 0.57
{'loss': 1.9513, 'grad_norm': 0.16843879222869873, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 1.5644675493240356, 'eval_runtime': 9.7147, 'eval_samples_per_second': 102.834, 'eval_steps_per_second': 6.485, 'epoch': 0.08}
{'loss': 1.4259, 'grad_norm': 0.08948464691638947, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3885794878005981, 'eval_runtime': 9.7793, 'eval_samples_per_second': 102.155, 'eval_steps_per_second': 6.442, 'epoch': 0.12}
{'loss': 1.3606, 'grad_norm': 0.05968733876943588, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2852916717529297, 'eval_runtime': 9.8309, 'eval_samples_per_second': 101.618, 'eval_steps_per_second': 6.408, 'epoch': 0.16}
{'loss': 1.2627, 'grad_norm': 0.052265796810388565, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.245003342628479, 'eval_runtime': 9.8445, 'eval_samples_per_second': 101.478, 'eval_steps_per_second': 6.4, 'epoch': 0.2}
{'loss': 1.1814, 'grad_norm': 0.03813258931040764, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2276908159255981, 'eval_runtime': 9.8638, 'eval_samples_per_second': 101.28, 'eval_steps_per_second': 6.387, 'epoch': 0.24}
{'loss': 1.2405, 'grad_norm': 0.04790522903203964, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.219261884689331, 'eval_runtime': 9.869, 'eval_samples_per_second': 101.226, 'eval_steps_per_second': 6.384, 'epoch': 0.28}
{'loss': 1.216, 'grad_norm': 0.048535577952861786, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2114289999008179, 'eval_runtime': 9.886, 'eval_samples_per_second': 101.052, 'eval_steps_per_second': 6.373, 'epoch': 0.32}
{'loss': 1.2258, 'grad_norm': 0.05014593526721001, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.206666350364685, 'eval_runtime': 9.8732, 'eval_samples_per_second': 101.183, 'eval_steps_per_second': 6.381, 'epoch': 0.36}
{'loss': 1.1899, 'grad_norm': 0.0498267337679863, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1979093551635742, 'eval_runtime': 9.8762, 'eval_samples_per_second': 101.153, 'eval_steps_per_second': 6.379, 'epoch': 0.4}
{'loss': 1.1956, 'grad_norm': 0.04549625515937805, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1923083066940308, 'eval_runtime': 9.8811, 'eval_samples_per_second': 101.102, 'eval_steps_per_second': 6.376, 'epoch': 0.44}
{'loss': 1.1656, 'grad_norm': 0.04807606339454651, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.185542106628418, 'eval_runtime': 9.8774, 'eval_samples_per_second': 101.14, 'eval_steps_per_second': 6.378, 'epoch': 0.48}
{'loss': 1.1568, 'grad_norm': 0.047291386872529984, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1807347536087036, 'eval_runtime': 9.8713, 'eval_samples_per_second': 101.202, 'eval_steps_per_second': 6.382, 'epoch': 0.52}
{'loss': 1.1837, 'grad_norm': 0.06561942398548126, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1770596504211426, 'eval_runtime': 9.861, 'eval_samples_per_second': 101.308, 'eval_steps_per_second': 6.389, 'epoch': 0.56}
{'loss': 1.1685, 'grad_norm': 0.05509921535849571, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1716883182525635, 'eval_runtime': 9.9063, 'eval_samples_per_second': 100.845, 'eval_steps_per_second': 6.36, 'epoch': 0.6}
{'loss': 1.1809, 'grad_norm': 0.05962851643562317, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1662814617156982, 'eval_runtime': 9.8859, 'eval_samples_per_second': 101.053, 'eval_steps_per_second': 6.373, 'epoch': 0.64}
{'loss': 1.1417, 'grad_norm': 0.054733287543058395, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1624596118927002, 'eval_runtime': 9.8865, 'eval_samples_per_second': 101.047, 'eval_steps_per_second': 6.372, 'epoch': 0.68}
{'loss': 1.1863, 'grad_norm': 0.05806445702910423, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1583590507507324, 'eval_runtime': 9.8961, 'eval_samples_per_second': 100.949, 'eval_steps_per_second': 6.366, 'epoch': 0.72}
{'loss': 1.1636, 'grad_norm': 0.05776762589812279, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1552839279174805, 'eval_runtime': 9.885, 'eval_samples_per_second': 101.063, 'eval_steps_per_second': 6.373, 'epoch': 0.76}
{'loss': 1.1787, 'grad_norm': 0.04667919501662254, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1523163318634033, 'eval_runtime': 9.8637, 'eval_samples_per_second': 101.281, 'eval_steps_per_second': 6.387, 'epoch': 0.8}
{'loss': 1.1282, 'grad_norm': 0.04889863356947899, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1498607397079468, 'eval_runtime': 9.8694, 'eval_samples_per_second': 101.222, 'eval_steps_per_second': 6.383, 'epoch': 0.84}
{'loss': 1.1288, 'grad_norm': 0.07204722613096237, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1477179527282715, 'eval_runtime': 9.8777, 'eval_samples_per_second': 101.137, 'eval_steps_per_second': 6.378, 'epoch': 0.88}
{'loss': 1.1823, 'grad_norm': 0.0531921312212944, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1460148096084595, 'eval_runtime': 9.8767, 'eval_samples_per_second': 101.147, 'eval_steps_per_second': 6.379, 'epoch': 0.92}
{'loss': 1.1335, 'grad_norm': 0.056074224412441254, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1447120904922485, 'eval_runtime': 9.8797, 'eval_samples_per_second': 101.117, 'eval_steps_per_second': 6.377, 'epoch': 0.96}
{'loss': 1.1504, 'grad_norm': 0.056612562388181686, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.144203782081604, 'eval_runtime': 9.8893, 'eval_samples_per_second': 101.018, 'eval_steps_per_second': 6.371, 'epoch': 1.0}
{'train_runtime': 605.9684, 'train_samples_per_second': 16.501, 'train_steps_per_second': 1.031, 'train_loss': 1.327367840576172, 'epoch': 1.0}
train_results:  {'eval_loss': [2.6608169078826904, 1.5644675493240356, 1.3885794878005981, 1.2852916717529297, 1.245003342628479, 1.2276908159255981, 1.219261884689331, 1.2114289999008179, 1.206666350364685, 1.1979093551635742, 1.1923083066940308, 1.185542106628418, 1.1807347536087036, 1.1770596504211426, 1.1716883182525635, 1.1662814617156982, 1.1624596118927002, 1.1583590507507324, 1.1552839279174805, 1.1523163318634033, 1.1498607397079468, 1.1477179527282715, 1.1460148096084595, 1.1447120904922485, 1.144203782081604], 'performance': [0.56, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:47,  1.69s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:12,  6.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:08,  6.15it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:13<00:08,  4.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:03,  5.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:14<00:00,  8.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.80it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.4800448417663574
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4811 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 1.1881772499621066
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 1.234295142466044
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 1.161637995969277
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 1.0170819501210269
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 1.2100033760448872
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.3112, dtype=torch.float64), 0, 0, tensor(0.6888, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.09999999999999999, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.6295e-17, dtype=torch.float64), tensor(2.8154e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3112, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6888, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.311
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.689

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999999,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.09999999999999999
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:40,  2.22s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:27,  3.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:04<00:16,  5.08it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:12,  5.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:09,  7.15it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:07,  7.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.37it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.52it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  9.09it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:02,  9.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  7.21it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:12<00:01,  7.60it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00,  8.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.45it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.0734, 'grad_norm': 0.16061298549175262, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 2.9562318325042725, 'eval_runtime': 7.6484, 'eval_samples_per_second': 130.615, 'eval_steps_per_second': 8.237, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:47,  2.10it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.36it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.21it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:14,  4.61it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.50it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  7.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  9.91it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 11.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  8.48it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:12<00:01,  8.33it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  9.47it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  7.84it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.9811, 'grad_norm': 0.059306662529706955, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.349446415901184, 'eval_runtime': 6.9421, 'eval_samples_per_second': 143.905, 'eval_steps_per_second': 9.075, 'epoch': 0.08}
{'loss': 1.223, 'grad_norm': 0.08960145711898804, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.16981041431427, 'eval_runtime': 6.9541, 'eval_samples_per_second': 143.657, 'eval_steps_per_second': 9.059, 'epoch': 0.12}
{'loss': 1.0988, 'grad_norm': 0.03962141275405884, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.075671672821045, 'eval_runtime': 6.9727, 'eval_samples_per_second': 143.272, 'eval_steps_per_second': 9.035, 'epoch': 0.16}
{'loss': 1.0593, 'grad_norm': 0.03420113027095795, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0412933826446533, 'eval_runtime': 7.0019, 'eval_samples_per_second': 142.675, 'eval_steps_per_second': 8.998, 'epoch': 0.2}
{'loss': 1.0068, 'grad_norm': 0.03739486262202263, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.014351487159729, 'eval_runtime': 6.9949, 'eval_samples_per_second': 142.818, 'eval_steps_per_second': 9.007, 'epoch': 0.24}
{'loss': 1.012, 'grad_norm': 0.035402461886405945, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9871674180030823, 'eval_runtime': 7.0079, 'eval_samples_per_second': 142.554, 'eval_steps_per_second': 8.99, 'epoch': 0.28}
{'loss': 0.9761, 'grad_norm': 0.05036066472530365, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.958237886428833, 'eval_runtime': 7.0214, 'eval_samples_per_second': 142.279, 'eval_steps_per_second': 8.973, 'epoch': 0.32}
{'loss': 0.9306, 'grad_norm': 0.0552518405020237, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9152817726135254, 'eval_runtime': 7.0241, 'eval_samples_per_second': 142.225, 'eval_steps_per_second': 8.969, 'epoch': 0.36}
{'loss': 0.8861, 'grad_norm': 0.0661926195025444, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.836799681186676, 'eval_runtime': 7.0257, 'eval_samples_per_second': 142.193, 'eval_steps_per_second': 8.967, 'epoch': 0.4}
{'loss': 0.8327, 'grad_norm': 0.05421622842550278, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7999441623687744, 'eval_runtime': 7.019, 'eval_samples_per_second': 142.328, 'eval_steps_per_second': 8.976, 'epoch': 0.44}
{'loss': 0.7948, 'grad_norm': 0.0575854554772377, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7608702778816223, 'eval_runtime': 7.0127, 'eval_samples_per_second': 142.455, 'eval_steps_per_second': 8.984, 'epoch': 0.48}
{'loss': 0.741, 'grad_norm': 0.06013988330960274, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7207872867584229, 'eval_runtime': 7.0196, 'eval_samples_per_second': 142.315, 'eval_steps_per_second': 8.975, 'epoch': 0.52}
{'loss': 0.6887, 'grad_norm': 0.05962565913796425, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6972614526748657, 'eval_runtime': 7.0157, 'eval_samples_per_second': 142.395, 'eval_steps_per_second': 8.98, 'epoch': 0.56}
{'loss': 0.6895, 'grad_norm': 0.07912863045930862, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6715298295021057, 'eval_runtime': 7.0283, 'eval_samples_per_second': 142.139, 'eval_steps_per_second': 8.964, 'epoch': 0.6}
{'loss': 0.688, 'grad_norm': 0.0772198960185051, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6507582068443298, 'eval_runtime': 7.0266, 'eval_samples_per_second': 142.173, 'eval_steps_per_second': 8.966, 'epoch': 0.64}
{'loss': 0.6591, 'grad_norm': 0.09255404025316238, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6299996376037598, 'eval_runtime': 7.0233, 'eval_samples_per_second': 142.241, 'eval_steps_per_second': 8.97, 'epoch': 0.68}
{'loss': 0.6182, 'grad_norm': 0.09176891297101974, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6086363196372986, 'eval_runtime': 7.0407, 'eval_samples_per_second': 141.889, 'eval_steps_per_second': 8.948, 'epoch': 0.72}
{'loss': 0.6247, 'grad_norm': 0.092697873711586, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5881715416908264, 'eval_runtime': 7.0538, 'eval_samples_per_second': 141.625, 'eval_steps_per_second': 8.931, 'epoch': 0.76}
{'loss': 0.6071, 'grad_norm': 0.11579377949237823, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5662948489189148, 'eval_runtime': 7.0533, 'eval_samples_per_second': 141.636, 'eval_steps_per_second': 8.932, 'epoch': 0.8}
{'loss': 0.593, 'grad_norm': 0.10131186246871948, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5498495697975159, 'eval_runtime': 7.0441, 'eval_samples_per_second': 141.822, 'eval_steps_per_second': 8.944, 'epoch': 0.84}
{'loss': 0.5742, 'grad_norm': 0.11034783720970154, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5295920968055725, 'eval_runtime': 7.0317, 'eval_samples_per_second': 142.071, 'eval_steps_per_second': 8.959, 'epoch': 0.88}
{'loss': 0.543, 'grad_norm': 0.12560997903347015, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5167669057846069, 'eval_runtime': 7.0307, 'eval_samples_per_second': 142.09, 'eval_steps_per_second': 8.961, 'epoch': 0.92}
{'loss': 0.5679, 'grad_norm': 0.12622949481010437, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5086029171943665, 'eval_runtime': 7.0308, 'eval_samples_per_second': 142.089, 'eval_steps_per_second': 8.961, 'epoch': 0.96}
{'loss': 0.5546, 'grad_norm': 0.1596911996603012, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5051066279411316, 'eval_runtime': 7.0357, 'eval_samples_per_second': 141.989, 'eval_steps_per_second': 8.954, 'epoch': 1.0}
{'train_runtime': 443.2346, 'train_samples_per_second': 22.559, 'train_steps_per_second': 1.41, 'train_loss': 0.9609508255004883, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9562318325042725, 1.349446415901184, 1.16981041431427, 1.075671672821045, 1.0412933826446533, 1.014351487159729, 0.9871674180030823, 0.958237886428833, 0.9152817726135254, 0.836799681186676, 0.7999441623687744, 0.7608702778816223, 0.7207872867584229, 0.6972614526748657, 0.6715298295021057, 0.6507582068443298, 0.6299996376037598, 0.6086363196372986, 0.5881715416908264, 0.5662948489189148, 0.5498495697975159, 0.5295920968055725, 0.5167669057846069, 0.5086029171943665, 0.5051066279411316], 'performance': [0.56, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:08,  5.54s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:10<00:46,  1.79it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:11<00:17,  3.90it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:12<00:08,  5.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:13<00:04,  7.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:01, 10.53it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:14<00:00, 13.96it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.83it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.4647204875946045
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9488 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 1.0657382119539627
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.9843299786311008
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.5701271861934745
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.975709781453709
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 1.1244284139776204
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.9508, dtype=torch.float64), tensor(0.0489, dtype=torch.float64), 0, 0, 32, 0, 0, 1, 0, 0, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(5.8279e-17, dtype=torch.float64), tensor(3.0330e-18, dtype=torch.float64), tensor(2.5941e-17, dtype=torch.float64), tensor(6.0274e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9508, dtype=torch.float64), tensor(0.0489, dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(1.0794e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.951
  wikitext: 0.049
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:27,  1.49s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:17,  5.33it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  8.10it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.19it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 11.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 12.11it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:03, 12.79it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 13.77it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 14.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 11.91it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 11.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 12.37it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.32it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.0511, 'grad_norm': 0.2550443410873413, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.810896158218384, 'eval_runtime': 4.2699, 'eval_samples_per_second': 233.965, 'eval_steps_per_second': 14.755, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:38,  2.56it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:06, 13.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.12it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 17.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 16.81it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 16.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 14.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 16.07it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.21it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 17.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 15.41it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 13.16it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 14.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.57it/s]
Evaluation performance at step 50: 0.55
{'loss': 2.5754, 'grad_norm': 0.07732297480106354, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 1.709395170211792, 'eval_runtime': 4.26, 'eval_samples_per_second': 234.507, 'eval_steps_per_second': 14.789, 'epoch': 0.08}
{'loss': 1.54, 'grad_norm': 0.15782718360424042, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3741014003753662, 'eval_runtime': 4.2713, 'eval_samples_per_second': 233.885, 'eval_steps_per_second': 14.749, 'epoch': 0.12}
{'loss': 1.2874, 'grad_norm': 0.09076222032308578, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.216611385345459, 'eval_runtime': 4.2778, 'eval_samples_per_second': 233.53, 'eval_steps_per_second': 14.727, 'epoch': 0.16}
{'loss': 1.2228, 'grad_norm': 0.04816910997033119, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1556941270828247, 'eval_runtime': 4.2704, 'eval_samples_per_second': 233.937, 'eval_steps_per_second': 14.753, 'epoch': 0.2}
{'loss': 1.156, 'grad_norm': 0.04689354449510574, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.11268150806427, 'eval_runtime': 4.2838, 'eval_samples_per_second': 233.203, 'eval_steps_per_second': 14.707, 'epoch': 0.24}
{'loss': 1.1567, 'grad_norm': 0.059432253241539, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0771961212158203, 'eval_runtime': 4.2948, 'eval_samples_per_second': 232.608, 'eval_steps_per_second': 14.669, 'epoch': 0.28}
{'loss': 1.0833, 'grad_norm': 0.060897499322891235, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0380510091781616, 'eval_runtime': 4.3047, 'eval_samples_per_second': 232.072, 'eval_steps_per_second': 14.635, 'epoch': 0.32}
{'loss': 1.0947, 'grad_norm': 0.05968117341399193, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9953130483627319, 'eval_runtime': 4.3035, 'eval_samples_per_second': 232.135, 'eval_steps_per_second': 14.639, 'epoch': 0.36}
{'loss': 1.0136, 'grad_norm': 0.07222479581832886, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9550216794013977, 'eval_runtime': 4.3041, 'eval_samples_per_second': 232.104, 'eval_steps_per_second': 14.637, 'epoch': 0.4}
{'loss': 0.9583, 'grad_norm': 0.10826422274112701, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.910683274269104, 'eval_runtime': 4.3117, 'eval_samples_per_second': 231.694, 'eval_steps_per_second': 14.611, 'epoch': 0.44}
{'loss': 0.8815, 'grad_norm': 0.10630456358194351, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8640168309211731, 'eval_runtime': 4.3124, 'eval_samples_per_second': 231.66, 'eval_steps_per_second': 14.609, 'epoch': 0.48}
{'loss': 0.8143, 'grad_norm': 0.10484236478805542, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8013778328895569, 'eval_runtime': 4.3171, 'eval_samples_per_second': 231.403, 'eval_steps_per_second': 14.593, 'epoch': 0.52}
{'loss': 0.8536, 'grad_norm': 0.145946204662323, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.73942631483078, 'eval_runtime': 4.3127, 'eval_samples_per_second': 231.641, 'eval_steps_per_second': 14.608, 'epoch': 0.56}
{'loss': 0.7684, 'grad_norm': 0.16633948683738708, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6640423536300659, 'eval_runtime': 4.3012, 'eval_samples_per_second': 232.258, 'eval_steps_per_second': 14.647, 'epoch': 0.6}
{'loss': 0.5998, 'grad_norm': 0.12959599494934082, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5942906141281128, 'eval_runtime': 4.303, 'eval_samples_per_second': 232.161, 'eval_steps_per_second': 14.641, 'epoch': 0.64}
{'loss': 0.6414, 'grad_norm': 0.14693044126033783, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5584596395492554, 'eval_runtime': 4.3118, 'eval_samples_per_second': 231.688, 'eval_steps_per_second': 14.611, 'epoch': 0.68}
{'loss': 0.6151, 'grad_norm': 0.17636029422283173, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5242422223091125, 'eval_runtime': 4.3024, 'eval_samples_per_second': 232.197, 'eval_steps_per_second': 14.643, 'epoch': 0.72}
{'loss': 0.5753, 'grad_norm': 0.17145319283008575, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.492339551448822, 'eval_runtime': 4.2977, 'eval_samples_per_second': 232.453, 'eval_steps_per_second': 14.659, 'epoch': 0.76}
{'loss': 0.5048, 'grad_norm': 0.12992806732654572, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.47016581892967224, 'eval_runtime': 4.3011, 'eval_samples_per_second': 232.267, 'eval_steps_per_second': 14.647, 'epoch': 0.8}
{'loss': 0.4634, 'grad_norm': 0.1311827301979065, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4506354331970215, 'eval_runtime': 4.3008, 'eval_samples_per_second': 232.284, 'eval_steps_per_second': 14.649, 'epoch': 0.84}
{'loss': 0.4343, 'grad_norm': 0.15103945136070251, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.4319221079349518, 'eval_runtime': 4.3172, 'eval_samples_per_second': 231.403, 'eval_steps_per_second': 14.593, 'epoch': 0.88}
{'loss': 0.4945, 'grad_norm': 0.1524243801832199, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4194057285785675, 'eval_runtime': 4.3009, 'eval_samples_per_second': 232.276, 'eval_steps_per_second': 14.648, 'epoch': 0.92}
{'loss': 0.4718, 'grad_norm': 0.18846774101257324, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.4112856686115265, 'eval_runtime': 4.3066, 'eval_samples_per_second': 231.971, 'eval_steps_per_second': 14.629, 'epoch': 0.96}
{'loss': 0.4331, 'grad_norm': 0.16135233640670776, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.40791475772857666, 'eval_runtime': 4.3106, 'eval_samples_per_second': 231.755, 'eval_steps_per_second': 14.615, 'epoch': 1.0}
{'train_runtime': 295.5168, 'train_samples_per_second': 33.829, 'train_steps_per_second': 2.115, 'train_loss': 1.0676243103027343, 'epoch': 1.0}
train_results:  {'eval_loss': [3.810896158218384, 1.709395170211792, 1.3741014003753662, 1.216611385345459, 1.1556941270828247, 1.11268150806427, 1.0771961212158203, 1.0380510091781616, 0.9953130483627319, 0.9550216794013977, 0.910683274269104, 0.8640168309211731, 0.8013778328895569, 0.73942631483078, 0.6640423536300659, 0.5942906141281128, 0.5584596395492554, 0.5242422223091125, 0.492339551448822, 0.47016581892967224, 0.4506354331970215, 0.4319221079349518, 0.4194057285785675, 0.4112856686115265, 0.40791475772857666], 'performance': [0.56, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:58,  4.84s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:40,  2.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:25,  2.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:17,  2.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:11,  3.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:05,  3.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:32<00:00,  3.53it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.11it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.4780852794647217
current iteration best possible performance (full train run):  0.378
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4726 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7134326282658644
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 1.191988278597228
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 1.0538777557906496
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 1.0108974439150193
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 1.1294678315359248
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.0719, dtype=torch.float64), tensor(0.5133, dtype=torch.float64), tensor(0.4147, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 1, 128, 9.8879238130678e-18, 1.4800000190734885, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6536e-18, dtype=torch.float64), tensor(0.0719, dtype=torch.float64), tensor(0.5133, dtype=torch.float64), tensor(0.4147, dtype=torch.float64), tensor(3.2141e-17, dtype=torch.float64), tensor(5.2957e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(9.8879e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.072
  truthfulqa_gen: 0.513
  wikitext: 0.415
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (9.8879238130678e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  9.8879238130678e-18
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:02,  1.85s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  3.99it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  6.93it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.13it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05,  9.94it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05, 10.18it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.02it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 12.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01,  9.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.87it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  9.94it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.13it/s]
Evaluation performance at step 25: 0.53
{'loss': 4.1898, 'grad_norm': 0.24295853078365326, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.53}
{'eval_loss': 3.367725133895874, 'eval_runtime': 8.2654, 'eval_samples_per_second': 120.865, 'eval_steps_per_second': 7.622, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:42,  1.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 10.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.24it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.76it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.28it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 15.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.27it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 16.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 18.29it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 18.00it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 17.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.02it/s]
Evaluation performance at step 50: 0.56
{'loss': 2.5929, 'grad_norm': 0.40710991621017456, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.9424636363983154, 'eval_runtime': 8.2665, 'eval_samples_per_second': 120.849, 'eval_steps_per_second': 7.621, 'epoch': 0.08}
{'loss': 1.7972, 'grad_norm': 0.24383985996246338, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6839009523391724, 'eval_runtime': 8.3255, 'eval_samples_per_second': 119.992, 'eval_steps_per_second': 7.567, 'epoch': 0.12}
{'loss': 1.6117, 'grad_norm': 0.08692486584186554, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5600439310073853, 'eval_runtime': 8.3724, 'eval_samples_per_second': 119.321, 'eval_steps_per_second': 7.525, 'epoch': 0.16}
{'loss': 1.5345, 'grad_norm': 0.0971946269273758, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4555717706680298, 'eval_runtime': 8.404, 'eval_samples_per_second': 118.872, 'eval_steps_per_second': 7.496, 'epoch': 0.2}
{'loss': 1.5295, 'grad_norm': 0.1157446950674057, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4165058135986328, 'eval_runtime': 8.4024, 'eval_samples_per_second': 118.895, 'eval_steps_per_second': 7.498, 'epoch': 0.24}
{'loss': 1.4267, 'grad_norm': 0.16012120246887207, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.393406867980957, 'eval_runtime': 8.4175, 'eval_samples_per_second': 118.681, 'eval_steps_per_second': 7.484, 'epoch': 0.28}
{'loss': 1.394, 'grad_norm': 0.1304922252893448, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.374535322189331, 'eval_runtime': 8.4107, 'eval_samples_per_second': 118.777, 'eval_steps_per_second': 7.49, 'epoch': 0.32}
{'loss': 1.3385, 'grad_norm': 0.0710393562912941, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3576596975326538, 'eval_runtime': 8.4164, 'eval_samples_per_second': 118.696, 'eval_steps_per_second': 7.485, 'epoch': 0.36}
{'loss': 1.3282, 'grad_norm': 0.11047860980033875, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3376284837722778, 'eval_runtime': 8.4277, 'eval_samples_per_second': 118.537, 'eval_steps_per_second': 7.475, 'epoch': 0.4}
{'loss': 1.2631, 'grad_norm': 0.08282547444105148, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3174331188201904, 'eval_runtime': 8.3814, 'eval_samples_per_second': 119.192, 'eval_steps_per_second': 7.517, 'epoch': 0.44}
{'loss': 1.3782, 'grad_norm': 0.11705535650253296, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2989689111709595, 'eval_runtime': 8.3693, 'eval_samples_per_second': 119.364, 'eval_steps_per_second': 7.527, 'epoch': 0.48}
{'loss': 1.2802, 'grad_norm': 0.11952199041843414, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2821940183639526, 'eval_runtime': 8.3638, 'eval_samples_per_second': 119.443, 'eval_steps_per_second': 7.532, 'epoch': 0.52}
{'loss': 1.2523, 'grad_norm': 0.1381480097770691, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2656540870666504, 'eval_runtime': 8.3557, 'eval_samples_per_second': 119.56, 'eval_steps_per_second': 7.54, 'epoch': 0.56}
{'loss': 1.3555, 'grad_norm': 0.09133896231651306, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2485988140106201, 'eval_runtime': 8.3767, 'eval_samples_per_second': 119.259, 'eval_steps_per_second': 7.521, 'epoch': 0.6}
{'loss': 1.2679, 'grad_norm': 0.0928676649928093, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2320636510849, 'eval_runtime': 8.3406, 'eval_samples_per_second': 119.776, 'eval_steps_per_second': 7.553, 'epoch': 0.64}
{'loss': 1.2584, 'grad_norm': 0.12569519877433777, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2178813219070435, 'eval_runtime': 8.3579, 'eval_samples_per_second': 119.528, 'eval_steps_per_second': 7.538, 'epoch': 0.68}
{'loss': 1.1895, 'grad_norm': 0.1321173906326294, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2058205604553223, 'eval_runtime': 8.3401, 'eval_samples_per_second': 119.783, 'eval_steps_per_second': 7.554, 'epoch': 0.72}
{'loss': 1.2085, 'grad_norm': 0.09054658561944962, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1907269954681396, 'eval_runtime': 8.3439, 'eval_samples_per_second': 119.729, 'eval_steps_per_second': 7.55, 'epoch': 0.76}
{'loss': 1.1441, 'grad_norm': 0.14287099242210388, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1780115365982056, 'eval_runtime': 8.3762, 'eval_samples_per_second': 119.267, 'eval_steps_per_second': 7.521, 'epoch': 0.8}
{'loss': 1.149, 'grad_norm': 0.18840353190898895, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1686075925827026, 'eval_runtime': 8.36, 'eval_samples_per_second': 119.498, 'eval_steps_per_second': 7.536, 'epoch': 0.84}
{'loss': 1.1884, 'grad_norm': 0.11304749548435211, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1571317911148071, 'eval_runtime': 8.3488, 'eval_samples_per_second': 119.658, 'eval_steps_per_second': 7.546, 'epoch': 0.88}
{'loss': 1.2042, 'grad_norm': 0.11189021915197372, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1514382362365723, 'eval_runtime': 8.3522, 'eval_samples_per_second': 119.61, 'eval_steps_per_second': 7.543, 'epoch': 0.92}
{'loss': 1.1236, 'grad_norm': 0.17172661423683167, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1464030742645264, 'eval_runtime': 8.3596, 'eval_samples_per_second': 119.504, 'eval_steps_per_second': 7.536, 'epoch': 0.96}
{'loss': 1.1085, 'grad_norm': 0.16830427944660187, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1456836462020874, 'eval_runtime': 8.3579, 'eval_samples_per_second': 119.527, 'eval_steps_per_second': 7.538, 'epoch': 1.0}
{'train_runtime': 514.2151, 'train_samples_per_second': 19.445, 'train_steps_per_second': 1.215, 'train_loss': 1.484579150390625, 'epoch': 1.0}
train_results:  {'eval_loss': [3.367725133895874, 1.9424636363983154, 1.6839009523391724, 1.5600439310073853, 1.4555717706680298, 1.4165058135986328, 1.393406867980957, 1.374535322189331, 1.3576596975326538, 1.3376284837722778, 1.3174331188201904, 1.2989689111709595, 1.2821940183639526, 1.2656540870666504, 1.2485988140106201, 1.2320636510849, 1.2178813219070435, 1.2058205604553223, 1.1907269954681396, 1.1780115365982056, 1.1686075925827026, 1.1571317911148071, 1.1514382362365723, 1.1464030742645264, 1.1456836462020874], 'performance': [0.53, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:12,  1.34s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 15.03it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:09,  5.26it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:04,  7.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:03,  4.88it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:14<00:00,  6.81it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.82it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.53, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.4888899326324463
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7845 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 1.0217412492320483
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.9020491033566422
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.9855051939873756
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.9714505736280703
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.6637249485386835
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.4509, dtype=torch.float64), 0, tensor(0.5491, dtype=torch.float64), 0, 32, 1, 1, 0, 1, 0, 128, 1.2143064331837614e-18, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.1085e-17, dtype=torch.float64), tensor(9.4728e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.9827e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4509, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5491, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.2143e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.451
  wikitext: 0
  mmlu: 0.549
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.2143064331837614e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  1.2143064331837614e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:07,  1.89s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:23,  3.90it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.71it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.12it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05,  9.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.48it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  7.97it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.28it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.66it/s]
Evaluation performance at step 25: 0.56
{'loss': 3.9428, 'grad_norm': 0.3837743401527405, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.019401788711548, 'eval_runtime': 9.0935, 'eval_samples_per_second': 109.858, 'eval_steps_per_second': 6.928, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:51,  1.91it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.39it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.56it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:25,  3.00it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:15,  4.28it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:10,  5.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:13<00:17,  2.93it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:18<00:18,  2.31it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:23<00:17,  2.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:23<00:09,  2.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:25<00:05,  3.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:26<00:02,  4.05it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:26<00:00,  5.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:26<00:00,  3.73it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.1895, 'grad_norm': 0.24719423055648804, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.612478256225586, 'eval_runtime': 9.1364, 'eval_samples_per_second': 109.343, 'eval_steps_per_second': 6.896, 'epoch': 0.08}
{'loss': 1.4169, 'grad_norm': 0.10180380940437317, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.3474093675613403, 'eval_runtime': 9.1313, 'eval_samples_per_second': 109.404, 'eval_steps_per_second': 6.899, 'epoch': 0.12}
{'loss': 1.2372, 'grad_norm': 0.09450526535511017, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2365903854370117, 'eval_runtime': 9.2083, 'eval_samples_per_second': 108.489, 'eval_steps_per_second': 6.842, 'epoch': 0.16}
{'loss': 1.1852, 'grad_norm': 0.05021215230226517, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2032036781311035, 'eval_runtime': 9.2163, 'eval_samples_per_second': 108.395, 'eval_steps_per_second': 6.836, 'epoch': 0.2}
{'loss': 1.1365, 'grad_norm': 0.06552787125110626, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1825604438781738, 'eval_runtime': 9.2195, 'eval_samples_per_second': 108.357, 'eval_steps_per_second': 6.833, 'epoch': 0.24}
{'loss': 1.1872, 'grad_norm': 0.05387192592024803, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1607613563537598, 'eval_runtime': 9.2337, 'eval_samples_per_second': 108.191, 'eval_steps_per_second': 6.823, 'epoch': 0.28}
{'loss': 1.1466, 'grad_norm': 0.06649565696716309, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1482380628585815, 'eval_runtime': 9.2381, 'eval_samples_per_second': 108.139, 'eval_steps_per_second': 6.82, 'epoch': 0.32}
{'loss': 1.0869, 'grad_norm': 0.04916786029934883, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1415704488754272, 'eval_runtime': 9.244, 'eval_samples_per_second': 108.07, 'eval_steps_per_second': 6.815, 'epoch': 0.36}
{'loss': 1.1239, 'grad_norm': 0.054958049207925797, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1360976696014404, 'eval_runtime': 9.2447, 'eval_samples_per_second': 108.062, 'eval_steps_per_second': 6.815, 'epoch': 0.4}
{'loss': 1.1331, 'grad_norm': 0.05724390223622322, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1282496452331543, 'eval_runtime': 9.2333, 'eval_samples_per_second': 108.196, 'eval_steps_per_second': 6.823, 'epoch': 0.44}
{'loss': 1.1093, 'grad_norm': 0.05213950201869011, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1224722862243652, 'eval_runtime': 9.2463, 'eval_samples_per_second': 108.044, 'eval_steps_per_second': 6.814, 'epoch': 0.48}
{'loss': 1.0985, 'grad_norm': 0.05373549461364746, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1182011365890503, 'eval_runtime': 9.2471, 'eval_samples_per_second': 108.034, 'eval_steps_per_second': 6.813, 'epoch': 0.52}
{'loss': 1.0931, 'grad_norm': 0.06125083193182945, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1138169765472412, 'eval_runtime': 9.2786, 'eval_samples_per_second': 107.667, 'eval_steps_per_second': 6.79, 'epoch': 0.56}
{'loss': 1.1045, 'grad_norm': 0.06465527415275574, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1108670234680176, 'eval_runtime': 9.2671, 'eval_samples_per_second': 107.8, 'eval_steps_per_second': 6.798, 'epoch': 0.6}
{'loss': 1.1316, 'grad_norm': 0.07336506247520447, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1064516305923462, 'eval_runtime': 9.2325, 'eval_samples_per_second': 108.205, 'eval_steps_per_second': 6.824, 'epoch': 0.64}
{'loss': 1.1288, 'grad_norm': 0.0595480240881443, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1036609411239624, 'eval_runtime': 9.2379, 'eval_samples_per_second': 108.141, 'eval_steps_per_second': 6.82, 'epoch': 0.68}
{'loss': 1.0693, 'grad_norm': 0.0682457759976387, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1011631488800049, 'eval_runtime': 9.2535, 'eval_samples_per_second': 107.96, 'eval_steps_per_second': 6.808, 'epoch': 0.72}
{'loss': 1.077, 'grad_norm': 0.07206427305936813, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0980464220046997, 'eval_runtime': 9.2445, 'eval_samples_per_second': 108.064, 'eval_steps_per_second': 6.815, 'epoch': 0.76}
{'loss': 1.1429, 'grad_norm': 0.05684814229607582, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0957894325256348, 'eval_runtime': 9.2602, 'eval_samples_per_second': 107.881, 'eval_steps_per_second': 6.803, 'epoch': 0.8}
{'loss': 1.091, 'grad_norm': 0.06274276226758957, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0941135883331299, 'eval_runtime': 9.2479, 'eval_samples_per_second': 108.025, 'eval_steps_per_second': 6.812, 'epoch': 0.84}
{'loss': 1.1455, 'grad_norm': 0.06701349467039108, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.09201180934906, 'eval_runtime': 9.2622, 'eval_samples_per_second': 107.858, 'eval_steps_per_second': 6.802, 'epoch': 0.88}
{'loss': 1.0739, 'grad_norm': 0.07869593799114227, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0906293392181396, 'eval_runtime': 9.28, 'eval_samples_per_second': 107.651, 'eval_steps_per_second': 6.789, 'epoch': 0.92}
{'loss': 1.0234, 'grad_norm': 0.07322745770215988, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0900306701660156, 'eval_runtime': 9.2373, 'eval_samples_per_second': 108.148, 'eval_steps_per_second': 6.82, 'epoch': 0.96}
{'loss': 1.0664, 'grad_norm': 0.06234900280833244, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0898323059082031, 'eval_runtime': 9.2213, 'eval_samples_per_second': 108.336, 'eval_steps_per_second': 6.832, 'epoch': 1.0}
{'train_runtime': 571.6387, 'train_samples_per_second': 17.492, 'train_steps_per_second': 1.093, 'train_loss': 1.2856305419921874, 'epoch': 1.0}
train_results:  {'eval_loss': [3.019401788711548, 1.612478256225586, 1.3474093675613403, 1.2365903854370117, 1.2032036781311035, 1.1825604438781738, 1.1607613563537598, 1.1482380628585815, 1.1415704488754272, 1.1360976696014404, 1.1282496452331543, 1.1224722862243652, 1.1182011365890503, 1.1138169765472412, 1.1108670234680176, 1.1064516305923462, 1.1036609411239624, 1.1011631488800049, 1.0980464220046997, 1.0957894325256348, 1.0941135883331299, 1.09201180934906, 1.0906293392181396, 1.0900306701660156, 1.0898323059082031], 'performance': [0.56, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:03,  5.49s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:26,  3.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:08<00:11,  5.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:09<00:07,  7.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:10<00:03,  9.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 12.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 16.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.76it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.482123613357544
current iteration best possible performance (full train run):  0.63
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9136 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 1.2051858016685497
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.8844045175095314
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 1.1772798080076576
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 1.0057437653786963
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 1.0595006275488488
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.3658, dtype=torch.float64), tensor(0.6342, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 1, 128, 4.510281053460344e-18, 1.4800000190734872, 1]
normalized proposed parameters for next round by BO: [tensor(8.2183e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2557e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3658, dtype=torch.float64), tensor(0.6342, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.0284e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(4.5103e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.366
  wikitext: 0.634
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.510281053460344e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  4.510281053460344e-18
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:02,  1.84s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  3.99it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.13it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.26it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.03it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:04, 10.54it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.29it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 10.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  9.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  9.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.19it/s]
Evaluation performance at step 25: 0.56
{'loss': 3.9913, 'grad_norm': 0.1551079899072647, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.2863094806671143, 'eval_runtime': 8.6855, 'eval_samples_per_second': 115.019, 'eval_steps_per_second': 7.253, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:40,  2.45it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.90it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:14,  4.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.34it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  6.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03, 10.40it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 11.67it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 12.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:00, 13.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 12.81it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.85it/s]
Evaluation performance at step 50: 0.6
{'loss': 2.6645, 'grad_norm': 0.27242758870124817, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 2.1632022857666016, 'eval_runtime': 8.6883, 'eval_samples_per_second': 114.982, 'eval_steps_per_second': 7.251, 'epoch': 0.08}
{'loss': 2.0398, 'grad_norm': 0.30240172147750854, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8834129571914673, 'eval_runtime': 8.7502, 'eval_samples_per_second': 114.168, 'eval_steps_per_second': 7.2, 'epoch': 0.12}
{'loss': 1.8026, 'grad_norm': 0.3158818483352661, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.763575553894043, 'eval_runtime': 8.784, 'eval_samples_per_second': 113.73, 'eval_steps_per_second': 7.172, 'epoch': 0.16}
{'loss': 1.7572, 'grad_norm': 0.20511053502559662, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6983418464660645, 'eval_runtime': 8.8037, 'eval_samples_per_second': 113.475, 'eval_steps_per_second': 7.156, 'epoch': 0.2}
{'loss': 1.7311, 'grad_norm': 0.211678609251976, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.6746934652328491, 'eval_runtime': 8.8144, 'eval_samples_per_second': 113.337, 'eval_steps_per_second': 7.147, 'epoch': 0.24}
{'loss': 1.7448, 'grad_norm': 0.08685696870088577, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6547092199325562, 'eval_runtime': 8.8074, 'eval_samples_per_second': 113.427, 'eval_steps_per_second': 7.153, 'epoch': 0.28}
{'loss': 1.6105, 'grad_norm': 0.10279518365859985, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.638934850692749, 'eval_runtime': 8.8164, 'eval_samples_per_second': 113.312, 'eval_steps_per_second': 7.146, 'epoch': 0.32}
{'loss': 1.7391, 'grad_norm': 0.070010706782341, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6242120265960693, 'eval_runtime': 8.8217, 'eval_samples_per_second': 113.243, 'eval_steps_per_second': 7.141, 'epoch': 0.36}
{'loss': 1.7264, 'grad_norm': 0.0934813916683197, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.6092207431793213, 'eval_runtime': 8.812, 'eval_samples_per_second': 113.369, 'eval_steps_per_second': 7.149, 'epoch': 0.4}
{'loss': 1.5996, 'grad_norm': 0.11507922410964966, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5943851470947266, 'eval_runtime': 8.8249, 'eval_samples_per_second': 113.202, 'eval_steps_per_second': 7.139, 'epoch': 0.44}
{'loss': 1.5709, 'grad_norm': 0.2288566529750824, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.58544921875, 'eval_runtime': 8.8297, 'eval_samples_per_second': 113.141, 'eval_steps_per_second': 7.135, 'epoch': 0.48}
{'loss': 1.5604, 'grad_norm': 0.14181706309318542, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5701639652252197, 'eval_runtime': 8.8117, 'eval_samples_per_second': 113.371, 'eval_steps_per_second': 7.15, 'epoch': 0.52}
{'loss': 1.5071, 'grad_norm': 0.13655886054039001, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.5558637380599976, 'eval_runtime': 8.7698, 'eval_samples_per_second': 113.914, 'eval_steps_per_second': 7.184, 'epoch': 0.56}
{'loss': 1.5676, 'grad_norm': 0.13812589645385742, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.5434690713882446, 'eval_runtime': 8.8001, 'eval_samples_per_second': 113.521, 'eval_steps_per_second': 7.159, 'epoch': 0.6}
{'loss': 1.5339, 'grad_norm': 0.0820210874080658, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5323320627212524, 'eval_runtime': 8.7803, 'eval_samples_per_second': 113.777, 'eval_steps_per_second': 7.175, 'epoch': 0.64}
{'loss': 1.5227, 'grad_norm': 0.15851758420467377, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.523474931716919, 'eval_runtime': 8.7935, 'eval_samples_per_second': 113.607, 'eval_steps_per_second': 7.164, 'epoch': 0.68}
{'loss': 1.5353, 'grad_norm': 0.11974062025547028, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.509758710861206, 'eval_runtime': 8.7721, 'eval_samples_per_second': 113.884, 'eval_steps_per_second': 7.182, 'epoch': 0.72}
{'loss': 1.5282, 'grad_norm': 0.08477456867694855, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.5003228187561035, 'eval_runtime': 8.7522, 'eval_samples_per_second': 114.143, 'eval_steps_per_second': 7.198, 'epoch': 0.76}
{'loss': 1.566, 'grad_norm': 0.24513551592826843, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4935942888259888, 'eval_runtime': 8.7254, 'eval_samples_per_second': 114.494, 'eval_steps_per_second': 7.22, 'epoch': 0.8}
{'loss': 1.5519, 'grad_norm': 0.13472531735897064, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.484287977218628, 'eval_runtime': 8.7258, 'eval_samples_per_second': 114.488, 'eval_steps_per_second': 7.22, 'epoch': 0.84}
{'loss': 1.4436, 'grad_norm': 0.11479558795690536, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.478866696357727, 'eval_runtime': 8.7396, 'eval_samples_per_second': 114.307, 'eval_steps_per_second': 7.209, 'epoch': 0.88}
{'loss': 1.5129, 'grad_norm': 0.10628906637430191, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4716302156448364, 'eval_runtime': 8.7402, 'eval_samples_per_second': 114.3, 'eval_steps_per_second': 7.208, 'epoch': 0.92}
{'loss': 1.5422, 'grad_norm': 0.11460337787866592, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.4693472385406494, 'eval_runtime': 8.7742, 'eval_samples_per_second': 113.856, 'eval_steps_per_second': 7.18, 'epoch': 0.96}
{'loss': 1.4912, 'grad_norm': 0.10960442572832108, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.468261480331421, 'eval_runtime': 8.7795, 'eval_samples_per_second': 113.787, 'eval_steps_per_second': 7.176, 'epoch': 1.0}
{'train_runtime': 527.9317, 'train_samples_per_second': 18.94, 'train_steps_per_second': 1.184, 'train_loss': 1.7536330993652345, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2863094806671143, 2.1632022857666016, 1.8834129571914673, 1.763575553894043, 1.6983418464660645, 1.6746934652328491, 1.6547092199325562, 1.638934850692749, 1.6242120265960693, 1.6092207431793213, 1.5943851470947266, 1.58544921875, 1.5701639652252197, 1.5558637380599976, 1.5434690713882446, 1.5323320627212524, 1.523474931716919, 1.509758710861206, 1.5003228187561035, 1.4935942888259888, 1.484287977218628, 1.478866696357727, 1.4716302156448364, 1.4693472385406494, 1.468261480331421], 'performance': [0.56, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:10,  5.56s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:22,  3.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:09,  7.14it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:09<00:07,  6.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  9.87it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 12.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 15.84it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.13it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.4884443283081055
current iteration best possible performance (full train run):  0.7244999999999999
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8927 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 1.1815914663059637
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 1.0661834360087898
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 1.2389055490564302
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 1.127992054096441
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.7174851930414641
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.3926, dtype=torch.float64), tensor(0.6074, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(2.4404e-17, dtype=torch.float64), tensor(2.7525e-17, dtype=torch.float64), tensor(1.3462e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.6016e-17, dtype=torch.float64), tensor(0.3926, dtype=torch.float64), tensor(0.6074, dtype=torch.float64), tensor(5.7446e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.393
  wikitext: 0.607
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:23,  2.06s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:25,  3.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:13,  6.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:11,  6.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  8.13it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:06,  8.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:05,  8.51it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  8.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.44it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 10.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  7.12it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:12<00:01,  7.41it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  7.80it/s]
Evaluation performance at step 25: 0.54
{'loss': 3.9119, 'grad_norm': 0.1572360247373581, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.3476433753967285, 'eval_runtime': 8.3993, 'eval_samples_per_second': 118.939, 'eval_steps_per_second': 7.501, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:43,  1.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:13,  7.00it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.46it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.74it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.34it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 14.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 15.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 15.72it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 14.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.08it/s]
Evaluation performance at step 50: 0.59
{'loss': 2.6681, 'grad_norm': 0.2712610363960266, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 2.1158745288848877, 'eval_runtime': 8.4073, 'eval_samples_per_second': 118.825, 'eval_steps_per_second': 7.493, 'epoch': 0.08}
{'loss': 2.0364, 'grad_norm': 0.1527682989835739, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.8533077239990234, 'eval_runtime': 8.4605, 'eval_samples_per_second': 118.078, 'eval_steps_per_second': 7.446, 'epoch': 0.12}
{'loss': 1.8506, 'grad_norm': 0.07451344281435013, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7285575866699219, 'eval_runtime': 8.4915, 'eval_samples_per_second': 117.647, 'eval_steps_per_second': 7.419, 'epoch': 0.16}
{'loss': 1.7689, 'grad_norm': 0.07338576763868332, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.6626956462860107, 'eval_runtime': 8.5211, 'eval_samples_per_second': 117.238, 'eval_steps_per_second': 7.393, 'epoch': 0.2}
{'loss': 1.7256, 'grad_norm': 0.10915500670671463, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.634319543838501, 'eval_runtime': 8.5166, 'eval_samples_per_second': 117.3, 'eval_steps_per_second': 7.397, 'epoch': 0.24}
{'loss': 1.7055, 'grad_norm': 0.11896529793739319, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.6145693063735962, 'eval_runtime': 8.4849, 'eval_samples_per_second': 117.739, 'eval_steps_per_second': 7.425, 'epoch': 0.28}
{'loss': 1.6105, 'grad_norm': 0.10855942964553833, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.5976271629333496, 'eval_runtime': 8.4864, 'eval_samples_per_second': 117.718, 'eval_steps_per_second': 7.424, 'epoch': 0.32}
{'loss': 1.5779, 'grad_norm': 0.15472033619880676, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.5802950859069824, 'eval_runtime': 8.4708, 'eval_samples_per_second': 117.935, 'eval_steps_per_second': 7.437, 'epoch': 0.36}
{'loss': 1.6576, 'grad_norm': 0.08340834081172943, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.5640860795974731, 'eval_runtime': 8.4735, 'eval_samples_per_second': 117.897, 'eval_steps_per_second': 7.435, 'epoch': 0.4}
{'loss': 1.6132, 'grad_norm': 0.08299855887889862, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.5466094017028809, 'eval_runtime': 8.4689, 'eval_samples_per_second': 117.961, 'eval_steps_per_second': 7.439, 'epoch': 0.44}
{'loss': 1.6072, 'grad_norm': 0.0752798542380333, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.5273792743682861, 'eval_runtime': 8.4662, 'eval_samples_per_second': 117.999, 'eval_steps_per_second': 7.441, 'epoch': 0.48}
{'loss': 1.5182, 'grad_norm': 0.08169679343700409, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5152667760849, 'eval_runtime': 8.492, 'eval_samples_per_second': 117.64, 'eval_steps_per_second': 7.419, 'epoch': 0.52}
{'loss': 1.5948, 'grad_norm': 0.09895259886980057, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.502516269683838, 'eval_runtime': 8.4738, 'eval_samples_per_second': 117.893, 'eval_steps_per_second': 7.435, 'epoch': 0.56}
{'loss': 1.5388, 'grad_norm': 0.11864176392555237, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.490430474281311, 'eval_runtime': 8.4685, 'eval_samples_per_second': 117.966, 'eval_steps_per_second': 7.439, 'epoch': 0.6}
{'loss': 1.4544, 'grad_norm': 0.11206136643886566, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.4754157066345215, 'eval_runtime': 8.4653, 'eval_samples_per_second': 118.012, 'eval_steps_per_second': 7.442, 'epoch': 0.64}
{'loss': 1.5257, 'grad_norm': 0.10215368866920471, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.4666099548339844, 'eval_runtime': 8.4739, 'eval_samples_per_second': 117.891, 'eval_steps_per_second': 7.435, 'epoch': 0.68}
{'loss': 1.4088, 'grad_norm': 0.11991306394338608, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.4505274295806885, 'eval_runtime': 8.4784, 'eval_samples_per_second': 117.829, 'eval_steps_per_second': 7.431, 'epoch': 0.72}
{'loss': 1.4422, 'grad_norm': 0.09537988901138306, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.440686583518982, 'eval_runtime': 8.4733, 'eval_samples_per_second': 117.899, 'eval_steps_per_second': 7.435, 'epoch': 0.76}
{'loss': 1.4756, 'grad_norm': 0.09436286985874176, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4294731616973877, 'eval_runtime': 8.4852, 'eval_samples_per_second': 117.734, 'eval_steps_per_second': 7.425, 'epoch': 0.8}
{'loss': 1.4657, 'grad_norm': 0.11580324918031693, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4192057847976685, 'eval_runtime': 8.4663, 'eval_samples_per_second': 117.997, 'eval_steps_per_second': 7.441, 'epoch': 0.84}
{'loss': 1.4771, 'grad_norm': 0.1412259191274643, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4130052328109741, 'eval_runtime': 8.4929, 'eval_samples_per_second': 117.628, 'eval_steps_per_second': 7.418, 'epoch': 0.88}
{'loss': 1.4732, 'grad_norm': 0.11812614649534225, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.4052326679229736, 'eval_runtime': 8.4998, 'eval_samples_per_second': 117.532, 'eval_steps_per_second': 7.412, 'epoch': 0.92}
{'loss': 1.4258, 'grad_norm': 0.14181716740131378, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.40171217918396, 'eval_runtime': 8.4885, 'eval_samples_per_second': 117.689, 'eval_steps_per_second': 7.422, 'epoch': 0.96}
{'loss': 1.3013, 'grad_norm': 0.10455956310033798, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4003640413284302, 'eval_runtime': 8.478, 'eval_samples_per_second': 117.834, 'eval_steps_per_second': 7.431, 'epoch': 1.0}
{'train_runtime': 529.5605, 'train_samples_per_second': 18.882, 'train_steps_per_second': 1.18, 'train_loss': 1.7133957397460937, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3476433753967285, 2.1158745288848877, 1.8533077239990234, 1.7285575866699219, 1.6626956462860107, 1.634319543838501, 1.6145693063735962, 1.5976271629333496, 1.5802950859069824, 1.5640860795974731, 1.5466094017028809, 1.5273792743682861, 1.5152667760849, 1.502516269683838, 1.490430474281311, 1.4754157066345215, 1.4666099548339844, 1.4505274295806885, 1.440686583518982, 1.4294731616973877, 1.4192057847976685, 1.4130052328109741, 1.4052326679229736, 1.40171217918396, 1.4003640413284302], 'performance': [0.54, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:11,  5.57s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.78it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:29,  2.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:21<00:20,  2.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:27<00:13,  2.69it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:32<00:06,  2.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:33<00:00,  4.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.00it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.4879426956176758
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5600 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 1.0120250928939267
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.8178332120324892
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 1.1975442514191335
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.8229737022872724
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 1.2171212235292546
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0581, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7595, dtype=torch.float64), tensor(0.1744, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.021277591853632226, 1.480000019073489, 1]
normalized proposed parameters for next round by BO: [tensor(0.0581, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3182e-18, dtype=torch.float64), tensor(0.0080, dtype=torch.float64), tensor(1.0097e-17, dtype=torch.float64), tensor(0.7595, dtype=torch.float64), tensor(0.1744, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4279e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2128, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.058
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.759
  wikitext: 0.174
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.021277591853632226,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.480000019073489,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.021277591853632226
lora alpha:  1.480000019073489
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9918
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  991
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:05,  1.26s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.89it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  8.02it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:09,  8.32it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.28it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:05,  9.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04,  9.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.76it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 11.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.43it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.72it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  9.76it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.42it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.6548, 'grad_norm': 0.34232571721076965, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.3972489833831787, 'eval_runtime': 6.1199, 'eval_samples_per_second': 161.931, 'eval_steps_per_second': 10.131, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:11,  1.38it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  8.99it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.41it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:05, 12.66it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.20it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 14.36it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 14.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.49it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 15.95it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.40it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 12.72it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 13.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 14.08it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.3215, 'grad_norm': 0.26559922099113464, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.5777533054351807, 'eval_runtime': 6.1544, 'eval_samples_per_second': 161.023, 'eval_steps_per_second': 10.074, 'epoch': 0.08}
{'loss': 1.3757, 'grad_norm': 0.1380937695503235, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.2776936292648315, 'eval_runtime': 6.213, 'eval_samples_per_second': 159.505, 'eval_steps_per_second': 9.979, 'epoch': 0.12}
{'loss': 1.1965, 'grad_norm': 0.11553411930799484, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 1.1273375749588013, 'eval_runtime': 6.1991, 'eval_samples_per_second': 159.862, 'eval_steps_per_second': 10.001, 'epoch': 0.16}
{'loss': 1.1498, 'grad_norm': 0.06849372386932373, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 1.0588864088058472, 'eval_runtime': 6.2073, 'eval_samples_per_second': 159.651, 'eval_steps_per_second': 9.988, 'epoch': 0.2}
{'loss': 1.1225, 'grad_norm': 0.08744185417890549, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 1.018373727798462, 'eval_runtime': 6.2167, 'eval_samples_per_second': 159.409, 'eval_steps_per_second': 9.973, 'epoch': 0.24}
{'loss': 1.002, 'grad_norm': 0.06469617784023285, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.9799850583076477, 'eval_runtime': 6.2143, 'eval_samples_per_second': 159.47, 'eval_steps_per_second': 9.977, 'epoch': 0.28}
{'loss': 0.9577, 'grad_norm': 0.07332295924425125, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.9508560299873352, 'eval_runtime': 6.2294, 'eval_samples_per_second': 159.085, 'eval_steps_per_second': 9.953, 'epoch': 0.32}
{'loss': 0.9217, 'grad_norm': 0.078798308968544, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.9217763543128967, 'eval_runtime': 6.2253, 'eval_samples_per_second': 159.19, 'eval_steps_per_second': 9.959, 'epoch': 0.36}
{'loss': 0.8575, 'grad_norm': 0.09097408503293991, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.8861841559410095, 'eval_runtime': 6.2202, 'eval_samples_per_second': 159.32, 'eval_steps_per_second': 9.968, 'epoch': 0.4}
{'loss': 0.9684, 'grad_norm': 0.10919294506311417, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.8509689569473267, 'eval_runtime': 6.1983, 'eval_samples_per_second': 159.881, 'eval_steps_per_second': 10.003, 'epoch': 0.44}
{'loss': 0.8959, 'grad_norm': 0.08363122493028641, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.8243014216423035, 'eval_runtime': 6.1904, 'eval_samples_per_second': 160.088, 'eval_steps_per_second': 10.016, 'epoch': 0.48}
{'loss': 0.8543, 'grad_norm': 0.12148633599281311, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.7914388179779053, 'eval_runtime': 6.1607, 'eval_samples_per_second': 160.858, 'eval_steps_per_second': 10.064, 'epoch': 0.52}
{'loss': 0.8734, 'grad_norm': 0.10321689397096634, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.7634028792381287, 'eval_runtime': 6.1597, 'eval_samples_per_second': 160.884, 'eval_steps_per_second': 10.065, 'epoch': 0.56}
{'loss': 0.8035, 'grad_norm': 0.11633425205945969, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.7352701425552368, 'eval_runtime': 6.1628, 'eval_samples_per_second': 160.804, 'eval_steps_per_second': 10.06, 'epoch': 0.6}
{'loss': 0.7381, 'grad_norm': 0.15596143901348114, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.702908992767334, 'eval_runtime': 6.1659, 'eval_samples_per_second': 160.724, 'eval_steps_per_second': 10.055, 'epoch': 0.65}
{'loss': 0.7512, 'grad_norm': 0.13285289704799652, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.6785839796066284, 'eval_runtime': 6.1664, 'eval_samples_per_second': 160.71, 'eval_steps_per_second': 10.054, 'epoch': 0.69}
{'loss': 0.7548, 'grad_norm': 0.16556909680366516, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.6581059694290161, 'eval_runtime': 6.1618, 'eval_samples_per_second': 160.829, 'eval_steps_per_second': 10.062, 'epoch': 0.73}
{'loss': 0.6245, 'grad_norm': 0.15632134675979614, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.6383773684501648, 'eval_runtime': 6.1615, 'eval_samples_per_second': 160.836, 'eval_steps_per_second': 10.062, 'epoch': 0.77}
{'loss': 0.636, 'grad_norm': 0.1538909673690796, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.6250419616699219, 'eval_runtime': 6.167, 'eval_samples_per_second': 160.695, 'eval_steps_per_second': 10.054, 'epoch': 0.81}
{'loss': 0.6698, 'grad_norm': 0.13671326637268066, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.6147112250328064, 'eval_runtime': 6.1665, 'eval_samples_per_second': 160.708, 'eval_steps_per_second': 10.054, 'epoch': 0.85}
{'loss': 0.6233, 'grad_norm': 0.18621499836444855, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.6042236685752869, 'eval_runtime': 6.1686, 'eval_samples_per_second': 160.652, 'eval_steps_per_second': 10.051, 'epoch': 0.89}
{'loss': 0.6433, 'grad_norm': 0.11864425987005234, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.5958978533744812, 'eval_runtime': 6.1737, 'eval_samples_per_second': 160.519, 'eval_steps_per_second': 10.043, 'epoch': 0.93}
{'loss': 0.6076, 'grad_norm': 0.12459725141525269, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.5922776460647583, 'eval_runtime': 6.169, 'eval_samples_per_second': 160.641, 'eval_steps_per_second': 10.05, 'epoch': 0.97}
{'train_runtime': 410.5573, 'train_samples_per_second': 24.157, 'train_steps_per_second': 1.51, 'train_loss': 1.0693514946968325, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3972489833831787, 1.5777533054351807, 1.2776936292648315, 1.1273375749588013, 1.0588864088058472, 1.018373727798462, 0.9799850583076477, 0.9508560299873352, 0.9217763543128967, 0.8861841559410095, 0.8509689569473267, 0.8243014216423035, 0.7914388179779053, 0.7634028792381287, 0.7352701425552368, 0.702908992767334, 0.6785839796066284, 0.6581059694290161, 0.6383773684501648, 0.6250419616699219, 0.6147112250328064, 0.6042236685752869, 0.5958978533744812, 0.5922776460647583], 'performance': [0.55, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:07,  1.28s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.75it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.02it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:09,  5.41it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:04,  7.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 10.49it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 13.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.41it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4904611110687256
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8285 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 1.1116707245162352
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 1.1826659733102052
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 1.0800914524073715
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.9544011083839627
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.5518326475749041
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0377, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5719, dtype=torch.float64), tensor(0.3904, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.010995560471403178, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0377, dtype=torch.float64), tensor(2.9541e-17, dtype=torch.float64), tensor(1.6972e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.1945e-18, dtype=torch.float64), tensor(0.5719, dtype=torch.float64), tensor(0.3904, dtype=torch.float64), tensor(1.4266e-17, dtype=torch.float64), tensor(2.0478e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.1100, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.038
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.572
  wikitext: 0.39
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.010995560471403178,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.010995560471403178
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:43,  1.66s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.75it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.58it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.22it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.12it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.69it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.07it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 11.89it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.77it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.99it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.3454, 'grad_norm': 0.2772567868232727, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.5020177364349365, 'eval_runtime': 7.4174, 'eval_samples_per_second': 134.684, 'eval_steps_per_second': 8.494, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:41,  1.03s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.17it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08, 10.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:07, 10.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.23it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 12.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 14.25it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:03, 13.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 16.33it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 14.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 16.19it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 15.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 14.89it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.44it/s]
Evaluation performance at step 50: 0.54
{'loss': 2.6601, 'grad_norm': 0.28782451152801514, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.54}
{'eval_loss': 1.9727987051010132, 'eval_runtime': 7.4509, 'eval_samples_per_second': 134.077, 'eval_steps_per_second': 8.455, 'epoch': 0.08}
{'loss': 1.8031, 'grad_norm': 0.21426217257976532, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6264506578445435, 'eval_runtime': 7.4934, 'eval_samples_per_second': 133.318, 'eval_steps_per_second': 8.407, 'epoch': 0.12}
{'loss': 1.5589, 'grad_norm': 0.10254502296447754, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5086966753005981, 'eval_runtime': 7.4635, 'eval_samples_per_second': 133.851, 'eval_steps_per_second': 8.441, 'epoch': 0.16}
{'loss': 1.5322, 'grad_norm': 0.09640298783779144, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3894332647323608, 'eval_runtime': 7.4601, 'eval_samples_per_second': 133.912, 'eval_steps_per_second': 8.445, 'epoch': 0.2}
{'loss': 1.2966, 'grad_norm': 0.0956576019525528, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.34331476688385, 'eval_runtime': 7.4717, 'eval_samples_per_second': 133.704, 'eval_steps_per_second': 8.432, 'epoch': 0.24}
{'loss': 1.3503, 'grad_norm': 0.20001718401908875, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3226722478866577, 'eval_runtime': 7.4692, 'eval_samples_per_second': 133.749, 'eval_steps_per_second': 8.435, 'epoch': 0.28}
{'loss': 1.3173, 'grad_norm': 0.1106439009308815, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3057422637939453, 'eval_runtime': 7.4577, 'eval_samples_per_second': 133.956, 'eval_steps_per_second': 8.448, 'epoch': 0.32}
{'loss': 1.3007, 'grad_norm': 0.1112428829073906, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2901618480682373, 'eval_runtime': 7.4657, 'eval_samples_per_second': 133.812, 'eval_steps_per_second': 8.439, 'epoch': 0.36}
{'loss': 1.289, 'grad_norm': 0.118381068110466, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2795612812042236, 'eval_runtime': 7.476, 'eval_samples_per_second': 133.627, 'eval_steps_per_second': 8.427, 'epoch': 0.4}
{'loss': 1.2433, 'grad_norm': 0.08734817057847977, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2684955596923828, 'eval_runtime': 7.4682, 'eval_samples_per_second': 133.766, 'eval_steps_per_second': 8.436, 'epoch': 0.44}
{'loss': 1.3456, 'grad_norm': 0.11224685609340668, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2590800523757935, 'eval_runtime': 7.4729, 'eval_samples_per_second': 133.683, 'eval_steps_per_second': 8.43, 'epoch': 0.48}
{'loss': 1.2169, 'grad_norm': 0.1134634017944336, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2522077560424805, 'eval_runtime': 7.4697, 'eval_samples_per_second': 133.741, 'eval_steps_per_second': 8.434, 'epoch': 0.52}
{'loss': 1.3232, 'grad_norm': 0.12178739905357361, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2427273988723755, 'eval_runtime': 7.4758, 'eval_samples_per_second': 133.631, 'eval_steps_per_second': 8.427, 'epoch': 0.56}
{'loss': 1.3793, 'grad_norm': 0.09508849680423737, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2369436025619507, 'eval_runtime': 7.4765, 'eval_samples_per_second': 133.619, 'eval_steps_per_second': 8.426, 'epoch': 0.6}
{'loss': 1.221, 'grad_norm': 0.09551964700222015, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2281781435012817, 'eval_runtime': 7.4699, 'eval_samples_per_second': 133.737, 'eval_steps_per_second': 8.434, 'epoch': 0.64}
{'loss': 1.3184, 'grad_norm': 0.10362397134304047, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2236649990081787, 'eval_runtime': 7.4717, 'eval_samples_per_second': 133.705, 'eval_steps_per_second': 8.432, 'epoch': 0.68}
{'loss': 1.219, 'grad_norm': 0.10687731206417084, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2183988094329834, 'eval_runtime': 7.4717, 'eval_samples_per_second': 133.705, 'eval_steps_per_second': 8.432, 'epoch': 0.72}
{'loss': 1.2411, 'grad_norm': 0.126855731010437, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2124689817428589, 'eval_runtime': 7.4662, 'eval_samples_per_second': 133.804, 'eval_steps_per_second': 8.438, 'epoch': 0.76}
{'loss': 1.2055, 'grad_norm': 0.1307533085346222, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2067501544952393, 'eval_runtime': 7.4711, 'eval_samples_per_second': 133.715, 'eval_steps_per_second': 8.432, 'epoch': 0.8}
{'loss': 1.2246, 'grad_norm': 0.12141554057598114, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2025450468063354, 'eval_runtime': 7.4768, 'eval_samples_per_second': 133.613, 'eval_steps_per_second': 8.426, 'epoch': 0.84}
{'loss': 1.2387, 'grad_norm': 0.10798196494579315, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2013227939605713, 'eval_runtime': 7.4681, 'eval_samples_per_second': 133.77, 'eval_steps_per_second': 8.436, 'epoch': 0.88}
{'loss': 1.2559, 'grad_norm': 0.14927347004413605, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1977324485778809, 'eval_runtime': 7.4675, 'eval_samples_per_second': 133.781, 'eval_steps_per_second': 8.437, 'epoch': 0.92}
{'loss': 1.2589, 'grad_norm': 0.1139393150806427, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1962792873382568, 'eval_runtime': 7.4637, 'eval_samples_per_second': 133.848, 'eval_steps_per_second': 8.441, 'epoch': 0.96}
{'loss': 1.2085, 'grad_norm': 0.14039792120456696, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.195590853691101, 'eval_runtime': 7.478, 'eval_samples_per_second': 133.592, 'eval_steps_per_second': 8.425, 'epoch': 1.0}
{'train_runtime': 475.3574, 'train_samples_per_second': 21.035, 'train_steps_per_second': 1.315, 'train_loss': 1.494142315673828, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5020177364349365, 1.9727987051010132, 1.6264506578445435, 1.5086966753005981, 1.3894332647323608, 1.34331476688385, 1.3226722478866577, 1.3057422637939453, 1.2901618480682373, 1.2795612812042236, 1.2684955596923828, 1.2590800523757935, 1.2522077560424805, 1.2427273988723755, 1.2369436025619507, 1.2281781435012817, 1.2236649990081787, 1.2183988094329834, 1.2124689817428589, 1.2067501544952393, 1.2025450468063354, 1.2013227939605713, 1.1977324485778809, 1.1962792873382568, 1.195590853691101], 'performance': [0.56, 0.54]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:43,  5.28s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:20,  3.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:08,  7.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:04, 10.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:02, 14.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 16.69it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 19.51it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.17it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.54]
current iteration observed (possibly low-fid or predicted) performance:  1.4912800788879395
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8940 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.9818079903675726
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 1.1155852311570822
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.8070276158650853
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 1.1304375078611193
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 1.1066328380868216
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0251, dtype=torch.float64), 0, 0, tensor(0.9749, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.04385404076228295, 1.4800000190734872, 1]
normalized proposed parameters for next round by BO: [tensor(0.0251, dtype=torch.float64), tensor(2.2289e-17, dtype=torch.float64), tensor(1.4062e-17, dtype=torch.float64), tensor(0.9749, dtype=torch.float64), tensor(2.9988e-18, dtype=torch.float64), tensor(8.7998e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.0409e-18, dtype=torch.float64), tensor(1.9825e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4385, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.025
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.975
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04385404076228295,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.04385404076228295
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:07,  1.90s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:23,  3.89it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:12,  6.76it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:10,  7.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  8.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  9.40it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.85it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.59it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  7.95it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.69it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  8.83it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.45it/s]
Evaluation performance at step 25: 0.54
{'loss': 5.1683, 'grad_norm': 0.4657082259654999, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.512624502182007, 'eval_runtime': 4.6729, 'eval_samples_per_second': 213.785, 'eval_steps_per_second': 13.482, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:00,  1.22s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:15,  5.84it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:11,  7.30it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:07<00:26,  2.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:08<00:16,  4.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:08<00:10,  5.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:09<00:06,  7.30it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:09<00:05,  8.60it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:10<00:03, 10.48it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:02, 11.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:12<00:02,  8.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:13<00:01,  8.36it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00,  9.95it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.32it/s]
Evaluation performance at step 50: 0.46
{'loss': 2.0628, 'grad_norm': 0.40183234214782715, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.46}
{'eval_loss': 1.2465248107910156, 'eval_runtime': 3.9765, 'eval_samples_per_second': 251.228, 'eval_steps_per_second': 15.843, 'epoch': 0.08}
{'loss': 1.049, 'grad_norm': 0.13671664893627167, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.953135073184967, 'eval_runtime': 3.9742, 'eval_samples_per_second': 251.374, 'eval_steps_per_second': 15.852, 'epoch': 0.12}
{'loss': 0.8555, 'grad_norm': 0.05437802895903587, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.791042149066925, 'eval_runtime': 3.9811, 'eval_samples_per_second': 250.937, 'eval_steps_per_second': 15.825, 'epoch': 0.16}
{'loss': 0.7718, 'grad_norm': 0.06500567495822906, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7678670287132263, 'eval_runtime': 3.9902, 'eval_samples_per_second': 250.365, 'eval_steps_per_second': 15.789, 'epoch': 0.2}
{'loss': 0.7456, 'grad_norm': 0.0551181435585022, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.754891037940979, 'eval_runtime': 3.9916, 'eval_samples_per_second': 250.277, 'eval_steps_per_second': 15.783, 'epoch': 0.24}
{'loss': 0.7571, 'grad_norm': 0.05216947942972183, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7466059923171997, 'eval_runtime': 3.9919, 'eval_samples_per_second': 250.257, 'eval_steps_per_second': 15.782, 'epoch': 0.28}
{'loss': 0.7505, 'grad_norm': 0.05773862078785896, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7437750101089478, 'eval_runtime': 3.9862, 'eval_samples_per_second': 250.616, 'eval_steps_per_second': 15.805, 'epoch': 0.32}
{'loss': 0.7389, 'grad_norm': 0.04653078690171242, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7376812696456909, 'eval_runtime': 3.9844, 'eval_samples_per_second': 250.73, 'eval_steps_per_second': 15.812, 'epoch': 0.36}
{'loss': 0.7363, 'grad_norm': 0.07626868784427643, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7334972620010376, 'eval_runtime': 3.9842, 'eval_samples_per_second': 250.742, 'eval_steps_per_second': 15.813, 'epoch': 0.4}
{'loss': 0.7193, 'grad_norm': 0.05176325887441635, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7294706702232361, 'eval_runtime': 3.9885, 'eval_samples_per_second': 250.469, 'eval_steps_per_second': 15.795, 'epoch': 0.44}
{'loss': 0.7412, 'grad_norm': 0.05491955950856209, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7248265743255615, 'eval_runtime': 3.9888, 'eval_samples_per_second': 250.45, 'eval_steps_per_second': 15.794, 'epoch': 0.48}
{'loss': 0.7362, 'grad_norm': 0.05590138956904411, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7223156094551086, 'eval_runtime': 3.9836, 'eval_samples_per_second': 250.781, 'eval_steps_per_second': 15.815, 'epoch': 0.52}
{'loss': 0.7266, 'grad_norm': 0.04741501063108444, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7186753749847412, 'eval_runtime': 3.9883, 'eval_samples_per_second': 250.484, 'eval_steps_per_second': 15.796, 'epoch': 0.56}
{'loss': 0.7243, 'grad_norm': 0.057308316230773926, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7155128717422485, 'eval_runtime': 3.9864, 'eval_samples_per_second': 250.599, 'eval_steps_per_second': 15.804, 'epoch': 0.6}
{'loss': 0.7462, 'grad_norm': 0.049116846174001694, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7132582664489746, 'eval_runtime': 3.9931, 'eval_samples_per_second': 250.179, 'eval_steps_per_second': 15.777, 'epoch': 0.64}
{'loss': 0.7055, 'grad_norm': 0.057484354823827744, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7105118036270142, 'eval_runtime': 3.9904, 'eval_samples_per_second': 250.353, 'eval_steps_per_second': 15.788, 'epoch': 0.68}
{'loss': 0.7363, 'grad_norm': 0.05848154425621033, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7072314620018005, 'eval_runtime': 4.0083, 'eval_samples_per_second': 249.23, 'eval_steps_per_second': 15.717, 'epoch': 0.72}
{'loss': 0.7306, 'grad_norm': 0.06445690989494324, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7054734230041504, 'eval_runtime': 3.9917, 'eval_samples_per_second': 250.268, 'eval_steps_per_second': 15.783, 'epoch': 0.76}
{'loss': 0.7119, 'grad_norm': 0.06939440965652466, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7036672830581665, 'eval_runtime': 3.9973, 'eval_samples_per_second': 249.918, 'eval_steps_per_second': 15.761, 'epoch': 0.8}
{'loss': 0.7058, 'grad_norm': 0.05342794954776764, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7018175721168518, 'eval_runtime': 3.9951, 'eval_samples_per_second': 250.054, 'eval_steps_per_second': 15.769, 'epoch': 0.84}
{'loss': 0.7237, 'grad_norm': 0.07066716998815536, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6996909379959106, 'eval_runtime': 3.9938, 'eval_samples_per_second': 250.139, 'eval_steps_per_second': 15.775, 'epoch': 0.88}
{'loss': 0.6906, 'grad_norm': 0.06163891777396202, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6986560821533203, 'eval_runtime': 3.9949, 'eval_samples_per_second': 250.072, 'eval_steps_per_second': 15.77, 'epoch': 0.92}
{'loss': 0.7009, 'grad_norm': 0.05680876225233078, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6981488466262817, 'eval_runtime': 3.9862, 'eval_samples_per_second': 250.617, 'eval_steps_per_second': 15.805, 'epoch': 0.96}
{'loss': 0.7334, 'grad_norm': 0.07701952755451202, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6980555057525635, 'eval_runtime': 4.0064, 'eval_samples_per_second': 249.351, 'eval_steps_per_second': 15.725, 'epoch': 1.0}
{'train_runtime': 302.5017, 'train_samples_per_second': 33.054, 'train_steps_per_second': 2.066, 'train_loss': 0.9787264984130859, 'epoch': 1.0}
train_results:  {'eval_loss': [3.512624502182007, 1.2465248107910156, 0.953135073184967, 0.791042149066925, 0.7678670287132263, 0.754891037940979, 0.7466059923171997, 0.7437750101089478, 0.7376812696456909, 0.7334972620010376, 0.7294706702232361, 0.7248265743255615, 0.7223156094551086, 0.7186753749847412, 0.7155128717422485, 0.7132582664489746, 0.7105118036270142, 0.7072314620018005, 0.7054734230041504, 0.7036672830581665, 0.7018175721168518, 0.6996909379959106, 0.6986560821533203, 0.6981488466262817, 0.6980555057525635], 'performance': [0.54, 0.46]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:10,  1.41it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:09,  5.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:04,  7.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01,  9.51it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 11.84it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.96it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.46]
current iteration observed (possibly low-fid or predicted) performance:  1.4859740734100342
current iteration best possible performance (full train run):  0.504
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3521 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 1.0924067135647295
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.9302125128677166
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 1.1354120039293516
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.9839336990813047
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.6295086217471217
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0204, dtype=torch.float64), 0, 0, 0, tensor(0.0554, dtype=torch.float64), tensor(0.9242, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.07395916901245965, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0204, dtype=torch.float64), tensor(2.4969e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0252e-17, dtype=torch.float64), tensor(0.0554, dtype=torch.float64), tensor(0.9242, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5310e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7396, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.02
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.055
  truthfulqa_gen: 0.924
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07395916901245965,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.07395916901245965
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:49,  1.11s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:16,  5.54it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  9.06it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.85it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.97it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.88it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.61it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 13.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 10.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.37it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.0559, 'grad_norm': 0.5306795835494995, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.6454660892486572, 'eval_runtime': 4.1066, 'eval_samples_per_second': 243.27, 'eval_steps_per_second': 15.341, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:48,  1.09s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.48it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:15,  4.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:09,  6.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  8.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:04, 10.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 12.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 15.40it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:01, 15.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 16.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 13.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 14.47it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 11.11it/s]
Evaluation performance at step 50: 0.45
{'loss': 2.3009, 'grad_norm': 0.39457058906555176, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.45}
{'eval_loss': 1.438773274421692, 'eval_runtime': 3.7848, 'eval_samples_per_second': 263.953, 'eval_steps_per_second': 16.646, 'epoch': 0.08}
{'loss': 1.1354, 'grad_norm': 0.15664184093475342, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0253729820251465, 'eval_runtime': 3.7805, 'eval_samples_per_second': 264.252, 'eval_steps_per_second': 16.665, 'epoch': 0.12}
{'loss': 0.9408, 'grad_norm': 0.1350744068622589, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8558329939842224, 'eval_runtime': 3.7866, 'eval_samples_per_second': 263.825, 'eval_steps_per_second': 16.638, 'epoch': 0.16}
{'loss': 0.8048, 'grad_norm': 0.0683363527059555, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8176968693733215, 'eval_runtime': 3.7966, 'eval_samples_per_second': 263.127, 'eval_steps_per_second': 16.594, 'epoch': 0.2}
{'loss': 0.7861, 'grad_norm': 0.07878120988607407, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7943670749664307, 'eval_runtime': 3.8002, 'eval_samples_per_second': 262.88, 'eval_steps_per_second': 16.578, 'epoch': 0.24}
{'loss': 0.7374, 'grad_norm': 0.07062701880931854, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7715821862220764, 'eval_runtime': 3.7915, 'eval_samples_per_second': 263.481, 'eval_steps_per_second': 16.616, 'epoch': 0.28}
{'loss': 0.7518, 'grad_norm': 0.07032665610313416, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7549181580543518, 'eval_runtime': 3.7923, 'eval_samples_per_second': 263.428, 'eval_steps_per_second': 16.613, 'epoch': 0.32}
{'loss': 0.7238, 'grad_norm': 0.07623149454593658, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7371224164962769, 'eval_runtime': 3.7953, 'eval_samples_per_second': 263.222, 'eval_steps_per_second': 16.6, 'epoch': 0.36}
{'loss': 0.7028, 'grad_norm': 0.0862669050693512, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7225253582000732, 'eval_runtime': 3.7927, 'eval_samples_per_second': 263.399, 'eval_steps_per_second': 16.611, 'epoch': 0.4}
{'loss': 0.6829, 'grad_norm': 0.09136547893285751, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7083156108856201, 'eval_runtime': 3.8089, 'eval_samples_per_second': 262.279, 'eval_steps_per_second': 16.54, 'epoch': 0.44}
{'loss': 0.7236, 'grad_norm': 0.12415707111358643, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6956323981285095, 'eval_runtime': 3.8057, 'eval_samples_per_second': 262.503, 'eval_steps_per_second': 16.554, 'epoch': 0.48}
{'loss': 0.6877, 'grad_norm': 0.11574088037014008, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6822085976600647, 'eval_runtime': 3.8102, 'eval_samples_per_second': 262.192, 'eval_steps_per_second': 16.535, 'epoch': 0.52}
{'loss': 0.6892, 'grad_norm': 0.11051103472709656, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6701675057411194, 'eval_runtime': 3.8164, 'eval_samples_per_second': 261.768, 'eval_steps_per_second': 16.508, 'epoch': 0.56}
{'loss': 0.6459, 'grad_norm': 0.13071194291114807, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6566702723503113, 'eval_runtime': 3.82, 'eval_samples_per_second': 261.519, 'eval_steps_per_second': 16.492, 'epoch': 0.6}
{'loss': 0.6454, 'grad_norm': 0.12307126075029373, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6439735889434814, 'eval_runtime': 3.8076, 'eval_samples_per_second': 262.369, 'eval_steps_per_second': 16.546, 'epoch': 0.64}
{'loss': 0.6254, 'grad_norm': 0.13379737734794617, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6318230032920837, 'eval_runtime': 3.8155, 'eval_samples_per_second': 261.83, 'eval_steps_per_second': 16.512, 'epoch': 0.68}
{'loss': 0.6154, 'grad_norm': 0.14197443425655365, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6189091205596924, 'eval_runtime': 3.807, 'eval_samples_per_second': 262.41, 'eval_steps_per_second': 16.548, 'epoch': 0.72}
{'loss': 0.606, 'grad_norm': 0.16856993734836578, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6113194227218628, 'eval_runtime': 3.8218, 'eval_samples_per_second': 261.395, 'eval_steps_per_second': 16.484, 'epoch': 0.76}
{'loss': 0.611, 'grad_norm': 0.17828671634197235, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6004423499107361, 'eval_runtime': 3.8261, 'eval_samples_per_second': 261.101, 'eval_steps_per_second': 16.466, 'epoch': 0.8}
{'loss': 0.598, 'grad_norm': 0.19341175258159637, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5920836329460144, 'eval_runtime': 3.8266, 'eval_samples_per_second': 261.065, 'eval_steps_per_second': 16.464, 'epoch': 0.84}
{'loss': 0.6328, 'grad_norm': 0.1764843463897705, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5845173597335815, 'eval_runtime': 3.8199, 'eval_samples_per_second': 261.525, 'eval_steps_per_second': 16.493, 'epoch': 0.88}
{'loss': 0.6091, 'grad_norm': 0.18942776322364807, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5800492763519287, 'eval_runtime': 3.8276, 'eval_samples_per_second': 260.998, 'eval_steps_per_second': 16.459, 'epoch': 0.92}
{'loss': 0.5865, 'grad_norm': 0.19918467104434967, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5749468803405762, 'eval_runtime': 3.8122, 'eval_samples_per_second': 262.051, 'eval_steps_per_second': 16.526, 'epoch': 0.96}
{'loss': 0.6007, 'grad_norm': 0.19144970178604126, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5738672018051147, 'eval_runtime': 3.8156, 'eval_samples_per_second': 261.819, 'eval_steps_per_second': 16.511, 'epoch': 1.0}
{'train_runtime': 283.5126, 'train_samples_per_second': 35.268, 'train_steps_per_second': 2.204, 'train_loss': 0.9399742660522461, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6454660892486572, 1.438773274421692, 1.0253729820251465, 0.8558329939842224, 0.8176968693733215, 0.7943670749664307, 0.7715821862220764, 0.7549181580543518, 0.7371224164962769, 0.7225253582000732, 0.7083156108856201, 0.6956323981285095, 0.6822085976600647, 0.6701675057411194, 0.6566702723503113, 0.6439735889434814, 0.6318230032920837, 0.6189091205596924, 0.6113194227218628, 0.6004423499107361, 0.5920836329460144, 0.5845173597335815, 0.5800492763519287, 0.5749468803405762, 0.5738672018051147], 'performance': [0.56, 0.45]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:58,  1.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 18.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 24.21it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 22.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 25.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 27.53it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 30.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 26.50it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.45]
current iteration observed (possibly low-fid or predicted) performance:  1.4933631420135498
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1139 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 1.0193110306057673
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 1.0488891369989082
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 1.1120078297645264
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 1.112715667696333
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.727645981417057
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0260, dtype=torch.float64), tensor(0.3821, dtype=torch.float64), 0, 0, tensor(0.0323, dtype=torch.float64), tensor(0.5596, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.05606954315896882, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0260, dtype=torch.float64), tensor(0.3821, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9350e-05, dtype=torch.float64), tensor(0.0323, dtype=torch.float64), tensor(0.5596, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5607, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.026
  gsm8k: 0.382
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.032
  truthfulqa_gen: 0.56
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05606954315896882,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.05606954315896882
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:04,  1.25s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:17,  5.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.57it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.56it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.45it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 13.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 10.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01, 10.60it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.35it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.72it/s]
Evaluation performance at step 25: 0.57
{'loss': 3.5068, 'grad_norm': 0.36856934428215027, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 2.641766309738159, 'eval_runtime': 9.1688, 'eval_samples_per_second': 108.957, 'eval_steps_per_second': 6.871, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:18,  1.27it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.16it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:07,  9.75it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.45it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 14.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 15.35it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 17.93it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 16.99it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 11.34it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:01, 10.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 12.35it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.75it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.8991, 'grad_norm': 0.24727900326251984, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.366789698600769, 'eval_runtime': 9.1793, 'eval_samples_per_second': 108.832, 'eval_steps_per_second': 6.863, 'epoch': 0.08}
{'loss': 1.1569, 'grad_norm': 0.07632885128259659, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0540924072265625, 'eval_runtime': 9.2166, 'eval_samples_per_second': 108.392, 'eval_steps_per_second': 6.836, 'epoch': 0.12}
{'loss': 0.9677, 'grad_norm': 0.0691419243812561, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9333293437957764, 'eval_runtime': 9.2689, 'eval_samples_per_second': 107.78, 'eval_steps_per_second': 6.797, 'epoch': 0.16}
{'loss': 0.9195, 'grad_norm': 0.0561857670545578, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8932676315307617, 'eval_runtime': 9.3025, 'eval_samples_per_second': 107.391, 'eval_steps_per_second': 6.772, 'epoch': 0.2}
{'loss': 0.8832, 'grad_norm': 0.05641317367553711, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.877493679523468, 'eval_runtime': 9.3184, 'eval_samples_per_second': 107.207, 'eval_steps_per_second': 6.761, 'epoch': 0.24}
{'loss': 0.8646, 'grad_norm': 0.05939299613237381, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8625550270080566, 'eval_runtime': 9.3177, 'eval_samples_per_second': 107.215, 'eval_steps_per_second': 6.761, 'epoch': 0.28}
{'loss': 0.855, 'grad_norm': 0.056855183094739914, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8537066578865051, 'eval_runtime': 9.3455, 'eval_samples_per_second': 106.897, 'eval_steps_per_second': 6.741, 'epoch': 0.32}
{'loss': 0.8596, 'grad_norm': 0.06367666274309158, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8485878705978394, 'eval_runtime': 9.3645, 'eval_samples_per_second': 106.68, 'eval_steps_per_second': 6.728, 'epoch': 0.36}
{'loss': 0.8588, 'grad_norm': 0.05069998651742935, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8428285717964172, 'eval_runtime': 9.3504, 'eval_samples_per_second': 106.84, 'eval_steps_per_second': 6.738, 'epoch': 0.4}
{'loss': 0.8413, 'grad_norm': 0.06260623037815094, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8371672034263611, 'eval_runtime': 9.3452, 'eval_samples_per_second': 106.899, 'eval_steps_per_second': 6.741, 'epoch': 0.44}
{'loss': 0.8577, 'grad_norm': 0.06907770782709122, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8338571786880493, 'eval_runtime': 9.3808, 'eval_samples_per_second': 106.494, 'eval_steps_per_second': 6.716, 'epoch': 0.48}
{'loss': 0.8279, 'grad_norm': 0.05706441402435303, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8280617594718933, 'eval_runtime': 9.3922, 'eval_samples_per_second': 106.365, 'eval_steps_per_second': 6.708, 'epoch': 0.52}
{'loss': 0.8265, 'grad_norm': 0.08387784659862518, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8259830474853516, 'eval_runtime': 9.4228, 'eval_samples_per_second': 106.019, 'eval_steps_per_second': 6.686, 'epoch': 0.56}
{'loss': 0.8425, 'grad_norm': 0.0615907721221447, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8211900591850281, 'eval_runtime': 9.3893, 'eval_samples_per_second': 106.398, 'eval_steps_per_second': 6.71, 'epoch': 0.6}
{'loss': 0.8423, 'grad_norm': 0.06649670749902725, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8183866143226624, 'eval_runtime': 9.339, 'eval_samples_per_second': 106.971, 'eval_steps_per_second': 6.746, 'epoch': 0.64}
{'loss': 0.8183, 'grad_norm': 0.06903635710477829, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8149281144142151, 'eval_runtime': 9.335, 'eval_samples_per_second': 107.017, 'eval_steps_per_second': 6.749, 'epoch': 0.68}
{'loss': 0.8067, 'grad_norm': 0.06419969350099564, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8116078972816467, 'eval_runtime': 9.336, 'eval_samples_per_second': 107.005, 'eval_steps_per_second': 6.748, 'epoch': 0.72}
{'loss': 0.8269, 'grad_norm': 0.06967411190271378, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8105693459510803, 'eval_runtime': 9.3395, 'eval_samples_per_second': 106.964, 'eval_steps_per_second': 6.746, 'epoch': 0.76}
{'loss': 0.821, 'grad_norm': 0.06335939466953278, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8074573874473572, 'eval_runtime': 9.3228, 'eval_samples_per_second': 107.157, 'eval_steps_per_second': 6.758, 'epoch': 0.8}
{'loss': 0.7881, 'grad_norm': 0.06737367808818817, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8052383661270142, 'eval_runtime': 9.3329, 'eval_samples_per_second': 107.041, 'eval_steps_per_second': 6.75, 'epoch': 0.84}
{'loss': 0.8151, 'grad_norm': 0.10937569290399551, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8034858703613281, 'eval_runtime': 9.3293, 'eval_samples_per_second': 107.083, 'eval_steps_per_second': 6.753, 'epoch': 0.88}
{'loss': 0.7954, 'grad_norm': 0.06536629796028137, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8024089932441711, 'eval_runtime': 9.3356, 'eval_samples_per_second': 107.009, 'eval_steps_per_second': 6.748, 'epoch': 0.92}
{'loss': 0.8042, 'grad_norm': 0.07651951164007187, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8019618988037109, 'eval_runtime': 9.3356, 'eval_samples_per_second': 107.009, 'eval_steps_per_second': 6.748, 'epoch': 0.96}
{'loss': 0.7852, 'grad_norm': 0.06809671223163605, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8014897108078003, 'eval_runtime': 9.3539, 'eval_samples_per_second': 106.801, 'eval_steps_per_second': 6.735, 'epoch': 1.0}
{'train_runtime': 551.0495, 'train_samples_per_second': 18.144, 'train_steps_per_second': 1.134, 'train_loss': 1.0028058746337891, 'epoch': 1.0}
train_results:  {'eval_loss': [2.641766309738159, 1.366789698600769, 1.0540924072265625, 0.9333293437957764, 0.8932676315307617, 0.877493679523468, 0.8625550270080566, 0.8537066578865051, 0.8485878705978394, 0.8428285717964172, 0.8371672034263611, 0.8338571786880493, 0.8280617594718933, 0.8259830474853516, 0.8211900591850281, 0.8183866143226624, 0.8149281144142151, 0.8116078972816467, 0.8105693459510803, 0.8074573874473572, 0.8052383661270142, 0.8034858703613281, 0.8024089932441711, 0.8019618988037109, 0.8014897108078003], 'performance': [0.57, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:00,  1.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.72it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 18.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 20.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 21.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 24.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 21.15it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.4883949756622314
current iteration best possible performance (full train run):  0.609
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7287 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 1.0295696907913734
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.7884188244507166
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 1.0261588003296094
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.6522663116302988
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 1.1713198887433545
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0219, dtype=torch.float64), 0, 0, 0, 0, tensor(0.9781, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.05091150911852535, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0219, dtype=torch.float64), tensor(2.1880e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8371e-17, dtype=torch.float64), tensor(0.9781, dtype=torch.float64), tensor(1.0089e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5091, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.022
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.978
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05091150911852535,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.05091150911852535
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:59,  1.21s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  5.02it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  8.18it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.80it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:05,  9.83it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.85it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 11.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.25it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  9.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.20it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.9232, 'grad_norm': 0.3803907334804535, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.4094626903533936, 'eval_runtime': 4.4051, 'eval_samples_per_second': 226.782, 'eval_steps_per_second': 14.302, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:03,  1.56it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.43it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.41it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.19it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 12.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 12.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:03, 13.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:08,  4.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:05,  5.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  6.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  6.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.15it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.0804, 'grad_norm': 0.3729233145713806, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 1.266032099723816, 'eval_runtime': 4.0717, 'eval_samples_per_second': 245.354, 'eval_steps_per_second': 15.473, 'epoch': 0.08}
{'loss': 1.1102, 'grad_norm': 0.12433337420225143, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9628411531448364, 'eval_runtime': 4.0717, 'eval_samples_per_second': 245.35, 'eval_steps_per_second': 15.473, 'epoch': 0.12}
{'loss': 0.8925, 'grad_norm': 0.07174816727638245, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7925758957862854, 'eval_runtime': 4.0682, 'eval_samples_per_second': 245.562, 'eval_steps_per_second': 15.486, 'epoch': 0.16}
{'loss': 0.7601, 'grad_norm': 0.05759227275848389, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7536283731460571, 'eval_runtime': 4.0632, 'eval_samples_per_second': 245.863, 'eval_steps_per_second': 15.505, 'epoch': 0.2}
{'loss': 0.7232, 'grad_norm': 0.05823484808206558, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7206889986991882, 'eval_runtime': 4.0699, 'eval_samples_per_second': 245.459, 'eval_steps_per_second': 15.479, 'epoch': 0.24}
{'loss': 0.6801, 'grad_norm': 0.06459401547908783, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6912068128585815, 'eval_runtime': 4.0775, 'eval_samples_per_second': 245.002, 'eval_steps_per_second': 15.451, 'epoch': 0.28}
{'loss': 0.6624, 'grad_norm': 0.08123183995485306, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6521509885787964, 'eval_runtime': 4.0744, 'eval_samples_per_second': 245.19, 'eval_steps_per_second': 15.462, 'epoch': 0.32}
{'loss': 0.6264, 'grad_norm': 0.084883913397789, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6094515919685364, 'eval_runtime': 4.0868, 'eval_samples_per_second': 244.445, 'eval_steps_per_second': 15.415, 'epoch': 0.36}
{'loss': 0.5907, 'grad_norm': 0.11121268570423126, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.559978723526001, 'eval_runtime': 4.0885, 'eval_samples_per_second': 244.342, 'eval_steps_per_second': 15.409, 'epoch': 0.4}
{'loss': 0.5601, 'grad_norm': 0.11143801361322403, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5065512657165527, 'eval_runtime': 4.0938, 'eval_samples_per_second': 244.03, 'eval_steps_per_second': 15.389, 'epoch': 0.44}
{'loss': 0.4987, 'grad_norm': 0.11001325398683548, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.456849604845047, 'eval_runtime': 4.0842, 'eval_samples_per_second': 244.599, 'eval_steps_per_second': 15.425, 'epoch': 0.48}
{'loss': 0.4402, 'grad_norm': 0.13136965036392212, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4159311354160309, 'eval_runtime': 4.1057, 'eval_samples_per_second': 243.321, 'eval_steps_per_second': 15.345, 'epoch': 0.52}
{'loss': 0.3944, 'grad_norm': 0.13694576919078827, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3861852288246155, 'eval_runtime': 4.0816, 'eval_samples_per_second': 244.757, 'eval_steps_per_second': 15.435, 'epoch': 0.56}
{'loss': 0.3753, 'grad_norm': 0.19432216882705688, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.34202077984809875, 'eval_runtime': 4.0814, 'eval_samples_per_second': 244.769, 'eval_steps_per_second': 15.436, 'epoch': 0.6}
{'loss': 0.3562, 'grad_norm': 0.1456565409898758, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.31216779351234436, 'eval_runtime': 4.0822, 'eval_samples_per_second': 244.72, 'eval_steps_per_second': 15.433, 'epoch': 0.64}
{'loss': 0.3307, 'grad_norm': 0.16762807965278625, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.28674936294555664, 'eval_runtime': 4.0804, 'eval_samples_per_second': 244.827, 'eval_steps_per_second': 15.44, 'epoch': 0.68}
{'loss': 0.2786, 'grad_norm': 0.17156581580638885, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.26521703600883484, 'eval_runtime': 4.0804, 'eval_samples_per_second': 244.829, 'eval_steps_per_second': 15.44, 'epoch': 0.72}
{'loss': 0.2863, 'grad_norm': 0.15986713767051697, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2447015345096588, 'eval_runtime': 4.0944, 'eval_samples_per_second': 243.992, 'eval_steps_per_second': 15.387, 'epoch': 0.76}
{'loss': 0.2571, 'grad_norm': 0.13769027590751648, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.2323797345161438, 'eval_runtime': 4.0966, 'eval_samples_per_second': 243.858, 'eval_steps_per_second': 15.378, 'epoch': 0.8}
{'loss': 0.2302, 'grad_norm': 0.12108538299798965, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.22151827812194824, 'eval_runtime': 4.0905, 'eval_samples_per_second': 244.226, 'eval_steps_per_second': 15.402, 'epoch': 0.84}
{'loss': 0.2259, 'grad_norm': 0.09982943534851074, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.21376512944698334, 'eval_runtime': 4.0883, 'eval_samples_per_second': 244.357, 'eval_steps_per_second': 15.41, 'epoch': 0.88}
{'loss': 0.2249, 'grad_norm': 0.11409379541873932, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.20885884761810303, 'eval_runtime': 4.1419, 'eval_samples_per_second': 241.193, 'eval_steps_per_second': 15.21, 'epoch': 0.92}
{'loss': 0.197, 'grad_norm': 0.14606289565563202, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.20456285774707794, 'eval_runtime': 4.097, 'eval_samples_per_second': 243.835, 'eval_steps_per_second': 15.377, 'epoch': 0.96}
{'loss': 0.2231, 'grad_norm': 0.10399843752384186, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.20323604345321655, 'eval_runtime': 4.0839, 'eval_samples_per_second': 244.619, 'eval_steps_per_second': 15.426, 'epoch': 1.0}
{'train_runtime': 303.458, 'train_samples_per_second': 32.95, 'train_steps_per_second': 2.06, 'train_loss': 0.7171105117797851, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4094626903533936, 1.266032099723816, 0.9628411531448364, 0.7925758957862854, 0.7536283731460571, 0.7206889986991882, 0.6912068128585815, 0.6521509885787964, 0.6094515919685364, 0.559978723526001, 0.5065512657165527, 0.456849604845047, 0.4159311354160309, 0.3861852288246155, 0.34202077984809875, 0.31216779351234436, 0.28674936294555664, 0.26521703600883484, 0.2447015345096588, 0.2323797345161438, 0.22151827812194824, 0.21376512944698334, 0.20885884761810303, 0.20456285774707794, 0.20323604345321655], 'performance': [0.55, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:01,  1.22s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.92it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 13.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:09,  5.15it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:04,  7.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  8.44it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 11.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  9.01it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.4931259155273438
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314, 1.4931259155273438]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6142 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.9995493338767151
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.9376596990592032
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 1.1717961947038908
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 1.1571862631479204
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.7908604514784803
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0200, dtype=torch.float64), 0, 0, 0, 0, tensor(0.8283, dtype=torch.float64), 0, 0, tensor(0.1517, dtype=torch.float64), 32, 0, 1, 0, 0, 0, 128, 0.04400387535664682, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0200, dtype=torch.float64), tensor(4.9907e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8283, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.4303e-18, dtype=torch.float64), tensor(0.1517, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4400, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.02
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.828
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.152

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04400387535664682,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.04400387535664682
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:35,  1.57s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.81it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 11.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.69it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.25it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 13.36it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 14.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.29it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01, 10.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 12.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.80it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.9924, 'grad_norm': 0.5112559199333191, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 4.454771041870117, 'eval_runtime': 4.9145, 'eval_samples_per_second': 203.277, 'eval_steps_per_second': 12.819, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.19it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:06, 14.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 18.28it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 17.96it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 17.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 16.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 17.05it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:03, 13.80it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 17.69it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:01, 18.09it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 18.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 18.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.51it/s]
Evaluation performance at step 50: 0.62
{'loss': 3.3202, 'grad_norm': 0.26931071281433105, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 2.329040765762329, 'eval_runtime': 4.9221, 'eval_samples_per_second': 202.962, 'eval_steps_per_second': 12.799, 'epoch': 0.08}
{'loss': 1.7935, 'grad_norm': 0.10118534415960312, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5296292304992676, 'eval_runtime': 4.9392, 'eval_samples_per_second': 202.261, 'eval_steps_per_second': 12.755, 'epoch': 0.12}
{'loss': 1.4241, 'grad_norm': 0.06609039753675461, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.330754280090332, 'eval_runtime': 4.9704, 'eval_samples_per_second': 200.991, 'eval_steps_per_second': 12.675, 'epoch': 0.16}
{'loss': 1.2358, 'grad_norm': 0.05211031436920166, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.206545114517212, 'eval_runtime': 5.0011, 'eval_samples_per_second': 199.755, 'eval_steps_per_second': 12.597, 'epoch': 0.2}
{'loss': 1.1638, 'grad_norm': 0.0681224837899208, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1616569757461548, 'eval_runtime': 5.0111, 'eval_samples_per_second': 199.359, 'eval_steps_per_second': 12.572, 'epoch': 0.24}
{'loss': 1.1595, 'grad_norm': 0.09214572608470917, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.132352352142334, 'eval_runtime': 5.0535, 'eval_samples_per_second': 197.684, 'eval_steps_per_second': 12.467, 'epoch': 0.28}
{'loss': 1.1148, 'grad_norm': 0.07806352525949478, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1102855205535889, 'eval_runtime': 5.0326, 'eval_samples_per_second': 198.504, 'eval_steps_per_second': 12.518, 'epoch': 0.32}
{'loss': 1.0878, 'grad_norm': 0.07524263858795166, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0919827222824097, 'eval_runtime': 5.0617, 'eval_samples_per_second': 197.366, 'eval_steps_per_second': 12.447, 'epoch': 0.36}
{'loss': 1.0526, 'grad_norm': 0.08648623526096344, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0751368999481201, 'eval_runtime': 5.0175, 'eval_samples_per_second': 199.105, 'eval_steps_per_second': 12.556, 'epoch': 0.4}
{'loss': 1.0584, 'grad_norm': 0.08155400305986404, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0615121126174927, 'eval_runtime': 5.0112, 'eval_samples_per_second': 199.353, 'eval_steps_per_second': 12.572, 'epoch': 0.44}
{'loss': 1.0501, 'grad_norm': 0.10558842122554779, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0447719097137451, 'eval_runtime': 5.0084, 'eval_samples_per_second': 199.463, 'eval_steps_per_second': 12.579, 'epoch': 0.48}
{'loss': 1.0349, 'grad_norm': 0.09190697222948074, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.027733325958252, 'eval_runtime': 5.0336, 'eval_samples_per_second': 198.466, 'eval_steps_per_second': 12.516, 'epoch': 0.52}
{'loss': 1.0347, 'grad_norm': 0.09426049888134003, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0144354104995728, 'eval_runtime': 5.0345, 'eval_samples_per_second': 198.431, 'eval_steps_per_second': 12.514, 'epoch': 0.56}
{'loss': 1.0108, 'grad_norm': 0.11624884605407715, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9968410730361938, 'eval_runtime': 5.0507, 'eval_samples_per_second': 197.793, 'eval_steps_per_second': 12.473, 'epoch': 0.6}
{'loss': 0.9737, 'grad_norm': 0.11554358154535294, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9787704944610596, 'eval_runtime': 5.0394, 'eval_samples_per_second': 198.238, 'eval_steps_per_second': 12.501, 'epoch': 0.64}
{'loss': 0.9735, 'grad_norm': 0.09796396642923355, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9557204842567444, 'eval_runtime': 5.0397, 'eval_samples_per_second': 198.228, 'eval_steps_per_second': 12.501, 'epoch': 0.68}
{'loss': 0.9601, 'grad_norm': 0.10787059366703033, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9253215193748474, 'eval_runtime': 5.0347, 'eval_samples_per_second': 198.423, 'eval_steps_per_second': 12.513, 'epoch': 0.72}
{'loss': 0.9016, 'grad_norm': 0.10383325815200806, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8882306218147278, 'eval_runtime': 5.0347, 'eval_samples_per_second': 198.424, 'eval_steps_per_second': 12.513, 'epoch': 0.76}
{'loss': 0.8872, 'grad_norm': 0.10247493535280228, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8677953481674194, 'eval_runtime': 5.0402, 'eval_samples_per_second': 198.208, 'eval_steps_per_second': 12.5, 'epoch': 0.8}
{'loss': 0.8601, 'grad_norm': 0.1233590617775917, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8562400341033936, 'eval_runtime': 5.0338, 'eval_samples_per_second': 198.458, 'eval_steps_per_second': 12.515, 'epoch': 0.84}
{'loss': 0.8673, 'grad_norm': 0.09936177730560303, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8486204743385315, 'eval_runtime': 5.0011, 'eval_samples_per_second': 199.755, 'eval_steps_per_second': 12.597, 'epoch': 0.88}
{'loss': 0.8291, 'grad_norm': 0.10141752660274506, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8440513014793396, 'eval_runtime': 5.0112, 'eval_samples_per_second': 199.353, 'eval_steps_per_second': 12.572, 'epoch': 0.92}
{'loss': 0.8233, 'grad_norm': 0.11126010119915009, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8403415679931641, 'eval_runtime': 5.0063, 'eval_samples_per_second': 199.547, 'eval_steps_per_second': 12.584, 'epoch': 0.96}
{'loss': 0.8281, 'grad_norm': 0.11896269023418427, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8398301601409912, 'eval_runtime': 5.0095, 'eval_samples_per_second': 199.421, 'eval_steps_per_second': 12.576, 'epoch': 1.0}
{'train_runtime': 347.1595, 'train_samples_per_second': 28.799, 'train_steps_per_second': 1.8, 'train_loss': 1.2974989562988282, 'epoch': 1.0}
train_results:  {'eval_loss': [4.454771041870117, 2.329040765762329, 1.5296292304992676, 1.330754280090332, 1.206545114517212, 1.1616569757461548, 1.132352352142334, 1.1102855205535889, 1.0919827222824097, 1.0751368999481201, 1.0615121126174927, 1.0447719097137451, 1.027733325958252, 1.0144354104995728, 0.9968410730361938, 0.9787704944610596, 0.9557204842567444, 0.9253215193748474, 0.8882306218147278, 0.8677953481674194, 0.8562400341033936, 0.8486204743385315, 0.8440513014793396, 0.8403415679931641, 0.8398301601409912], 'performance': [0.55, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:48,  2.04it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.91it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 25.33it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:01, 28.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 30.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 31.21it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 34.77it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 30.13it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.4895248413085938
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314, 1.4931259155273438, 1.4895248413085938]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0500 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 1.0732981408213127
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.9727669205220573
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 1.1246408636126635
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 1.0956058512285016
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.6359984755127066
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0326, dtype=torch.float64), 0, 0, 0, tensor(0.0383, dtype=torch.float64), tensor(0.1940, dtype=torch.float64), tensor(0.5309, dtype=torch.float64), tensor(0.1987, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 1, 128, 0.051684007394717194, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0326, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4468e-17, dtype=torch.float64), tensor(0.0055, dtype=torch.float64), tensor(0.0383, dtype=torch.float64), tensor(0.1940, dtype=torch.float64), tensor(0.5309, dtype=torch.float64), tensor(0.1987, dtype=torch.float64), tensor(7.8665e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5168, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.033
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.038
  truthfulqa_gen: 0.194
  wikitext: 0.531
  mmlu: 0.199
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.051684007394717194,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.051684007394717194
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9943
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  994
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:00,  1.82s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  4.02it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.01it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.68it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.13it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05,  9.94it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.54it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.65it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  9.31it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.65it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.97it/s]
Evaluation performance at step 25: 0.56
{'loss': 3.7215, 'grad_norm': 0.28723815083503723, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.0660245418548584, 'eval_runtime': 9.2885, 'eval_samples_per_second': 107.014, 'eval_steps_per_second': 6.783, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:56,  1.74it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.89it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:07,  9.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.97it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 12.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:13,  3.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:09<00:08,  4.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:05,  6.55it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:03,  7.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  8.30it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  9.05it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  9.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.03it/s]
Evaluation performance at step 50: 0.6
{'loss': 2.5499, 'grad_norm': 0.2725708782672882, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 2.088475227355957, 'eval_runtime': 9.3151, 'eval_samples_per_second': 106.708, 'eval_steps_per_second': 6.763, 'epoch': 0.08}
{'loss': 2.0088, 'grad_norm': 0.10125821083784103, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.8518805503845215, 'eval_runtime': 9.3455, 'eval_samples_per_second': 106.362, 'eval_steps_per_second': 6.741, 'epoch': 0.12}
{'loss': 1.7767, 'grad_norm': 0.09125687181949615, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.724886178970337, 'eval_runtime': 9.3483, 'eval_samples_per_second': 106.329, 'eval_steps_per_second': 6.739, 'epoch': 0.16}
{'loss': 1.7189, 'grad_norm': 0.17375582456588745, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.6500182151794434, 'eval_runtime': 9.3562, 'eval_samples_per_second': 106.24, 'eval_steps_per_second': 6.733, 'epoch': 0.2}
{'loss': 1.6487, 'grad_norm': 0.0701829269528389, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.6163434982299805, 'eval_runtime': 9.3639, 'eval_samples_per_second': 106.153, 'eval_steps_per_second': 6.728, 'epoch': 0.24}
{'loss': 1.597, 'grad_norm': 0.09493094682693481, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.598833680152893, 'eval_runtime': 9.3794, 'eval_samples_per_second': 105.977, 'eval_steps_per_second': 6.717, 'epoch': 0.28}
{'loss': 1.6177, 'grad_norm': 0.11374231427907944, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.5836267471313477, 'eval_runtime': 9.3894, 'eval_samples_per_second': 105.864, 'eval_steps_per_second': 6.71, 'epoch': 0.32}
{'loss': 1.6582, 'grad_norm': 0.0929778441786766, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.572746753692627, 'eval_runtime': 9.3904, 'eval_samples_per_second': 105.853, 'eval_steps_per_second': 6.709, 'epoch': 0.36}
{'loss': 1.6504, 'grad_norm': 0.07465902715921402, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.562569499015808, 'eval_runtime': 9.3858, 'eval_samples_per_second': 105.904, 'eval_steps_per_second': 6.712, 'epoch': 0.4}
{'loss': 1.5964, 'grad_norm': 0.07430069148540497, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.555390477180481, 'eval_runtime': 9.3997, 'eval_samples_per_second': 105.748, 'eval_steps_per_second': 6.702, 'epoch': 0.44}
{'loss': 1.4884, 'grad_norm': 0.09416842460632324, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.549126148223877, 'eval_runtime': 9.4052, 'eval_samples_per_second': 105.686, 'eval_steps_per_second': 6.698, 'epoch': 0.48}
{'loss': 1.6539, 'grad_norm': 0.06904629617929459, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 1.5416791439056396, 'eval_runtime': 9.39, 'eval_samples_per_second': 105.858, 'eval_steps_per_second': 6.709, 'epoch': 0.52}
{'loss': 1.5483, 'grad_norm': 0.12861046195030212, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 1.536628246307373, 'eval_runtime': 9.4125, 'eval_samples_per_second': 105.605, 'eval_steps_per_second': 6.693, 'epoch': 0.56}
{'loss': 1.5874, 'grad_norm': 0.09667864441871643, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 1.5310208797454834, 'eval_runtime': 9.4088, 'eval_samples_per_second': 105.646, 'eval_steps_per_second': 6.696, 'epoch': 0.6}
{'loss': 1.5186, 'grad_norm': 0.06901955604553223, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 1.5265514850616455, 'eval_runtime': 9.4096, 'eval_samples_per_second': 105.636, 'eval_steps_per_second': 6.695, 'epoch': 0.64}
{'loss': 1.6049, 'grad_norm': 0.06101476028561592, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 1.5227117538452148, 'eval_runtime': 9.4216, 'eval_samples_per_second': 105.502, 'eval_steps_per_second': 6.687, 'epoch': 0.68}
{'loss': 1.5267, 'grad_norm': 0.07211986184120178, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 1.5183531045913696, 'eval_runtime': 9.4111, 'eval_samples_per_second': 105.62, 'eval_steps_per_second': 6.694, 'epoch': 0.72}
{'loss': 1.588, 'grad_norm': 0.08957726508378983, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 1.5154128074645996, 'eval_runtime': 9.4182, 'eval_samples_per_second': 105.54, 'eval_steps_per_second': 6.689, 'epoch': 0.76}
{'loss': 1.5242, 'grad_norm': 0.10823329538106918, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 1.513164758682251, 'eval_runtime': 9.405, 'eval_samples_per_second': 105.688, 'eval_steps_per_second': 6.699, 'epoch': 0.8}
{'loss': 1.5261, 'grad_norm': 0.11636808514595032, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 1.5105098485946655, 'eval_runtime': 9.372, 'eval_samples_per_second': 106.061, 'eval_steps_per_second': 6.722, 'epoch': 0.84}
{'loss': 1.5865, 'grad_norm': 0.09097309410572052, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 1.50930655002594, 'eval_runtime': 9.3923, 'eval_samples_per_second': 105.832, 'eval_steps_per_second': 6.708, 'epoch': 0.88}
{'loss': 1.5644, 'grad_norm': 0.0686257854104042, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 1.5079519748687744, 'eval_runtime': 9.3813, 'eval_samples_per_second': 105.955, 'eval_steps_per_second': 6.715, 'epoch': 0.92}
{'loss': 1.5169, 'grad_norm': 0.09483117610216141, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 1.5071884393692017, 'eval_runtime': 9.384, 'eval_samples_per_second': 105.925, 'eval_steps_per_second': 6.714, 'epoch': 0.96}
{'train_runtime': 557.1358, 'train_samples_per_second': 17.847, 'train_steps_per_second': 1.116, 'train_loss': 1.73721646495954, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0660245418548584, 2.088475227355957, 1.8518805503845215, 1.724886178970337, 1.6500182151794434, 1.6163434982299805, 1.598833680152893, 1.5836267471313477, 1.572746753692627, 1.562569499015808, 1.555390477180481, 1.549126148223877, 1.5416791439056396, 1.536628246307373, 1.5310208797454834, 1.5265514850616455, 1.5227117538452148, 1.5183531045913696, 1.5154128074645996, 1.513164758682251, 1.5105098485946655, 1.50930655002594, 1.5079519748687744, 1.5071884393692017], 'performance': [0.56, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:58,  1.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 23.01it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:07,  7.18it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:03, 10.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 13.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 17.47it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.4837315082550049
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314, 1.4931259155273438, 1.4895248413085938, 1.4837315082550049]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4339 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.8211508532013824
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 1.1440823707196799
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.5426446915089628
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.780748167748913
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 1.1772767509660316
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0109, dtype=torch.float64), 0, 0, tensor(0.3128, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(0.6503, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.04150508454220725, 1.4800000190734868, 1]
normalized proposed parameters for next round by BO: [tensor(0.0109, dtype=torch.float64), tensor(2.9062e-17, dtype=torch.float64), tensor(1.6246e-17, dtype=torch.float64), tensor(0.3128, dtype=torch.float64), tensor(0.0260, dtype=torch.float64), tensor(0.6503, dtype=torch.float64), tensor(7.9161e-18, dtype=torch.float64), tensor(6.7539e-18, dtype=torch.float64), tensor(3.5793e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.4151, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.011
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.313
  triviaqa: 0.026
  truthfulqa_gen: 0.65
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04150508454220725,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.04150508454220725
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:43,  1.65s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.50it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.71it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.40it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.33it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.92it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 12.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.77it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.90it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.80it/s]
Evaluation performance at step 25: 0.58
{'loss': 5.1715, 'grad_norm': 0.5364007353782654, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.58}
{'eval_loss': 3.675950050354004, 'eval_runtime': 3.6103, 'eval_samples_per_second': 276.71, 'eval_steps_per_second': 17.45, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:50,  1.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.76it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.85it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 14.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 15.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.27it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 16.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 16.67it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 12.73it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 14.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.66it/s]
Evaluation performance at step 50: 0.44
{'loss': 2.3201, 'grad_norm': 0.43489694595336914, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.44}
{'eval_loss': 1.4200000762939453, 'eval_runtime': 3.5906, 'eval_samples_per_second': 278.223, 'eval_steps_per_second': 17.546, 'epoch': 0.08}
{'loss': 1.1755, 'grad_norm': 0.12481159716844559, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0239595174789429, 'eval_runtime': 3.5647, 'eval_samples_per_second': 280.251, 'eval_steps_per_second': 17.673, 'epoch': 0.12}
{'loss': 0.9406, 'grad_norm': 0.1330179125070572, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8615427017211914, 'eval_runtime': 3.5716, 'eval_samples_per_second': 279.708, 'eval_steps_per_second': 17.639, 'epoch': 0.16}
{'loss': 0.8326, 'grad_norm': 0.0747925192117691, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8317472338676453, 'eval_runtime': 3.5612, 'eval_samples_per_second': 280.525, 'eval_steps_per_second': 17.691, 'epoch': 0.2}
{'loss': 0.834, 'grad_norm': 0.0631958395242691, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8173221349716187, 'eval_runtime': 3.5633, 'eval_samples_per_second': 280.358, 'eval_steps_per_second': 17.68, 'epoch': 0.24}
{'loss': 0.7981, 'grad_norm': 0.07602740824222565, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8019195795059204, 'eval_runtime': 3.5602, 'eval_samples_per_second': 280.599, 'eval_steps_per_second': 17.695, 'epoch': 0.28}
{'loss': 0.8251, 'grad_norm': 0.06900829076766968, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7879437208175659, 'eval_runtime': 3.5583, 'eval_samples_per_second': 280.751, 'eval_steps_per_second': 17.705, 'epoch': 0.32}
{'loss': 0.7764, 'grad_norm': 0.07241537421941757, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7756749391555786, 'eval_runtime': 3.5762, 'eval_samples_per_second': 279.344, 'eval_steps_per_second': 17.616, 'epoch': 0.36}
{'loss': 0.7688, 'grad_norm': 0.07819440215826035, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7680333852767944, 'eval_runtime': 3.5493, 'eval_samples_per_second': 281.466, 'eval_steps_per_second': 17.75, 'epoch': 0.4}
{'loss': 0.7434, 'grad_norm': 0.08145309239625931, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7557175159454346, 'eval_runtime': 3.5571, 'eval_samples_per_second': 280.848, 'eval_steps_per_second': 17.711, 'epoch': 0.44}
{'loss': 0.7677, 'grad_norm': 0.13218063116073608, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7493665814399719, 'eval_runtime': 3.5542, 'eval_samples_per_second': 281.073, 'eval_steps_per_second': 17.725, 'epoch': 0.48}
{'loss': 0.7654, 'grad_norm': 0.09540987014770508, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7408223152160645, 'eval_runtime': 3.5514, 'eval_samples_per_second': 281.296, 'eval_steps_per_second': 17.739, 'epoch': 0.52}
{'loss': 0.7506, 'grad_norm': 0.09847350418567657, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7342556118965149, 'eval_runtime': 3.5563, 'eval_samples_per_second': 280.913, 'eval_steps_per_second': 17.715, 'epoch': 0.56}
{'loss': 0.7259, 'grad_norm': 0.09319699555635452, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7264028191566467, 'eval_runtime': 3.5521, 'eval_samples_per_second': 281.239, 'eval_steps_per_second': 17.736, 'epoch': 0.6}
{'loss': 0.7183, 'grad_norm': 0.09485423564910889, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7218111753463745, 'eval_runtime': 3.558, 'eval_samples_per_second': 280.777, 'eval_steps_per_second': 17.707, 'epoch': 0.64}
{'loss': 0.7181, 'grad_norm': 0.09461096674203873, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7159171104431152, 'eval_runtime': 3.5621, 'eval_samples_per_second': 280.452, 'eval_steps_per_second': 17.686, 'epoch': 0.68}
{'loss': 0.745, 'grad_norm': 0.09333103150129318, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7102481126785278, 'eval_runtime': 3.5666, 'eval_samples_per_second': 280.1, 'eval_steps_per_second': 17.664, 'epoch': 0.72}
{'loss': 0.6905, 'grad_norm': 0.11448998004198074, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7055055499076843, 'eval_runtime': 3.5677, 'eval_samples_per_second': 280.015, 'eval_steps_per_second': 17.659, 'epoch': 0.76}
{'loss': 0.7288, 'grad_norm': 0.11841342598199844, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7014859318733215, 'eval_runtime': 3.5638, 'eval_samples_per_second': 280.322, 'eval_steps_per_second': 17.678, 'epoch': 0.8}
{'loss': 0.7015, 'grad_norm': 0.1049085184931755, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6978670358657837, 'eval_runtime': 3.5704, 'eval_samples_per_second': 279.798, 'eval_steps_per_second': 17.645, 'epoch': 0.84}
{'loss': 0.7242, 'grad_norm': 0.12488752603530884, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6945012807846069, 'eval_runtime': 3.5732, 'eval_samples_per_second': 279.579, 'eval_steps_per_second': 17.631, 'epoch': 0.88}
{'loss': 0.7035, 'grad_norm': 0.13078272342681885, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6919962167739868, 'eval_runtime': 3.5743, 'eval_samples_per_second': 279.498, 'eval_steps_per_second': 17.626, 'epoch': 0.92}
{'loss': 0.717, 'grad_norm': 0.11128165572881699, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6905372738838196, 'eval_runtime': 3.5663, 'eval_samples_per_second': 280.119, 'eval_steps_per_second': 17.665, 'epoch': 0.96}
{'loss': 0.6886, 'grad_norm': 0.12014979869127274, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6901602745056152, 'eval_runtime': 3.5649, 'eval_samples_per_second': 280.236, 'eval_steps_per_second': 17.673, 'epoch': 1.0}
{'train_runtime': 276.1958, 'train_samples_per_second': 36.199, 'train_steps_per_second': 2.263, 'train_loss': 1.0132543701171874, 'epoch': 1.0}
train_results:  {'eval_loss': [3.675950050354004, 1.4200000762939453, 1.0239595174789429, 0.8615427017211914, 0.8317472338676453, 0.8173221349716187, 0.8019195795059204, 0.7879437208175659, 0.7756749391555786, 0.7680333852767944, 0.7557175159454346, 0.7493665814399719, 0.7408223152160645, 0.7342556118965149, 0.7264028191566467, 0.7218111753463745, 0.7159171104431152, 0.7102481126785278, 0.7055055499076843, 0.7014859318733215, 0.6978670358657837, 0.6945012807846069, 0.6919962167739868, 0.6905372738838196, 0.6901602745056152], 'performance': [0.58, 0.44]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:34,  1.04it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 20.00it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:05, 10.02it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 12.84it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 16.35it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 20.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.02it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.58, 0.44]
current iteration observed (possibly low-fid or predicted) performance:  1.4919477701187134
current iteration best possible performance (full train run):  0.651
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314, 1.4931259155273438, 1.4895248413085938, 1.4837315082550049, 1.4919477701187134]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5959 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 1.1718464087512839
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 1.0236724748252444
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.8492472898803591
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.8586632385051707
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 1.087176273865925
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0258, dtype=torch.float64), 0, 0, 0, 0, tensor(0.9742, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.032591677257890556, 1.48000001907349, 1]
normalized proposed parameters for next round by BO: [tensor(0.0258, dtype=torch.float64), tensor(1.2847e-17, dtype=torch.float64), tensor(1.1395e-17, dtype=torch.float64), tensor(2.9652e-16, dtype=torch.float64), tensor(6.6600e-17, dtype=torch.float64), tensor(0.9742, dtype=torch.float64), tensor(4.0134e-18, dtype=torch.float64), tensor(1.3829e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3259, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.026
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.974
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.032591677257890556,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.48000001907349,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.032591677257890556
lora alpha:  1.48000001907349
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:41,  1.63s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.39it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.74it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 12.13it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.16it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.92it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.95it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 10.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.01it/s]
Evaluation performance at step 25: 0.57
{'loss': 5.006, 'grad_norm': 0.48427167534828186, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 3.610612630844116, 'eval_runtime': 3.8555, 'eval_samples_per_second': 259.113, 'eval_steps_per_second': 16.34, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:46,  1.08s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.60it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 12.69it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:04, 13.82it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.11it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 17.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 20.38it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 18.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 19.85it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 14.55it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 15.92it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.98it/s]
Evaluation performance at step 50: 0.47
{'loss': 2.2795, 'grad_norm': 0.4249808192253113, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.47}
{'eval_loss': 1.4035444259643555, 'eval_runtime': 3.8484, 'eval_samples_per_second': 259.587, 'eval_steps_per_second': 16.37, 'epoch': 0.08}
{'loss': 1.1259, 'grad_norm': 0.11018730700016022, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9853782057762146, 'eval_runtime': 3.8412, 'eval_samples_per_second': 260.077, 'eval_steps_per_second': 16.401, 'epoch': 0.12}
{'loss': 0.9115, 'grad_norm': 0.1202070489525795, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8232954740524292, 'eval_runtime': 3.8568, 'eval_samples_per_second': 259.025, 'eval_steps_per_second': 16.335, 'epoch': 0.16}
{'loss': 0.7941, 'grad_norm': 0.06863181293010712, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7791289687156677, 'eval_runtime': 3.8627, 'eval_samples_per_second': 258.627, 'eval_steps_per_second': 16.31, 'epoch': 0.2}
{'loss': 0.7874, 'grad_norm': 0.07514084875583649, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7527189254760742, 'eval_runtime': 3.8606, 'eval_samples_per_second': 258.765, 'eval_steps_per_second': 16.318, 'epoch': 0.24}
{'loss': 0.7372, 'grad_norm': 0.07448387891054153, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7292885184288025, 'eval_runtime': 3.8373, 'eval_samples_per_second': 260.342, 'eval_steps_per_second': 16.418, 'epoch': 0.28}
{'loss': 0.7099, 'grad_norm': 0.07660066336393356, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7122539281845093, 'eval_runtime': 3.8408, 'eval_samples_per_second': 260.101, 'eval_steps_per_second': 16.403, 'epoch': 0.32}
{'loss': 0.6963, 'grad_norm': 0.07963185757398605, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6943858861923218, 'eval_runtime': 3.8457, 'eval_samples_per_second': 259.771, 'eval_steps_per_second': 16.382, 'epoch': 0.36}
{'loss': 0.6757, 'grad_norm': 0.08570744842290878, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.67798912525177, 'eval_runtime': 3.8458, 'eval_samples_per_second': 259.766, 'eval_steps_per_second': 16.382, 'epoch': 0.4}
{'loss': 0.7001, 'grad_norm': 0.09174402058124542, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6622007489204407, 'eval_runtime': 3.8582, 'eval_samples_per_second': 258.929, 'eval_steps_per_second': 16.329, 'epoch': 0.44}
{'loss': 0.6745, 'grad_norm': 0.1086941659450531, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.646705687046051, 'eval_runtime': 3.8492, 'eval_samples_per_second': 259.533, 'eval_steps_per_second': 16.367, 'epoch': 0.48}
{'loss': 0.6413, 'grad_norm': 0.13319583237171173, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.635905921459198, 'eval_runtime': 3.8514, 'eval_samples_per_second': 259.384, 'eval_steps_per_second': 16.358, 'epoch': 0.52}
{'loss': 0.6261, 'grad_norm': 0.13038000464439392, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6236774921417236, 'eval_runtime': 3.8502, 'eval_samples_per_second': 259.466, 'eval_steps_per_second': 16.363, 'epoch': 0.56}
{'loss': 0.6162, 'grad_norm': 0.14971187710762024, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6088148951530457, 'eval_runtime': 3.8626, 'eval_samples_per_second': 258.635, 'eval_steps_per_second': 16.31, 'epoch': 0.6}
{'loss': 0.6356, 'grad_norm': 0.10181356221437454, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6001848578453064, 'eval_runtime': 3.8659, 'eval_samples_per_second': 258.414, 'eval_steps_per_second': 16.296, 'epoch': 0.64}
{'loss': 0.6167, 'grad_norm': 0.14003989100456238, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5812773704528809, 'eval_runtime': 3.869, 'eval_samples_per_second': 258.205, 'eval_steps_per_second': 16.283, 'epoch': 0.68}
{'loss': 0.5893, 'grad_norm': 0.13693466782569885, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5711638331413269, 'eval_runtime': 3.8699, 'eval_samples_per_second': 258.143, 'eval_steps_per_second': 16.279, 'epoch': 0.72}
{'loss': 0.5872, 'grad_norm': 0.15596455335617065, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5575112700462341, 'eval_runtime': 3.8585, 'eval_samples_per_second': 258.91, 'eval_steps_per_second': 16.328, 'epoch': 0.76}
{'loss': 0.566, 'grad_norm': 0.16308078169822693, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5454232096672058, 'eval_runtime': 3.8728, 'eval_samples_per_second': 257.951, 'eval_steps_per_second': 16.267, 'epoch': 0.8}
{'loss': 0.5257, 'grad_norm': 0.16343671083450317, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5371508002281189, 'eval_runtime': 3.8708, 'eval_samples_per_second': 258.084, 'eval_steps_per_second': 16.276, 'epoch': 0.84}
{'loss': 0.5643, 'grad_norm': 0.16952326893806458, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5296628475189209, 'eval_runtime': 3.8703, 'eval_samples_per_second': 258.122, 'eval_steps_per_second': 16.278, 'epoch': 0.88}
{'loss': 0.5339, 'grad_norm': 0.19009053707122803, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5217564702033997, 'eval_runtime': 3.8607, 'eval_samples_per_second': 258.764, 'eval_steps_per_second': 16.318, 'epoch': 0.92}
{'loss': 0.5352, 'grad_norm': 0.21685358881950378, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5172258615493774, 'eval_runtime': 3.8856, 'eval_samples_per_second': 257.102, 'eval_steps_per_second': 16.214, 'epoch': 0.96}
{'loss': 0.5259, 'grad_norm': 0.1972872018814087, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5154101848602295, 'eval_runtime': 3.8559, 'eval_samples_per_second': 259.083, 'eval_steps_per_second': 16.339, 'epoch': 1.0}
{'train_runtime': 293.74, 'train_samples_per_second': 34.04, 'train_steps_per_second': 2.128, 'train_loss': 0.9064608139038086, 'epoch': 1.0}
train_results:  {'eval_loss': [3.610612630844116, 1.4035444259643555, 0.9853782057762146, 0.8232954740524292, 0.7791289687156677, 0.7527189254760742, 0.7292885184288025, 0.7122539281845093, 0.6943858861923218, 0.67798912525177, 0.6622007489204407, 0.646705687046051, 0.635905921459198, 0.6236774921417236, 0.6088148951530457, 0.6001848578453064, 0.5812773704528809, 0.5711638331413269, 0.5575112700462341, 0.5454232096672058, 0.5371508002281189, 0.5296628475189209, 0.5217564702033997, 0.5172258615493774, 0.5154101848602295], 'performance': [0.57, 0.47]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:00,  1.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.73it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 21.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 24.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 27.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 30.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 24.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.47]
current iteration observed (possibly low-fid or predicted) performance:  1.4945029020309448
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314, 1.4931259155273438, 1.4895248413085938, 1.4837315082550049, 1.4919477701187134, 1.4945029020309448]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2438 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.9756599043971039
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.9937659076891023
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 1.0400621775828
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 1.146602685647268
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 1.1649594551007518
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0147, dtype=torch.float64), 0, 0, 0, tensor(0.0189, dtype=torch.float64), tensor(0.9664, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.02958109019200819, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0147, dtype=torch.float64), tensor(3.4902e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0189, dtype=torch.float64), tensor(0.9664, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2958, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.015
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.019
  truthfulqa_gen: 0.966
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.02958109019200819,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.02958109019200819
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:02,  1.85s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:23,  3.94it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:12,  6.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:10,  7.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  8.74it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  9.23it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  8.69it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  8.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.53it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 10.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  8.32it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  8.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.45it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.9586, 'grad_norm': 0.3865456283092499, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.299952745437622, 'eval_runtime': 4.3158, 'eval_samples_per_second': 231.475, 'eval_steps_per_second': 14.598, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:01,  1.60it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.32it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 12.61it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:05, 13.21it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.10it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:03, 12.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:08,  4.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:05,  5.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  6.82it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.44it/s]
Evaluation performance at step 50: 0.44
{'loss': 1.9686, 'grad_norm': 0.31863439083099365, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.44}
{'eval_loss': 1.2239400148391724, 'eval_runtime': 3.8388, 'eval_samples_per_second': 260.239, 'eval_steps_per_second': 16.411, 'epoch': 0.08}
{'loss': 1.0647, 'grad_norm': 0.11648077517747879, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9363553524017334, 'eval_runtime': 3.8406, 'eval_samples_per_second': 260.112, 'eval_steps_per_second': 16.403, 'epoch': 0.12}
{'loss': 0.8271, 'grad_norm': 0.06018241122364998, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7728762626647949, 'eval_runtime': 3.8462, 'eval_samples_per_second': 259.739, 'eval_steps_per_second': 16.38, 'epoch': 0.16}
{'loss': 0.7759, 'grad_norm': 0.06871224194765091, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7346083521842957, 'eval_runtime': 3.8452, 'eval_samples_per_second': 259.805, 'eval_steps_per_second': 16.384, 'epoch': 0.2}
{'loss': 0.7238, 'grad_norm': 0.058358971029520035, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7057890295982361, 'eval_runtime': 3.8426, 'eval_samples_per_second': 259.978, 'eval_steps_per_second': 16.395, 'epoch': 0.24}
{'loss': 0.702, 'grad_norm': 0.06767726689577103, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6752973794937134, 'eval_runtime': 3.8407, 'eval_samples_per_second': 260.111, 'eval_steps_per_second': 16.403, 'epoch': 0.28}
{'loss': 0.6406, 'grad_norm': 0.07312985509634018, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6431845426559448, 'eval_runtime': 3.8449, 'eval_samples_per_second': 259.823, 'eval_steps_per_second': 16.385, 'epoch': 0.32}
{'loss': 0.6277, 'grad_norm': 0.08720105141401291, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6019414067268372, 'eval_runtime': 3.8482, 'eval_samples_per_second': 259.603, 'eval_steps_per_second': 16.371, 'epoch': 0.36}
{'loss': 0.6142, 'grad_norm': 0.11068800091743469, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5595149397850037, 'eval_runtime': 3.8461, 'eval_samples_per_second': 259.744, 'eval_steps_per_second': 16.38, 'epoch': 0.4}
{'loss': 0.5575, 'grad_norm': 0.10013172030448914, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5179097652435303, 'eval_runtime': 3.8556, 'eval_samples_per_second': 259.104, 'eval_steps_per_second': 16.34, 'epoch': 0.44}
{'loss': 0.5183, 'grad_norm': 0.13774743676185608, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4719037115573883, 'eval_runtime': 3.8563, 'eval_samples_per_second': 259.057, 'eval_steps_per_second': 16.337, 'epoch': 0.48}
{'loss': 0.4849, 'grad_norm': 0.15844109654426575, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.43090513348579407, 'eval_runtime': 3.8573, 'eval_samples_per_second': 258.988, 'eval_steps_per_second': 16.333, 'epoch': 0.52}
{'loss': 0.4306, 'grad_norm': 0.13946150243282318, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.39874163269996643, 'eval_runtime': 3.8603, 'eval_samples_per_second': 258.789, 'eval_steps_per_second': 16.32, 'epoch': 0.56}
{'loss': 0.4176, 'grad_norm': 0.1592230647802353, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.362115740776062, 'eval_runtime': 3.8568, 'eval_samples_per_second': 259.024, 'eval_steps_per_second': 16.335, 'epoch': 0.6}
{'loss': 0.3832, 'grad_norm': 0.18830226361751556, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.32468342781066895, 'eval_runtime': 3.8646, 'eval_samples_per_second': 258.502, 'eval_steps_per_second': 16.302, 'epoch': 0.64}
{'loss': 0.3423, 'grad_norm': 0.17665493488311768, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3020274341106415, 'eval_runtime': 3.8601, 'eval_samples_per_second': 258.8, 'eval_steps_per_second': 16.321, 'epoch': 0.68}
{'loss': 0.3013, 'grad_norm': 0.11660421639680862, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.28348007798194885, 'eval_runtime': 3.8647, 'eval_samples_per_second': 258.496, 'eval_steps_per_second': 16.302, 'epoch': 0.72}
{'loss': 0.3025, 'grad_norm': 0.12589892745018005, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2663811147212982, 'eval_runtime': 3.8547, 'eval_samples_per_second': 259.165, 'eval_steps_per_second': 16.344, 'epoch': 0.76}
{'loss': 0.2585, 'grad_norm': 0.2025221288204193, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.24984505772590637, 'eval_runtime': 3.8759, 'eval_samples_per_second': 257.748, 'eval_steps_per_second': 16.254, 'epoch': 0.8}
{'loss': 0.2596, 'grad_norm': 0.14331750571727753, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.23739437758922577, 'eval_runtime': 3.863, 'eval_samples_per_second': 258.607, 'eval_steps_per_second': 16.309, 'epoch': 0.84}
{'loss': 0.263, 'grad_norm': 0.17996026575565338, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.22980338335037231, 'eval_runtime': 3.8555, 'eval_samples_per_second': 259.109, 'eval_steps_per_second': 16.34, 'epoch': 0.88}
{'loss': 0.246, 'grad_norm': 0.16942374408245087, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2237139642238617, 'eval_runtime': 3.8743, 'eval_samples_per_second': 257.852, 'eval_steps_per_second': 16.261, 'epoch': 0.92}
{'loss': 0.2222, 'grad_norm': 0.1604328155517578, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2192247211933136, 'eval_runtime': 3.8625, 'eval_samples_per_second': 258.641, 'eval_steps_per_second': 16.311, 'epoch': 0.96}
{'loss': 0.229, 'grad_norm': 0.12814819812774658, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.21742971241474152, 'eval_runtime': 3.8619, 'eval_samples_per_second': 258.68, 'eval_steps_per_second': 16.313, 'epoch': 1.0}
{'train_runtime': 307.9536, 'train_samples_per_second': 32.469, 'train_steps_per_second': 2.03, 'train_loss': 0.7247860900878906, 'epoch': 1.0}
train_results:  {'eval_loss': [3.299952745437622, 1.2239400148391724, 0.9363553524017334, 0.7728762626647949, 0.7346083521842957, 0.7057890295982361, 0.6752973794937134, 0.6431845426559448, 0.6019414067268372, 0.5595149397850037, 0.5179097652435303, 0.4719037115573883, 0.43090513348579407, 0.39874163269996643, 0.362115740776062, 0.32468342781066895, 0.3020274341106415, 0.28348007798194885, 0.2663811147212982, 0.24984505772590637, 0.23739437758922577, 0.22980338335037231, 0.2237139642238617, 0.2192247211933136, 0.21742971241474152], 'performance': [0.56, 0.44]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:18,  1.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:12,  6.69it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:10,  4.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:05,  6.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  8.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 11.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  9.05it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.44]
current iteration observed (possibly low-fid or predicted) performance:  1.4921175241470337
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314, 1.4931259155273438, 1.4895248413085938, 1.4837315082550049, 1.4919477701187134, 1.4945029020309448, 1.4921175241470337]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2431 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 1.1114617349479987
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 1.0863554434828735
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.9002052474822462
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 1.0799601170910198
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.45477740125831423
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7345, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2655, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 2, 3.469446951953612e-19, 30.628957133504635, 1]
normalized proposed parameters for next round by BO: [tensor(0.7345, dtype=torch.float64), tensor(2.1429e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5657e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2655, dtype=torch.float64), tensor(4.9658e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(3.4694e-18, dtype=torch.float64), tensor(0.6381, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.734
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.266
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (3.469446951953612e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (30.628957133504635,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  3.469446951953612e-19
lora alpha:  30.628957133504635
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 1,507,328 || all params: 8,031,768,576 || trainable%: 0.0188
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:47,  2.08it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.77it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.49it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:21,  3.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:12,  5.17it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  7.03it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:02, 11.79it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 12.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 13.21it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 13.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 14.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.72it/s]
Evaluation performance at step 25: 0.56
{'loss': 3.2946, 'grad_norm': 9.580991744995117, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 1.3272457122802734, 'eval_runtime': 4.8194, 'eval_samples_per_second': 207.289, 'eval_steps_per_second': 13.072, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:09,  1.42it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  8.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:05, 13.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.35it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.16it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 17.70it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 16.77it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 15.08it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 15.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 15.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.71it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.0565, 'grad_norm': 4.4216837882995605, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 0.9104551076889038, 'eval_runtime': 4.8234, 'eval_samples_per_second': 207.114, 'eval_steps_per_second': 13.061, 'epoch': 0.08}
{'loss': 0.8871, 'grad_norm': 1.4517208337783813, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8713953495025635, 'eval_runtime': 4.7982, 'eval_samples_per_second': 208.202, 'eval_steps_per_second': 13.13, 'epoch': 0.12}
{'loss': 0.8734, 'grad_norm': 1.3942230939865112, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8496704697608948, 'eval_runtime': 4.7985, 'eval_samples_per_second': 208.188, 'eval_steps_per_second': 13.129, 'epoch': 0.16}
{'loss': 0.8635, 'grad_norm': 1.1886001825332642, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8302488327026367, 'eval_runtime': 4.8068, 'eval_samples_per_second': 207.829, 'eval_steps_per_second': 13.106, 'epoch': 0.2}
{'loss': 0.8353, 'grad_norm': 1.2166962623596191, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.81448894739151, 'eval_runtime': 4.801, 'eval_samples_per_second': 208.083, 'eval_steps_per_second': 13.122, 'epoch': 0.24}
{'loss': 0.822, 'grad_norm': 1.4253082275390625, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8011913895606995, 'eval_runtime': 4.805, 'eval_samples_per_second': 207.908, 'eval_steps_per_second': 13.111, 'epoch': 0.28}
{'loss': 0.8138, 'grad_norm': 1.3561431169509888, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.793380618095398, 'eval_runtime': 4.8166, 'eval_samples_per_second': 207.408, 'eval_steps_per_second': 13.08, 'epoch': 0.32}
{'loss': 0.8311, 'grad_norm': 1.2042222023010254, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.778565526008606, 'eval_runtime': 4.8125, 'eval_samples_per_second': 207.586, 'eval_steps_per_second': 13.091, 'epoch': 0.36}
{'loss': 0.8068, 'grad_norm': 1.2576912641525269, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7665872573852539, 'eval_runtime': 4.8104, 'eval_samples_per_second': 207.676, 'eval_steps_per_second': 13.097, 'epoch': 0.4}
{'loss': 0.8084, 'grad_norm': 1.505417823791504, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7540574073791504, 'eval_runtime': 4.8233, 'eval_samples_per_second': 207.12, 'eval_steps_per_second': 13.062, 'epoch': 0.44}
{'loss': 0.7785, 'grad_norm': 1.3609484434127808, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7454661726951599, 'eval_runtime': 4.8139, 'eval_samples_per_second': 207.525, 'eval_steps_per_second': 13.087, 'epoch': 0.48}
{'loss': 0.7668, 'grad_norm': 1.1545473337173462, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7392776012420654, 'eval_runtime': 4.8196, 'eval_samples_per_second': 207.277, 'eval_steps_per_second': 13.072, 'epoch': 0.52}
{'loss': 0.7643, 'grad_norm': 1.1121095418930054, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7320367097854614, 'eval_runtime': 4.82, 'eval_samples_per_second': 207.262, 'eval_steps_per_second': 13.071, 'epoch': 0.56}
{'loss': 0.7584, 'grad_norm': 1.4251246452331543, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7223286628723145, 'eval_runtime': 4.8223, 'eval_samples_per_second': 207.161, 'eval_steps_per_second': 13.064, 'epoch': 0.6}
{'loss': 0.7527, 'grad_norm': 1.0944876670837402, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7158870100975037, 'eval_runtime': 4.8493, 'eval_samples_per_second': 206.007, 'eval_steps_per_second': 12.991, 'epoch': 0.64}
{'loss': 0.757, 'grad_norm': 1.3007270097732544, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7068502306938171, 'eval_runtime': 4.8308, 'eval_samples_per_second': 206.8, 'eval_steps_per_second': 13.041, 'epoch': 0.68}
{'loss': 0.7489, 'grad_norm': 1.180612325668335, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6991755962371826, 'eval_runtime': 4.8394, 'eval_samples_per_second': 206.432, 'eval_steps_per_second': 13.018, 'epoch': 0.72}
{'loss': 0.752, 'grad_norm': 1.394889235496521, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6954794526100159, 'eval_runtime': 4.8452, 'eval_samples_per_second': 206.183, 'eval_steps_per_second': 13.003, 'epoch': 0.76}
{'loss': 0.75, 'grad_norm': 1.16102135181427, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6893815398216248, 'eval_runtime': 4.8354, 'eval_samples_per_second': 206.601, 'eval_steps_per_second': 13.029, 'epoch': 0.8}
{'loss': 0.7286, 'grad_norm': 1.4328323602676392, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6832804083824158, 'eval_runtime': 4.8403, 'eval_samples_per_second': 206.392, 'eval_steps_per_second': 13.016, 'epoch': 0.84}
{'loss': 0.7259, 'grad_norm': 1.546434998512268, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6795687079429626, 'eval_runtime': 4.8233, 'eval_samples_per_second': 207.119, 'eval_steps_per_second': 13.062, 'epoch': 0.88}
{'loss': 0.7271, 'grad_norm': 1.272957444190979, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6760635375976562, 'eval_runtime': 4.8172, 'eval_samples_per_second': 207.38, 'eval_steps_per_second': 13.078, 'epoch': 0.92}
{'loss': 0.7294, 'grad_norm': 1.2183729410171509, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6734787821769714, 'eval_runtime': 4.8124, 'eval_samples_per_second': 207.589, 'eval_steps_per_second': 13.091, 'epoch': 0.96}
{'loss': 0.7375, 'grad_norm': 1.5950514078140259, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6726641058921814, 'eval_runtime': 4.8205, 'eval_samples_per_second': 207.238, 'eval_steps_per_second': 13.069, 'epoch': 1.0}
{'train_runtime': 339.643, 'train_samples_per_second': 29.44, 'train_steps_per_second': 1.84, 'train_loss': 0.8947841033935547, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3272457122802734, 0.9104551076889038, 0.8713953495025635, 0.8496704697608948, 0.8302488327026367, 0.81448894739151, 0.8011913895606995, 0.793380618095398, 0.778565526008606, 0.7665872573852539, 0.7540574073791504, 0.7454661726951599, 0.7392776012420654, 0.7320367097854614, 0.7223286628723145, 0.7158870100975037, 0.7068502306938171, 0.6991755962371826, 0.6954794526100159, 0.6893815398216248, 0.6832804083824158, 0.6795687079429626, 0.6760635375976562, 0.6734787821769714, 0.6726641058921814], 'performance': [0.56, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:47,  1.09s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 19.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 19.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:01, 17.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 22.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.92it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.2448489665985107
current iteration best possible performance (full train run):  0.735
max performance so far:  0.8925
BO observations:  [1.4170141220092773, 1.4707750082015991, 1.4716897010803223, 1.4683763980865479, 1.4805963039398193, 1.486405849456787, 1.4819879531860352, 1.4871513843536377, 1.4756464958190918, 1.4931979179382324, 1.4928396940231323, 1.4800448417663574, 1.4647204875946045, 1.4780852794647217, 1.4888899326324463, 1.482123613357544, 1.4884443283081055, 1.4879426956176758, 1.4904611110687256, 1.4912800788879395, 1.4859740734100342, 1.4933631420135498, 1.4883949756622314, 1.4931259155273438, 1.4895248413085938, 1.4837315082550049, 1.4919477701187134, 1.4945029020309448, 1.4921175241470337, 1.2448489665985107]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0194 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 1.0964779807703815
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.6890161582768559
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.9197551591727473
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 1.169644786956839
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 1.2604446467596992
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0285, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6940, dtype=torch.float64), 0, 0, tensor(0.2775, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 128, 1.3049400446339339e-17, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0.0285, dtype=torch.float64), tensor(9.1227e-17, dtype=torch.float64), tensor(1.0015e-18, dtype=torch.float64), tensor(6.5267e-17, dtype=torch.float64), tensor(5.2523e-18, dtype=torch.float64), tensor(0.6940, dtype=torch.float64), tensor(1.1226e-15, dtype=torch.float64), tensor(1.8466e-17, dtype=torch.float64), tensor(0.2775, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.3049e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.7140000000000001, 0.7875000000000001, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/truthfulqa_gen/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 20, 1, 1, 1, 1, 1, 18, 0.05137445104835325, 2, 1]
Checking history sample input_X_between_0_1:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.5137445104835324, 0.041666666666666664, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 22, 1, 1, 0, 0, 1, 9, 0.09530863992118319, 22, 1]
Checking history sample input_X_between_0_1:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 0.6875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0703125, 0.9530863992118318, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 7, 0, 0, 0, 0, 1, 45, 0.012049704078718804, 22, 1]
Checking history sample input_X_between_0_1:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 0.21875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3515625, 0.12049704078718804, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 2, 1, 1, 1, 0, 0, 32, 0.03789572912213354, 26, 1]
Checking history sample input_X_between_0_1:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.3789572912213354, 0.5416666666666666, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 8, 0, 0, 1, 1, 1, 57, 0.05639372568359048, 47, 0]
Checking history sample input_X_between_0_1:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4453125, 0.5639372568359048, 0.9791666666666666, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 18, 1, 1, 0, 0, 1, 112, 0.0011300351648876107, 2, 1]
Checking history sample input_X_between_0_1:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 0.5625, 1.0, 1.0, 0.0, 0.0, 1.0, 0.875, 0.011300351648876106, 0.041666666666666664, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 11, 0, 1, 1, 0, 1, 121, 0.04409228366491266, 38, 0]
Checking history sample input_X_between_0_1:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 0.34375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9453125, 0.4409228366491266, 0.7916666666666666, 0.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 19, 1, 0, 0, 1, 0, 20, 0.057419890903339765, 17, 1]
Checking history sample input_X_between_0_1:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 0.59375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15625, 0.5741989090333977, 0.3541666666666667, 1.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 13, 0, 1, 0, 0, 1, 54, 0.07044921211215552, 48, 0]
Checking history sample input_X_between_0_1:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 0.40625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.421875, 0.7044921211215551, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 9, 1, 0, 1, 1, 0, 17, 0.07776680881547844, 40, 0]
Checking history sample input_X_between_0_1:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 0.28125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1328125, 0.7776680881547844, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 1, 0, 0, 0, 1, 0, 60, 0.001514616808966751, 14, 1]
Checking history sample input_X_between_0_1:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.46875, 0.015146168089667511, 0.2916666666666667, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 18, 1, 1, 1, 0, 1, 45, 0.001287895623877422, 34, 1]
Checking history sample input_X_between_0_1:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 0.5625, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3515625, 0.01287895623877422, 0.7083333333333334, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 8, 0, 0, 0, 1, 1, 51, 0.08835721159033366, 35, 0]
Checking history sample input_X_between_0_1:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3984375, 0.8835721159033366, 0.7291666666666666, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 2, 0, 0, 0, 1, 1, 28, 0.09450434861769766, 16, 1]
Checking history sample input_X_between_0_1:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21875, 0.9450434861769765, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 2, 0, 0, 0, 1, 0, 27, 0.08742966606550949, 14, 0]
Checking history sample input_X_between_0_1:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2109375, 0.8742966606550948, 0.2916666666666667, 0.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 30, 0, 0, 0, 1, 0, 18, 0.07914275308569024, 23, 0]
Checking history sample input_X_between_0_1:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.140625, 0.7914275308569024, 0.4791666666666667, 0.0]
Checking history sample performance at 625 steps:  0.66
Checking history sample input_X:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 4, 1, 1, 1, 1, 1, 126, 0.005789639303569194, 25, 1]
Checking history sample input_X_between_0_1:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.05789639303569194, 0.5208333333333334, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 9, 1, 1, 1, 1, 1, 31, 0.03322680456132531, 22, 1]
Checking history sample input_X_between_0_1:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2421875, 0.33226804561325307, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 15, 0, 1, 0, 1, 1, 49, 0.0008783405064032635, 29, 1]
Checking history sample input_X_between_0_1:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 0.46875, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3828125, 0.008783405064032634, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.69
Checking history sample input_X:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 1, 0, 1, 1, 0, 1, 106, 0.08696702158391928, 5, 0]
Checking history sample input_X_between_0_1:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 0.03125, 0.0, 1.0, 1.0, 0.0, 1.0, 0.828125, 0.8696702158391928, 0.10416666666666667, 0.0]
Checking history sample performance at 625 steps:  0.46
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8527 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.7832053610253471
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.7851656893104026
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.7850647982096228
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.7732329629714589
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.7849718788372212
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.1052, dtype=torch.float64), tensor(0.0540, dtype=torch.float64), tensor(0.0802, dtype=torch.float64), 0, tensor(0.4351, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), tensor(0.0267, dtype=torch.float64), tensor(0.2083, dtype=torch.float64), 32, 0, 0, 0, 1, 0, 114, 8.868754143115545e-20, 44.250211883418324, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0., dtype=torch.float64), tensor(0.1052, dtype=torch.float64), tensor(0.0540, dtype=torch.float64), tensor(0.0802, dtype=torch.float64), tensor(2.5258e-18, dtype=torch.float64), tensor(0.4351, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), tensor(0.0267, dtype=torch.float64), tensor(0.2083, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8888, dtype=torch.float64), tensor(8.8688e-19, dtype=torch.float64), tensor(0.9219, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.105
  rowan_hellaswag: 0.054
  sciq: 0.08
  triviaqa: 0
  truthfulqa_gen: 0.435
  wikitext: 0.09
  mmlu: 0.027
  arc_challenge: 0.208

LoRA Parameters:
  lora_r: (114,)
  lora_dropout: (8.868754143115545e-20,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (44.250211883418324,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  114
lora dropout:  8.868754143115545e-20
lora alpha:  44.250211883418324
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 67,239,936 || all params: 8,097,501,184 || trainable%: 0.8304
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:44,  2.20it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 11.36it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.88it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:07,  9.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 11.03it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.32it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:03, 13.21it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 15.47it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 15.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 15.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 16.51it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 16.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 14.21it/s]
Evaluation performance at step 25: 0.56
{'loss': 3.0678, 'grad_norm': 0.9102330207824707, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 1.6472097635269165, 'eval_runtime': 8.6867, 'eval_samples_per_second': 115.004, 'eval_steps_per_second': 7.252, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:42,  2.32it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:06, 13.06it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.30it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 18.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 20.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 20.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 22.41it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 16.42it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 18.59it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:03<00:01, 19.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 20.70it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 20.18it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 22.51it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.03it/s]
Evaluation performance at step 50: 0.66
{'loss': 1.3469, 'grad_norm': 0.38299262523651123, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.66}
{'eval_loss': 1.1794828176498413, 'eval_runtime': 8.6434, 'eval_samples_per_second': 115.579, 'eval_steps_per_second': 7.289, 'epoch': 0.08}
{'loss': 1.0799, 'grad_norm': 0.3202807605266571, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1043015718460083, 'eval_runtime': 8.743, 'eval_samples_per_second': 114.263, 'eval_steps_per_second': 7.206, 'epoch': 0.12}
{'loss': 1.0787, 'grad_norm': 0.31591418385505676, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0690338611602783, 'eval_runtime': 8.7629, 'eval_samples_per_second': 114.003, 'eval_steps_per_second': 7.189, 'epoch': 0.16}
{'loss': 1.1069, 'grad_norm': 0.32936927676200867, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.038957953453064, 'eval_runtime': 8.8022, 'eval_samples_per_second': 113.495, 'eval_steps_per_second': 7.157, 'epoch': 0.2}
{'loss': 1.0747, 'grad_norm': 0.37418195605278015, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0210939645767212, 'eval_runtime': 8.7959, 'eval_samples_per_second': 113.576, 'eval_steps_per_second': 7.162, 'epoch': 0.24}
{'loss': 0.9923, 'grad_norm': 0.269581139087677, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0072872638702393, 'eval_runtime': 8.7912, 'eval_samples_per_second': 113.637, 'eval_steps_per_second': 7.166, 'epoch': 0.28}
{'loss': 1.0158, 'grad_norm': 0.2506341338157654, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9891154766082764, 'eval_runtime': 8.8074, 'eval_samples_per_second': 113.428, 'eval_steps_per_second': 7.153, 'epoch': 0.32}
{'loss': 0.9546, 'grad_norm': 0.2541608512401581, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9736521244049072, 'eval_runtime': 8.8458, 'eval_samples_per_second': 112.935, 'eval_steps_per_second': 7.122, 'epoch': 0.36}
{'loss': 0.9732, 'grad_norm': 0.23350617289543152, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9563912153244019, 'eval_runtime': 8.8269, 'eval_samples_per_second': 113.177, 'eval_steps_per_second': 7.137, 'epoch': 0.4}
{'loss': 0.9741, 'grad_norm': 0.3032476305961609, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9432122111320496, 'eval_runtime': 8.7856, 'eval_samples_per_second': 113.709, 'eval_steps_per_second': 7.171, 'epoch': 0.44}
{'loss': 0.9746, 'grad_norm': 0.3179260194301605, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9304616451263428, 'eval_runtime': 8.7963, 'eval_samples_per_second': 113.57, 'eval_steps_per_second': 7.162, 'epoch': 0.48}
{'loss': 0.9875, 'grad_norm': 0.32016658782958984, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9162521958351135, 'eval_runtime': 8.7966, 'eval_samples_per_second': 113.567, 'eval_steps_per_second': 7.162, 'epoch': 0.52}
{'loss': 0.9111, 'grad_norm': 0.23361781239509583, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9043422341346741, 'eval_runtime': 8.7888, 'eval_samples_per_second': 113.668, 'eval_steps_per_second': 7.168, 'epoch': 0.56}
{'loss': 0.9433, 'grad_norm': 0.35979634523391724, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8915393948554993, 'eval_runtime': 8.8043, 'eval_samples_per_second': 113.467, 'eval_steps_per_second': 7.156, 'epoch': 0.6}
{'loss': 0.8964, 'grad_norm': 0.2858985364437103, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.879307746887207, 'eval_runtime': 8.7967, 'eval_samples_per_second': 113.566, 'eval_steps_per_second': 7.162, 'epoch': 0.64}
{'loss': 0.917, 'grad_norm': 0.43459370732307434, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8667356967926025, 'eval_runtime': 8.7997, 'eval_samples_per_second': 113.526, 'eval_steps_per_second': 7.159, 'epoch': 0.68}
{'loss': 0.8783, 'grad_norm': 0.2593398988246918, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8566864728927612, 'eval_runtime': 8.7993, 'eval_samples_per_second': 113.532, 'eval_steps_per_second': 7.16, 'epoch': 0.72}
{'loss': 0.8778, 'grad_norm': 0.3602519631385803, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8451282978057861, 'eval_runtime': 8.8042, 'eval_samples_per_second': 113.468, 'eval_steps_per_second': 7.156, 'epoch': 0.76}
{'loss': 0.9905, 'grad_norm': 0.3672626316547394, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8374577760696411, 'eval_runtime': 8.8032, 'eval_samples_per_second': 113.482, 'eval_steps_per_second': 7.157, 'epoch': 0.8}
{'loss': 0.8605, 'grad_norm': 0.3678734600543976, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.826316773891449, 'eval_runtime': 8.7891, 'eval_samples_per_second': 113.663, 'eval_steps_per_second': 7.168, 'epoch': 0.84}
{'loss': 0.9081, 'grad_norm': 0.4331144392490387, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8200792670249939, 'eval_runtime': 8.79, 'eval_samples_per_second': 113.652, 'eval_steps_per_second': 7.167, 'epoch': 0.88}
{'loss': 0.8216, 'grad_norm': 0.42172345519065857, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8149793148040771, 'eval_runtime': 8.787, 'eval_samples_per_second': 113.69, 'eval_steps_per_second': 7.17, 'epoch': 0.92}
{'loss': 0.8148, 'grad_norm': 0.41758596897125244, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8123374581336975, 'eval_runtime': 8.8166, 'eval_samples_per_second': 113.309, 'eval_steps_per_second': 7.146, 'epoch': 0.96}
{'loss': 0.8558, 'grad_norm': 0.34344756603240967, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8099187016487122, 'eval_runtime': 8.8503, 'eval_samples_per_second': 112.878, 'eval_steps_per_second': 7.118, 'epoch': 1.0}
{'train_runtime': 533.1288, 'train_samples_per_second': 18.752, 'train_steps_per_second': 1.172, 'train_loss': 1.0520868041992189, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6472097635269165, 1.1794828176498413, 1.1043015718460083, 1.0690338611602783, 1.038957953453064, 1.0210939645767212, 1.0072872638702393, 0.9891154766082764, 0.9736521244049072, 0.9563912153244019, 0.9432122111320496, 0.9304616451263428, 0.9162521958351135, 0.9043422341346741, 0.8915393948554993, 0.879307746887207, 0.8667356967926025, 0.8566864728927612, 0.8451282978057861, 0.8374577760696411, 0.826316773891449, 0.8200792670249939, 0.8149793148040771, 0.8123374581336975, 0.8099187016487122], 'performance': [0.56, 0.66]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:02,  1.58it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.18it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 22.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:01, 25.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 29.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 30.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 33.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 28.24it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.66]
current iteration observed (possibly low-fid or predicted) performance:  1.4315956830978394
current iteration best possible performance (full train run):  0.7665
max performance so far:  0.7665
BO observations:  [1.4315956830978394]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1990 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.4116110579135879
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 1.36339508644605
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.4158338349049764
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.4035252137242042
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.339946277561724
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.2960e-18, dtype=torch.float64), tensor(1.8911e-17, dtype=torch.float64), tensor(1.7744e-17, dtype=torch.float64), tensor(5.0333e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.5850e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 1.0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:34,  1.05it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  8.44it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.51it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:04, 15.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 16.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 17.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 20.18it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 19.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 22.50it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 20.76it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 21.90it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:01, 10.17it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 12.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.79it/s]
Evaluation performance at step 25: 0.44
{'loss': 3.4197, 'grad_norm': 0.8703210353851318, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.44}
{'eval_loss': 1.2089009284973145, 'eval_runtime': 3.4192, 'eval_samples_per_second': 292.469, 'eval_steps_per_second': 18.426, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:50,  1.98it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 12.16it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 16.59it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:03, 20.61it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 22.25it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 21.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 24.04it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 23.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 23.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:03<00:01, 21.57it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 23.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 21.51it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 21.44it/s]
Evaluation performance at step 50: 0.59
{'loss': 0.8793, 'grad_norm': 0.37194734811782837, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 0.7733300924301147, 'eval_runtime': 3.4107, 'eval_samples_per_second': 293.194, 'eval_steps_per_second': 18.471, 'epoch': 0.08}
{'loss': 0.6989, 'grad_norm': 0.35914355516433716, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.6710794568061829, 'eval_runtime': 3.416, 'eval_samples_per_second': 292.741, 'eval_steps_per_second': 18.443, 'epoch': 0.12}
{'loss': 0.6211, 'grad_norm': 0.3124374449253082, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.5926260948181152, 'eval_runtime': 3.4207, 'eval_samples_per_second': 292.342, 'eval_steps_per_second': 18.418, 'epoch': 0.16}
{'loss': 0.5481, 'grad_norm': 0.3953920900821686, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.5128477215766907, 'eval_runtime': 3.4335, 'eval_samples_per_second': 291.25, 'eval_steps_per_second': 18.349, 'epoch': 0.2}
{'loss': 0.4618, 'grad_norm': 0.4251965880393982, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.4445233643054962, 'eval_runtime': 3.4357, 'eval_samples_per_second': 291.062, 'eval_steps_per_second': 18.337, 'epoch': 0.24}
{'loss': 0.4201, 'grad_norm': 0.37561336159706116, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.3713596761226654, 'eval_runtime': 3.4492, 'eval_samples_per_second': 289.921, 'eval_steps_per_second': 18.265, 'epoch': 0.28}
{'loss': 0.3829, 'grad_norm': 0.33470189571380615, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.31101420521736145, 'eval_runtime': 3.4276, 'eval_samples_per_second': 291.75, 'eval_steps_per_second': 18.38, 'epoch': 0.32}
{'loss': 0.2965, 'grad_norm': 0.352859765291214, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.27804210782051086, 'eval_runtime': 3.4318, 'eval_samples_per_second': 291.394, 'eval_steps_per_second': 18.358, 'epoch': 0.36}
{'loss': 0.2676, 'grad_norm': 0.3023260533809662, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.24686500430107117, 'eval_runtime': 3.442, 'eval_samples_per_second': 290.533, 'eval_steps_per_second': 18.304, 'epoch': 0.4}
{'loss': 0.2478, 'grad_norm': 0.23577024042606354, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.22462844848632812, 'eval_runtime': 3.4397, 'eval_samples_per_second': 290.727, 'eval_steps_per_second': 18.316, 'epoch': 0.44}
{'loss': 0.213, 'grad_norm': 0.35205796360969543, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.2090989649295807, 'eval_runtime': 3.4454, 'eval_samples_per_second': 290.242, 'eval_steps_per_second': 18.285, 'epoch': 0.48}
{'loss': 0.2151, 'grad_norm': 0.4128406345844269, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.19535955786705017, 'eval_runtime': 3.4417, 'eval_samples_per_second': 290.552, 'eval_steps_per_second': 18.305, 'epoch': 0.52}
{'loss': 0.1863, 'grad_norm': 0.2184072881937027, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.18059878051280975, 'eval_runtime': 3.4437, 'eval_samples_per_second': 290.386, 'eval_steps_per_second': 18.294, 'epoch': 0.56}
{'loss': 0.1888, 'grad_norm': 0.22050222754478455, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.171871155500412, 'eval_runtime': 3.449, 'eval_samples_per_second': 289.937, 'eval_steps_per_second': 18.266, 'epoch': 0.6}
{'loss': 0.1689, 'grad_norm': 0.29946285486221313, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.16678336262702942, 'eval_runtime': 3.4472, 'eval_samples_per_second': 290.092, 'eval_steps_per_second': 18.276, 'epoch': 0.64}
{'loss': 0.1655, 'grad_norm': 0.19104301929473877, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.16008277237415314, 'eval_runtime': 3.454, 'eval_samples_per_second': 289.519, 'eval_steps_per_second': 18.24, 'epoch': 0.68}
{'loss': 0.1594, 'grad_norm': 0.1924924999475479, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.15467219054698944, 'eval_runtime': 3.4499, 'eval_samples_per_second': 289.866, 'eval_steps_per_second': 18.262, 'epoch': 0.72}
{'loss': 0.1553, 'grad_norm': 0.2574358582496643, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.15162138640880585, 'eval_runtime': 3.4529, 'eval_samples_per_second': 289.61, 'eval_steps_per_second': 18.245, 'epoch': 0.76}
{'loss': 0.1558, 'grad_norm': 0.26359036564826965, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.1477414220571518, 'eval_runtime': 3.4532, 'eval_samples_per_second': 289.59, 'eval_steps_per_second': 18.244, 'epoch': 0.8}
{'loss': 0.1495, 'grad_norm': 0.1424734890460968, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.1454516053199768, 'eval_runtime': 3.4532, 'eval_samples_per_second': 289.589, 'eval_steps_per_second': 18.244, 'epoch': 0.84}
{'loss': 0.146, 'grad_norm': 0.14778631925582886, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.14241167902946472, 'eval_runtime': 3.4432, 'eval_samples_per_second': 290.426, 'eval_steps_per_second': 18.297, 'epoch': 0.88}
{'loss': 0.1448, 'grad_norm': 0.439430832862854, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.1406560093164444, 'eval_runtime': 3.4496, 'eval_samples_per_second': 289.891, 'eval_steps_per_second': 18.263, 'epoch': 0.92}
{'loss': 0.144, 'grad_norm': 0.165735125541687, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.1390494406223297, 'eval_runtime': 3.452, 'eval_samples_per_second': 289.685, 'eval_steps_per_second': 18.25, 'epoch': 0.96}
{'loss': 0.1426, 'grad_norm': 0.11782708764076233, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.13870792090892792, 'eval_runtime': 3.4492, 'eval_samples_per_second': 289.922, 'eval_steps_per_second': 18.265, 'epoch': 1.0}
{'train_runtime': 258.5321, 'train_samples_per_second': 38.68, 'train_steps_per_second': 2.417, 'train_loss': 0.42315164909362796, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2089009284973145, 0.7733300924301147, 0.6710794568061829, 0.5926260948181152, 0.5128477215766907, 0.4445233643054962, 0.3713596761226654, 0.31101420521736145, 0.27804210782051086, 0.24686500430107117, 0.22462844848632812, 0.2090989649295807, 0.19535955786705017, 0.18059878051280975, 0.171871155500412, 0.16678336262702942, 0.16008277237415314, 0.15467219054698944, 0.15162138640880585, 0.1477414220571518, 0.1454516053199768, 0.14241167902946472, 0.1406560093164444, 0.1390494406223297, 0.13870792090892792], 'performance': [0.44, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:56,  1.74it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 19.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 27.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 31.10it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 32.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 32.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 37.91it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 32.14it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.44, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.475605845451355
current iteration best possible performance (full train run):  0.9345000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3636 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.3618497310656732
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 1.2619715611298084
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.3524366101514076
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 1.3351818711322585
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 1.1276448615041679
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4492, dtype=torch.float64), 0, 0, tensor(0.5508, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 1.5959455978986636e-17, 47.99999999999952, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.3235e-16, dtype=torch.float64), tensor(0.4492, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2962e-16, dtype=torch.float64), tensor(0.5508, dtype=torch.float64), tensor(8.1653e-17, dtype=torch.float64), tensor(1.1625e-17, dtype=torch.float64), tensor(4.1321e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.5959e-16, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.449
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.551
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.5959455978986636e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (47.99999999999952,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  1.5959455978986636e-17
lora alpha:  47.99999999999952
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.61it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 12.44it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 16.31it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:03, 19.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 20.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 21.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 23.68it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 14.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:02, 15.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 15.66it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:01, 17.91it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 16.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 17.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.47it/s]
Evaluation performance at step 25: 0.46
{'loss': 3.4643, 'grad_norm': 0.7067471742630005, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.46}
{'eval_loss': 2.0880441665649414, 'eval_runtime': 9.4388, 'eval_samples_per_second': 105.84, 'eval_steps_per_second': 6.675, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:59,  1.65it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  8.67it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:26,  3.12it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:15,  4.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:09<00:21,  3.17it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:13<00:22,  2.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:21,  2.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:17<00:13,  3.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:18<00:07,  4.50it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:22<00:08,  3.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:25<00:06,  2.78it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:26<00:02,  3.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:26<00:00,  4.94it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:26<00:00,  3.73it/s]
Evaluation performance at step 50: 0.42
{'loss': 1.7387, 'grad_norm': 0.3520379066467285, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.42}
{'eval_loss': 1.5902128219604492, 'eval_runtime': 9.449, 'eval_samples_per_second': 105.725, 'eval_steps_per_second': 6.667, 'epoch': 0.08}
{'loss': 1.5228, 'grad_norm': 0.23267175257205963, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5222290754318237, 'eval_runtime': 9.5364, 'eval_samples_per_second': 104.757, 'eval_steps_per_second': 6.606, 'epoch': 0.12}
{'loss': 1.4817, 'grad_norm': 0.2510133683681488, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4916245937347412, 'eval_runtime': 9.5535, 'eval_samples_per_second': 104.569, 'eval_steps_per_second': 6.594, 'epoch': 0.16}
{'loss': 1.4651, 'grad_norm': 0.21790345013141632, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.472352385520935, 'eval_runtime': 9.5811, 'eval_samples_per_second': 104.268, 'eval_steps_per_second': 6.575, 'epoch': 0.2}
{'loss': 1.4598, 'grad_norm': 0.22043046355247498, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4604315757751465, 'eval_runtime': 9.5772, 'eval_samples_per_second': 104.31, 'eval_steps_per_second': 6.578, 'epoch': 0.24}
{'loss': 1.4651, 'grad_norm': 0.21769636869430542, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4371908903121948, 'eval_runtime': 9.5939, 'eval_samples_per_second': 104.128, 'eval_steps_per_second': 6.567, 'epoch': 0.28}
{'loss': 1.4434, 'grad_norm': 0.2444109469652176, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4242419004440308, 'eval_runtime': 9.6158, 'eval_samples_per_second': 103.891, 'eval_steps_per_second': 6.552, 'epoch': 0.32}
{'loss': 1.386, 'grad_norm': 0.24503958225250244, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4100017547607422, 'eval_runtime': 9.6231, 'eval_samples_per_second': 103.812, 'eval_steps_per_second': 6.547, 'epoch': 0.36}
{'loss': 1.386, 'grad_norm': 0.23253175616264343, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3944404125213623, 'eval_runtime': 9.6171, 'eval_samples_per_second': 103.877, 'eval_steps_per_second': 6.551, 'epoch': 0.4}
{'loss': 1.4223, 'grad_norm': 0.2580716907978058, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3770827054977417, 'eval_runtime': 9.6529, 'eval_samples_per_second': 103.493, 'eval_steps_per_second': 6.527, 'epoch': 0.44}
{'loss': 1.3838, 'grad_norm': 0.24473005533218384, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3682843446731567, 'eval_runtime': 9.6481, 'eval_samples_per_second': 103.543, 'eval_steps_per_second': 6.53, 'epoch': 0.48}
{'loss': 1.4044, 'grad_norm': 0.3241185247898102, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3500038385391235, 'eval_runtime': 9.6599, 'eval_samples_per_second': 103.418, 'eval_steps_per_second': 6.522, 'epoch': 0.52}
{'loss': 1.3644, 'grad_norm': 0.34778672456741333, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3408808708190918, 'eval_runtime': 9.6502, 'eval_samples_per_second': 103.521, 'eval_steps_per_second': 6.528, 'epoch': 0.56}
{'loss': 1.3812, 'grad_norm': 0.26368793845176697, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.327880620956421, 'eval_runtime': 9.65, 'eval_samples_per_second': 103.524, 'eval_steps_per_second': 6.529, 'epoch': 0.6}
{'loss': 1.3972, 'grad_norm': 0.24493755400180817, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3229949474334717, 'eval_runtime': 9.6423, 'eval_samples_per_second': 103.606, 'eval_steps_per_second': 6.534, 'epoch': 0.64}
{'loss': 1.3432, 'grad_norm': 0.2715815305709839, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3109599351882935, 'eval_runtime': 9.6507, 'eval_samples_per_second': 103.515, 'eval_steps_per_second': 6.528, 'epoch': 0.68}
{'loss': 1.3566, 'grad_norm': 0.23978941142559052, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3033143281936646, 'eval_runtime': 9.6243, 'eval_samples_per_second': 103.799, 'eval_steps_per_second': 6.546, 'epoch': 0.72}
{'loss': 1.3231, 'grad_norm': 0.2750328779220581, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2963529825210571, 'eval_runtime': 9.5976, 'eval_samples_per_second': 104.089, 'eval_steps_per_second': 6.564, 'epoch': 0.76}
{'loss': 1.334, 'grad_norm': 0.30652204155921936, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2893146276474, 'eval_runtime': 9.6072, 'eval_samples_per_second': 103.985, 'eval_steps_per_second': 6.558, 'epoch': 0.8}
{'loss': 1.3434, 'grad_norm': 0.2494238168001175, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2828130722045898, 'eval_runtime': 9.6061, 'eval_samples_per_second': 103.997, 'eval_steps_per_second': 6.558, 'epoch': 0.84}
{'loss': 1.3112, 'grad_norm': 0.26588574051856995, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2786164283752441, 'eval_runtime': 9.6108, 'eval_samples_per_second': 103.945, 'eval_steps_per_second': 6.555, 'epoch': 0.88}
{'loss': 1.2811, 'grad_norm': 0.25331273674964905, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.27315092086792, 'eval_runtime': 9.6012, 'eval_samples_per_second': 104.05, 'eval_steps_per_second': 6.562, 'epoch': 0.92}
{'loss': 1.3421, 'grad_norm': 0.21475693583488464, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2710447311401367, 'eval_runtime': 9.6146, 'eval_samples_per_second': 103.904, 'eval_steps_per_second': 6.553, 'epoch': 0.96}
{'loss': 1.3238, 'grad_norm': 0.28405946493148804, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2698677778244019, 'eval_runtime': 9.6085, 'eval_samples_per_second': 103.97, 'eval_steps_per_second': 6.557, 'epoch': 1.0}
{'train_runtime': 577.3901, 'train_samples_per_second': 17.318, 'train_steps_per_second': 1.082, 'train_loss': 1.4849817199707032, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0880441665649414, 1.5902128219604492, 1.5222290754318237, 1.4916245937347412, 1.472352385520935, 1.4604315757751465, 1.4371908903121948, 1.4242419004440308, 1.4100017547607422, 1.3944404125213623, 1.3770827054977417, 1.3682843446731567, 1.3500038385391235, 1.3408808708190918, 1.327880620956421, 1.3229949474334717, 1.3109599351882935, 1.3033143281936646, 1.2963529825210571, 1.2893146276474, 1.2828130722045898, 1.2786164283752441, 1.27315092086792, 1.2710447311401367, 1.2698677778244019], 'performance': [0.46, 0.42]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<08:01,  4.86s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:20,  3.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:10<00:18,  3.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:11<00:09,  5.37it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:16<00:08,  4.35it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:17<00:03,  6.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:17<00:00,  8.34it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:17<00:00,  5.65it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.46, 0.42]
current iteration observed (possibly low-fid or predicted) performance:  1.4680230617523193
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9114 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 1.0106023753963655
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.172494816698612
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 1.0086309876776622
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.1318598411053635
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.2094157598949393
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.4490, dtype=torch.float64), 0, tensor(0.5510, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 0, 128, 3.1225022567582517e-18, 47.99999999999997, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.2707e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.1127e-19, dtype=torch.float64), tensor(0.4490, dtype=torch.float64), tensor(1.8246e-16, dtype=torch.float64), tensor(0.5510, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.1225e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.449
  wikitext: 0
  mmlu: 0.551
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.1225022567582517e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (47.99999999999997,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  3.1225022567582517e-18
lora alpha:  47.99999999999997
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:42,  2.33it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.83it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:21,  3.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:09,  6.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:11<00:15,  3.25it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:16<00:16,  2.54it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:21<00:15,  2.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:21<00:09,  2.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:22<00:05,  3.75it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:23<00:02,  4.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:23<00:00,  5.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:23<00:00,  4.19it/s]
Evaluation performance at step 25: 0.52
{'loss': 2.7789, 'grad_norm': 0.7910539507865906, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.52}
{'eval_loss': 1.409147024154663, 'eval_runtime': 8.7831, 'eval_samples_per_second': 113.741, 'eval_steps_per_second': 7.173, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:08,  1.44it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:08,  9.91it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.37it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.03it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:05, 11.26it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.74it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:05,  8.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:03, 10.33it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:02, 11.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 11.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 13.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 14.49it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.93it/s]
Evaluation performance at step 50: 0.72
{'loss': 1.263, 'grad_norm': 0.7547098994255066, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.72}
{'eval_loss': 1.1614116430282593, 'eval_runtime': 8.8237, 'eval_samples_per_second': 113.218, 'eval_steps_per_second': 7.14, 'epoch': 0.08}
{'loss': 1.1608, 'grad_norm': 0.3003288805484772, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.104597568511963, 'eval_runtime': 8.8466, 'eval_samples_per_second': 112.925, 'eval_steps_per_second': 7.121, 'epoch': 0.12}
{'loss': 1.1363, 'grad_norm': 0.26158109307289124, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.079071044921875, 'eval_runtime': 8.8242, 'eval_samples_per_second': 113.211, 'eval_steps_per_second': 7.139, 'epoch': 0.16}
{'loss': 1.126, 'grad_norm': 0.28773730993270874, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.059891700744629, 'eval_runtime': 8.8346, 'eval_samples_per_second': 113.078, 'eval_steps_per_second': 7.131, 'epoch': 0.2}
{'loss': 1.0444, 'grad_norm': 0.30376964807510376, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0387135744094849, 'eval_runtime': 8.8592, 'eval_samples_per_second': 112.764, 'eval_steps_per_second': 7.111, 'epoch': 0.24}
{'loss': 1.099, 'grad_norm': 0.22934216260910034, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0239765644073486, 'eval_runtime': 8.9049, 'eval_samples_per_second': 112.185, 'eval_steps_per_second': 7.075, 'epoch': 0.28}
{'loss': 1.0375, 'grad_norm': 0.2647676467895508, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0058001279830933, 'eval_runtime': 8.9155, 'eval_samples_per_second': 112.052, 'eval_steps_per_second': 7.066, 'epoch': 0.32}
{'loss': 1.0071, 'grad_norm': 0.35886383056640625, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9922666549682617, 'eval_runtime': 8.9077, 'eval_samples_per_second': 112.151, 'eval_steps_per_second': 7.073, 'epoch': 0.36}
{'loss': 0.9873, 'grad_norm': 0.2726440727710724, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9680518507957458, 'eval_runtime': 8.9157, 'eval_samples_per_second': 112.049, 'eval_steps_per_second': 7.066, 'epoch': 0.4}
{'loss': 1.02, 'grad_norm': 0.3071146309375763, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9512442350387573, 'eval_runtime': 8.9236, 'eval_samples_per_second': 111.95, 'eval_steps_per_second': 7.06, 'epoch': 0.44}
{'loss': 1.0007, 'grad_norm': 0.27777695655822754, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9329354763031006, 'eval_runtime': 8.9311, 'eval_samples_per_second': 111.856, 'eval_steps_per_second': 7.054, 'epoch': 0.48}
{'loss': 0.9391, 'grad_norm': 0.2940233051776886, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9212576746940613, 'eval_runtime': 8.9442, 'eval_samples_per_second': 111.692, 'eval_steps_per_second': 7.044, 'epoch': 0.52}
{'loss': 1.0146, 'grad_norm': 0.3173816204071045, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9033681750297546, 'eval_runtime': 8.9581, 'eval_samples_per_second': 111.519, 'eval_steps_per_second': 7.033, 'epoch': 0.56}
{'loss': 0.9541, 'grad_norm': 0.3195123076438904, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8899955749511719, 'eval_runtime': 8.9323, 'eval_samples_per_second': 111.841, 'eval_steps_per_second': 7.053, 'epoch': 0.6}
{'loss': 0.98, 'grad_norm': 0.3172057867050171, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8760983943939209, 'eval_runtime': 8.9038, 'eval_samples_per_second': 112.2, 'eval_steps_per_second': 7.076, 'epoch': 0.64}
{'loss': 1.0173, 'grad_norm': 0.4205934405326843, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8637163043022156, 'eval_runtime': 8.9087, 'eval_samples_per_second': 112.138, 'eval_steps_per_second': 7.072, 'epoch': 0.68}
{'loss': 0.9834, 'grad_norm': 0.3479491174221039, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.852304220199585, 'eval_runtime': 8.9051, 'eval_samples_per_second': 112.183, 'eval_steps_per_second': 7.075, 'epoch': 0.72}
{'loss': 0.9133, 'grad_norm': 0.3694467544555664, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8413258194923401, 'eval_runtime': 8.9159, 'eval_samples_per_second': 112.047, 'eval_steps_per_second': 7.066, 'epoch': 0.76}
{'loss': 0.9472, 'grad_norm': 0.25226670503616333, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8333253264427185, 'eval_runtime': 8.9131, 'eval_samples_per_second': 112.082, 'eval_steps_per_second': 7.068, 'epoch': 0.8}
{'loss': 0.8881, 'grad_norm': 0.3064856231212616, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8236701488494873, 'eval_runtime': 8.9044, 'eval_samples_per_second': 112.191, 'eval_steps_per_second': 7.075, 'epoch': 0.84}
{'loss': 0.9426, 'grad_norm': 0.35150346159935, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8153756856918335, 'eval_runtime': 8.9093, 'eval_samples_per_second': 112.13, 'eval_steps_per_second': 7.071, 'epoch': 0.88}
{'loss': 0.91, 'grad_norm': 0.2903066873550415, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.807631254196167, 'eval_runtime': 8.9042, 'eval_samples_per_second': 112.194, 'eval_steps_per_second': 7.075, 'epoch': 0.92}
{'loss': 0.877, 'grad_norm': 0.3167373538017273, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8042972087860107, 'eval_runtime': 8.9159, 'eval_samples_per_second': 112.047, 'eval_steps_per_second': 7.066, 'epoch': 0.96}
{'loss': 0.8587, 'grad_norm': 0.33670735359191895, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8028530478477478, 'eval_runtime': 8.9213, 'eval_samples_per_second': 111.979, 'eval_steps_per_second': 7.062, 'epoch': 1.0}
{'train_runtime': 549.9253, 'train_samples_per_second': 18.182, 'train_steps_per_second': 1.137, 'train_loss': 1.0754448089599609, 'epoch': 1.0}
train_results:  {'eval_loss': [1.409147024154663, 1.1614116430282593, 1.104597568511963, 1.079071044921875, 1.059891700744629, 1.0387135744094849, 1.0239765644073486, 1.0058001279830933, 0.9922666549682617, 0.9680518507957458, 0.9512442350387573, 0.9329354763031006, 0.9212576746940613, 0.9033681750297546, 0.8899955749511719, 0.8760983943939209, 0.8637163043022156, 0.852304220199585, 0.8413258194923401, 0.8333253264427185, 0.8236701488494873, 0.8153756856918335, 0.807631254196167, 0.8042972087860107, 0.8028530478477478], 'performance': [0.52, 0.72]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:51,  1.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:04, 20.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 24.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:01, 26.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 24.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 27.68it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 31.83it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 27.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.52, 0.72]
current iteration observed (possibly low-fid or predicted) performance:  1.4795663356781006
current iteration best possible performance (full train run):  0.7454999999999999
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8771 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.2943746186472511
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 1.2574908204962123
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.225100397589395
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.8770094357714332
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 1.1609160289500886
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.1164, dtype=torch.float64), tensor(0.8836, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734919, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.3696e-17, dtype=torch.float64), tensor(5.4878e-17, dtype=torch.float64), tensor(0.1164, dtype=torch.float64), tensor(0.8836, dtype=torch.float64), tensor(4.6813e-17, dtype=torch.float64), tensor(5.2715e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.116
  wikitext: 0.884
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734919,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734919
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:43,  1.65s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.44it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.98it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.26it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 11.82it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.00it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.54it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 10.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.10it/s]
Evaluation performance at step 25: 0.55
{'loss': 3.4761, 'grad_norm': 0.1613401472568512, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.2600808143615723, 'eval_runtime': 8.9605, 'eval_samples_per_second': 111.489, 'eval_steps_per_second': 7.031, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.68it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.67it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:22,  3.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.90it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  6.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.56it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:04,  9.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 11.58it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 12.85it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 13.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 14.64it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 14.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.52it/s]
Evaluation performance at step 50: 0.59
{'loss': 2.8232, 'grad_norm': 0.10902782529592514, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 2.3955280780792236, 'eval_runtime': 8.9079, 'eval_samples_per_second': 112.148, 'eval_steps_per_second': 7.072, 'epoch': 0.08}
{'loss': 2.278, 'grad_norm': 0.12016498297452927, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1613142490386963, 'eval_runtime': 8.9609, 'eval_samples_per_second': 111.484, 'eval_steps_per_second': 7.031, 'epoch': 0.12}
{'loss': 2.0805, 'grad_norm': 0.12217787653207779, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1082565784454346, 'eval_runtime': 9.0332, 'eval_samples_per_second': 110.592, 'eval_steps_per_second': 6.974, 'epoch': 0.16}
{'loss': 2.0977, 'grad_norm': 0.25484687089920044, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0809695720672607, 'eval_runtime': 9.0673, 'eval_samples_per_second': 110.176, 'eval_steps_per_second': 6.948, 'epoch': 0.2}
{'loss': 1.9956, 'grad_norm': 0.133850559592247, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0575978755950928, 'eval_runtime': 9.0705, 'eval_samples_per_second': 110.137, 'eval_steps_per_second': 6.946, 'epoch': 0.24}
{'loss': 2.0973, 'grad_norm': 0.08111274242401123, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.0421743392944336, 'eval_runtime': 9.0623, 'eval_samples_per_second': 110.237, 'eval_steps_per_second': 6.952, 'epoch': 0.28}
{'loss': 2.077, 'grad_norm': 0.08266691863536835, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.030071496963501, 'eval_runtime': 8.9995, 'eval_samples_per_second': 111.007, 'eval_steps_per_second': 7.0, 'epoch': 0.32}
{'loss': 2.0153, 'grad_norm': 0.1654321700334549, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.0289244651794434, 'eval_runtime': 8.9815, 'eval_samples_per_second': 111.229, 'eval_steps_per_second': 7.014, 'epoch': 0.36}
{'loss': 2.0303, 'grad_norm': 0.07379242777824402, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0167338848114014, 'eval_runtime': 8.9922, 'eval_samples_per_second': 111.097, 'eval_steps_per_second': 7.006, 'epoch': 0.4}
{'loss': 2.0087, 'grad_norm': 0.15916980803012848, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.009398937225342, 'eval_runtime': 8.9794, 'eval_samples_per_second': 111.255, 'eval_steps_per_second': 7.016, 'epoch': 0.44}
{'loss': 1.9691, 'grad_norm': 0.1480470895767212, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0078535079956055, 'eval_runtime': 8.9706, 'eval_samples_per_second': 111.364, 'eval_steps_per_second': 7.023, 'epoch': 0.48}
{'loss': 2.0315, 'grad_norm': 0.192505344748497, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.001939058303833, 'eval_runtime': 8.9819, 'eval_samples_per_second': 111.224, 'eval_steps_per_second': 7.014, 'epoch': 0.52}
{'loss': 2.0131, 'grad_norm': 0.08296816051006317, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9969693422317505, 'eval_runtime': 8.9869, 'eval_samples_per_second': 111.162, 'eval_steps_per_second': 7.01, 'epoch': 0.56}
{'loss': 2.0059, 'grad_norm': 0.5327293276786804, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9932191371917725, 'eval_runtime': 8.9801, 'eval_samples_per_second': 111.246, 'eval_steps_per_second': 7.016, 'epoch': 0.6}
{'loss': 1.9702, 'grad_norm': 0.12879957258701324, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.9890164136886597, 'eval_runtime': 8.9791, 'eval_samples_per_second': 111.259, 'eval_steps_per_second': 7.016, 'epoch': 0.64}
{'loss': 2.0117, 'grad_norm': 0.11927232891321182, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9868614673614502, 'eval_runtime': 8.9825, 'eval_samples_per_second': 111.216, 'eval_steps_per_second': 7.014, 'epoch': 0.68}
{'loss': 1.9971, 'grad_norm': 0.12197957932949066, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9837803840637207, 'eval_runtime': 8.9914, 'eval_samples_per_second': 111.106, 'eval_steps_per_second': 7.007, 'epoch': 0.72}
{'loss': 1.9746, 'grad_norm': 0.12979774177074432, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9811698198318481, 'eval_runtime': 9.0147, 'eval_samples_per_second': 110.819, 'eval_steps_per_second': 6.989, 'epoch': 0.76}
{'loss': 2.0043, 'grad_norm': 0.14848913252353668, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.9795200824737549, 'eval_runtime': 9.0048, 'eval_samples_per_second': 110.941, 'eval_steps_per_second': 6.996, 'epoch': 0.8}
{'loss': 1.9131, 'grad_norm': 0.10012280195951462, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.978057622909546, 'eval_runtime': 9.0082, 'eval_samples_per_second': 110.898, 'eval_steps_per_second': 6.994, 'epoch': 0.84}
{'loss': 1.9095, 'grad_norm': 0.08867546916007996, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9768455028533936, 'eval_runtime': 9.0034, 'eval_samples_per_second': 110.957, 'eval_steps_per_second': 6.997, 'epoch': 0.88}
{'loss': 1.8755, 'grad_norm': 0.1611221581697464, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9763091802597046, 'eval_runtime': 9.0088, 'eval_samples_per_second': 110.891, 'eval_steps_per_second': 6.993, 'epoch': 0.92}
{'loss': 1.9853, 'grad_norm': 0.14265136420726776, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9749839305877686, 'eval_runtime': 8.9879, 'eval_samples_per_second': 111.15, 'eval_steps_per_second': 7.009, 'epoch': 0.96}
{'loss': 1.9576, 'grad_norm': 0.143804132938385, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9746198654174805, 'eval_runtime': 8.9883, 'eval_samples_per_second': 111.144, 'eval_steps_per_second': 7.009, 'epoch': 1.0}
{'train_runtime': 530.9607, 'train_samples_per_second': 18.832, 'train_steps_per_second': 1.177, 'train_loss': 2.103930285644531, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2600808143615723, 2.3955280780792236, 2.1613142490386963, 2.1082565784454346, 2.0809695720672607, 2.0575978755950928, 2.0421743392944336, 2.030071496963501, 2.0289244651794434, 2.0167338848114014, 2.009398937225342, 2.0078535079956055, 2.001939058303833, 1.9969693422317505, 1.9932191371917725, 1.9890164136886597, 1.9868614673614502, 1.9837803840637207, 1.9811698198318481, 1.9795200824737549, 1.978057622909546, 1.9768455028533936, 1.9763091802597046, 1.9749839305877686, 1.9746198654174805], 'performance': [0.55, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:54,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 19.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:11<00:13,  3.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:11<00:05,  5.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:12<00:02,  8.48it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00, 11.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  7.93it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.4879405498504639
current iteration best possible performance (full train run):  0.672
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9098 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.3095479188481525
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.9883020135956444
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 1.2981934924701455
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.7604262711062415
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.1143924431013694
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(1.0000, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.3484e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.6498e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 1.0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:42,  1.64s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.51it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.86it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.84it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.24it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.17it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.43it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.78it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01, 10.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.24it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.283, 'grad_norm': 0.5986527800559998, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.734898805618286, 'eval_runtime': 4.5522, 'eval_samples_per_second': 219.456, 'eval_steps_per_second': 13.84, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:21,  1.43s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:16,  5.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.66it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.94it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.90it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 13.08it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 13.40it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.72it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 14.57it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:03,  4.77it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:02,  5.25it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  6.77it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.40it/s]
Evaluation performance at step 50: 0.44
{'loss': 2.2877, 'grad_norm': 0.4732852876186371, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.44}
{'eval_loss': 1.2543487548828125, 'eval_runtime': 3.163, 'eval_samples_per_second': 315.839, 'eval_steps_per_second': 19.918, 'epoch': 0.08}
{'loss': 1.0466, 'grad_norm': 0.14461249113082886, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9255308508872986, 'eval_runtime': 3.1766, 'eval_samples_per_second': 314.488, 'eval_steps_per_second': 19.833, 'epoch': 0.12}
{'loss': 0.8614, 'grad_norm': 0.12913760542869568, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7639915347099304, 'eval_runtime': 3.1756, 'eval_samples_per_second': 314.582, 'eval_steps_per_second': 19.839, 'epoch': 0.16}
{'loss': 0.7457, 'grad_norm': 0.0730552077293396, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7428885102272034, 'eval_runtime': 3.1953, 'eval_samples_per_second': 312.643, 'eval_steps_per_second': 19.716, 'epoch': 0.2}
{'loss': 0.7617, 'grad_norm': 0.057105034589767456, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.734209418296814, 'eval_runtime': 3.1777, 'eval_samples_per_second': 314.378, 'eval_steps_per_second': 19.826, 'epoch': 0.24}
{'loss': 0.7534, 'grad_norm': 0.06027347967028618, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7291221618652344, 'eval_runtime': 3.1746, 'eval_samples_per_second': 314.684, 'eval_steps_per_second': 19.845, 'epoch': 0.28}
{'loss': 0.7312, 'grad_norm': 0.04931241273880005, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7266557812690735, 'eval_runtime': 3.1846, 'eval_samples_per_second': 313.695, 'eval_steps_per_second': 19.783, 'epoch': 0.32}
{'loss': 0.7287, 'grad_norm': 0.06581084430217743, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7242581844329834, 'eval_runtime': 3.1948, 'eval_samples_per_second': 312.696, 'eval_steps_per_second': 19.72, 'epoch': 0.36}
{'loss': 0.7316, 'grad_norm': 0.056858208030462265, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7201467752456665, 'eval_runtime': 3.1706, 'eval_samples_per_second': 315.082, 'eval_steps_per_second': 19.87, 'epoch': 0.4}
{'loss': 0.7101, 'grad_norm': 0.05919709801673889, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7179485559463501, 'eval_runtime': 3.1748, 'eval_samples_per_second': 314.667, 'eval_steps_per_second': 19.844, 'epoch': 0.44}
{'loss': 0.7223, 'grad_norm': 0.06899888068437576, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7159957885742188, 'eval_runtime': 3.2065, 'eval_samples_per_second': 311.555, 'eval_steps_per_second': 19.648, 'epoch': 0.48}
{'loss': 0.7195, 'grad_norm': 0.05550166592001915, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.715393602848053, 'eval_runtime': 3.1952, 'eval_samples_per_second': 312.654, 'eval_steps_per_second': 19.717, 'epoch': 0.52}
{'loss': 0.7218, 'grad_norm': 0.0661713182926178, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7117394208908081, 'eval_runtime': 3.1941, 'eval_samples_per_second': 312.762, 'eval_steps_per_second': 19.724, 'epoch': 0.56}
{'loss': 0.7167, 'grad_norm': 0.05967605486512184, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7105282545089722, 'eval_runtime': 3.1911, 'eval_samples_per_second': 313.062, 'eval_steps_per_second': 19.743, 'epoch': 0.6}
{'loss': 0.7183, 'grad_norm': 0.05820941925048828, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7081055641174316, 'eval_runtime': 3.2067, 'eval_samples_per_second': 311.536, 'eval_steps_per_second': 19.646, 'epoch': 0.64}
{'loss': 0.7234, 'grad_norm': 0.06129172816872597, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7064871191978455, 'eval_runtime': 3.1978, 'eval_samples_per_second': 312.398, 'eval_steps_per_second': 19.701, 'epoch': 0.68}
{'loss': 0.7166, 'grad_norm': 0.06454622745513916, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7052413821220398, 'eval_runtime': 3.213, 'eval_samples_per_second': 310.928, 'eval_steps_per_second': 19.608, 'epoch': 0.72}
{'loss': 0.7286, 'grad_norm': 0.05646086484193802, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7041817307472229, 'eval_runtime': 3.2049, 'eval_samples_per_second': 311.712, 'eval_steps_per_second': 19.658, 'epoch': 0.76}
{'loss': 0.7259, 'grad_norm': 0.0642009824514389, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7030172348022461, 'eval_runtime': 3.1922, 'eval_samples_per_second': 312.948, 'eval_steps_per_second': 19.735, 'epoch': 0.8}
{'loss': 0.7041, 'grad_norm': 0.07417301833629608, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7022165060043335, 'eval_runtime': 3.2004, 'eval_samples_per_second': 312.148, 'eval_steps_per_second': 19.685, 'epoch': 0.84}
{'loss': 0.7215, 'grad_norm': 0.05279083922505379, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7017292380332947, 'eval_runtime': 3.199, 'eval_samples_per_second': 312.288, 'eval_steps_per_second': 19.694, 'epoch': 0.88}
{'loss': 0.6967, 'grad_norm': 0.05411297827959061, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7008090019226074, 'eval_runtime': 3.2171, 'eval_samples_per_second': 310.527, 'eval_steps_per_second': 19.583, 'epoch': 0.92}
{'loss': 0.6982, 'grad_norm': 0.06839840114116669, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7003089189529419, 'eval_runtime': 3.1974, 'eval_samples_per_second': 312.441, 'eval_steps_per_second': 19.703, 'epoch': 0.96}
{'loss': 0.6935, 'grad_norm': 0.06584572792053223, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.700193464756012, 'eval_runtime': 3.2183, 'eval_samples_per_second': 310.412, 'eval_steps_per_second': 19.576, 'epoch': 1.0}
{'train_runtime': 270.4106, 'train_samples_per_second': 36.977, 'train_steps_per_second': 2.311, 'train_loss': 0.9859344024658203, 'epoch': 1.0}
train_results:  {'eval_loss': [3.734898805618286, 1.2543487548828125, 0.9255308508872986, 0.7639915347099304, 0.7428885102272034, 0.734209418296814, 0.7291221618652344, 0.7266557812690735, 0.7242581844329834, 0.7201467752456665, 0.7179485559463501, 0.7159957885742188, 0.715393602848053, 0.7117394208908081, 0.7105282545089722, 0.7081055641174316, 0.7064871191978455, 0.7052413821220398, 0.7041817307472229, 0.7030172348022461, 0.7022165060043335, 0.7017292380332947, 0.7008090019226074, 0.7003089189529419, 0.700193464756012], 'performance': [0.56, 0.44]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:13,  1.35it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.62it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 14.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 19.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 23.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.67it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.44]
current iteration observed (possibly low-fid or predicted) performance:  1.4887475967407227
current iteration best possible performance (full train run):  0.4515
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0650 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.2031568453720065
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.8173159549760931
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.0359400194674495
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.9053154759719795
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.0491661871473728
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8654, dtype=torch.float64), 0, 0, 0, tensor(0.1346, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734912, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.8654, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9340e-17, dtype=torch.float64), tensor(2.0242e-17, dtype=torch.float64), tensor(0.1346, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.0895e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.865
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.135
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734912,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734912
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:39,  1.61s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:19,  4.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  8.20it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.94it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.96it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.26it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:01, 13.53it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.82it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01, 10.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.36it/s]
Evaluation performance at step 25: 0.54
{'loss': 2.6432, 'grad_norm': 0.23355421423912048, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 2.0765879154205322, 'eval_runtime': 9.6219, 'eval_samples_per_second': 103.826, 'eval_steps_per_second': 6.548, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:54,  1.16s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08, 10.26it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.50it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 12.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.44it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.01it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 13.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 14.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 10.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01, 10.22it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 12.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.23it/s]
Evaluation performance at step 50: 0.55
{'loss': 1.5735, 'grad_norm': 0.17526084184646606, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 1.2026029825210571, 'eval_runtime': 9.6223, 'eval_samples_per_second': 103.821, 'eval_steps_per_second': 6.547, 'epoch': 0.08}
{'loss': 1.0602, 'grad_norm': 0.051998917013406754, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9552661180496216, 'eval_runtime': 9.7355, 'eval_samples_per_second': 102.615, 'eval_steps_per_second': 6.471, 'epoch': 0.12}
{'loss': 0.9322, 'grad_norm': 0.06317365914583206, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8929539918899536, 'eval_runtime': 9.7447, 'eval_samples_per_second': 102.517, 'eval_steps_per_second': 6.465, 'epoch': 0.16}
{'loss': 0.8951, 'grad_norm': 0.04838395118713379, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8693543076515198, 'eval_runtime': 9.7661, 'eval_samples_per_second': 102.292, 'eval_steps_per_second': 6.451, 'epoch': 0.2}
{'loss': 0.8783, 'grad_norm': 0.039984121918678284, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8604356050491333, 'eval_runtime': 9.807, 'eval_samples_per_second': 101.866, 'eval_steps_per_second': 6.424, 'epoch': 0.24}
{'loss': 0.8722, 'grad_norm': 0.042218226939439774, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8545968532562256, 'eval_runtime': 9.797, 'eval_samples_per_second': 101.97, 'eval_steps_per_second': 6.431, 'epoch': 0.28}
{'loss': 0.859, 'grad_norm': 0.0440804585814476, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8498877286911011, 'eval_runtime': 9.8039, 'eval_samples_per_second': 101.898, 'eval_steps_per_second': 6.426, 'epoch': 0.32}
{'loss': 0.8379, 'grad_norm': 0.041035085916519165, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8453335165977478, 'eval_runtime': 9.8135, 'eval_samples_per_second': 101.798, 'eval_steps_per_second': 6.42, 'epoch': 0.36}
{'loss': 0.8464, 'grad_norm': 0.04689958691596985, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8408348560333252, 'eval_runtime': 9.8056, 'eval_samples_per_second': 101.881, 'eval_steps_per_second': 6.425, 'epoch': 0.4}
{'loss': 0.853, 'grad_norm': 0.043846916407346725, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8366020917892456, 'eval_runtime': 9.8329, 'eval_samples_per_second': 101.598, 'eval_steps_per_second': 6.407, 'epoch': 0.44}
{'loss': 0.8692, 'grad_norm': 0.04245052486658096, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8338824510574341, 'eval_runtime': 9.8348, 'eval_samples_per_second': 101.578, 'eval_steps_per_second': 6.406, 'epoch': 0.48}
{'loss': 0.8442, 'grad_norm': 0.044642042368650436, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8311396837234497, 'eval_runtime': 9.8486, 'eval_samples_per_second': 101.436, 'eval_steps_per_second': 6.397, 'epoch': 0.52}
{'loss': 0.8266, 'grad_norm': 0.03923363983631134, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8290852904319763, 'eval_runtime': 9.8496, 'eval_samples_per_second': 101.426, 'eval_steps_per_second': 6.396, 'epoch': 0.56}
{'loss': 0.83, 'grad_norm': 0.047837089747190475, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.826694130897522, 'eval_runtime': 9.8355, 'eval_samples_per_second': 101.571, 'eval_steps_per_second': 6.405, 'epoch': 0.6}
{'loss': 0.8276, 'grad_norm': 0.048659782856702805, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8251245617866516, 'eval_runtime': 9.8353, 'eval_samples_per_second': 101.573, 'eval_steps_per_second': 6.406, 'epoch': 0.64}
{'loss': 0.8131, 'grad_norm': 0.04548730328679085, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.823624312877655, 'eval_runtime': 9.8376, 'eval_samples_per_second': 101.549, 'eval_steps_per_second': 6.404, 'epoch': 0.68}
{'loss': 0.8191, 'grad_norm': 0.05265280231833458, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8215265870094299, 'eval_runtime': 9.8766, 'eval_samples_per_second': 101.149, 'eval_steps_per_second': 6.379, 'epoch': 0.72}
{'loss': 0.8388, 'grad_norm': 0.053405992686748505, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8197560906410217, 'eval_runtime': 9.8424, 'eval_samples_per_second': 101.499, 'eval_steps_per_second': 6.401, 'epoch': 0.76}
{'loss': 0.828, 'grad_norm': 0.052167896181344986, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8180360794067383, 'eval_runtime': 9.8572, 'eval_samples_per_second': 101.348, 'eval_steps_per_second': 6.391, 'epoch': 0.8}
{'loss': 0.8366, 'grad_norm': 0.04979688301682472, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8169891834259033, 'eval_runtime': 9.8443, 'eval_samples_per_second': 101.48, 'eval_steps_per_second': 6.4, 'epoch': 0.84}
{'loss': 0.8286, 'grad_norm': 0.04756012558937073, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8161940574645996, 'eval_runtime': 9.7857, 'eval_samples_per_second': 102.088, 'eval_steps_per_second': 6.438, 'epoch': 0.88}
{'loss': 0.829, 'grad_norm': 0.05069318041205406, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8156108856201172, 'eval_runtime': 9.7822, 'eval_samples_per_second': 102.124, 'eval_steps_per_second': 6.44, 'epoch': 0.92}
{'loss': 0.8235, 'grad_norm': 0.04767732694745064, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8151150941848755, 'eval_runtime': 9.7889, 'eval_samples_per_second': 102.054, 'eval_steps_per_second': 6.436, 'epoch': 0.96}
{'loss': 0.8194, 'grad_norm': 0.052906334400177, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.815009593963623, 'eval_runtime': 9.7895, 'eval_samples_per_second': 102.048, 'eval_steps_per_second': 6.435, 'epoch': 1.0}
{'train_runtime': 572.9959, 'train_samples_per_second': 17.45, 'train_steps_per_second': 1.091, 'train_loss': 0.9553866668701172, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0765879154205322, 1.2026029825210571, 0.9552661180496216, 0.8929539918899536, 0.8693543076515198, 0.8604356050491333, 0.8545968532562256, 0.8498877286911011, 0.8453335165977478, 0.8408348560333252, 0.8366020917892456, 0.8338824510574341, 0.8311396837234497, 0.8290852904319763, 0.826694130897522, 0.8251245617866516, 0.823624312877655, 0.8215265870094299, 0.8197560906410217, 0.8180360794067383, 0.8169891834259033, 0.8161940574645996, 0.8156108856201172, 0.8151150941848755, 0.815009593963623], 'performance': [0.54, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:05,  1.51it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:27,  3.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:11,  5.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  6.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.82it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 12.30it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 16.55it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.03it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.4820997714996338
current iteration best possible performance (full train run):  0.5355000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9391 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.1996042706953052
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 1.087313369964603
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.1993816473333563
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 1.1079567136892647
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.6735647184925859
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.7081, dtype=torch.float64), 0, tensor(0.2919, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(7.3570e-17, dtype=torch.float64), tensor(1.0738e-17, dtype=torch.float64), tensor(3.6633e-17, dtype=torch.float64), tensor(7.2937e-17, dtype=torch.float64), tensor(1.6826e-16, dtype=torch.float64), tensor(0.7081, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2919, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.708
  wikitext: 0
  mmlu: 0.292
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:01,  1.83s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:21,  4.33it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.36it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:09,  7.95it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.36it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.09it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.56it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.60it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.25it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01,  9.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.60it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.53it/s]
Evaluation performance at step 25: 0.53
{'loss': 4.2892, 'grad_norm': 0.3994573652744293, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.53}
{'eval_loss': 3.0813493728637695, 'eval_runtime': 8.1774, 'eval_samples_per_second': 122.165, 'eval_steps_per_second': 7.704, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:48,  2.03it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.58it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.18it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.17it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:14,  4.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.08it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  6.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.07it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03, 10.47it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 11.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 10.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  9.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00, 11.60it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.27it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.0922, 'grad_norm': 0.3508926033973694, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.3909811973571777, 'eval_runtime': 8.1925, 'eval_samples_per_second': 121.941, 'eval_steps_per_second': 7.69, 'epoch': 0.08}
{'loss': 1.299, 'grad_norm': 0.08562129735946655, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1494990587234497, 'eval_runtime': 8.227, 'eval_samples_per_second': 121.429, 'eval_steps_per_second': 7.658, 'epoch': 0.12}
{'loss': 1.1734, 'grad_norm': 0.08407068997621536, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0230246782302856, 'eval_runtime': 8.2789, 'eval_samples_per_second': 120.668, 'eval_steps_per_second': 7.61, 'epoch': 0.16}
{'loss': 1.046, 'grad_norm': 0.04600781947374344, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9977894425392151, 'eval_runtime': 8.3137, 'eval_samples_per_second': 120.164, 'eval_steps_per_second': 7.578, 'epoch': 0.2}
{'loss': 1.0362, 'grad_norm': 0.059647493064403534, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9746263027191162, 'eval_runtime': 8.2946, 'eval_samples_per_second': 120.439, 'eval_steps_per_second': 7.595, 'epoch': 0.24}
{'loss': 0.9995, 'grad_norm': 0.06389456987380981, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9560055732727051, 'eval_runtime': 8.3047, 'eval_samples_per_second': 120.293, 'eval_steps_per_second': 7.586, 'epoch': 0.28}
{'loss': 0.981, 'grad_norm': 0.05087195709347725, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9353297352790833, 'eval_runtime': 8.3502, 'eval_samples_per_second': 119.638, 'eval_steps_per_second': 7.545, 'epoch': 0.32}
{'loss': 0.95, 'grad_norm': 0.06250646710395813, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9175781011581421, 'eval_runtime': 8.333, 'eval_samples_per_second': 119.884, 'eval_steps_per_second': 7.56, 'epoch': 0.36}
{'loss': 0.9082, 'grad_norm': 0.055911313742399216, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9002274870872498, 'eval_runtime': 8.3497, 'eval_samples_per_second': 119.646, 'eval_steps_per_second': 7.545, 'epoch': 0.4}
{'loss': 0.8802, 'grad_norm': 0.07145791500806808, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8840206265449524, 'eval_runtime': 8.3314, 'eval_samples_per_second': 119.908, 'eval_steps_per_second': 7.562, 'epoch': 0.44}
{'loss': 0.8922, 'grad_norm': 0.08106904476881027, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8640039563179016, 'eval_runtime': 8.3224, 'eval_samples_per_second': 120.038, 'eval_steps_per_second': 7.57, 'epoch': 0.48}
{'loss': 0.8887, 'grad_norm': 0.06643813103437424, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8442619442939758, 'eval_runtime': 8.2997, 'eval_samples_per_second': 120.366, 'eval_steps_per_second': 7.591, 'epoch': 0.52}
{'loss': 0.8367, 'grad_norm': 0.08550365269184113, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8246890902519226, 'eval_runtime': 8.3031, 'eval_samples_per_second': 120.317, 'eval_steps_per_second': 7.588, 'epoch': 0.56}
{'loss': 0.9339, 'grad_norm': 0.07309907674789429, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.808866024017334, 'eval_runtime': 8.3037, 'eval_samples_per_second': 120.308, 'eval_steps_per_second': 7.587, 'epoch': 0.6}
{'loss': 0.8535, 'grad_norm': 0.0978386327624321, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7903817296028137, 'eval_runtime': 8.3018, 'eval_samples_per_second': 120.335, 'eval_steps_per_second': 7.589, 'epoch': 0.64}
{'loss': 0.825, 'grad_norm': 0.07790521532297134, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7801569700241089, 'eval_runtime': 8.3295, 'eval_samples_per_second': 119.935, 'eval_steps_per_second': 7.563, 'epoch': 0.68}
{'loss': 0.8617, 'grad_norm': 0.1161612793803215, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.758450448513031, 'eval_runtime': 8.3231, 'eval_samples_per_second': 120.028, 'eval_steps_per_second': 7.569, 'epoch': 0.72}
{'loss': 0.795, 'grad_norm': 0.08631305396556854, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7431523203849792, 'eval_runtime': 8.3236, 'eval_samples_per_second': 120.021, 'eval_steps_per_second': 7.569, 'epoch': 0.76}
{'loss': 0.8391, 'grad_norm': 0.10728432238101959, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7277853488922119, 'eval_runtime': 8.3223, 'eval_samples_per_second': 120.039, 'eval_steps_per_second': 7.57, 'epoch': 0.8}
{'loss': 0.7273, 'grad_norm': 0.09968750178813934, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7172648906707764, 'eval_runtime': 8.3105, 'eval_samples_per_second': 120.209, 'eval_steps_per_second': 7.581, 'epoch': 0.84}
{'loss': 0.8019, 'grad_norm': 0.08892150968313217, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7071788907051086, 'eval_runtime': 8.3133, 'eval_samples_per_second': 120.169, 'eval_steps_per_second': 7.578, 'epoch': 0.88}
{'loss': 0.7642, 'grad_norm': 0.12733697891235352, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7008633017539978, 'eval_runtime': 8.3126, 'eval_samples_per_second': 120.179, 'eval_steps_per_second': 7.579, 'epoch': 0.92}
{'loss': 0.7489, 'grad_norm': 0.16975709795951843, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6947364807128906, 'eval_runtime': 8.3207, 'eval_samples_per_second': 120.062, 'eval_steps_per_second': 7.571, 'epoch': 0.96}
{'loss': 0.7467, 'grad_norm': 0.1304914653301239, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6931642293930054, 'eval_runtime': 8.314, 'eval_samples_per_second': 120.159, 'eval_steps_per_second': 7.578, 'epoch': 1.0}
{'train_runtime': 512.2534, 'train_samples_per_second': 19.52, 'train_steps_per_second': 1.22, 'train_loss': 1.086789566040039, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0813493728637695, 1.3909811973571777, 1.1494990587234497, 1.0230246782302856, 0.9977894425392151, 0.9746263027191162, 0.9560055732727051, 0.9353297352790833, 0.9175781011581421, 0.9002274870872498, 0.8840206265449524, 0.8640039563179016, 0.8442619442939758, 0.8246890902519226, 0.808866024017334, 0.7903817296028137, 0.7801569700241089, 0.758450448513031, 0.7431523203849792, 0.7277853488922119, 0.7172648906707764, 0.7071788907051086, 0.7008633017539978, 0.6947364807128906, 0.6931642293930054], 'performance': [0.53, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:52,  1.14s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:07,  9.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 16.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 21.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.59it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.53, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4879324436187744
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7989 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.7129233381690958
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.9300682271367732
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.194951369601902
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.2693263209393977
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.2293709586323003
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 32, 0, 0, 0, 0, 1, 128, 2.9582283945787947e-32, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.0556e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.7822e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7793e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.9582e-31, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 1.0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.9582283945787947e-32,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  2.9582283945787947e-32
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:53,  1.76s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:21,  4.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.09it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:09,  8.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 12.00it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 12.42it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 13.48it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:01, 14.70it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 11.40it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01, 10.35it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.39it/s]
Evaluation performance at step 25: 0.54
{'loss': 3.471, 'grad_norm': 0.09315844625234604, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.087942123413086, 'eval_runtime': 9.1101, 'eval_samples_per_second': 109.768, 'eval_steps_per_second': 6.915, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:33,  3.97s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:36,  2.51it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:18,  4.56it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:12,  5.97it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:08,  7.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:06,  8.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:05, 10.06it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:04, 10.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03, 11.52it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 12.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  9.08it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  9.50it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00, 10.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.06it/s]
Evaluation performance at step 50: 0.58
{'loss': 2.3011, 'grad_norm': 0.02521311491727829, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.8359533548355103, 'eval_runtime': 9.1149, 'eval_samples_per_second': 109.711, 'eval_steps_per_second': 6.912, 'epoch': 0.08}
{'loss': 1.7626, 'grad_norm': 0.023754065856337547, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6639466285705566, 'eval_runtime': 9.1586, 'eval_samples_per_second': 109.187, 'eval_steps_per_second': 6.879, 'epoch': 0.12}
{'loss': 1.5982, 'grad_norm': 0.028756815940141678, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5919159650802612, 'eval_runtime': 9.1579, 'eval_samples_per_second': 109.196, 'eval_steps_per_second': 6.879, 'epoch': 0.16}
{'loss': 1.5452, 'grad_norm': 0.027033228427171707, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.505271553993225, 'eval_runtime': 9.1853, 'eval_samples_per_second': 108.869, 'eval_steps_per_second': 6.859, 'epoch': 0.2}
{'loss': 1.4751, 'grad_norm': 0.026860302314162254, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4420241117477417, 'eval_runtime': 9.2179, 'eval_samples_per_second': 108.485, 'eval_steps_per_second': 6.835, 'epoch': 0.24}
{'loss': 1.4518, 'grad_norm': 0.030112851411104202, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4151921272277832, 'eval_runtime': 9.208, 'eval_samples_per_second': 108.601, 'eval_steps_per_second': 6.842, 'epoch': 0.28}
{'loss': 1.437, 'grad_norm': 0.027569714933633804, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3965390920639038, 'eval_runtime': 9.2271, 'eval_samples_per_second': 108.376, 'eval_steps_per_second': 6.828, 'epoch': 0.32}
{'loss': 1.3985, 'grad_norm': 0.028162015601992607, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3763576745986938, 'eval_runtime': 9.2143, 'eval_samples_per_second': 108.527, 'eval_steps_per_second': 6.837, 'epoch': 0.36}
{'loss': 1.4129, 'grad_norm': 0.02804466150701046, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.352249264717102, 'eval_runtime': 9.2189, 'eval_samples_per_second': 108.473, 'eval_steps_per_second': 6.834, 'epoch': 0.4}
{'loss': 1.3771, 'grad_norm': 0.03362716734409332, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3214309215545654, 'eval_runtime': 9.196, 'eval_samples_per_second': 108.743, 'eval_steps_per_second': 6.851, 'epoch': 0.44}
{'loss': 1.3136, 'grad_norm': 0.030022919178009033, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2811400890350342, 'eval_runtime': 9.2332, 'eval_samples_per_second': 108.305, 'eval_steps_per_second': 6.823, 'epoch': 0.48}
{'loss': 1.2653, 'grad_norm': 0.027812592685222626, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2643696069717407, 'eval_runtime': 9.1964, 'eval_samples_per_second': 108.738, 'eval_steps_per_second': 6.85, 'epoch': 0.52}
{'loss': 1.2671, 'grad_norm': 0.03212926536798477, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2486510276794434, 'eval_runtime': 9.2134, 'eval_samples_per_second': 108.537, 'eval_steps_per_second': 6.838, 'epoch': 0.56}
{'loss': 1.2067, 'grad_norm': 0.031106987968087196, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.232440710067749, 'eval_runtime': 9.2317, 'eval_samples_per_second': 108.322, 'eval_steps_per_second': 6.824, 'epoch': 0.6}
{'loss': 1.3005, 'grad_norm': 0.03238362818956375, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2221190929412842, 'eval_runtime': 9.2371, 'eval_samples_per_second': 108.259, 'eval_steps_per_second': 6.82, 'epoch': 0.64}
{'loss': 1.2366, 'grad_norm': 0.03198789432644844, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2164989709854126, 'eval_runtime': 9.2794, 'eval_samples_per_second': 107.766, 'eval_steps_per_second': 6.789, 'epoch': 0.68}
{'loss': 1.2626, 'grad_norm': 0.0336335264146328, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2135776281356812, 'eval_runtime': 9.2637, 'eval_samples_per_second': 107.948, 'eval_steps_per_second': 6.801, 'epoch': 0.72}
{'loss': 1.2699, 'grad_norm': 0.03300922363996506, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2105880975723267, 'eval_runtime': 9.244, 'eval_samples_per_second': 108.178, 'eval_steps_per_second': 6.815, 'epoch': 0.76}
{'loss': 1.2387, 'grad_norm': 0.04003429785370827, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2083619832992554, 'eval_runtime': 9.218, 'eval_samples_per_second': 108.483, 'eval_steps_per_second': 6.834, 'epoch': 0.8}
{'loss': 1.2115, 'grad_norm': 0.031268347054719925, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2071119546890259, 'eval_runtime': 9.2202, 'eval_samples_per_second': 108.457, 'eval_steps_per_second': 6.833, 'epoch': 0.84}
{'loss': 1.2293, 'grad_norm': 0.034462809562683105, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2057644128799438, 'eval_runtime': 9.2311, 'eval_samples_per_second': 108.33, 'eval_steps_per_second': 6.825, 'epoch': 0.88}
{'loss': 1.2193, 'grad_norm': 0.03126772493124008, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.20442795753479, 'eval_runtime': 9.3093, 'eval_samples_per_second': 107.419, 'eval_steps_per_second': 6.767, 'epoch': 0.92}
{'loss': 1.2612, 'grad_norm': 0.03563313186168671, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2035244703292847, 'eval_runtime': 9.2588, 'eval_samples_per_second': 108.006, 'eval_steps_per_second': 6.804, 'epoch': 0.96}
{'loss': 1.2597, 'grad_norm': 0.03207068145275116, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2033085823059082, 'eval_runtime': 9.22, 'eval_samples_per_second': 108.46, 'eval_steps_per_second': 6.833, 'epoch': 1.0}
{'train_runtime': 542.2607, 'train_samples_per_second': 18.441, 'train_steps_per_second': 1.153, 'train_loss': 1.4709068481445313, 'epoch': 1.0}
train_results:  {'eval_loss': [3.087942123413086, 1.8359533548355103, 1.6639466285705566, 1.5919159650802612, 1.505271553993225, 1.4420241117477417, 1.4151921272277832, 1.3965390920639038, 1.3763576745986938, 1.352249264717102, 1.3214309215545654, 1.2811400890350342, 1.2643696069717407, 1.2486510276794434, 1.232440710067749, 1.2221190929412842, 1.2164989709854126, 1.2135776281356812, 1.2105880975723267, 1.2083619832992554, 1.2071119546890259, 1.2057644128799438, 1.20442795753479, 1.2035244703292847, 1.2033085823059082], 'performance': [0.54, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:49,  1.11s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:27,  3.02it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:11,  5.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:13<00:03,  6.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00,  8.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.36it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.4614763259887695
current iteration best possible performance (full train run):  0.546
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0895 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 1.2046997539075133
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9888575470872711
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.1611731056926262
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7397158014172032
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 1.1374149503640831
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [0, 0, 0, tensor(0.5176, dtype=torch.float64), 0, 0, tensor(0.4824, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 0, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.9606e-16, dtype=torch.float64), tensor(1.0285e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5176, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4824, dtype=torch.float64), tensor(2.7213e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.518
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.482
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:27,  1.49s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.95it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.92it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:06, 11.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.96it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 13.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.59it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 15.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 15.82it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 14.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 16.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.97it/s]
Evaluation performance at step 25: 0.55
{'loss': 3.4775, 'grad_norm': 0.3560742139816284, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 2.3385515213012695, 'eval_runtime': 7.6797, 'eval_samples_per_second': 130.082, 'eval_steps_per_second': 8.203, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:34,  3.98s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:33,  2.73it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:08<00:35,  2.33it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:08<00:19,  3.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:09<00:12,  5.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:09<00:08,  7.11it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:10<00:06,  8.21it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:10<00:04,  9.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:14<00:07,  4.38it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:15<00:04,  5.59it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:15<00:02,  6.99it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:19<00:02,  4.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:20<00:00,  5.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:20<00:00,  4.92it/s]
Evaluation performance at step 50: 0.47
{'loss': 2.039, 'grad_norm': 0.25381699204444885, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.47}
{'eval_loss': 1.7867059707641602, 'eval_runtime': 7.67, 'eval_samples_per_second': 130.249, 'eval_steps_per_second': 8.214, 'epoch': 0.08}
{'loss': 1.7419, 'grad_norm': 0.299608439207077, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6537959575653076, 'eval_runtime': 7.7124, 'eval_samples_per_second': 129.532, 'eval_steps_per_second': 8.169, 'epoch': 0.12}
{'loss': 1.5995, 'grad_norm': 0.3739052712917328, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5520232915878296, 'eval_runtime': 7.7585, 'eval_samples_per_second': 128.763, 'eval_steps_per_second': 8.12, 'epoch': 0.16}
{'loss': 1.6404, 'grad_norm': 0.24915768206119537, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4739537239074707, 'eval_runtime': 7.7787, 'eval_samples_per_second': 128.428, 'eval_steps_per_second': 8.099, 'epoch': 0.2}
{'loss': 1.5684, 'grad_norm': 0.24583663046360016, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4045507907867432, 'eval_runtime': 7.7804, 'eval_samples_per_second': 128.4, 'eval_steps_per_second': 8.097, 'epoch': 0.24}
{'loss': 1.4538, 'grad_norm': 0.3515641391277313, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.388153314590454, 'eval_runtime': 7.8441, 'eval_samples_per_second': 127.357, 'eval_steps_per_second': 8.032, 'epoch': 0.28}
{'loss': 1.4818, 'grad_norm': 0.2524757385253906, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.378254771232605, 'eval_runtime': 7.8268, 'eval_samples_per_second': 127.638, 'eval_steps_per_second': 8.049, 'epoch': 0.32}
{'loss': 1.4427, 'grad_norm': 0.24641019105911255, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3659049272537231, 'eval_runtime': 7.8137, 'eval_samples_per_second': 127.852, 'eval_steps_per_second': 8.063, 'epoch': 0.36}
{'loss': 1.4686, 'grad_norm': 0.2944468855857849, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.345271110534668, 'eval_runtime': 7.8166, 'eval_samples_per_second': 127.805, 'eval_steps_per_second': 8.06, 'epoch': 0.4}
{'loss': 1.456, 'grad_norm': 0.24340376257896423, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3326715230941772, 'eval_runtime': 7.8132, 'eval_samples_per_second': 127.861, 'eval_steps_per_second': 8.063, 'epoch': 0.44}
{'loss': 1.4066, 'grad_norm': 0.2529749274253845, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.323408603668213, 'eval_runtime': 7.7961, 'eval_samples_per_second': 128.14, 'eval_steps_per_second': 8.081, 'epoch': 0.48}
{'loss': 1.3864, 'grad_norm': 0.2660312056541443, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3162498474121094, 'eval_runtime': 7.7767, 'eval_samples_per_second': 128.46, 'eval_steps_per_second': 8.101, 'epoch': 0.52}
{'loss': 1.4339, 'grad_norm': 0.2923908233642578, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3058923482894897, 'eval_runtime': 7.7818, 'eval_samples_per_second': 128.376, 'eval_steps_per_second': 8.096, 'epoch': 0.56}
{'loss': 1.4251, 'grad_norm': 0.33242738246917725, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2947107553482056, 'eval_runtime': 7.8005, 'eval_samples_per_second': 128.068, 'eval_steps_per_second': 8.076, 'epoch': 0.6}
{'loss': 1.3319, 'grad_norm': 0.29352182149887085, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.286544680595398, 'eval_runtime': 7.7941, 'eval_samples_per_second': 128.174, 'eval_steps_per_second': 8.083, 'epoch': 0.64}
{'loss': 1.3976, 'grad_norm': 0.3131715953350067, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2826424837112427, 'eval_runtime': 7.7951, 'eval_samples_per_second': 128.157, 'eval_steps_per_second': 8.082, 'epoch': 0.68}
{'loss': 1.3996, 'grad_norm': 0.2363220751285553, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2716647386550903, 'eval_runtime': 7.785, 'eval_samples_per_second': 128.323, 'eval_steps_per_second': 8.092, 'epoch': 0.72}
{'loss': 1.3422, 'grad_norm': 0.2695826590061188, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2667179107666016, 'eval_runtime': 7.7831, 'eval_samples_per_second': 128.355, 'eval_steps_per_second': 8.094, 'epoch': 0.76}
{'loss': 1.4734, 'grad_norm': 0.2722558379173279, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2562452554702759, 'eval_runtime': 7.7927, 'eval_samples_per_second': 128.197, 'eval_steps_per_second': 8.084, 'epoch': 0.8}
{'loss': 1.3814, 'grad_norm': 0.40358611941337585, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2527222633361816, 'eval_runtime': 7.8076, 'eval_samples_per_second': 127.952, 'eval_steps_per_second': 8.069, 'epoch': 0.84}
{'loss': 1.4426, 'grad_norm': 0.2493196576833725, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.245535969734192, 'eval_runtime': 7.8008, 'eval_samples_per_second': 128.064, 'eval_steps_per_second': 8.076, 'epoch': 0.88}
{'loss': 1.4135, 'grad_norm': 0.28142255544662476, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2417166233062744, 'eval_runtime': 7.7824, 'eval_samples_per_second': 128.367, 'eval_steps_per_second': 8.095, 'epoch': 0.92}
{'loss': 1.3236, 'grad_norm': 0.34390348196029663, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2399256229400635, 'eval_runtime': 7.7999, 'eval_samples_per_second': 128.078, 'eval_steps_per_second': 8.077, 'epoch': 0.96}
{'loss': 1.3104, 'grad_norm': 0.28505879640579224, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2394007444381714, 'eval_runtime': 7.7924, 'eval_samples_per_second': 128.201, 'eval_steps_per_second': 8.085, 'epoch': 1.0}
{'train_runtime': 480.5487, 'train_samples_per_second': 20.807, 'train_steps_per_second': 1.301, 'train_loss': 1.5535083862304688, 'epoch': 1.0}
train_results:  {'eval_loss': [2.3385515213012695, 1.7867059707641602, 1.6537959575653076, 1.5520232915878296, 1.4739537239074707, 1.4045507907867432, 1.388153314590454, 1.378254771232605, 1.3659049272537231, 1.345271110534668, 1.3326715230941772, 1.323408603668213, 1.3162498474121094, 1.3058923482894897, 1.2947107553482056, 1.286544680595398, 1.2826424837112427, 1.2716647386550903, 1.2667179107666016, 1.2562452554702759, 1.2527222633361816, 1.245535969734192, 1.2417166233062744, 1.2399256229400635, 1.2394007444381714], 'performance': [0.55, 0.47]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:49,  2.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 19.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 26.16it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 22.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 25.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.02it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 25.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 24.14it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.47]
current iteration observed (possibly low-fid or predicted) performance:  1.4583327770233154
current iteration best possible performance (full train run):  0.525
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0202 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.9466198854943175
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.8600880214380764
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.9753786621097646
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.8133953351024256
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 1.0753372678422646
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.5358, dtype=torch.float64), 0, 0, tensor(0.4642, dtype=torch.float64), 32, 0, 1, 0, 0, 1, 128, 3.145712932229657e-17, 1.480000019073495, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5358, dtype=torch.float64), tensor(9.7095e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4642, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.1457e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.536
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.464

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.145712932229657e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.480000019073495,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  128
lora dropout:  3.145712932229657e-17
lora alpha:  1.480000019073495
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:43,  1.65s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.44it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.61it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:03, 11.63it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.50it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.39it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  9.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.50it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.3268, 'grad_norm': 0.2394467145204544, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.508664846420288, 'eval_runtime': 6.188, 'eval_samples_per_second': 161.443, 'eval_steps_per_second': 10.181, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:41,  2.39it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.76it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.14it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:21,  3.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  5.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  6.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.07it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.26it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.27it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 11.61it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 12.84it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 14.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 14.42it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.53it/s]
Evaluation performance at step 50: 0.61
{'loss': 2.3834, 'grad_norm': 0.11194933950901031, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.556296706199646, 'eval_runtime': 6.2257, 'eval_samples_per_second': 160.464, 'eval_steps_per_second': 10.119, 'epoch': 0.08}
{'loss': 1.3333, 'grad_norm': 0.04884400963783264, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2555646896362305, 'eval_runtime': 6.2773, 'eval_samples_per_second': 159.145, 'eval_steps_per_second': 10.036, 'epoch': 0.12}
{'loss': 1.1991, 'grad_norm': 0.039535414427518845, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1199696063995361, 'eval_runtime': 6.2782, 'eval_samples_per_second': 159.121, 'eval_steps_per_second': 10.035, 'epoch': 0.16}
{'loss': 1.0788, 'grad_norm': 0.041319072246551514, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.077070951461792, 'eval_runtime': 6.3062, 'eval_samples_per_second': 158.415, 'eval_steps_per_second': 9.99, 'epoch': 0.2}
{'loss': 1.0271, 'grad_norm': 0.043925464153289795, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0481947660446167, 'eval_runtime': 6.2907, 'eval_samples_per_second': 158.807, 'eval_steps_per_second': 10.015, 'epoch': 0.24}
{'loss': 1.0232, 'grad_norm': 0.05016599968075752, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0219594240188599, 'eval_runtime': 6.2956, 'eval_samples_per_second': 158.682, 'eval_steps_per_second': 10.007, 'epoch': 0.28}
{'loss': 0.9948, 'grad_norm': 0.04089066758751869, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9962030053138733, 'eval_runtime': 6.306, 'eval_samples_per_second': 158.42, 'eval_steps_per_second': 9.99, 'epoch': 0.32}
{'loss': 0.9544, 'grad_norm': 0.055952221155166626, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9611548781394958, 'eval_runtime': 6.3018, 'eval_samples_per_second': 158.526, 'eval_steps_per_second': 9.997, 'epoch': 0.36}
{'loss': 0.9357, 'grad_norm': 0.05783757567405701, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8981080651283264, 'eval_runtime': 6.3062, 'eval_samples_per_second': 158.417, 'eval_steps_per_second': 9.99, 'epoch': 0.4}
{'loss': 0.8864, 'grad_norm': 0.05696014314889908, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8391508460044861, 'eval_runtime': 6.3047, 'eval_samples_per_second': 158.453, 'eval_steps_per_second': 9.993, 'epoch': 0.44}
{'loss': 0.8321, 'grad_norm': 0.05191272497177124, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8002476096153259, 'eval_runtime': 6.306, 'eval_samples_per_second': 158.42, 'eval_steps_per_second': 9.99, 'epoch': 0.48}
{'loss': 0.7717, 'grad_norm': 0.05151728168129921, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7629746198654175, 'eval_runtime': 6.3014, 'eval_samples_per_second': 158.536, 'eval_steps_per_second': 9.998, 'epoch': 0.52}
{'loss': 0.785, 'grad_norm': 0.05667734891176224, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7445805072784424, 'eval_runtime': 6.3044, 'eval_samples_per_second': 158.46, 'eval_steps_per_second': 9.993, 'epoch': 0.56}
{'loss': 0.7537, 'grad_norm': 0.05915163457393646, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7336041927337646, 'eval_runtime': 6.3218, 'eval_samples_per_second': 158.024, 'eval_steps_per_second': 9.965, 'epoch': 0.6}
{'loss': 0.7433, 'grad_norm': 0.0644015446305275, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7220879793167114, 'eval_runtime': 6.3089, 'eval_samples_per_second': 158.347, 'eval_steps_per_second': 9.986, 'epoch': 0.64}
{'loss': 0.7213, 'grad_norm': 0.060986828058958054, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7121356129646301, 'eval_runtime': 6.2914, 'eval_samples_per_second': 158.788, 'eval_steps_per_second': 10.014, 'epoch': 0.68}
{'loss': 0.7214, 'grad_norm': 0.0725841075181961, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7025818824768066, 'eval_runtime': 6.3016, 'eval_samples_per_second': 158.531, 'eval_steps_per_second': 9.997, 'epoch': 0.72}
{'loss': 0.6983, 'grad_norm': 0.06988395750522614, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6921256184577942, 'eval_runtime': 6.2988, 'eval_samples_per_second': 158.602, 'eval_steps_per_second': 10.002, 'epoch': 0.76}
{'loss': 0.7013, 'grad_norm': 0.06690710037946701, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6835587024688721, 'eval_runtime': 6.2878, 'eval_samples_per_second': 158.88, 'eval_steps_per_second': 10.019, 'epoch': 0.8}
{'loss': 0.6954, 'grad_norm': 0.07431308180093765, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6761022210121155, 'eval_runtime': 6.2969, 'eval_samples_per_second': 158.65, 'eval_steps_per_second': 10.005, 'epoch': 0.84}
{'loss': 0.7002, 'grad_norm': 0.07316497713327408, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6702051162719727, 'eval_runtime': 6.3123, 'eval_samples_per_second': 158.262, 'eval_steps_per_second': 9.98, 'epoch': 0.88}
{'loss': 0.6579, 'grad_norm': 0.08216439187526703, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6642327308654785, 'eval_runtime': 6.3355, 'eval_samples_per_second': 157.682, 'eval_steps_per_second': 9.944, 'epoch': 0.92}
{'loss': 0.6827, 'grad_norm': 0.09633952379226685, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6613057851791382, 'eval_runtime': 6.3188, 'eval_samples_per_second': 158.101, 'eval_steps_per_second': 9.97, 'epoch': 0.96}
{'loss': 0.6692, 'grad_norm': 0.0776599794626236, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6600440144538879, 'eval_runtime': 6.3086, 'eval_samples_per_second': 158.356, 'eval_steps_per_second': 9.986, 'epoch': 1.0}
{'train_runtime': 407.1288, 'train_samples_per_second': 24.56, 'train_steps_per_second': 1.535, 'train_loss': 1.0510549285888673, 'epoch': 1.0}
train_results:  {'eval_loss': [3.508664846420288, 1.556296706199646, 1.2555646896362305, 1.1199696063995361, 1.077070951461792, 1.0481947660446167, 1.0219594240188599, 0.9962030053138733, 0.9611548781394958, 0.8981080651283264, 0.8391508460044861, 0.8002476096153259, 0.7629746198654175, 0.7445805072784424, 0.7336041927337646, 0.7220879793167114, 0.7121356129646301, 0.7025818824768066, 0.6921256184577942, 0.6835587024688721, 0.6761022210121155, 0.6702051162719727, 0.6642327308654785, 0.6613057851791382, 0.6600440144538879], 'performance': [0.55, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:59,  2.42s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.66it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 13.27it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 13.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 17.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 20.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 22.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.07it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.4840221405029297
current iteration best possible performance (full train run):  0.672
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9912 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 1.1771828145988494
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.9206623226447445
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.9738322130325542
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 1.070800771834601
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 1.1433271968621368
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.2094, dtype=torch.float64), 0, tensor(0.7906, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 0, 1, 128, 0.0, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.4644e-16, dtype=torch.float64), tensor(0.2094, dtype=torch.float64), tensor(7.6420e-17, dtype=torch.float64), tensor(0.7906, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.8003e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.209
  triviaqa: 0
  truthfulqa_gen: 0.791
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:15,  1.97s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:24,  3.70it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:13,  6.09it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:10,  6.84it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  7.78it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:06,  8.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.24it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.41it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.39it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02,  9.92it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  7.19it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.56it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  7.97it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.9988, 'grad_norm': 0.2964807152748108, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.466798782348633, 'eval_runtime': 3.6884, 'eval_samples_per_second': 270.851, 'eval_steps_per_second': 17.081, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:56,  1.74it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09, 10.02it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.46it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.13it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:14,  4.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.09it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:06,  7.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:04,  8.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 11.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 12.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 11.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01, 10.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 10.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.40it/s]
Evaluation performance at step 50: 0.56
{'loss': 2.1666, 'grad_norm': 0.07859653234481812, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.4915268421173096, 'eval_runtime': 3.6952, 'eval_samples_per_second': 270.35, 'eval_steps_per_second': 17.049, 'epoch': 0.08}
{'loss': 1.3169, 'grad_norm': 0.06547751277685165, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2076042890548706, 'eval_runtime': 3.6976, 'eval_samples_per_second': 270.173, 'eval_steps_per_second': 17.038, 'epoch': 0.12}
{'loss': 1.1752, 'grad_norm': 0.05448877811431885, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1246147155761719, 'eval_runtime': 3.6988, 'eval_samples_per_second': 270.09, 'eval_steps_per_second': 17.033, 'epoch': 0.16}
{'loss': 1.0959, 'grad_norm': 0.045784950256347656, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.074541449546814, 'eval_runtime': 3.717, 'eval_samples_per_second': 268.766, 'eval_steps_per_second': 16.949, 'epoch': 0.2}
{'loss': 1.0111, 'grad_norm': 0.05302529036998749, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.024836778640747, 'eval_runtime': 3.7456, 'eval_samples_per_second': 266.712, 'eval_steps_per_second': 16.82, 'epoch': 0.24}
{'loss': 0.9913, 'grad_norm': 0.05818978697061539, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9595106840133667, 'eval_runtime': 3.7358, 'eval_samples_per_second': 267.414, 'eval_steps_per_second': 16.864, 'epoch': 0.28}
{'loss': 0.9275, 'grad_norm': 0.09251435101032257, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8399466276168823, 'eval_runtime': 3.733, 'eval_samples_per_second': 267.616, 'eval_steps_per_second': 16.877, 'epoch': 0.32}
{'loss': 0.8022, 'grad_norm': 0.08651140332221985, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7423729300498962, 'eval_runtime': 3.7401, 'eval_samples_per_second': 267.102, 'eval_steps_per_second': 16.844, 'epoch': 0.36}
{'loss': 0.7062, 'grad_norm': 0.07677960395812988, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6572474837303162, 'eval_runtime': 3.7365, 'eval_samples_per_second': 267.364, 'eval_steps_per_second': 16.861, 'epoch': 0.4}
{'loss': 0.6674, 'grad_norm': 0.09670008718967438, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.586658775806427, 'eval_runtime': 3.7049, 'eval_samples_per_second': 269.646, 'eval_steps_per_second': 17.005, 'epoch': 0.44}
{'loss': 0.5792, 'grad_norm': 0.09112315624952316, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.550661563873291, 'eval_runtime': 3.7047, 'eval_samples_per_second': 269.655, 'eval_steps_per_second': 17.005, 'epoch': 0.48}
{'loss': 0.5638, 'grad_norm': 0.11156710982322693, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5196743011474609, 'eval_runtime': 3.6887, 'eval_samples_per_second': 270.831, 'eval_steps_per_second': 17.079, 'epoch': 0.52}
{'loss': 0.5196, 'grad_norm': 0.10894245654344559, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.48431557416915894, 'eval_runtime': 3.6981, 'eval_samples_per_second': 270.138, 'eval_steps_per_second': 17.036, 'epoch': 0.56}
{'loss': 0.5044, 'grad_norm': 0.11706981807947159, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.45588868856430054, 'eval_runtime': 3.6918, 'eval_samples_per_second': 270.601, 'eval_steps_per_second': 17.065, 'epoch': 0.6}
{'loss': 0.4725, 'grad_norm': 0.10957801342010498, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.42975103855133057, 'eval_runtime': 3.6962, 'eval_samples_per_second': 270.277, 'eval_steps_per_second': 17.045, 'epoch': 0.64}
{'loss': 0.4347, 'grad_norm': 0.10962462425231934, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.405438631772995, 'eval_runtime': 3.6961, 'eval_samples_per_second': 270.283, 'eval_steps_per_second': 17.045, 'epoch': 0.68}
{'loss': 0.4535, 'grad_norm': 0.13681955635547638, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3864004909992218, 'eval_runtime': 3.6955, 'eval_samples_per_second': 270.326, 'eval_steps_per_second': 17.048, 'epoch': 0.72}
{'loss': 0.4108, 'grad_norm': 0.12023480236530304, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.36708229780197144, 'eval_runtime': 3.7041, 'eval_samples_per_second': 269.701, 'eval_steps_per_second': 17.008, 'epoch': 0.76}
{'loss': 0.3961, 'grad_norm': 0.12775152921676636, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3511491119861603, 'eval_runtime': 3.6964, 'eval_samples_per_second': 270.266, 'eval_steps_per_second': 17.044, 'epoch': 0.8}
{'loss': 0.4059, 'grad_norm': 0.14407150447368622, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3350710868835449, 'eval_runtime': 3.7061, 'eval_samples_per_second': 269.557, 'eval_steps_per_second': 16.999, 'epoch': 0.84}
{'loss': 0.3807, 'grad_norm': 0.1289909929037094, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.32235363125801086, 'eval_runtime': 3.732, 'eval_samples_per_second': 267.684, 'eval_steps_per_second': 16.881, 'epoch': 0.88}
{'loss': 0.358, 'grad_norm': 0.1736755669116974, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.31679901480674744, 'eval_runtime': 3.7071, 'eval_samples_per_second': 269.486, 'eval_steps_per_second': 16.995, 'epoch': 0.92}
{'loss': 0.3378, 'grad_norm': 0.15669262409210205, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.31304606795310974, 'eval_runtime': 3.7038, 'eval_samples_per_second': 269.726, 'eval_steps_per_second': 17.01, 'epoch': 0.96}
{'loss': 0.3438, 'grad_norm': 0.12821736931800842, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3107786476612091, 'eval_runtime': 3.7012, 'eval_samples_per_second': 269.911, 'eval_steps_per_second': 17.021, 'epoch': 1.0}
{'train_runtime': 287.7249, 'train_samples_per_second': 34.752, 'train_steps_per_second': 2.172, 'train_loss': 0.880787548828125, 'epoch': 1.0}
train_results:  {'eval_loss': [3.466798782348633, 1.4915268421173096, 1.2076042890548706, 1.1246147155761719, 1.074541449546814, 1.024836778640747, 0.9595106840133667, 0.8399466276168823, 0.7423729300498962, 0.6572474837303162, 0.586658775806427, 0.550661563873291, 0.5196743011474609, 0.48431557416915894, 0.45588868856430054, 0.42975103855133057, 0.405438631772995, 0.3864004909992218, 0.36708229780197144, 0.3511491119861603, 0.3350710868835449, 0.32235363125801086, 0.31679901480674744, 0.31304606795310974, 0.3107786476612091], 'performance': [0.56, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:09,  1.92s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  8.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:04, 13.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.52it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.54it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 21.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 23.63it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.62it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.4821724891662598
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1788 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 1.1591867800595417
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 1.2166813859769652
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 1.1257858325790318
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 1.0019949043935688
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 1.1826911024194646
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.2915, dtype=torch.float64), tensor(0.0799, dtype=torch.float64), tensor(0.5856, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.480000019073487, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.8164e-17, dtype=torch.float64), tensor(5.6782e-17, dtype=torch.float64), tensor(0.2915, dtype=torch.float64), tensor(0.0799, dtype=torch.float64), tensor(0.5856, dtype=torch.float64), tensor(0.0430, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3300e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.291
  triviaqa: 0.08
  truthfulqa_gen: 0.586
  wikitext: 0.043
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:44,  1.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:15,  5.75it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.40it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:07,  9.69it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.25it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 11.97it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.29it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.49it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 13.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.41it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01,  9.81it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.75it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.0461, 'grad_norm': 0.46275579929351807, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.6771433353424072, 'eval_runtime': 4.0016, 'eval_samples_per_second': 249.651, 'eval_steps_per_second': 15.744, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:44,  1.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:13,  6.84it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.34it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 13.32it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.33it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 14.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 17.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 20.64it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 18.47it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.83it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 13.80it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 15.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.96it/s]
Evaluation performance at step 50: 0.42
{'loss': 2.366, 'grad_norm': 0.4133027195930481, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.42}
{'eval_loss': 1.4646836519241333, 'eval_runtime': 4.0041, 'eval_samples_per_second': 249.493, 'eval_steps_per_second': 15.734, 'epoch': 0.08}
{'loss': 1.2466, 'grad_norm': 0.14604851603507996, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1100045442581177, 'eval_runtime': 4.0039, 'eval_samples_per_second': 249.506, 'eval_steps_per_second': 15.735, 'epoch': 0.12}
{'loss': 1.0541, 'grad_norm': 0.11360374093055725, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9655171632766724, 'eval_runtime': 4.0151, 'eval_samples_per_second': 248.811, 'eval_steps_per_second': 15.691, 'epoch': 0.16}
{'loss': 0.9191, 'grad_norm': 0.07371041178703308, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9238213300704956, 'eval_runtime': 4.0002, 'eval_samples_per_second': 249.74, 'eval_steps_per_second': 15.749, 'epoch': 0.2}
{'loss': 0.9255, 'grad_norm': 0.06394250690937042, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9075868725776672, 'eval_runtime': 4.0055, 'eval_samples_per_second': 249.409, 'eval_steps_per_second': 15.728, 'epoch': 0.24}
{'loss': 0.9002, 'grad_norm': 0.060566723346710205, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8925259709358215, 'eval_runtime': 4.0087, 'eval_samples_per_second': 249.21, 'eval_steps_per_second': 15.716, 'epoch': 0.28}
{'loss': 0.9208, 'grad_norm': 0.06855519115924835, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8815215826034546, 'eval_runtime': 4.0195, 'eval_samples_per_second': 248.538, 'eval_steps_per_second': 15.674, 'epoch': 0.32}
{'loss': 0.8664, 'grad_norm': 0.07187206298112869, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8713694214820862, 'eval_runtime': 4.0167, 'eval_samples_per_second': 248.71, 'eval_steps_per_second': 15.684, 'epoch': 0.36}
{'loss': 0.9267, 'grad_norm': 0.0696096271276474, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.862979531288147, 'eval_runtime': 4.021, 'eval_samples_per_second': 248.449, 'eval_steps_per_second': 15.668, 'epoch': 0.4}
{'loss': 0.9138, 'grad_norm': 0.13688991963863373, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8520894050598145, 'eval_runtime': 4.0304, 'eval_samples_per_second': 247.866, 'eval_steps_per_second': 15.631, 'epoch': 0.44}
{'loss': 0.7995, 'grad_norm': 0.08740144968032837, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8429367542266846, 'eval_runtime': 4.0294, 'eval_samples_per_second': 247.93, 'eval_steps_per_second': 15.635, 'epoch': 0.48}
{'loss': 0.8406, 'grad_norm': 0.0825217217206955, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8360344767570496, 'eval_runtime': 4.0236, 'eval_samples_per_second': 248.283, 'eval_steps_per_second': 15.658, 'epoch': 0.52}
{'loss': 0.8223, 'grad_norm': 0.0900050476193428, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8287651538848877, 'eval_runtime': 4.0278, 'eval_samples_per_second': 248.024, 'eval_steps_per_second': 15.641, 'epoch': 0.56}
{'loss': 0.8274, 'grad_norm': 0.09040595591068268, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8218986988067627, 'eval_runtime': 4.0264, 'eval_samples_per_second': 248.112, 'eval_steps_per_second': 15.647, 'epoch': 0.6}
{'loss': 0.8243, 'grad_norm': 0.11363150179386139, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8153054714202881, 'eval_runtime': 4.037, 'eval_samples_per_second': 247.461, 'eval_steps_per_second': 15.606, 'epoch': 0.64}
{'loss': 0.8395, 'grad_norm': 0.0913202241063118, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8104031682014465, 'eval_runtime': 4.0282, 'eval_samples_per_second': 248.004, 'eval_steps_per_second': 15.64, 'epoch': 0.68}
{'loss': 0.8301, 'grad_norm': 0.09792438894510269, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8048052787780762, 'eval_runtime': 4.0368, 'eval_samples_per_second': 247.475, 'eval_steps_per_second': 15.607, 'epoch': 0.72}
{'loss': 0.7744, 'grad_norm': 0.10534507781267166, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7995505928993225, 'eval_runtime': 4.0326, 'eval_samples_per_second': 247.73, 'eval_steps_per_second': 15.623, 'epoch': 0.76}
{'loss': 0.8217, 'grad_norm': 0.12137038260698318, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7962659001350403, 'eval_runtime': 4.0407, 'eval_samples_per_second': 247.232, 'eval_steps_per_second': 15.591, 'epoch': 0.8}
{'loss': 0.8128, 'grad_norm': 0.11677826941013336, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7918562889099121, 'eval_runtime': 4.0343, 'eval_samples_per_second': 247.624, 'eval_steps_per_second': 15.616, 'epoch': 0.84}
{'loss': 0.843, 'grad_norm': 0.10870636254549026, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7890465259552002, 'eval_runtime': 4.0348, 'eval_samples_per_second': 247.594, 'eval_steps_per_second': 15.614, 'epoch': 0.88}
{'loss': 0.7952, 'grad_norm': 0.10881337523460388, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7872515916824341, 'eval_runtime': 4.0375, 'eval_samples_per_second': 247.43, 'eval_steps_per_second': 15.604, 'epoch': 0.92}
{'loss': 0.7425, 'grad_norm': 0.13902074098587036, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.785180389881134, 'eval_runtime': 4.0382, 'eval_samples_per_second': 247.385, 'eval_steps_per_second': 15.601, 'epoch': 0.96}
{'loss': 0.8179, 'grad_norm': 0.12607154250144958, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.784393846988678, 'eval_runtime': 4.0345, 'eval_samples_per_second': 247.616, 'eval_steps_per_second': 15.615, 'epoch': 1.0}
{'train_runtime': 299.1017, 'train_samples_per_second': 33.427, 'train_steps_per_second': 2.09, 'train_loss': 1.0990494079589843, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6771433353424072, 1.4646836519241333, 1.1100045442581177, 0.9655171632766724, 0.9238213300704956, 0.9075868725776672, 0.8925259709358215, 0.8815215826034546, 0.8713694214820862, 0.862979531288147, 0.8520894050598145, 0.8429367542266846, 0.8360344767570496, 0.8287651538848877, 0.8218986988067627, 0.8153054714202881, 0.8104031682014465, 0.8048052787780762, 0.7995505928993225, 0.7962659001350403, 0.7918562889099121, 0.7890465259552002, 0.7872515916824341, 0.785180389881134, 0.784393846988678], 'performance': [0.56, 0.42]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:09,  1.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 16.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 23.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 10.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 14.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 21.89it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.90it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.42]
current iteration observed (possibly low-fid or predicted) performance:  1.491274118423462
current iteration best possible performance (full train run):  0.672
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0104 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 1.1029783903743056
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.9543404313033519
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.5738474494522542
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 1.0038008595491164
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 1.0251843268988523
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0560, dtype=torch.float64), 0, 0, tensor(0.3903, dtype=torch.float64), 0, tensor(0.5538, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 1.5612511283791264e-18, 1.4800000190734877, 0]
normalized proposed parameters for next round by BO: [tensor(0.0560, dtype=torch.float64), tensor(1.0678e-17, dtype=torch.float64), tensor(3.0809e-17, dtype=torch.float64), tensor(0.3903, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5538, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.2644e-18, dtype=torch.float64), tensor(3.0852e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.5613e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.056
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.39
  triviaqa: 0
  truthfulqa_gen: 0.554
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.5612511283791264e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734877,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  1.5612511283791264e-18
lora alpha:  1.4800000190734877
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:41,  1.63s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.50it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.78it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.11it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.51it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 10.88it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 11.85it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.15it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.88it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.64it/s]
Evaluation performance at step 25: 0.55
{'loss': 5.0797, 'grad_norm': 0.5398498177528381, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.654827833175659, 'eval_runtime': 5.3841, 'eval_samples_per_second': 185.548, 'eval_steps_per_second': 11.701, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:50,  1.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.50it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.86it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.69it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 14.63it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 15.42it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.31it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 16.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 16.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 12.87it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 14.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.69it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.3593, 'grad_norm': 0.413244366645813, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 1.4449399709701538, 'eval_runtime': 3.9186, 'eval_samples_per_second': 254.936, 'eval_steps_per_second': 16.077, 'epoch': 0.08}
{'loss': 1.2088, 'grad_norm': 0.12598834931850433, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0403729677200317, 'eval_runtime': 3.916, 'eval_samples_per_second': 255.106, 'eval_steps_per_second': 16.088, 'epoch': 0.12}
{'loss': 0.9643, 'grad_norm': 0.12492996454238892, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8765231370925903, 'eval_runtime': 3.9312, 'eval_samples_per_second': 254.12, 'eval_steps_per_second': 16.026, 'epoch': 0.16}
{'loss': 0.8595, 'grad_norm': 0.07517984509468079, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8252489566802979, 'eval_runtime': 3.9334, 'eval_samples_per_second': 253.977, 'eval_steps_per_second': 16.017, 'epoch': 0.2}
{'loss': 0.8274, 'grad_norm': 0.06948971748352051, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8031491637229919, 'eval_runtime': 3.9293, 'eval_samples_per_second': 254.247, 'eval_steps_per_second': 16.034, 'epoch': 0.24}
{'loss': 0.8346, 'grad_norm': 0.06554414331912994, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7910031080245972, 'eval_runtime': 3.9304, 'eval_samples_per_second': 254.17, 'eval_steps_per_second': 16.029, 'epoch': 0.28}
{'loss': 0.8263, 'grad_norm': 0.07095416635274887, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7777692079544067, 'eval_runtime': 3.9418, 'eval_samples_per_second': 253.439, 'eval_steps_per_second': 15.983, 'epoch': 0.32}
{'loss': 0.804, 'grad_norm': 0.07392647117376328, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7699635624885559, 'eval_runtime': 3.9433, 'eval_samples_per_second': 253.338, 'eval_steps_per_second': 15.976, 'epoch': 0.36}
{'loss': 0.7829, 'grad_norm': 0.06925074756145477, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7598214149475098, 'eval_runtime': 3.945, 'eval_samples_per_second': 253.234, 'eval_steps_per_second': 15.97, 'epoch': 0.4}
{'loss': 0.7603, 'grad_norm': 0.08473266661167145, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7554643154144287, 'eval_runtime': 3.9546, 'eval_samples_per_second': 252.619, 'eval_steps_per_second': 15.931, 'epoch': 0.44}
{'loss': 0.755, 'grad_norm': 0.09288948029279709, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7519826292991638, 'eval_runtime': 3.9496, 'eval_samples_per_second': 252.936, 'eval_steps_per_second': 15.951, 'epoch': 0.48}
{'loss': 0.7556, 'grad_norm': 0.07958988845348358, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7439308762550354, 'eval_runtime': 3.9529, 'eval_samples_per_second': 252.726, 'eval_steps_per_second': 15.938, 'epoch': 0.52}
{'loss': 0.7427, 'grad_norm': 0.10713411122560501, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7400102019309998, 'eval_runtime': 3.9465, 'eval_samples_per_second': 253.136, 'eval_steps_per_second': 15.964, 'epoch': 0.56}
{'loss': 0.7633, 'grad_norm': 0.07098712027072906, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.735718309879303, 'eval_runtime': 3.9518, 'eval_samples_per_second': 252.797, 'eval_steps_per_second': 15.942, 'epoch': 0.6}
{'loss': 0.7423, 'grad_norm': 0.08803436905145645, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7284834980964661, 'eval_runtime': 3.9486, 'eval_samples_per_second': 253.0, 'eval_steps_per_second': 15.955, 'epoch': 0.64}
{'loss': 0.763, 'grad_norm': 0.0841865986585617, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7249709963798523, 'eval_runtime': 3.9564, 'eval_samples_per_second': 252.502, 'eval_steps_per_second': 15.924, 'epoch': 0.68}
{'loss': 0.7261, 'grad_norm': 0.08216474950313568, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7214944362640381, 'eval_runtime': 3.9489, 'eval_samples_per_second': 252.981, 'eval_steps_per_second': 15.954, 'epoch': 0.72}
{'loss': 0.7489, 'grad_norm': 0.0915093868970871, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7181112170219421, 'eval_runtime': 3.9525, 'eval_samples_per_second': 252.753, 'eval_steps_per_second': 15.939, 'epoch': 0.76}
{'loss': 0.7347, 'grad_norm': 0.08550064265727997, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7151626944541931, 'eval_runtime': 3.9595, 'eval_samples_per_second': 252.307, 'eval_steps_per_second': 15.911, 'epoch': 0.8}
{'loss': 0.7409, 'grad_norm': 0.09755726903676987, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7126914262771606, 'eval_runtime': 3.9542, 'eval_samples_per_second': 252.645, 'eval_steps_per_second': 15.933, 'epoch': 0.84}
{'loss': 0.7506, 'grad_norm': 0.12325619906187057, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7101688385009766, 'eval_runtime': 3.9635, 'eval_samples_per_second': 252.052, 'eval_steps_per_second': 15.895, 'epoch': 0.88}
{'loss': 0.7461, 'grad_norm': 0.10691255331039429, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.709092378616333, 'eval_runtime': 3.9559, 'eval_samples_per_second': 252.534, 'eval_steps_per_second': 15.926, 'epoch': 0.92}
{'loss': 0.7086, 'grad_norm': 0.10192761570215225, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7079335451126099, 'eval_runtime': 3.9668, 'eval_samples_per_second': 251.838, 'eval_steps_per_second': 15.882, 'epoch': 0.96}
{'loss': 0.7327, 'grad_norm': 0.10862544924020767, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7070561647415161, 'eval_runtime': 3.9561, 'eval_samples_per_second': 252.525, 'eval_steps_per_second': 15.925, 'epoch': 1.0}
{'train_runtime': 293.2531, 'train_samples_per_second': 34.093, 'train_steps_per_second': 2.131, 'train_loss': 1.0287103668212891, 'epoch': 1.0}
train_results:  {'eval_loss': [3.654827833175659, 1.4449399709701538, 1.0403729677200317, 0.8765231370925903, 0.8252489566802979, 0.8031491637229919, 0.7910031080245972, 0.7777692079544067, 0.7699635624885559, 0.7598214149475098, 0.7554643154144287, 0.7519826292991638, 0.7439308762550354, 0.7400102019309998, 0.735718309879303, 0.7284834980964661, 0.7249709963798523, 0.7214944362640381, 0.7181112170219421, 0.7151626944541931, 0.7126914262771606, 0.7101688385009766, 0.709092378616333, 0.7079335451126099, 0.7070561647415161], 'performance': [0.55, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:00,  1.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 16.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.51it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 23.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 23.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  7.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00,  9.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.60it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.4915637969970703
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1319 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7218959667121243
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 1.1916224402006914
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.8200762945587248
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.8756071290127454
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.9485788986783028
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.5420, dtype=torch.float64), 0, tensor(0.4580, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 1.2490009027032991e-17, 1.480000019073493, 0]
normalized proposed parameters for next round by BO: [tensor(1.2284e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.0153e-17, dtype=torch.float64), tensor(0.5420, dtype=torch.float64), tensor(3.7234e-17, dtype=torch.float64), tensor(0.4580, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.2490e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.542
  triviaqa: 0
  truthfulqa_gen: 0.458
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.2490009027032991e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (1.480000019073493,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  1.2490009027032991e-17
lora alpha:  1.480000019073493
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:21,  1.43s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:17,  5.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.83it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:07,  9.75it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.60it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 12.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 12.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 13.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.02it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 15.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 12.12it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 11.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 12.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.61it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.2323, 'grad_norm': 0.6143282055854797, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.928400993347168, 'eval_runtime': 3.2187, 'eval_samples_per_second': 310.376, 'eval_steps_per_second': 19.573, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:36,  1.03it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.39it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.31it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 13.72it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.38it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 14.97it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.63it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.08it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:02,  5.25it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00,  6.84it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.11it/s]
Evaluation performance at step 50: 0.43
{'loss': 2.4793, 'grad_norm': 0.44866448640823364, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.43}
{'eval_loss': 1.4532841444015503, 'eval_runtime': 3.2173, 'eval_samples_per_second': 310.51, 'eval_steps_per_second': 19.582, 'epoch': 0.08}
{'loss': 1.1888, 'grad_norm': 0.11881363391876221, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9996230602264404, 'eval_runtime': 3.2254, 'eval_samples_per_second': 309.731, 'eval_steps_per_second': 19.533, 'epoch': 0.12}
{'loss': 0.9416, 'grad_norm': 0.13961130380630493, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8445727229118347, 'eval_runtime': 3.2261, 'eval_samples_per_second': 309.666, 'eval_steps_per_second': 19.528, 'epoch': 0.16}
{'loss': 0.7948, 'grad_norm': 0.050883613526821136, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.809971809387207, 'eval_runtime': 3.2349, 'eval_samples_per_second': 308.818, 'eval_steps_per_second': 19.475, 'epoch': 0.2}
{'loss': 0.8049, 'grad_norm': 0.04796627536416054, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7968291640281677, 'eval_runtime': 3.2439, 'eval_samples_per_second': 307.967, 'eval_steps_per_second': 19.421, 'epoch': 0.24}
{'loss': 0.7956, 'grad_norm': 0.06885025650262833, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7869929075241089, 'eval_runtime': 3.2479, 'eval_samples_per_second': 307.587, 'eval_steps_per_second': 19.397, 'epoch': 0.28}
{'loss': 0.7818, 'grad_norm': 0.07614239305257797, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7779667973518372, 'eval_runtime': 3.2523, 'eval_samples_per_second': 307.17, 'eval_steps_per_second': 19.371, 'epoch': 0.32}
{'loss': 0.7963, 'grad_norm': 0.07378486543893814, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.772643506526947, 'eval_runtime': 3.2377, 'eval_samples_per_second': 308.552, 'eval_steps_per_second': 19.458, 'epoch': 0.36}
{'loss': 0.7523, 'grad_norm': 0.05848133563995361, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7637637853622437, 'eval_runtime': 3.245, 'eval_samples_per_second': 307.859, 'eval_steps_per_second': 19.415, 'epoch': 0.4}
{'loss': 0.7947, 'grad_norm': 0.053805284202098846, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.757919192314148, 'eval_runtime': 3.2388, 'eval_samples_per_second': 308.45, 'eval_steps_per_second': 19.452, 'epoch': 0.44}
{'loss': 0.7839, 'grad_norm': 0.05746590346097946, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7549216747283936, 'eval_runtime': 3.2487, 'eval_samples_per_second': 307.508, 'eval_steps_per_second': 19.392, 'epoch': 0.48}
{'loss': 0.7569, 'grad_norm': 0.06952986866235733, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7481943964958191, 'eval_runtime': 3.2544, 'eval_samples_per_second': 306.968, 'eval_steps_per_second': 19.358, 'epoch': 0.52}
{'loss': 0.7539, 'grad_norm': 0.06457426398992538, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.74419105052948, 'eval_runtime': 3.2607, 'eval_samples_per_second': 306.374, 'eval_steps_per_second': 19.321, 'epoch': 0.56}
{'loss': 0.7631, 'grad_norm': 0.07669780403375626, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7397834062576294, 'eval_runtime': 3.2534, 'eval_samples_per_second': 307.068, 'eval_steps_per_second': 19.365, 'epoch': 0.6}
{'loss': 0.7242, 'grad_norm': 0.0632026419043541, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7363548874855042, 'eval_runtime': 3.2615, 'eval_samples_per_second': 306.305, 'eval_steps_per_second': 19.317, 'epoch': 0.64}
{'loss': 0.7609, 'grad_norm': 0.07003335654735565, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7330076694488525, 'eval_runtime': 3.2741, 'eval_samples_per_second': 305.126, 'eval_steps_per_second': 19.242, 'epoch': 0.68}
{'loss': 0.744, 'grad_norm': 0.06511985510587692, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7297987341880798, 'eval_runtime': 3.2595, 'eval_samples_per_second': 306.486, 'eval_steps_per_second': 19.328, 'epoch': 0.72}
{'loss': 0.7352, 'grad_norm': 0.07227537781000137, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7267656922340393, 'eval_runtime': 3.3066, 'eval_samples_per_second': 302.121, 'eval_steps_per_second': 19.053, 'epoch': 0.76}
{'loss': 0.7572, 'grad_norm': 0.07974766939878464, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7258316874504089, 'eval_runtime': 3.2695, 'eval_samples_per_second': 305.549, 'eval_steps_per_second': 19.269, 'epoch': 0.8}
{'loss': 0.7389, 'grad_norm': 0.08293000608682632, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7233842611312866, 'eval_runtime': 3.2729, 'eval_samples_per_second': 305.237, 'eval_steps_per_second': 19.249, 'epoch': 0.84}
{'loss': 0.7598, 'grad_norm': 0.07458358258008957, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7209413051605225, 'eval_runtime': 3.2627, 'eval_samples_per_second': 306.19, 'eval_steps_per_second': 19.309, 'epoch': 0.88}
{'loss': 0.7176, 'grad_norm': 0.08803774416446686, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7202448844909668, 'eval_runtime': 3.2658, 'eval_samples_per_second': 305.894, 'eval_steps_per_second': 19.291, 'epoch': 0.92}
{'loss': 0.7361, 'grad_norm': 0.08155973255634308, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.719346821308136, 'eval_runtime': 3.2665, 'eval_samples_per_second': 305.833, 'eval_steps_per_second': 19.287, 'epoch': 0.96}
{'loss': 0.7482, 'grad_norm': 0.09766926616430283, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7189682722091675, 'eval_runtime': 3.2693, 'eval_samples_per_second': 305.57, 'eval_steps_per_second': 19.27, 'epoch': 1.0}
{'train_runtime': 252.0993, 'train_samples_per_second': 39.663, 'train_steps_per_second': 2.479, 'train_loss': 1.0336845123291016, 'epoch': 1.0}
train_results:  {'eval_loss': [3.928400993347168, 1.4532841444015503, 0.9996230602264404, 0.8445727229118347, 0.809971809387207, 0.7968291640281677, 0.7869929075241089, 0.7779667973518372, 0.772643506526947, 0.7637637853622437, 0.757919192314148, 0.7549216747283936, 0.7481943964958191, 0.74419105052948, 0.7397834062576294, 0.7363548874855042, 0.7330076694488525, 0.7297987341880798, 0.7267656922340393, 0.7258316874504089, 0.7233842611312866, 0.7209413051605225, 0.7202448844909668, 0.719346821308136, 0.7189682722091675], 'performance': [0.56, 0.43]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:42,  1.03s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 20.24it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 19.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 22.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 24.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.43]
current iteration observed (possibly low-fid or predicted) performance:  1.4815982580184937
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9841 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.9454370380768682
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.946928634141184
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.9074619013136151
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.9472949449142674
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.6320107616195817
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0460, dtype=torch.float64), 0, 0, 0, 0, tensor(0.4537, dtype=torch.float64), tensor(0.5003, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 1, 128, 1.1796119636642291e-17, 1.480000019073488, 1]
normalized proposed parameters for next round by BO: [tensor(0.0460, dtype=torch.float64), tensor(2.3912e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4537, dtype=torch.float64), tensor(0.5003, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.1796e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.046
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.454
  wikitext: 0.5
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.1796119636642291e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  1.1796119636642291e-17
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:09,  1.92s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:23,  3.87it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:12,  6.72it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:10,  7.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  8.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  9.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.14it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.21it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  7.93it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.42it/s]
Evaluation performance at step 25: 0.57
{'loss': 4.0924, 'grad_norm': 0.28139057755470276, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 3.319415330886841, 'eval_runtime': 8.3297, 'eval_samples_per_second': 119.933, 'eval_steps_per_second': 7.563, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:40,  1.02s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.05it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 13.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 13.46it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 14.95it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 14.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 14.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 16.74it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.92it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 17.74it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 16.91it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.01it/s]
Evaluation performance at step 50: 0.57
{'loss': 2.5528, 'grad_norm': 0.24725289642810822, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 1.9925657510757446, 'eval_runtime': 8.3209, 'eval_samples_per_second': 120.058, 'eval_steps_per_second': 7.571, 'epoch': 0.08}
{'loss': 1.8071, 'grad_norm': 0.2221841663122177, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7248764038085938, 'eval_runtime': 8.3752, 'eval_samples_per_second': 119.281, 'eval_steps_per_second': 7.522, 'epoch': 0.12}
{'loss': 1.7413, 'grad_norm': 0.11078885197639465, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5829752683639526, 'eval_runtime': 8.3652, 'eval_samples_per_second': 119.424, 'eval_steps_per_second': 7.531, 'epoch': 0.16}
{'loss': 1.5221, 'grad_norm': 0.12501604855060577, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4920790195465088, 'eval_runtime': 8.389, 'eval_samples_per_second': 119.084, 'eval_steps_per_second': 7.51, 'epoch': 0.2}
{'loss': 1.5348, 'grad_norm': 0.18063847720623016, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.46382474899292, 'eval_runtime': 8.3991, 'eval_samples_per_second': 118.941, 'eval_steps_per_second': 7.501, 'epoch': 0.24}
{'loss': 1.5015, 'grad_norm': 0.08456394076347351, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4447475671768188, 'eval_runtime': 8.3967, 'eval_samples_per_second': 118.975, 'eval_steps_per_second': 7.503, 'epoch': 0.28}
{'loss': 1.4495, 'grad_norm': 0.08141128718852997, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4275184869766235, 'eval_runtime': 8.4073, 'eval_samples_per_second': 118.826, 'eval_steps_per_second': 7.494, 'epoch': 0.32}
{'loss': 1.4871, 'grad_norm': 0.07140281796455383, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4123870134353638, 'eval_runtime': 8.4085, 'eval_samples_per_second': 118.808, 'eval_steps_per_second': 7.492, 'epoch': 0.36}
{'loss': 1.3449, 'grad_norm': 0.11805673688650131, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3981549739837646, 'eval_runtime': 8.4077, 'eval_samples_per_second': 118.82, 'eval_steps_per_second': 7.493, 'epoch': 0.4}
{'loss': 1.4378, 'grad_norm': 0.08234455436468124, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.385675311088562, 'eval_runtime': 8.4243, 'eval_samples_per_second': 118.585, 'eval_steps_per_second': 7.478, 'epoch': 0.44}
{'loss': 1.4457, 'grad_norm': 0.15125156939029694, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.3679227828979492, 'eval_runtime': 8.399, 'eval_samples_per_second': 118.943, 'eval_steps_per_second': 7.501, 'epoch': 0.48}
{'loss': 1.5166, 'grad_norm': 0.13159799575805664, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3537888526916504, 'eval_runtime': 8.4253, 'eval_samples_per_second': 118.571, 'eval_steps_per_second': 7.477, 'epoch': 0.52}
{'loss': 1.4455, 'grad_norm': 0.158424511551857, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3408249616622925, 'eval_runtime': 8.4147, 'eval_samples_per_second': 118.72, 'eval_steps_per_second': 7.487, 'epoch': 0.56}
{'loss': 1.278, 'grad_norm': 0.11548396944999695, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.324387788772583, 'eval_runtime': 8.4549, 'eval_samples_per_second': 118.157, 'eval_steps_per_second': 7.451, 'epoch': 0.6}
{'loss': 1.2293, 'grad_norm': 0.16670429706573486, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3054436445236206, 'eval_runtime': 8.4593, 'eval_samples_per_second': 118.095, 'eval_steps_per_second': 7.447, 'epoch': 0.64}
{'loss': 1.3759, 'grad_norm': 0.142411008477211, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2919429540634155, 'eval_runtime': 8.4586, 'eval_samples_per_second': 118.105, 'eval_steps_per_second': 7.448, 'epoch': 0.68}
{'loss': 1.3339, 'grad_norm': 0.11820094287395477, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2781150341033936, 'eval_runtime': 8.4396, 'eval_samples_per_second': 118.37, 'eval_steps_per_second': 7.465, 'epoch': 0.72}
{'loss': 1.3085, 'grad_norm': 0.11410965770483017, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.2647813558578491, 'eval_runtime': 8.4292, 'eval_samples_per_second': 118.517, 'eval_steps_per_second': 7.474, 'epoch': 0.76}
{'loss': 1.3343, 'grad_norm': 0.14801788330078125, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.2548242807388306, 'eval_runtime': 8.4211, 'eval_samples_per_second': 118.63, 'eval_steps_per_second': 7.481, 'epoch': 0.8}
{'loss': 1.3282, 'grad_norm': 0.1360846608877182, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2477082014083862, 'eval_runtime': 8.4353, 'eval_samples_per_second': 118.431, 'eval_steps_per_second': 7.469, 'epoch': 0.84}
{'loss': 1.3526, 'grad_norm': 0.1359013170003891, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2407021522521973, 'eval_runtime': 8.4195, 'eval_samples_per_second': 118.654, 'eval_steps_per_second': 7.483, 'epoch': 0.88}
{'loss': 1.3077, 'grad_norm': 0.11205535382032394, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.2353975772857666, 'eval_runtime': 8.4016, 'eval_samples_per_second': 118.906, 'eval_steps_per_second': 7.499, 'epoch': 0.92}
{'loss': 1.2721, 'grad_norm': 0.1451702117919922, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2295061349868774, 'eval_runtime': 8.4159, 'eval_samples_per_second': 118.704, 'eval_steps_per_second': 7.486, 'epoch': 0.96}
{'loss': 1.2534, 'grad_norm': 0.17929062247276306, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2283741235733032, 'eval_runtime': 8.4047, 'eval_samples_per_second': 118.862, 'eval_steps_per_second': 7.496, 'epoch': 1.0}
{'train_runtime': 507.6696, 'train_samples_per_second': 19.694, 'train_steps_per_second': 1.231, 'train_loss': 1.570120440673828, 'epoch': 1.0}
train_results:  {'eval_loss': [3.319415330886841, 1.9925657510757446, 1.7248764038085938, 1.5829752683639526, 1.4920790195465088, 1.46382474899292, 1.4447475671768188, 1.4275184869766235, 1.4123870134353638, 1.3981549739837646, 1.385675311088562, 1.3679227828979492, 1.3537888526916504, 1.3408249616622925, 1.324387788772583, 1.3054436445236206, 1.2919429540634155, 1.2781150341033936, 1.2647813558578491, 1.2548242807388306, 1.2477082014083862, 1.2407021522521973, 1.2353975772857666, 1.2295061349868774, 1.2283741235733032], 'performance': [0.57, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:45,  1.06s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.02it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 19.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:09,  5.48it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:04,  8.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.75it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 13.72it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.79it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.4881203174591064
current iteration best possible performance (full train run):  0.6825000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2268 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 1.172434793650564
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.8080540106348317
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 1.1305345906532875
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.9857084590566898
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.9775184070628251
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0356, dtype=torch.float64), tensor(0.5244, dtype=torch.float64), 0, 0, 0, tensor(0.4400, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 3.903127820947816e-19, 1.4800000190734868, 0]
normalized proposed parameters for next round by BO: [tensor(0.0356, dtype=torch.float64), tensor(0.5244, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.6702e-18, dtype=torch.float64), tensor(0.4400, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.9508e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.9031e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.036
  gsm8k: 0.524
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.44
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.903127820947816e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734868,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  3.903127820947816e-19
lora alpha:  1.4800000190734868
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:47,  1.69s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:21,  4.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:11,  7.49it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:09,  8.23it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.25it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.62it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.55it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.67it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.74it/s]
Evaluation performance at step 25: 0.55
{'loss': 3.0666, 'grad_norm': 0.26721277832984924, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 2.4152703285217285, 'eval_runtime': 9.576, 'eval_samples_per_second': 104.324, 'eval_steps_per_second': 6.579, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:46,  1.07s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.34it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.43it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 13.25it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.23it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.69it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.35it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.03it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 10.93it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:01, 10.21it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 12.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.82it/s]
Evaluation performance at step 50: 0.54
{'loss': 1.813, 'grad_norm': 0.23911458253860474, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.54}
{'eval_loss': 1.319190502166748, 'eval_runtime': 9.6128, 'eval_samples_per_second': 103.924, 'eval_steps_per_second': 6.554, 'epoch': 0.08}
{'loss': 1.1353, 'grad_norm': 0.056203484535217285, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0233821868896484, 'eval_runtime': 9.7073, 'eval_samples_per_second': 102.912, 'eval_steps_per_second': 6.49, 'epoch': 0.12}
{'loss': 0.9857, 'grad_norm': 0.06985605508089066, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9285629987716675, 'eval_runtime': 9.7333, 'eval_samples_per_second': 102.638, 'eval_steps_per_second': 6.473, 'epoch': 0.16}
{'loss': 0.9116, 'grad_norm': 0.05354272574186325, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8923872709274292, 'eval_runtime': 9.7492, 'eval_samples_per_second': 102.47, 'eval_steps_per_second': 6.462, 'epoch': 0.2}
{'loss': 0.8739, 'grad_norm': 0.05055135115981102, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8752486109733582, 'eval_runtime': 9.72, 'eval_samples_per_second': 102.778, 'eval_steps_per_second': 6.481, 'epoch': 0.24}
{'loss': 0.8521, 'grad_norm': 0.057746585458517075, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8651039004325867, 'eval_runtime': 9.7391, 'eval_samples_per_second': 102.576, 'eval_steps_per_second': 6.469, 'epoch': 0.28}
{'loss': 0.8735, 'grad_norm': 0.05295321345329285, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8584182262420654, 'eval_runtime': 9.743, 'eval_samples_per_second': 102.535, 'eval_steps_per_second': 6.466, 'epoch': 0.32}
{'loss': 0.8446, 'grad_norm': 0.046760644763708115, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8506810665130615, 'eval_runtime': 9.737, 'eval_samples_per_second': 102.599, 'eval_steps_per_second': 6.47, 'epoch': 0.36}
{'loss': 0.8484, 'grad_norm': 0.04544743523001671, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8466176390647888, 'eval_runtime': 9.7368, 'eval_samples_per_second': 102.601, 'eval_steps_per_second': 6.47, 'epoch': 0.4}
{'loss': 0.8625, 'grad_norm': 0.051220495253801346, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8431400060653687, 'eval_runtime': 9.7336, 'eval_samples_per_second': 102.634, 'eval_steps_per_second': 6.472, 'epoch': 0.44}
{'loss': 0.8534, 'grad_norm': 0.05345376953482628, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8395914435386658, 'eval_runtime': 9.7396, 'eval_samples_per_second': 102.571, 'eval_steps_per_second': 6.468, 'epoch': 0.48}
{'loss': 0.8368, 'grad_norm': 0.04923151433467865, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8346429467201233, 'eval_runtime': 9.746, 'eval_samples_per_second': 102.503, 'eval_steps_per_second': 6.464, 'epoch': 0.52}
{'loss': 0.8367, 'grad_norm': 0.05555503070354462, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8323177695274353, 'eval_runtime': 9.7281, 'eval_samples_per_second': 102.692, 'eval_steps_per_second': 6.476, 'epoch': 0.56}
{'loss': 0.8209, 'grad_norm': 0.06610661000013351, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8296162486076355, 'eval_runtime': 9.7287, 'eval_samples_per_second': 102.685, 'eval_steps_per_second': 6.476, 'epoch': 0.6}
{'loss': 0.8369, 'grad_norm': 0.06340700387954712, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8267946839332581, 'eval_runtime': 9.7616, 'eval_samples_per_second': 102.34, 'eval_steps_per_second': 6.454, 'epoch': 0.64}
{'loss': 0.8481, 'grad_norm': 0.053589947521686554, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8244887590408325, 'eval_runtime': 9.747, 'eval_samples_per_second': 102.493, 'eval_steps_per_second': 6.464, 'epoch': 0.68}
{'loss': 0.828, 'grad_norm': 0.051201216876506805, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8218889832496643, 'eval_runtime': 9.7391, 'eval_samples_per_second': 102.576, 'eval_steps_per_second': 6.469, 'epoch': 0.72}
{'loss': 0.8184, 'grad_norm': 0.0632329136133194, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8197360038757324, 'eval_runtime': 9.7302, 'eval_samples_per_second': 102.67, 'eval_steps_per_second': 6.475, 'epoch': 0.76}
{'loss': 0.8034, 'grad_norm': 0.06935548037290573, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8183823227882385, 'eval_runtime': 9.7281, 'eval_samples_per_second': 102.693, 'eval_steps_per_second': 6.476, 'epoch': 0.8}
{'loss': 0.8133, 'grad_norm': 0.06003311648964882, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8168546557426453, 'eval_runtime': 9.7404, 'eval_samples_per_second': 102.563, 'eval_steps_per_second': 6.468, 'epoch': 0.84}
{'loss': 0.8064, 'grad_norm': 0.07054275274276733, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8156733512878418, 'eval_runtime': 9.735, 'eval_samples_per_second': 102.62, 'eval_steps_per_second': 6.472, 'epoch': 0.88}
{'loss': 0.8205, 'grad_norm': 0.07070042937994003, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8145977258682251, 'eval_runtime': 9.7461, 'eval_samples_per_second': 102.502, 'eval_steps_per_second': 6.464, 'epoch': 0.92}
{'loss': 0.8318, 'grad_norm': 0.05520046502351761, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.814228892326355, 'eval_runtime': 9.7603, 'eval_samples_per_second': 102.354, 'eval_steps_per_second': 6.455, 'epoch': 0.96}
{'loss': 0.8356, 'grad_norm': 0.06874864548444748, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8135782480239868, 'eval_runtime': 9.7424, 'eval_samples_per_second': 102.542, 'eval_steps_per_second': 6.467, 'epoch': 1.0}
{'train_runtime': 568.1065, 'train_samples_per_second': 17.599, 'train_steps_per_second': 1.1, 'train_loss': 0.9862981323242187, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4152703285217285, 1.319190502166748, 1.0233821868896484, 0.9285629987716675, 0.8923872709274292, 0.8752486109733582, 0.8651039004325867, 0.8584182262420654, 0.8506810665130615, 0.8466176390647888, 0.8431400060653687, 0.8395914435386658, 0.8346429467201233, 0.8323177695274353, 0.8296162486076355, 0.8267946839332581, 0.8244887590408325, 0.8218889832496643, 0.8197360038757324, 0.8183823227882385, 0.8168546557426453, 0.8156733512878418, 0.8145977258682251, 0.814228892326355, 0.8135782480239868], 'performance': [0.55, 0.54]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:00,  1.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:27,  3.02it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:10,  6.11it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:05,  8.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.57it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 12.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 17.43it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.82it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.54]
current iteration observed (possibly low-fid or predicted) performance:  1.4866738319396973
current iteration best possible performance (full train run):  0.5880000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0010 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 1.1213172676120817
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 1.0586719614629505
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 1.2694707738012556
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 1.0471696919363942
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.7047831651129315
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0150, dtype=torch.float64), 0, 0, 0, tensor(0.1489, dtype=torch.float64), tensor(0.4856, dtype=torch.float64), tensor(0.3504, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.014785424765510613, 1.480000019073487, 0]
normalized proposed parameters for next round by BO: [tensor(0.0150, dtype=torch.float64), tensor(1.3291e-16, dtype=torch.float64), tensor(4.8017e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1489, dtype=torch.float64), tensor(0.4856, dtype=torch.float64), tensor(0.3504, dtype=torch.float64), tensor(3.8167e-16, dtype=torch.float64), tensor(7.5085e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.1479, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.015
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.149
  truthfulqa_gen: 0.486
  wikitext: 0.35
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.014785424765510613,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.014785424765510613
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:00,  1.83s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.54it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.74it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:09,  8.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.45it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04,  9.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.25it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.88it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.14it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  9.01it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.59, 'grad_norm': 0.5551990866661072, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.5130858421325684, 'eval_runtime': 7.0901, 'eval_samples_per_second': 140.9, 'eval_steps_per_second': 8.886, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:44,  1.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.62it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 14.06it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.86it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 16.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.55it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 17.02it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.96it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 18.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 18.22it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 18.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 19.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.43it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.6264, 'grad_norm': 0.3485497534275055, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.9444204568862915, 'eval_runtime': 7.0889, 'eval_samples_per_second': 140.925, 'eval_steps_per_second': 8.887, 'epoch': 0.08}
{'loss': 1.6819, 'grad_norm': 0.22670002281665802, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.640380859375, 'eval_runtime': 7.1177, 'eval_samples_per_second': 140.354, 'eval_steps_per_second': 8.851, 'epoch': 0.12}
{'loss': 1.5481, 'grad_norm': 0.10294990986585617, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5240241289138794, 'eval_runtime': 7.1442, 'eval_samples_per_second': 139.834, 'eval_steps_per_second': 8.818, 'epoch': 0.16}
{'loss': 1.4306, 'grad_norm': 0.09923330694437027, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.416077733039856, 'eval_runtime': 7.1638, 'eval_samples_per_second': 139.452, 'eval_steps_per_second': 8.794, 'epoch': 0.2}
{'loss': 1.3435, 'grad_norm': 0.07949597388505936, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3700884580612183, 'eval_runtime': 7.1614, 'eval_samples_per_second': 139.498, 'eval_steps_per_second': 8.797, 'epoch': 0.24}
{'loss': 1.444, 'grad_norm': 0.10239950567483902, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3421101570129395, 'eval_runtime': 7.1582, 'eval_samples_per_second': 139.56, 'eval_steps_per_second': 8.801, 'epoch': 0.28}
{'loss': 1.3237, 'grad_norm': 0.07248362898826599, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3204630613327026, 'eval_runtime': 7.153, 'eval_samples_per_second': 139.661, 'eval_steps_per_second': 8.807, 'epoch': 0.32}
{'loss': 1.3029, 'grad_norm': 0.09241782128810883, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3047207593917847, 'eval_runtime': 7.1525, 'eval_samples_per_second': 139.671, 'eval_steps_per_second': 8.808, 'epoch': 0.36}
{'loss': 1.3172, 'grad_norm': 0.10684357583522797, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2929966449737549, 'eval_runtime': 7.1661, 'eval_samples_per_second': 139.406, 'eval_steps_per_second': 8.791, 'epoch': 0.4}
{'loss': 1.3665, 'grad_norm': 0.10984408855438232, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.288295030593872, 'eval_runtime': 7.1626, 'eval_samples_per_second': 139.474, 'eval_steps_per_second': 8.796, 'epoch': 0.44}
{'loss': 1.3567, 'grad_norm': 0.07864511758089066, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2752894163131714, 'eval_runtime': 7.1607, 'eval_samples_per_second': 139.511, 'eval_steps_per_second': 8.798, 'epoch': 0.48}
{'loss': 1.2577, 'grad_norm': 0.1067337766289711, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2693763971328735, 'eval_runtime': 7.1696, 'eval_samples_per_second': 139.338, 'eval_steps_per_second': 8.787, 'epoch': 0.52}
{'loss': 1.4009, 'grad_norm': 0.10272333025932312, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2609721422195435, 'eval_runtime': 7.1854, 'eval_samples_per_second': 139.031, 'eval_steps_per_second': 8.768, 'epoch': 0.56}
{'loss': 1.3085, 'grad_norm': 0.10531191527843475, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.255499243736267, 'eval_runtime': 7.2186, 'eval_samples_per_second': 138.393, 'eval_steps_per_second': 8.727, 'epoch': 0.6}
{'loss': 1.2931, 'grad_norm': 0.1220877543091774, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.250355839729309, 'eval_runtime': 7.2202, 'eval_samples_per_second': 138.361, 'eval_steps_per_second': 8.725, 'epoch': 0.64}
{'loss': 1.3329, 'grad_norm': 0.10531619191169739, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.244344711303711, 'eval_runtime': 7.1975, 'eval_samples_per_second': 138.797, 'eval_steps_per_second': 8.753, 'epoch': 0.68}
{'loss': 1.2593, 'grad_norm': 0.11416281759738922, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2394005060195923, 'eval_runtime': 7.2164, 'eval_samples_per_second': 138.435, 'eval_steps_per_second': 8.73, 'epoch': 0.72}
{'loss': 1.2649, 'grad_norm': 0.09103158861398697, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.234367847442627, 'eval_runtime': 7.2202, 'eval_samples_per_second': 138.363, 'eval_steps_per_second': 8.726, 'epoch': 0.76}
{'loss': 1.2075, 'grad_norm': 0.0818866491317749, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.232081651687622, 'eval_runtime': 7.2218, 'eval_samples_per_second': 138.331, 'eval_steps_per_second': 8.724, 'epoch': 0.8}
{'loss': 1.3155, 'grad_norm': 0.10187654942274094, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.2282081842422485, 'eval_runtime': 7.2207, 'eval_samples_per_second': 138.352, 'eval_steps_per_second': 8.725, 'epoch': 0.84}
{'loss': 1.253, 'grad_norm': 0.10989149659872055, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.2258659601211548, 'eval_runtime': 7.2001, 'eval_samples_per_second': 138.749, 'eval_steps_per_second': 8.75, 'epoch': 0.88}
{'loss': 1.2845, 'grad_norm': 0.09887132793664932, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.224103331565857, 'eval_runtime': 7.208, 'eval_samples_per_second': 138.597, 'eval_steps_per_second': 8.74, 'epoch': 0.92}
{'loss': 1.3079, 'grad_norm': 0.11769867688417435, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.2224795818328857, 'eval_runtime': 7.1969, 'eval_samples_per_second': 138.809, 'eval_steps_per_second': 8.754, 'epoch': 0.96}
{'loss': 1.1876, 'grad_norm': 0.11743312329053879, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.2216929197311401, 'eval_runtime': 7.2073, 'eval_samples_per_second': 138.609, 'eval_steps_per_second': 8.741, 'epoch': 1.0}
{'train_runtime': 452.4116, 'train_samples_per_second': 22.099, 'train_steps_per_second': 1.381, 'train_loss': 1.5201889678955078, 'epoch': 1.0}
train_results:  {'eval_loss': [3.5130858421325684, 1.9444204568862915, 1.640380859375, 1.5240241289138794, 1.416077733039856, 1.3700884580612183, 1.3421101570129395, 1.3204630613327026, 1.3047207593917847, 1.2929966449737549, 1.288295030593872, 1.2752894163131714, 1.2693763971328735, 1.2609721422195435, 1.255499243736267, 1.250355839729309, 1.244344711303711, 1.2394005060195923, 1.234367847442627, 1.232081651687622, 1.2282081842422485, 1.2258659601211548, 1.224103331565857, 1.2224795818328857, 1.2216929197311401], 'performance': [0.56, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:11,  1.93s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  9.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 15.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 15.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 19.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 22.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.12it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.489715337753296
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8350 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.9108138623260015
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.6882651153672543
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 1.1378633209537852
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.6440900634040643
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 1.1718739666468352
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.9953, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 0, 1, 128, 0.011498953512302743, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0047, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9953, dtype=torch.float64), tensor(1.4223e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1150, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.995
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.011498953512302743,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.011498953512302743
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9952
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  995
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:14,  1.97s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  3.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:13,  6.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:10,  6.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  8.11it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:06,  8.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  8.70it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.08it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02,  9.69it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  8.09it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.04it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.1512, 'grad_norm': 0.3578183054924011, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.8946189880371094, 'eval_runtime': 3.3942, 'eval_samples_per_second': 293.147, 'eval_steps_per_second': 18.561, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:41,  2.39it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 12.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:11,  7.33it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.13it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:15,  4.30it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  5.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  7.21it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:09<00:05,  7.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  9.47it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:02, 10.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 12.32it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:00, 12.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 12.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.38it/s]
Evaluation performance at step 50: 0.56
{'loss': 2.559, 'grad_norm': 0.09409866482019424, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.6599594354629517, 'eval_runtime': 3.3899, 'eval_samples_per_second': 293.516, 'eval_steps_per_second': 18.584, 'epoch': 0.08}
{'loss': 1.4432, 'grad_norm': 0.06714693456888199, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.3004791736602783, 'eval_runtime': 3.3863, 'eval_samples_per_second': 293.828, 'eval_steps_per_second': 18.604, 'epoch': 0.12}
{'loss': 1.1905, 'grad_norm': 0.04704858362674713, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.1293003559112549, 'eval_runtime': 3.3945, 'eval_samples_per_second': 293.124, 'eval_steps_per_second': 18.56, 'epoch': 0.16}
{'loss': 1.1025, 'grad_norm': 0.05108548700809479, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.0709034204483032, 'eval_runtime': 3.393, 'eval_samples_per_second': 293.254, 'eval_steps_per_second': 18.568, 'epoch': 0.2}
{'loss': 1.0501, 'grad_norm': 0.05487971752882004, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.0254329442977905, 'eval_runtime': 3.3965, 'eval_samples_per_second': 292.947, 'eval_steps_per_second': 18.548, 'epoch': 0.24}
{'loss': 0.9957, 'grad_norm': 0.06846421211957932, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.9788906574249268, 'eval_runtime': 3.411, 'eval_samples_per_second': 291.703, 'eval_steps_per_second': 18.47, 'epoch': 0.28}
{'loss': 0.9668, 'grad_norm': 0.07664620131254196, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.9203048944473267, 'eval_runtime': 3.3888, 'eval_samples_per_second': 293.612, 'eval_steps_per_second': 18.59, 'epoch': 0.32}
{'loss': 0.8793, 'grad_norm': 0.0771218091249466, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.8305052518844604, 'eval_runtime': 3.3911, 'eval_samples_per_second': 293.413, 'eval_steps_per_second': 18.578, 'epoch': 0.36}
{'loss': 0.7571, 'grad_norm': 0.08283600211143494, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.710453987121582, 'eval_runtime': 3.3917, 'eval_samples_per_second': 293.363, 'eval_steps_per_second': 18.575, 'epoch': 0.4}
{'loss': 0.6911, 'grad_norm': 0.10219467431306839, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.635100245475769, 'eval_runtime': 3.393, 'eval_samples_per_second': 293.253, 'eval_steps_per_second': 18.568, 'epoch': 0.44}
{'loss': 0.594, 'grad_norm': 0.11785785853862762, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.5461449027061462, 'eval_runtime': 3.3933, 'eval_samples_per_second': 293.221, 'eval_steps_per_second': 18.566, 'epoch': 0.48}
{'loss': 0.5124, 'grad_norm': 0.12163063138723373, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.48917505145072937, 'eval_runtime': 3.3909, 'eval_samples_per_second': 293.436, 'eval_steps_per_second': 18.579, 'epoch': 0.52}
{'loss': 0.4777, 'grad_norm': 0.11564278602600098, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.44154974818229675, 'eval_runtime': 3.4037, 'eval_samples_per_second': 292.333, 'eval_steps_per_second': 18.51, 'epoch': 0.56}
{'loss': 0.4625, 'grad_norm': 0.1591043770313263, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.40291309356689453, 'eval_runtime': 3.4228, 'eval_samples_per_second': 290.697, 'eval_steps_per_second': 18.406, 'epoch': 0.6}
{'loss': 0.397, 'grad_norm': 0.17073209583759308, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.3644792437553406, 'eval_runtime': 3.3972, 'eval_samples_per_second': 292.888, 'eval_steps_per_second': 18.545, 'epoch': 0.64}
{'loss': 0.3692, 'grad_norm': 0.1900615394115448, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.3291514217853546, 'eval_runtime': 3.3979, 'eval_samples_per_second': 292.83, 'eval_steps_per_second': 18.541, 'epoch': 0.68}
{'loss': 0.3253, 'grad_norm': 0.1578797996044159, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.30182167887687683, 'eval_runtime': 3.398, 'eval_samples_per_second': 292.822, 'eval_steps_per_second': 18.541, 'epoch': 0.72}
{'loss': 0.3058, 'grad_norm': 0.14703284204006195, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.2753029763698578, 'eval_runtime': 3.4015, 'eval_samples_per_second': 292.517, 'eval_steps_per_second': 18.521, 'epoch': 0.76}
{'loss': 0.2846, 'grad_norm': 0.16010360419750214, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.2557661831378937, 'eval_runtime': 3.4006, 'eval_samples_per_second': 292.594, 'eval_steps_per_second': 18.526, 'epoch': 0.8}
{'loss': 0.2721, 'grad_norm': 0.1896253526210785, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.23800794780254364, 'eval_runtime': 3.4123, 'eval_samples_per_second': 291.594, 'eval_steps_per_second': 18.463, 'epoch': 0.84}
{'loss': 0.2428, 'grad_norm': 0.15155728161334991, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.22557510435581207, 'eval_runtime': 3.3983, 'eval_samples_per_second': 292.796, 'eval_steps_per_second': 18.539, 'epoch': 0.88}
{'loss': 0.2533, 'grad_norm': 0.15112368762493134, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.21595069766044617, 'eval_runtime': 3.4039, 'eval_samples_per_second': 292.31, 'eval_steps_per_second': 18.508, 'epoch': 0.92}
{'loss': 0.2315, 'grad_norm': 0.16918227076530457, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.21048608422279358, 'eval_runtime': 3.3999, 'eval_samples_per_second': 292.652, 'eval_steps_per_second': 18.53, 'epoch': 0.96}
{'train_runtime': 268.5431, 'train_samples_per_second': 37.059, 'train_steps_per_second': 2.316, 'train_loss': 0.8728454588310511, 'epoch': 1.0}
train_results:  {'eval_loss': [3.8946189880371094, 1.6599594354629517, 1.3004791736602783, 1.1293003559112549, 1.0709034204483032, 1.0254329442977905, 0.9788906574249268, 0.9203048944473267, 0.8305052518844604, 0.710453987121582, 0.635100245475769, 0.5461449027061462, 0.48917505145072937, 0.44154974818229675, 0.40291309356689453, 0.3644792437553406, 0.3291514217853546, 0.30182167887687683, 0.2753029763698578, 0.2557661831378937, 0.23800794780254364, 0.22557510435581207, 0.21595069766044617, 0.21048608422279358], 'performance': [0.56, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:09,  1.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.62it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 21.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 24.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 27.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 30.06it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 33.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 27.61it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.4896281957626343
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3489 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 1.0829511903951774
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 1.1383055346567947
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 1.041505798480748
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.8942278884929978
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.5649310065693469
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0348, dtype=torch.float64), 0, 0, 0, tensor(0.0948, dtype=torch.float64), tensor(0.7229, dtype=torch.float64), 0, tensor(0.1475, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 0, 128, 0.0715183488212877, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0348, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.9936e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0948, dtype=torch.float64), tensor(0.7229, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1475, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7152, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.035
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.095
  truthfulqa_gen: 0.723
  wikitext: 0
  mmlu: 0.148
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0715183488212877,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0715183488212877
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:39,  1.61s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:19,  4.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.92it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.34it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.27it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.79it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01, 10.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.28it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.7046, 'grad_norm': 0.4485490322113037, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.4688150882720947, 'eval_runtime': 6.7759, 'eval_samples_per_second': 147.433, 'eval_steps_per_second': 9.298, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:53,  1.86it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.75it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:07<00:26,  2.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:16,  4.13it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:08<00:10,  5.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:09<00:08,  5.93it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:10<00:05,  7.31it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:15<00:10,  3.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:16<00:06,  4.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:16<00:03,  5.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:18<00:02,  5.50it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:18<00:00,  7.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18<00:00,  5.41it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.3352, 'grad_norm': 0.315560907125473, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 1.559987187385559, 'eval_runtime': 6.7598, 'eval_samples_per_second': 147.785, 'eval_steps_per_second': 9.32, 'epoch': 0.08}
{'loss': 1.3324, 'grad_norm': 0.11080418527126312, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1819794178009033, 'eval_runtime': 6.8033, 'eval_samples_per_second': 146.84, 'eval_steps_per_second': 9.26, 'epoch': 0.12}
{'loss': 1.1262, 'grad_norm': 0.1127258837223053, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.032217025756836, 'eval_runtime': 6.7986, 'eval_samples_per_second': 146.943, 'eval_steps_per_second': 9.267, 'epoch': 0.16}
{'loss': 1.0268, 'grad_norm': 0.0822019875049591, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.988777220249176, 'eval_runtime': 6.8345, 'eval_samples_per_second': 146.171, 'eval_steps_per_second': 9.218, 'epoch': 0.2}
{'loss': 0.9642, 'grad_norm': 0.09451624006032944, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9634653329849243, 'eval_runtime': 6.8526, 'eval_samples_per_second': 145.785, 'eval_steps_per_second': 9.194, 'epoch': 0.24}
{'loss': 0.9547, 'grad_norm': 0.08057963103055954, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9468270540237427, 'eval_runtime': 6.8573, 'eval_samples_per_second': 145.683, 'eval_steps_per_second': 9.187, 'epoch': 0.28}
{'loss': 0.9621, 'grad_norm': 0.07282165437936783, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.935897946357727, 'eval_runtime': 6.8751, 'eval_samples_per_second': 145.307, 'eval_steps_per_second': 9.164, 'epoch': 0.32}
{'loss': 0.9675, 'grad_norm': 0.07116232812404633, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9237298965454102, 'eval_runtime': 6.8722, 'eval_samples_per_second': 145.368, 'eval_steps_per_second': 9.167, 'epoch': 0.36}
{'loss': 0.9644, 'grad_norm': 0.08590684086084366, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9126859903335571, 'eval_runtime': 6.8494, 'eval_samples_per_second': 145.852, 'eval_steps_per_second': 9.198, 'epoch': 0.4}
{'loss': 0.9149, 'grad_norm': 0.08507609367370605, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9066745638847351, 'eval_runtime': 6.8766, 'eval_samples_per_second': 145.274, 'eval_steps_per_second': 9.161, 'epoch': 0.44}
{'loss': 0.873, 'grad_norm': 0.08865632116794586, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8997555375099182, 'eval_runtime': 6.8735, 'eval_samples_per_second': 145.34, 'eval_steps_per_second': 9.166, 'epoch': 0.48}
{'loss': 0.8973, 'grad_norm': 0.10628752410411835, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8904256224632263, 'eval_runtime': 6.8731, 'eval_samples_per_second': 145.35, 'eval_steps_per_second': 9.166, 'epoch': 0.52}
{'loss': 0.9046, 'grad_norm': 0.10173621028661728, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8815856575965881, 'eval_runtime': 6.8766, 'eval_samples_per_second': 145.276, 'eval_steps_per_second': 9.162, 'epoch': 0.56}
{'loss': 0.8801, 'grad_norm': 0.08457034081220627, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8765782117843628, 'eval_runtime': 6.8908, 'eval_samples_per_second': 144.975, 'eval_steps_per_second': 9.143, 'epoch': 0.6}
{'loss': 0.8421, 'grad_norm': 0.0966658964753151, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8713809251785278, 'eval_runtime': 6.8635, 'eval_samples_per_second': 145.552, 'eval_steps_per_second': 9.179, 'epoch': 0.64}
{'loss': 0.8877, 'grad_norm': 0.11397582292556763, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8632071018218994, 'eval_runtime': 6.8694, 'eval_samples_per_second': 145.428, 'eval_steps_per_second': 9.171, 'epoch': 0.68}
{'loss': 0.842, 'grad_norm': 0.1195778027176857, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8569624423980713, 'eval_runtime': 6.8782, 'eval_samples_per_second': 145.241, 'eval_steps_per_second': 9.159, 'epoch': 0.72}
{'loss': 0.7878, 'grad_norm': 0.10868827998638153, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8512220978736877, 'eval_runtime': 6.878, 'eval_samples_per_second': 145.246, 'eval_steps_per_second': 9.16, 'epoch': 0.76}
{'loss': 0.9101, 'grad_norm': 0.17370212078094482, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8463201522827148, 'eval_runtime': 6.8802, 'eval_samples_per_second': 145.199, 'eval_steps_per_second': 9.157, 'epoch': 0.8}
{'loss': 0.8748, 'grad_norm': 0.09030813723802567, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8436858057975769, 'eval_runtime': 6.8397, 'eval_samples_per_second': 146.059, 'eval_steps_per_second': 9.211, 'epoch': 0.84}
{'loss': 0.8519, 'grad_norm': 0.09524761885404587, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8373123407363892, 'eval_runtime': 6.8547, 'eval_samples_per_second': 145.74, 'eval_steps_per_second': 9.191, 'epoch': 0.88}
{'loss': 0.8607, 'grad_norm': 0.12283243983983994, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8345726132392883, 'eval_runtime': 6.8649, 'eval_samples_per_second': 145.523, 'eval_steps_per_second': 9.177, 'epoch': 0.92}
{'loss': 0.8675, 'grad_norm': 0.14127632975578308, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8323372602462769, 'eval_runtime': 6.8096, 'eval_samples_per_second': 146.705, 'eval_steps_per_second': 9.252, 'epoch': 0.96}
{'loss': 0.8597, 'grad_norm': 0.11974170058965683, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8315201997756958, 'eval_runtime': 6.8272, 'eval_samples_per_second': 146.326, 'eval_steps_per_second': 9.228, 'epoch': 1.0}
{'train_runtime': 457.0559, 'train_samples_per_second': 21.875, 'train_steps_per_second': 1.367, 'train_loss': 1.1356836791992186, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4688150882720947, 1.559987187385559, 1.1819794178009033, 1.032217025756836, 0.988777220249176, 0.9634653329849243, 0.9468270540237427, 0.935897946357727, 0.9237298965454102, 0.9126859903335571, 0.9066745638847351, 0.8997555375099182, 0.8904256224632263, 0.8815856575965881, 0.8765782117843628, 0.8713809251785278, 0.8632071018218994, 0.8569624423980713, 0.8512220978736877, 0.8463201522827148, 0.8436858057975769, 0.8373123407363892, 0.8345726132392883, 0.8323372602462769, 0.8315201997756958], 'performance': [0.56, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:18,  1.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.13it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 20.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:06,  5.69it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  7.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.73it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.84it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.4902009963989258
current iteration best possible performance (full train run):  0.7244999999999999
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5801 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 1.0078542173196063
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 1.0862794884685685
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.811827033674685
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 1.1148995025150181
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 1.0841115716273617
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0254, dtype=torch.float64), 0, 0, 0, tensor(0.0481, dtype=torch.float64), tensor(0.5192, dtype=torch.float64), 0, 0, tensor(0.4073, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 0.045865767956081084, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0254, dtype=torch.float64), tensor(3.7081e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0481, dtype=torch.float64), tensor(0.5192, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4073, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.4587, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.025
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.048
  truthfulqa_gen: 0.519
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.407

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.045865767956081084,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.045865767956081084
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:40,  1.62s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.54it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.88it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.71it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.37it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.27it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.20it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.29it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.85it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.3885, 'grad_norm': 0.4211876392364502, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.290835380554199, 'eval_runtime': 6.1184, 'eval_samples_per_second': 163.278, 'eval_steps_per_second': 10.297, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:02,  1.58it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.45it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.01it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:21,  3.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  5.03it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  6.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.07it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:02, 12.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 12.61it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 13.35it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 14.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 15.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.77it/s]
Evaluation performance at step 50: 0.52
{'loss': 2.1784, 'grad_norm': 0.3150278627872467, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.3798741102218628, 'eval_runtime': 6.1292, 'eval_samples_per_second': 162.99, 'eval_steps_per_second': 10.279, 'epoch': 0.08}
{'loss': 1.1321, 'grad_norm': 0.10579632222652435, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0281829833984375, 'eval_runtime': 6.1651, 'eval_samples_per_second': 162.041, 'eval_steps_per_second': 10.219, 'epoch': 0.12}
{'loss': 0.9569, 'grad_norm': 0.09377577155828476, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9108810424804688, 'eval_runtime': 6.1967, 'eval_samples_per_second': 161.214, 'eval_steps_per_second': 10.167, 'epoch': 0.16}
{'loss': 0.8763, 'grad_norm': 0.06858428567647934, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8735622763633728, 'eval_runtime': 6.1699, 'eval_samples_per_second': 161.915, 'eval_steps_per_second': 10.211, 'epoch': 0.2}
{'loss': 0.8696, 'grad_norm': 0.05412735417485237, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8599609136581421, 'eval_runtime': 6.1817, 'eval_samples_per_second': 161.606, 'eval_steps_per_second': 10.191, 'epoch': 0.24}
{'loss': 0.8454, 'grad_norm': 0.07210273295640945, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8475452065467834, 'eval_runtime': 6.1968, 'eval_samples_per_second': 161.212, 'eval_steps_per_second': 10.167, 'epoch': 0.28}
{'loss': 0.857, 'grad_norm': 0.05402223393321037, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8370949625968933, 'eval_runtime': 6.2041, 'eval_samples_per_second': 161.022, 'eval_steps_per_second': 10.155, 'epoch': 0.32}
{'loss': 0.8406, 'grad_norm': 0.055993691086769104, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8310580849647522, 'eval_runtime': 6.2049, 'eval_samples_per_second': 161.002, 'eval_steps_per_second': 10.153, 'epoch': 0.36}
{'loss': 0.8099, 'grad_norm': 0.07122120261192322, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.822197675704956, 'eval_runtime': 6.1983, 'eval_samples_per_second': 161.173, 'eval_steps_per_second': 10.164, 'epoch': 0.4}
{'loss': 0.8217, 'grad_norm': 0.06607230007648468, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.81698077917099, 'eval_runtime': 6.2027, 'eval_samples_per_second': 161.06, 'eval_steps_per_second': 10.157, 'epoch': 0.44}
{'loss': 0.8015, 'grad_norm': 0.068339042365551, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8109492659568787, 'eval_runtime': 6.201, 'eval_samples_per_second': 161.103, 'eval_steps_per_second': 10.16, 'epoch': 0.48}
{'loss': 0.7946, 'grad_norm': 0.06997965276241302, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.806876003742218, 'eval_runtime': 6.202, 'eval_samples_per_second': 161.076, 'eval_steps_per_second': 10.158, 'epoch': 0.52}
{'loss': 0.8106, 'grad_norm': 0.08100195974111557, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8016334772109985, 'eval_runtime': 6.2023, 'eval_samples_per_second': 161.069, 'eval_steps_per_second': 10.157, 'epoch': 0.56}
{'loss': 0.8195, 'grad_norm': 0.07318586111068726, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.796908438205719, 'eval_runtime': 6.2096, 'eval_samples_per_second': 160.879, 'eval_steps_per_second': 10.146, 'epoch': 0.6}
{'loss': 0.7912, 'grad_norm': 0.06624174863100052, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.79393470287323, 'eval_runtime': 6.2034, 'eval_samples_per_second': 161.042, 'eval_steps_per_second': 10.156, 'epoch': 0.64}
{'loss': 0.8101, 'grad_norm': 0.08078571408987045, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7883250713348389, 'eval_runtime': 6.2033, 'eval_samples_per_second': 161.044, 'eval_steps_per_second': 10.156, 'epoch': 0.68}
{'loss': 0.7672, 'grad_norm': 0.08824991434812546, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7850740551948547, 'eval_runtime': 6.2072, 'eval_samples_per_second': 160.943, 'eval_steps_per_second': 10.15, 'epoch': 0.72}
{'loss': 0.7783, 'grad_norm': 0.07267805933952332, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7831328511238098, 'eval_runtime': 6.2196, 'eval_samples_per_second': 160.621, 'eval_steps_per_second': 10.129, 'epoch': 0.76}
{'loss': 0.7848, 'grad_norm': 0.07472221553325653, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7797622084617615, 'eval_runtime': 6.2644, 'eval_samples_per_second': 159.474, 'eval_steps_per_second': 10.057, 'epoch': 0.8}
{'loss': 0.7924, 'grad_norm': 0.07898487150669098, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7770322561264038, 'eval_runtime': 6.2316, 'eval_samples_per_second': 160.312, 'eval_steps_per_second': 10.11, 'epoch': 0.84}
{'loss': 0.7663, 'grad_norm': 0.06665800511837006, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7746686935424805, 'eval_runtime': 6.2264, 'eval_samples_per_second': 160.445, 'eval_steps_per_second': 10.118, 'epoch': 0.88}
{'loss': 0.7653, 'grad_norm': 0.08625368028879166, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7732457518577576, 'eval_runtime': 6.2192, 'eval_samples_per_second': 160.632, 'eval_steps_per_second': 10.13, 'epoch': 0.92}
{'loss': 0.783, 'grad_norm': 0.09015771001577377, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7722784280776978, 'eval_runtime': 6.2002, 'eval_samples_per_second': 161.125, 'eval_steps_per_second': 10.161, 'epoch': 0.96}
{'loss': 0.7869, 'grad_norm': 0.09631316363811493, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7715515494346619, 'eval_runtime': 6.2003, 'eval_samples_per_second': 161.12, 'eval_steps_per_second': 10.161, 'epoch': 1.0}
{'train_runtime': 406.8898, 'train_samples_per_second': 24.572, 'train_steps_per_second': 1.536, 'train_loss': 1.0251148406982422, 'epoch': 1.0}
train_results:  {'eval_loss': [3.290835380554199, 1.3798741102218628, 1.0281829833984375, 0.9108810424804688, 0.8735622763633728, 0.8599609136581421, 0.8475452065467834, 0.8370949625968933, 0.8310580849647522, 0.822197675704956, 0.81698077917099, 0.8109492659568787, 0.806876003742218, 0.8016334772109985, 0.796908438205719, 0.79393470287323, 0.7883250713348389, 0.7850740551948547, 0.7831328511238098, 0.7797622084617615, 0.7770322561264038, 0.7746686935424805, 0.7732457518577576, 0.7722784280776978, 0.7715515494346619], 'performance': [0.54, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:55,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:14,  4.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:09<00:09,  5.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:04,  7.57it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 10.21it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 13.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.24it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.4891033172607422
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6369 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 1.0893276451292877
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.9284391419872784
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 1.077580188495559
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.9627729946552712
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.6571059865088602
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0306, dtype=torch.float64), 0, 0, 0, tensor(0.0549, dtype=torch.float64), tensor(0.9145, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.05766034835689902, 1.4800000190734932, 0]
normalized proposed parameters for next round by BO: [tensor(0.0306, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7043e-16, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(0.9145, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5812e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.5766, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.031
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.055
  truthfulqa_gen: 0.914
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05766034835689902,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734932,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.05766034835689902
lora alpha:  1.4800000190734932
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:47,  1.69s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.34it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.56it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.94it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.35it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.73it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.62it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.82it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.79it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.21it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.58it/s]
Evaluation performance at step 25: 0.55
{'loss': 5.0253, 'grad_norm': 0.4910275340080261, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.639173746109009, 'eval_runtime': 4.7313, 'eval_samples_per_second': 211.146, 'eval_steps_per_second': 13.316, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:46,  1.07s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.50it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:04, 13.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.37it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.74it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.85it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.68it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 13.46it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 14.87it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.37it/s]
Evaluation performance at step 50: 0.47
{'loss': 2.3391, 'grad_norm': 0.4070436656475067, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.47}
{'eval_loss': 1.4374322891235352, 'eval_runtime': 3.8184, 'eval_samples_per_second': 261.625, 'eval_steps_per_second': 16.499, 'epoch': 0.08}
{'loss': 1.1846, 'grad_norm': 0.14877468347549438, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0272499322891235, 'eval_runtime': 3.8178, 'eval_samples_per_second': 261.668, 'eval_steps_per_second': 16.502, 'epoch': 0.12}
{'loss': 0.9648, 'grad_norm': 0.12940442562103271, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8598067164421082, 'eval_runtime': 3.8222, 'eval_samples_per_second': 261.365, 'eval_steps_per_second': 16.482, 'epoch': 0.16}
{'loss': 0.8158, 'grad_norm': 0.06543926149606705, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8167532682418823, 'eval_runtime': 3.8274, 'eval_samples_per_second': 261.012, 'eval_steps_per_second': 16.46, 'epoch': 0.2}
{'loss': 0.8043, 'grad_norm': 0.09382935613393784, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7899982333183289, 'eval_runtime': 3.8359, 'eval_samples_per_second': 260.433, 'eval_steps_per_second': 16.424, 'epoch': 0.24}
{'loss': 0.7515, 'grad_norm': 0.06799495220184326, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7691957950592041, 'eval_runtime': 3.8238, 'eval_samples_per_second': 261.258, 'eval_steps_per_second': 16.476, 'epoch': 0.28}
{'loss': 0.7238, 'grad_norm': 0.08992889523506165, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7544721364974976, 'eval_runtime': 3.8178, 'eval_samples_per_second': 261.671, 'eval_steps_per_second': 16.502, 'epoch': 0.32}
{'loss': 0.6968, 'grad_norm': 0.08230967074632645, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7427327036857605, 'eval_runtime': 3.8311, 'eval_samples_per_second': 260.764, 'eval_steps_per_second': 16.445, 'epoch': 0.36}
{'loss': 0.7248, 'grad_norm': 0.09289295971393585, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7249284386634827, 'eval_runtime': 3.8342, 'eval_samples_per_second': 260.55, 'eval_steps_per_second': 16.431, 'epoch': 0.4}
{'loss': 0.7165, 'grad_norm': 0.09300703555345535, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.712284505367279, 'eval_runtime': 3.8447, 'eval_samples_per_second': 259.841, 'eval_steps_per_second': 16.386, 'epoch': 0.44}
{'loss': 0.7089, 'grad_norm': 0.11101777106523514, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7004167437553406, 'eval_runtime': 3.8591, 'eval_samples_per_second': 258.866, 'eval_steps_per_second': 16.325, 'epoch': 0.48}
{'loss': 0.701, 'grad_norm': 0.10405970364809036, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6866005659103394, 'eval_runtime': 3.8482, 'eval_samples_per_second': 259.603, 'eval_steps_per_second': 16.371, 'epoch': 0.52}
{'loss': 0.6586, 'grad_norm': 0.12587545812129974, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.674983561038971, 'eval_runtime': 3.8531, 'eval_samples_per_second': 259.275, 'eval_steps_per_second': 16.351, 'epoch': 0.56}
{'loss': 0.6446, 'grad_norm': 0.13747510313987732, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6623772382736206, 'eval_runtime': 3.8557, 'eval_samples_per_second': 259.099, 'eval_steps_per_second': 16.34, 'epoch': 0.6}
{'loss': 0.6621, 'grad_norm': 0.12283157557249069, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6517268419265747, 'eval_runtime': 3.8586, 'eval_samples_per_second': 258.905, 'eval_steps_per_second': 16.327, 'epoch': 0.64}
{'loss': 0.6636, 'grad_norm': 0.14625723659992218, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6430282592773438, 'eval_runtime': 3.8467, 'eval_samples_per_second': 259.701, 'eval_steps_per_second': 16.378, 'epoch': 0.68}
{'loss': 0.6165, 'grad_norm': 0.1315278261899948, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6317294836044312, 'eval_runtime': 3.8424, 'eval_samples_per_second': 259.993, 'eval_steps_per_second': 16.396, 'epoch': 0.72}
{'loss': 0.6528, 'grad_norm': 0.1504938304424286, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6233028769493103, 'eval_runtime': 3.8529, 'eval_samples_per_second': 259.282, 'eval_steps_per_second': 16.351, 'epoch': 0.76}
{'loss': 0.6048, 'grad_norm': 0.17427656054496765, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6161788105964661, 'eval_runtime': 3.8394, 'eval_samples_per_second': 260.197, 'eval_steps_per_second': 16.409, 'epoch': 0.8}
{'loss': 0.6407, 'grad_norm': 0.16889700293540955, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6058140397071838, 'eval_runtime': 3.8381, 'eval_samples_per_second': 260.288, 'eval_steps_per_second': 16.415, 'epoch': 0.84}
{'loss': 0.5952, 'grad_norm': 0.1784832626581192, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5990237593650818, 'eval_runtime': 3.8331, 'eval_samples_per_second': 260.625, 'eval_steps_per_second': 16.436, 'epoch': 0.88}
{'loss': 0.6234, 'grad_norm': 0.17369592189788818, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5934842824935913, 'eval_runtime': 3.8477, 'eval_samples_per_second': 259.637, 'eval_steps_per_second': 16.373, 'epoch': 0.92}
{'loss': 0.5925, 'grad_norm': 0.1781006157398224, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5900686979293823, 'eval_runtime': 3.8243, 'eval_samples_per_second': 261.223, 'eval_steps_per_second': 16.474, 'epoch': 0.96}
{'loss': 0.5683, 'grad_norm': 0.20158851146697998, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5887801051139832, 'eval_runtime': 3.8308, 'eval_samples_per_second': 260.783, 'eval_steps_per_second': 16.446, 'epoch': 1.0}
{'train_runtime': 286.5358, 'train_samples_per_second': 34.896, 'train_steps_per_second': 2.181, 'train_loss': 0.9472111282348633, 'epoch': 1.0}
train_results:  {'eval_loss': [3.639173746109009, 1.4374322891235352, 1.0272499322891235, 0.8598067164421082, 0.8167532682418823, 0.7899982333183289, 0.7691957950592041, 0.7544721364974976, 0.7427327036857605, 0.7249284386634827, 0.712284505367279, 0.7004167437553406, 0.6866005659103394, 0.674983561038971, 0.6623772382736206, 0.6517268419265747, 0.6430282592773438, 0.6317294836044312, 0.6233028769493103, 0.6161788105964661, 0.6058140397071838, 0.5990237593650818, 0.5934842824935913, 0.5900686979293823, 0.5887801051139832], 'performance': [0.55, 0.47]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:25,  1.16it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.79it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 21.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 24.83it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 28.17it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 29.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 32.42it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 26.65it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.47]
current iteration observed (possibly low-fid or predicted) performance:  1.4934120178222656
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.5247 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 0.9890189212659461
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.8116223063976347
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.9879131510860113
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.96933441150441
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.7282847627790258
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0260, dtype=torch.float64), 0, tensor(0.2771, dtype=torch.float64), tensor(0.0844, dtype=torch.float64), tensor(0.0593, dtype=torch.float64), tensor(0.4703, dtype=torch.float64), 0, tensor(0.0829, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 1, 128, 0.04439906689195769, 1.4800000190734888, 1]
normalized proposed parameters for next round by BO: [tensor(0.0260, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2771, dtype=torch.float64), tensor(0.0844, dtype=torch.float64), tensor(0.0593, dtype=torch.float64), tensor(0.4703, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0829, dtype=torch.float64), tensor(5.1083e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4440, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.026
  gsm8k: 0
  rowan_hellaswag: 0.277
  sciq: 0.084
  triviaqa: 0.059
  truthfulqa_gen: 0.47
  wikitext: 0
  mmlu: 0.083
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04439906689195769,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734888,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.04439906689195769
lora alpha:  1.4800000190734888
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:59,  1.81s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  4.03it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.22it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.84it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.27it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.03it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:04, 10.48it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.46it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.20it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.54it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  9.02it/s]
Evaluation performance at step 25: 0.56
{'loss': 4.3906, 'grad_norm': 0.3252737820148468, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.4000651836395264, 'eval_runtime': 9.9509, 'eval_samples_per_second': 100.393, 'eval_steps_per_second': 6.331, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:42,  2.35it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 11.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 13.90it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:24,  3.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:14,  4.56it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.15it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.77it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  9.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 10.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 11.83it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:00, 11.26it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 12.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.61it/s]
Evaluation performance at step 50: 0.55
{'loss': 2.5915, 'grad_norm': 0.24026069045066833, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 2.0031447410583496, 'eval_runtime': 9.9107, 'eval_samples_per_second': 100.8, 'eval_steps_per_second': 6.357, 'epoch': 0.08}
{'loss': 1.7961, 'grad_norm': 0.08338287472724915, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6253077983856201, 'eval_runtime': 9.9751, 'eval_samples_per_second': 100.15, 'eval_steps_per_second': 6.316, 'epoch': 0.12}
{'loss': 1.5273, 'grad_norm': 0.07777385413646698, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4865801334381104, 'eval_runtime': 10.0843, 'eval_samples_per_second': 99.065, 'eval_steps_per_second': 6.247, 'epoch': 0.16}
{'loss': 1.4538, 'grad_norm': 0.05940517038106918, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4449800252914429, 'eval_runtime': 10.0636, 'eval_samples_per_second': 99.268, 'eval_steps_per_second': 6.26, 'epoch': 0.2}
{'loss': 1.4729, 'grad_norm': 0.060510553419589996, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4193856716156006, 'eval_runtime': 10.0659, 'eval_samples_per_second': 99.246, 'eval_steps_per_second': 6.259, 'epoch': 0.24}
{'loss': 1.4195, 'grad_norm': 0.05731397122144699, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4048542976379395, 'eval_runtime': 10.0678, 'eval_samples_per_second': 99.227, 'eval_steps_per_second': 6.258, 'epoch': 0.28}
{'loss': 1.4021, 'grad_norm': 0.06406821310520172, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3939237594604492, 'eval_runtime': 10.026, 'eval_samples_per_second': 99.641, 'eval_steps_per_second': 6.284, 'epoch': 0.32}
{'loss': 1.3475, 'grad_norm': 0.05138310790061951, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.383750557899475, 'eval_runtime': 10.0113, 'eval_samples_per_second': 99.788, 'eval_steps_per_second': 6.293, 'epoch': 0.36}
{'loss': 1.3564, 'grad_norm': 0.05824728310108185, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3757920265197754, 'eval_runtime': 10.0132, 'eval_samples_per_second': 99.768, 'eval_steps_per_second': 6.292, 'epoch': 0.4}
{'loss': 1.3389, 'grad_norm': 0.05905090272426605, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.367824912071228, 'eval_runtime': 10.0169, 'eval_samples_per_second': 99.731, 'eval_steps_per_second': 6.289, 'epoch': 0.44}
{'loss': 1.4402, 'grad_norm': 0.05889542028307915, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.36037015914917, 'eval_runtime': 10.0445, 'eval_samples_per_second': 99.457, 'eval_steps_per_second': 6.272, 'epoch': 0.48}
{'loss': 1.3413, 'grad_norm': 0.07194564491510391, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.353745698928833, 'eval_runtime': 10.0907, 'eval_samples_per_second': 99.003, 'eval_steps_per_second': 6.243, 'epoch': 0.52}
{'loss': 1.3092, 'grad_norm': 0.06582910567522049, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3480055332183838, 'eval_runtime': 10.0778, 'eval_samples_per_second': 99.129, 'eval_steps_per_second': 6.251, 'epoch': 0.56}
{'loss': 1.3573, 'grad_norm': 0.0787418782711029, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.341496229171753, 'eval_runtime': 10.0802, 'eval_samples_per_second': 99.105, 'eval_steps_per_second': 6.25, 'epoch': 0.6}
{'loss': 1.3467, 'grad_norm': 0.05916711315512657, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.3357861042022705, 'eval_runtime': 10.0646, 'eval_samples_per_second': 99.259, 'eval_steps_per_second': 6.26, 'epoch': 0.64}
{'loss': 1.3595, 'grad_norm': 0.06843863427639008, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3302637338638306, 'eval_runtime': 10.0526, 'eval_samples_per_second': 99.377, 'eval_steps_per_second': 6.267, 'epoch': 0.68}
{'loss': 1.2486, 'grad_norm': 0.060805659741163254, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3225312232971191, 'eval_runtime': 10.0429, 'eval_samples_per_second': 99.473, 'eval_steps_per_second': 6.273, 'epoch': 0.72}
{'loss': 1.3699, 'grad_norm': 0.0797165036201477, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3162627220153809, 'eval_runtime': 10.0231, 'eval_samples_per_second': 99.669, 'eval_steps_per_second': 6.285, 'epoch': 0.76}
{'loss': 1.2667, 'grad_norm': 0.0790891945362091, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3127609491348267, 'eval_runtime': 10.022, 'eval_samples_per_second': 99.681, 'eval_steps_per_second': 6.286, 'epoch': 0.8}
{'loss': 1.3307, 'grad_norm': 0.06520746648311615, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.308714509010315, 'eval_runtime': 10.0265, 'eval_samples_per_second': 99.636, 'eval_steps_per_second': 6.283, 'epoch': 0.84}
{'loss': 1.3222, 'grad_norm': 0.0808403268456459, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3050110340118408, 'eval_runtime': 10.0162, 'eval_samples_per_second': 99.738, 'eval_steps_per_second': 6.29, 'epoch': 0.88}
{'loss': 1.3389, 'grad_norm': 0.06175798922777176, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3027693033218384, 'eval_runtime': 10.025, 'eval_samples_per_second': 99.651, 'eval_steps_per_second': 6.284, 'epoch': 0.92}
{'loss': 1.3036, 'grad_norm': 0.06584420800209045, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.300736427307129, 'eval_runtime': 10.0239, 'eval_samples_per_second': 99.662, 'eval_steps_per_second': 6.285, 'epoch': 0.96}
{'loss': 1.2867, 'grad_norm': 0.09436296671628952, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.300082802772522, 'eval_runtime': 10.0214, 'eval_samples_per_second': 99.687, 'eval_steps_per_second': 6.287, 'epoch': 1.0}
{'train_runtime': 595.4612, 'train_samples_per_second': 16.789, 'train_steps_per_second': 1.05, 'train_loss': 1.5487164520263672, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4000651836395264, 2.0031447410583496, 1.6253077983856201, 1.4865801334381104, 1.4449800252914429, 1.4193856716156006, 1.4048542976379395, 1.3939237594604492, 1.383750557899475, 1.3757920265197754, 1.367824912071228, 1.36037015914917, 1.353745698928833, 1.3480055332183838, 1.341496229171753, 1.3357861042022705, 1.3302637338638306, 1.3225312232971191, 1.3162627220153809, 1.3127609491348267, 1.308714509010315, 1.3050110340118408, 1.3027693033218384, 1.300736427307129, 1.300082802772522], 'performance': [0.56, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:59,  1.20s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 12.21it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 13.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 16.94it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 20.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.40it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.4869228601455688
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3586 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.9634610557765293
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.8184895228820898
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.7196986137292561
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.6670284431119278
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 1.155960118396667
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0295, dtype=torch.float64), 0, 0, 0, tensor(0.0797, dtype=torch.float64), tensor(0.5653, dtype=torch.float64), 0, tensor(0.3255, dtype=torch.float64), 0, 32, 0, 1, 0, 0, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0295, dtype=torch.float64), tensor(5.6557e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.0399e-17, dtype=torch.float64), tensor(0.0797, dtype=torch.float64), tensor(0.5653, dtype=torch.float64), tensor(6.0474e-18, dtype=torch.float64), tensor(0.3255, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.029
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.08
  truthfulqa_gen: 0.565
  wikitext: 0
  mmlu: 0.326
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:38,  1.60s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.53it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.63it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.25it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.34it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 11.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.70it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.18it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.46it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.73it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.433, 'grad_norm': 0.3076721429824829, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.559105396270752, 'eval_runtime': 7.6406, 'eval_samples_per_second': 130.748, 'eval_steps_per_second': 8.245, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:44,  2.23it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:20,  3.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  5.13it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  7.00it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  8.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  8.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 11.37it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 12.67it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 13.81it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 13.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 13.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.74it/s]
Evaluation performance at step 50: 0.57
{'loss': 2.4905, 'grad_norm': 0.10931433737277985, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 1.8644282817840576, 'eval_runtime': 7.6075, 'eval_samples_per_second': 131.318, 'eval_steps_per_second': 8.281, 'epoch': 0.08}
{'loss': 1.6601, 'grad_norm': 0.058746807277202606, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.545868158340454, 'eval_runtime': 7.655, 'eval_samples_per_second': 130.502, 'eval_steps_per_second': 8.23, 'epoch': 0.12}
{'loss': 1.492, 'grad_norm': 0.04667208716273308, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4008742570877075, 'eval_runtime': 7.6604, 'eval_samples_per_second': 130.411, 'eval_steps_per_second': 8.224, 'epoch': 0.16}
{'loss': 1.3627, 'grad_norm': 0.05615503340959549, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3456072807312012, 'eval_runtime': 7.683, 'eval_samples_per_second': 130.027, 'eval_steps_per_second': 8.2, 'epoch': 0.2}
{'loss': 1.318, 'grad_norm': 0.055014718323946, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3053603172302246, 'eval_runtime': 7.6743, 'eval_samples_per_second': 130.175, 'eval_steps_per_second': 8.209, 'epoch': 0.24}
{'loss': 1.313, 'grad_norm': 0.05670962482690811, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2707600593566895, 'eval_runtime': 7.6757, 'eval_samples_per_second': 130.151, 'eval_steps_per_second': 8.208, 'epoch': 0.28}
{'loss': 1.2291, 'grad_norm': 0.05215320736169815, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2343889474868774, 'eval_runtime': 7.6965, 'eval_samples_per_second': 129.799, 'eval_steps_per_second': 8.185, 'epoch': 0.32}
{'loss': 1.2888, 'grad_norm': 0.06695585697889328, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1928917169570923, 'eval_runtime': 7.6938, 'eval_samples_per_second': 129.845, 'eval_steps_per_second': 8.188, 'epoch': 0.36}
{'loss': 1.1808, 'grad_norm': 0.0856253057718277, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1257328987121582, 'eval_runtime': 7.6889, 'eval_samples_per_second': 129.927, 'eval_steps_per_second': 8.194, 'epoch': 0.4}
{'loss': 1.1057, 'grad_norm': 0.06785933673381805, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0632054805755615, 'eval_runtime': 7.6976, 'eval_samples_per_second': 129.78, 'eval_steps_per_second': 8.184, 'epoch': 0.44}
{'loss': 1.0416, 'grad_norm': 0.0747113972902298, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0231695175170898, 'eval_runtime': 7.7043, 'eval_samples_per_second': 129.667, 'eval_steps_per_second': 8.177, 'epoch': 0.48}
{'loss': 0.9966, 'grad_norm': 0.062263574451208115, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9817079305648804, 'eval_runtime': 7.693, 'eval_samples_per_second': 129.858, 'eval_steps_per_second': 8.189, 'epoch': 0.52}
{'loss': 1.0008, 'grad_norm': 0.08097287267446518, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9663745760917664, 'eval_runtime': 7.7013, 'eval_samples_per_second': 129.719, 'eval_steps_per_second': 8.18, 'epoch': 0.56}
{'loss': 1.0246, 'grad_norm': 0.05779341608285904, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9582279324531555, 'eval_runtime': 7.6933, 'eval_samples_per_second': 129.853, 'eval_steps_per_second': 8.189, 'epoch': 0.6}
{'loss': 0.9647, 'grad_norm': 0.0673103854060173, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9472687840461731, 'eval_runtime': 7.6999, 'eval_samples_per_second': 129.743, 'eval_steps_per_second': 8.182, 'epoch': 0.64}
{'loss': 0.9772, 'grad_norm': 0.08408443629741669, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.940861165523529, 'eval_runtime': 7.7434, 'eval_samples_per_second': 129.013, 'eval_steps_per_second': 8.136, 'epoch': 0.68}
{'loss': 0.9328, 'grad_norm': 0.09920281916856766, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9331843852996826, 'eval_runtime': 7.7752, 'eval_samples_per_second': 128.486, 'eval_steps_per_second': 8.103, 'epoch': 0.72}
{'loss': 0.9159, 'grad_norm': 0.06740226596593857, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9245688319206238, 'eval_runtime': 7.7355, 'eval_samples_per_second': 129.144, 'eval_steps_per_second': 8.144, 'epoch': 0.76}
{'loss': 0.9482, 'grad_norm': 0.07385522127151489, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9166573882102966, 'eval_runtime': 7.7381, 'eval_samples_per_second': 129.101, 'eval_steps_per_second': 8.142, 'epoch': 0.8}
{'loss': 0.9741, 'grad_norm': 0.09517938643693924, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9111226797103882, 'eval_runtime': 7.7354, 'eval_samples_per_second': 129.147, 'eval_steps_per_second': 8.144, 'epoch': 0.84}
{'loss': 0.957, 'grad_norm': 0.06913914531469345, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9053020477294922, 'eval_runtime': 7.7735, 'eval_samples_per_second': 128.514, 'eval_steps_per_second': 8.104, 'epoch': 0.88}
{'loss': 0.9814, 'grad_norm': 0.08869371563196182, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9002260565757751, 'eval_runtime': 7.7846, 'eval_samples_per_second': 128.331, 'eval_steps_per_second': 8.093, 'epoch': 0.92}
{'loss': 0.9529, 'grad_norm': 0.10324671864509583, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8985044360160828, 'eval_runtime': 7.8019, 'eval_samples_per_second': 128.045, 'eval_steps_per_second': 8.075, 'epoch': 0.96}
{'loss': 0.9104, 'grad_norm': 0.1049213856458664, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8973714113235474, 'eval_runtime': 7.799, 'eval_samples_per_second': 128.093, 'eval_steps_per_second': 8.078, 'epoch': 1.0}
{'train_runtime': 469.1663, 'train_samples_per_second': 21.31, 'train_steps_per_second': 1.332, 'train_loss': 1.2980707122802735, 'epoch': 1.0}
train_results:  {'eval_loss': [3.559105396270752, 1.8644282817840576, 1.545868158340454, 1.4008742570877075, 1.3456072807312012, 1.3053603172302246, 1.2707600593566895, 1.2343889474868774, 1.1928917169570923, 1.1257328987121582, 1.0632054805755615, 1.0231695175170898, 0.9817079305648804, 0.9663745760917664, 0.9582279324531555, 0.9472687840461731, 0.940861165523529, 0.9331843852996826, 0.9245688319206238, 0.9166573882102966, 0.9111226797103882, 0.9053020477294922, 0.9002260565757751, 0.8985044360160828, 0.8973714113235474], 'performance': [0.54, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:06,  1.49it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 19.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 19.86it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 22.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 19.98it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.4820952415466309
current iteration best possible performance (full train run):  0.672
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688, 1.4820952415466309]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5182 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.9677498901344548
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.9240840010103036
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 1.128465169125973
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 1.1221943813566433
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.6853868130295327
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0157, dtype=torch.float64), 0, 0, 0, 0, tensor(0.9843, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.017511757971282706, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(0.0157, dtype=torch.float64), tensor(1.1579e-16, dtype=torch.float64), tensor(1.8856e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9843, dtype=torch.float64), tensor(8.9999e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1751, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.016
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.984
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.017511757971282706,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.017511757971282706
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:40,  1.62s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.70it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.07it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.04it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.05it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.05it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.73it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.0537, 'grad_norm': 0.5381594896316528, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.626565933227539, 'eval_runtime': 3.9085, 'eval_samples_per_second': 255.598, 'eval_steps_per_second': 16.119, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:05,  1.51it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.12it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 13.62it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 14.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.16it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 17.26it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 20.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 18.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 19.34it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 14.06it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 15.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.43it/s]
Evaluation performance at step 50: 0.45
{'loss': 2.2647, 'grad_norm': 0.4034021198749542, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.45}
{'eval_loss': 1.394336462020874, 'eval_runtime': 3.6779, 'eval_samples_per_second': 271.62, 'eval_steps_per_second': 17.129, 'epoch': 0.08}
{'loss': 1.111, 'grad_norm': 0.11888100206851959, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9842073321342468, 'eval_runtime': 3.6722, 'eval_samples_per_second': 272.048, 'eval_steps_per_second': 17.156, 'epoch': 0.12}
{'loss': 0.9296, 'grad_norm': 0.14021971821784973, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8238789439201355, 'eval_runtime': 3.6854, 'eval_samples_per_second': 271.07, 'eval_steps_per_second': 17.094, 'epoch': 0.16}
{'loss': 0.817, 'grad_norm': 0.06670331954956055, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7751926779747009, 'eval_runtime': 3.6878, 'eval_samples_per_second': 270.891, 'eval_steps_per_second': 17.083, 'epoch': 0.2}
{'loss': 0.7802, 'grad_norm': 0.06799955666065216, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7526354193687439, 'eval_runtime': 3.6704, 'eval_samples_per_second': 272.177, 'eval_steps_per_second': 17.164, 'epoch': 0.24}
{'loss': 0.7415, 'grad_norm': 0.07441027462482452, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7337506413459778, 'eval_runtime': 3.6688, 'eval_samples_per_second': 272.293, 'eval_steps_per_second': 17.172, 'epoch': 0.28}
{'loss': 0.7063, 'grad_norm': 0.07168064266443253, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7159642577171326, 'eval_runtime': 3.6705, 'eval_samples_per_second': 272.169, 'eval_steps_per_second': 17.164, 'epoch': 0.32}
{'loss': 0.7292, 'grad_norm': 0.08005441725254059, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6931341886520386, 'eval_runtime': 3.679, 'eval_samples_per_second': 271.539, 'eval_steps_per_second': 17.124, 'epoch': 0.36}
{'loss': 0.6653, 'grad_norm': 0.09029240906238556, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6770734190940857, 'eval_runtime': 3.6939, 'eval_samples_per_second': 270.443, 'eval_steps_per_second': 17.055, 'epoch': 0.4}
{'loss': 0.666, 'grad_norm': 0.11573243886232376, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6593721508979797, 'eval_runtime': 3.6952, 'eval_samples_per_second': 270.349, 'eval_steps_per_second': 17.049, 'epoch': 0.44}
{'loss': 0.6437, 'grad_norm': 0.09882007539272308, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6436483860015869, 'eval_runtime': 3.6925, 'eval_samples_per_second': 270.545, 'eval_steps_per_second': 17.061, 'epoch': 0.48}
{'loss': 0.6204, 'grad_norm': 0.12217407673597336, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6323622465133667, 'eval_runtime': 3.7107, 'eval_samples_per_second': 269.222, 'eval_steps_per_second': 16.978, 'epoch': 0.52}
{'loss': 0.6293, 'grad_norm': 0.13651752471923828, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6171212792396545, 'eval_runtime': 3.6942, 'eval_samples_per_second': 270.426, 'eval_steps_per_second': 17.054, 'epoch': 0.56}
{'loss': 0.6027, 'grad_norm': 0.11609507352113724, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6040403842926025, 'eval_runtime': 3.6938, 'eval_samples_per_second': 270.451, 'eval_steps_per_second': 17.055, 'epoch': 0.6}
{'loss': 0.6337, 'grad_norm': 0.14175812900066376, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5909882187843323, 'eval_runtime': 3.7067, 'eval_samples_per_second': 269.512, 'eval_steps_per_second': 16.996, 'epoch': 0.64}
{'loss': 0.6118, 'grad_norm': 0.17636671662330627, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5768847465515137, 'eval_runtime': 3.6978, 'eval_samples_per_second': 270.163, 'eval_steps_per_second': 17.037, 'epoch': 0.68}
{'loss': 0.5621, 'grad_norm': 0.16188488900661469, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5676038265228271, 'eval_runtime': 3.7128, 'eval_samples_per_second': 269.068, 'eval_steps_per_second': 16.968, 'epoch': 0.72}
{'loss': 0.576, 'grad_norm': 0.1695793867111206, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.553456723690033, 'eval_runtime': 3.6973, 'eval_samples_per_second': 270.195, 'eval_steps_per_second': 17.039, 'epoch': 0.76}
{'loss': 0.5356, 'grad_norm': 0.16292744874954224, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5442743301391602, 'eval_runtime': 3.7061, 'eval_samples_per_second': 269.558, 'eval_steps_per_second': 16.999, 'epoch': 0.8}
{'loss': 0.552, 'grad_norm': 0.18973878026008606, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5359357595443726, 'eval_runtime': 3.7242, 'eval_samples_per_second': 268.244, 'eval_steps_per_second': 16.916, 'epoch': 0.84}
{'loss': 0.5482, 'grad_norm': 0.1815425455570221, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5268453359603882, 'eval_runtime': 3.7538, 'eval_samples_per_second': 266.13, 'eval_steps_per_second': 16.783, 'epoch': 0.88}
{'loss': 0.5263, 'grad_norm': 0.17036326229572296, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5224534869194031, 'eval_runtime': 3.7455, 'eval_samples_per_second': 266.718, 'eval_steps_per_second': 16.82, 'epoch': 0.92}
{'loss': 0.5291, 'grad_norm': 0.17237532138824463, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5171360373497009, 'eval_runtime': 3.7564, 'eval_samples_per_second': 265.944, 'eval_steps_per_second': 16.771, 'epoch': 0.96}
{'loss': 0.556, 'grad_norm': 0.1945829540491104, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5151407122612, 'eval_runtime': 3.7433, 'eval_samples_per_second': 266.874, 'eval_steps_per_second': 16.83, 'epoch': 1.0}
{'train_runtime': 284.6093, 'train_samples_per_second': 35.132, 'train_steps_per_second': 2.196, 'train_loss': 0.9036535278320312, 'epoch': 1.0}
train_results:  {'eval_loss': [3.626565933227539, 1.394336462020874, 0.9842073321342468, 0.8238789439201355, 0.7751926779747009, 0.7526354193687439, 0.7337506413459778, 0.7159642577171326, 0.6931341886520386, 0.6770734190940857, 0.6593721508979797, 0.6436483860015869, 0.6323622465133667, 0.6171212792396545, 0.6040403842926025, 0.5909882187843323, 0.5768847465515137, 0.5676038265228271, 0.553456723690033, 0.5442743301391602, 0.5359357595443726, 0.5268453359603882, 0.5224534869194031, 0.5171360373497009, 0.5151407122612], 'performance': [0.56, 0.45]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:34,  1.56s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 18.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 21.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 23.37it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 25.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 19.54it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.45]
current iteration observed (possibly low-fid or predicted) performance:  1.4946320056915283
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688, 1.4820952415466309, 1.4946320056915283]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4339 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 1.0346614446587106
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.627956795634662
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 1.0950840821539125
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.8492949276220058
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.603969593710407
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0138, dtype=torch.float64), 0, 0, 0, 0, tensor(0.5969, dtype=torch.float64), tensor(0.3893, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.052621615115908994, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0138, dtype=torch.float64), tensor(7.7331e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5969, dtype=torch.float64), tensor(0.3893, dtype=torch.float64), tensor(8.9196e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.5262, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.014
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.597
  wikitext: 0.389
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.052621615115908994,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.052621615115908994
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:08,  1.30s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:17,  5.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.57it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.85it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.27it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.61it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.43it/s]
Evaluation performance at step 25: 0.58
{'loss': 4.3924, 'grad_norm': 0.35866743326187134, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.58}
{'eval_loss': 3.54077410697937, 'eval_runtime': 6.9791, 'eval_samples_per_second': 143.142, 'eval_steps_per_second': 9.027, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:48,  1.10s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.38it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.86it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 13.31it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.34it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.52it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 17.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 20.44it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 18.27it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 18.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 18.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 18.73it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.99it/s]
Evaluation performance at step 50: 0.53
{'loss': 2.5513, 'grad_norm': 0.3872440457344055, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.53}
{'eval_loss': 1.927101492881775, 'eval_runtime': 6.9759, 'eval_samples_per_second': 143.208, 'eval_steps_per_second': 9.031, 'epoch': 0.08}
{'loss': 1.7083, 'grad_norm': 0.11859250068664551, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6084116697311401, 'eval_runtime': 7.0271, 'eval_samples_per_second': 142.165, 'eval_steps_per_second': 8.965, 'epoch': 0.12}
{'loss': 1.6199, 'grad_norm': 0.1169145330786705, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.491467833518982, 'eval_runtime': 7.0293, 'eval_samples_per_second': 142.119, 'eval_steps_per_second': 8.962, 'epoch': 0.16}
{'loss': 1.445, 'grad_norm': 0.12338684499263763, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3676120042800903, 'eval_runtime': 7.0459, 'eval_samples_per_second': 141.785, 'eval_steps_per_second': 8.941, 'epoch': 0.2}
{'loss': 1.4106, 'grad_norm': 0.07796748727560043, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3219225406646729, 'eval_runtime': 7.0531, 'eval_samples_per_second': 141.639, 'eval_steps_per_second': 8.932, 'epoch': 0.24}
{'loss': 1.3318, 'grad_norm': 0.06107146665453911, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.29342782497406, 'eval_runtime': 7.0562, 'eval_samples_per_second': 141.578, 'eval_steps_per_second': 8.928, 'epoch': 0.28}
{'loss': 1.3256, 'grad_norm': 0.09484027326107025, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.273770809173584, 'eval_runtime': 7.0509, 'eval_samples_per_second': 141.684, 'eval_steps_per_second': 8.935, 'epoch': 0.32}
{'loss': 1.3624, 'grad_norm': 0.0897519662976265, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2613954544067383, 'eval_runtime': 7.0897, 'eval_samples_per_second': 140.908, 'eval_steps_per_second': 8.886, 'epoch': 0.36}
{'loss': 1.2843, 'grad_norm': 0.08226801455020905, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2482109069824219, 'eval_runtime': 7.0784, 'eval_samples_per_second': 141.134, 'eval_steps_per_second': 8.9, 'epoch': 0.4}
{'loss': 1.324, 'grad_norm': 0.09497461467981339, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2414132356643677, 'eval_runtime': 7.0874, 'eval_samples_per_second': 140.954, 'eval_steps_per_second': 8.889, 'epoch': 0.44}
{'loss': 1.1938, 'grad_norm': 0.09038394689559937, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2298253774642944, 'eval_runtime': 7.0781, 'eval_samples_per_second': 141.14, 'eval_steps_per_second': 8.901, 'epoch': 0.48}
{'loss': 1.3155, 'grad_norm': 0.0845925360918045, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2233905792236328, 'eval_runtime': 7.0711, 'eval_samples_per_second': 141.279, 'eval_steps_per_second': 8.909, 'epoch': 0.52}
{'loss': 1.327, 'grad_norm': 0.09268646687269211, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2134931087493896, 'eval_runtime': 7.0658, 'eval_samples_per_second': 141.385, 'eval_steps_per_second': 8.916, 'epoch': 0.56}
{'loss': 1.3109, 'grad_norm': 0.09820569306612015, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2049212455749512, 'eval_runtime': 7.0627, 'eval_samples_per_second': 141.448, 'eval_steps_per_second': 8.92, 'epoch': 0.6}
{'loss': 1.1928, 'grad_norm': 0.15478840470314026, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1985191106796265, 'eval_runtime': 7.0682, 'eval_samples_per_second': 141.338, 'eval_steps_per_second': 8.913, 'epoch': 0.64}
{'loss': 1.2102, 'grad_norm': 0.10899577289819717, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1901540756225586, 'eval_runtime': 7.0752, 'eval_samples_per_second': 141.197, 'eval_steps_per_second': 8.904, 'epoch': 0.68}
{'loss': 1.2227, 'grad_norm': 0.11151626706123352, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.184337854385376, 'eval_runtime': 7.0748, 'eval_samples_per_second': 141.206, 'eval_steps_per_second': 8.905, 'epoch': 0.72}
{'loss': 1.1511, 'grad_norm': 0.18123576045036316, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.178877353668213, 'eval_runtime': 7.0785, 'eval_samples_per_second': 141.132, 'eval_steps_per_second': 8.9, 'epoch': 0.76}
{'loss': 1.2333, 'grad_norm': 0.09583599865436554, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1734803915023804, 'eval_runtime': 7.0806, 'eval_samples_per_second': 141.09, 'eval_steps_per_second': 8.898, 'epoch': 0.8}
{'loss': 1.2412, 'grad_norm': 0.1372261643409729, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1683753728866577, 'eval_runtime': 7.0765, 'eval_samples_per_second': 141.171, 'eval_steps_per_second': 8.903, 'epoch': 0.84}
{'loss': 1.3805, 'grad_norm': 0.08282747119665146, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1654211282730103, 'eval_runtime': 7.0836, 'eval_samples_per_second': 141.029, 'eval_steps_per_second': 8.894, 'epoch': 0.88}
{'loss': 1.2086, 'grad_norm': 0.11865351349115372, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1615089178085327, 'eval_runtime': 7.0985, 'eval_samples_per_second': 140.735, 'eval_steps_per_second': 8.875, 'epoch': 0.92}
{'loss': 1.2225, 'grad_norm': 0.13096821308135986, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1598974466323853, 'eval_runtime': 7.0648, 'eval_samples_per_second': 141.405, 'eval_steps_per_second': 8.917, 'epoch': 0.96}
{'loss': 1.2186, 'grad_norm': 0.1080232784152031, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.159353494644165, 'eval_runtime': 7.0526, 'eval_samples_per_second': 141.651, 'eval_steps_per_second': 8.933, 'epoch': 1.0}
{'train_runtime': 445.1427, 'train_samples_per_second': 22.46, 'train_steps_per_second': 1.404, 'train_loss': 1.487373046875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.54077410697937, 1.927101492881775, 1.6084116697311401, 1.491467833518982, 1.3676120042800903, 1.3219225406646729, 1.29342782497406, 1.273770809173584, 1.2613954544067383, 1.2482109069824219, 1.2414132356643677, 1.2298253774642944, 1.2233905792236328, 1.2134931087493896, 1.2049212455749512, 1.1985191106796265, 1.1901540756225586, 1.184337854385376, 1.178877353668213, 1.1734803915023804, 1.1683753728866577, 1.1654211282730103, 1.1615089178085327, 1.1598974466323853, 1.159353494644165], 'performance': [0.58, 0.53]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:32,  5.17s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:20,  4.06it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:08,  7.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:11<00:10,  4.71it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:11<00:04,  7.06it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:12<00:01,  9.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00, 12.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  7.80it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.58, 0.53]
current iteration observed (possibly low-fid or predicted) performance:  1.4916050434112549
current iteration best possible performance (full train run):  0.63
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688, 1.4820952415466309, 1.4946320056915283, 1.4916050434112549]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8187 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.7742731337897989
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 1.0492723971993678
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.5142305814801574
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.6968855000261993
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 1.152456833592964
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0112, dtype=torch.float64), 0, 0, tensor(0.0875, dtype=torch.float64), tensor(0.0485, dtype=torch.float64), tensor(0.3926, dtype=torch.float64), tensor(0.2976, dtype=torch.float64), 0, tensor(0.1627, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 0.02484031902837845, 1.4800000190734866, 1]
normalized proposed parameters for next round by BO: [tensor(0.0112, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.2092e-17, dtype=torch.float64), tensor(0.0875, dtype=torch.float64), tensor(0.0485, dtype=torch.float64), tensor(0.3926, dtype=torch.float64), tensor(0.2976, dtype=torch.float64), tensor(1.3002e-17, dtype=torch.float64), tensor(0.1627, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2484, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.011
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.087
  triviaqa: 0.049
  truthfulqa_gen: 0.393
  wikitext: 0.298
  mmlu: 0
  arc_challenge: 0.163

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.02484031902837845,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.02484031902837845
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:39,  1.61s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:19,  4.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.78it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.42it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.27it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.20it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.28it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 10.82it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.02it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.3271, 'grad_norm': 0.2695559859275818, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.4104504585266113, 'eval_runtime': 7.3099, 'eval_samples_per_second': 136.663, 'eval_steps_per_second': 8.618, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:49,  1.10s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.65it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 13.42it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.12it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.65it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.72it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 17.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.49it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 18.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 17.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.65it/s]
Evaluation performance at step 50: 0.55
{'loss': 2.5071, 'grad_norm': 0.31103387475013733, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 1.8021295070648193, 'eval_runtime': 7.3145, 'eval_samples_per_second': 136.579, 'eval_steps_per_second': 8.613, 'epoch': 0.08}
{'loss': 1.5973, 'grad_norm': 0.12856841087341309, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.458699345588684, 'eval_runtime': 7.3559, 'eval_samples_per_second': 135.81, 'eval_steps_per_second': 8.565, 'epoch': 0.12}
{'loss': 1.4115, 'grad_norm': 0.09886892139911652, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3587210178375244, 'eval_runtime': 7.3979, 'eval_samples_per_second': 135.038, 'eval_steps_per_second': 8.516, 'epoch': 0.16}
{'loss': 1.382, 'grad_norm': 0.11898390203714371, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2722216844558716, 'eval_runtime': 7.3952, 'eval_samples_per_second': 135.088, 'eval_steps_per_second': 8.519, 'epoch': 0.2}
{'loss': 1.2546, 'grad_norm': 0.062258120626211166, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2330763339996338, 'eval_runtime': 7.3906, 'eval_samples_per_second': 135.172, 'eval_steps_per_second': 8.524, 'epoch': 0.24}
{'loss': 1.2299, 'grad_norm': 0.07957573980093002, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.206254243850708, 'eval_runtime': 7.4123, 'eval_samples_per_second': 134.775, 'eval_steps_per_second': 8.499, 'epoch': 0.28}
{'loss': 1.2584, 'grad_norm': 0.08075529336929321, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1901429891586304, 'eval_runtime': 7.4076, 'eval_samples_per_second': 134.862, 'eval_steps_per_second': 8.505, 'epoch': 0.32}
{'loss': 1.2669, 'grad_norm': 0.08596973121166229, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.179213047027588, 'eval_runtime': 7.4046, 'eval_samples_per_second': 134.916, 'eval_steps_per_second': 8.508, 'epoch': 0.36}
{'loss': 1.2201, 'grad_norm': 0.06928902864456177, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1696559190750122, 'eval_runtime': 7.4101, 'eval_samples_per_second': 134.815, 'eval_steps_per_second': 8.502, 'epoch': 0.4}
{'loss': 1.1751, 'grad_norm': 0.10875160992145538, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.162546157836914, 'eval_runtime': 7.3811, 'eval_samples_per_second': 135.345, 'eval_steps_per_second': 8.535, 'epoch': 0.44}
{'loss': 1.2706, 'grad_norm': 0.09960049390792847, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1556888818740845, 'eval_runtime': 7.3565, 'eval_samples_per_second': 135.799, 'eval_steps_per_second': 8.564, 'epoch': 0.48}
{'loss': 1.1719, 'grad_norm': 0.12353147566318512, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1497700214385986, 'eval_runtime': 7.3481, 'eval_samples_per_second': 135.954, 'eval_steps_per_second': 8.574, 'epoch': 0.52}
{'loss': 1.1614, 'grad_norm': 0.09029503166675568, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.146714687347412, 'eval_runtime': 7.3548, 'eval_samples_per_second': 135.829, 'eval_steps_per_second': 8.566, 'epoch': 0.56}
{'loss': 1.1471, 'grad_norm': 0.08575613796710968, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.141266107559204, 'eval_runtime': 7.359, 'eval_samples_per_second': 135.752, 'eval_steps_per_second': 8.561, 'epoch': 0.6}
{'loss': 1.2077, 'grad_norm': 0.11276566237211227, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1359130144119263, 'eval_runtime': 7.3891, 'eval_samples_per_second': 135.199, 'eval_steps_per_second': 8.526, 'epoch': 0.64}
{'loss': 1.1511, 'grad_norm': 0.056870363652706146, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.132214069366455, 'eval_runtime': 7.4394, 'eval_samples_per_second': 134.286, 'eval_steps_per_second': 8.468, 'epoch': 0.68}
{'loss': 1.1608, 'grad_norm': 0.08136562258005142, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1306959390640259, 'eval_runtime': 7.4223, 'eval_samples_per_second': 134.595, 'eval_steps_per_second': 8.488, 'epoch': 0.72}
{'loss': 1.1682, 'grad_norm': 0.07499082386493683, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.12644624710083, 'eval_runtime': 7.4038, 'eval_samples_per_second': 134.932, 'eval_steps_per_second': 8.509, 'epoch': 0.76}
{'loss': 1.1752, 'grad_norm': 0.09062821418046951, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1245173215866089, 'eval_runtime': 7.3948, 'eval_samples_per_second': 135.095, 'eval_steps_per_second': 8.52, 'epoch': 0.8}
{'loss': 1.1317, 'grad_norm': 0.083941750228405, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1220901012420654, 'eval_runtime': 7.3782, 'eval_samples_per_second': 135.4, 'eval_steps_per_second': 8.539, 'epoch': 0.84}
{'loss': 1.199, 'grad_norm': 0.11842437833547592, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1197234392166138, 'eval_runtime': 7.366, 'eval_samples_per_second': 135.623, 'eval_steps_per_second': 8.553, 'epoch': 0.88}
{'loss': 1.2448, 'grad_norm': 0.08900035917758942, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1186225414276123, 'eval_runtime': 7.3479, 'eval_samples_per_second': 135.956, 'eval_steps_per_second': 8.574, 'epoch': 0.92}
{'loss': 1.1867, 'grad_norm': 0.07561353594064713, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1174688339233398, 'eval_runtime': 7.3476, 'eval_samples_per_second': 135.962, 'eval_steps_per_second': 8.574, 'epoch': 0.96}
{'loss': 1.1584, 'grad_norm': 0.10299613326787949, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1172313690185547, 'eval_runtime': 7.3481, 'eval_samples_per_second': 135.954, 'eval_steps_per_second': 8.574, 'epoch': 1.0}
{'train_runtime': 454.3489, 'train_samples_per_second': 22.001, 'train_steps_per_second': 1.376, 'train_loss': 1.4065904693603515, 'epoch': 1.0}
train_results:  {'eval_loss': [3.4104504585266113, 1.8021295070648193, 1.458699345588684, 1.3587210178375244, 1.2722216844558716, 1.2330763339996338, 1.206254243850708, 1.1901429891586304, 1.179213047027588, 1.1696559190750122, 1.162546157836914, 1.1556888818740845, 1.1497700214385986, 1.146714687347412, 1.141266107559204, 1.1359130144119263, 1.132214069366455, 1.1306959390640259, 1.12644624710083, 1.1245173215866089, 1.1220901012420654, 1.1197234392166138, 1.1186225414276123, 1.1174688339233398, 1.1172313690185547], 'performance': [0.54, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:56,  1.74it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 18.74it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 24.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:05,  8.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 12.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 14.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 19.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.4891393184661865
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688, 1.4820952415466309, 1.4946320056915283, 1.4916050434112549, 1.4891393184661865]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3091 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 1.1437164897702607
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 1.0302386550916018
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.832013527430587
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.8900836872439805
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 1.1070108911126055
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0147, dtype=torch.float64), 0, 0, tensor(0.2790, dtype=torch.float64), tensor(0.0150, dtype=torch.float64), tensor(0.6913, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 1, 0, 128, 0.03919765927266111, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0147, dtype=torch.float64), tensor(1.8196e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2790, dtype=torch.float64), tensor(0.0150, dtype=torch.float64), tensor(0.6913, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3920, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.015
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.279
  triviaqa: 0.015
  truthfulqa_gen: 0.691
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03919765927266111,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.03919765927266111
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:01,  1.83s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  3.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.06it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.11it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  9.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.38it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.22it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  8.07it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.02it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.54it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.1206, 'grad_norm': 0.522861659526825, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.647265672683716, 'eval_runtime': 3.7313, 'eval_samples_per_second': 267.734, 'eval_steps_per_second': 16.884, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:07,  1.47it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 11.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:07,  9.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.82it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 11.84it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.52it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 13.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 15.86it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 14.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 14.36it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 11.28it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 12.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 12.43it/s]
Evaluation performance at step 50: 0.45
{'loss': 2.3054, 'grad_norm': 0.41062721610069275, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.45}
{'eval_loss': 1.4304592609405518, 'eval_runtime': 3.7318, 'eval_samples_per_second': 267.702, 'eval_steps_per_second': 16.882, 'epoch': 0.08}
{'loss': 1.15, 'grad_norm': 0.12151820212602615, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0439532995224, 'eval_runtime': 3.7692, 'eval_samples_per_second': 265.045, 'eval_steps_per_second': 16.715, 'epoch': 0.12}
{'loss': 0.9321, 'grad_norm': 0.13646522164344788, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8883240222930908, 'eval_runtime': 3.7482, 'eval_samples_per_second': 266.528, 'eval_steps_per_second': 16.808, 'epoch': 0.16}
{'loss': 0.8245, 'grad_norm': 0.07046804577112198, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8469627499580383, 'eval_runtime': 3.7713, 'eval_samples_per_second': 264.895, 'eval_steps_per_second': 16.705, 'epoch': 0.2}
{'loss': 0.8177, 'grad_norm': 0.047262076288461685, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8286514282226562, 'eval_runtime': 3.7398, 'eval_samples_per_second': 267.123, 'eval_steps_per_second': 16.846, 'epoch': 0.24}
{'loss': 0.7709, 'grad_norm': 0.08194352686405182, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8143196702003479, 'eval_runtime': 3.7404, 'eval_samples_per_second': 267.082, 'eval_steps_per_second': 16.843, 'epoch': 0.28}
{'loss': 0.7781, 'grad_norm': 0.06802265346050262, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.798879086971283, 'eval_runtime': 3.7418, 'eval_samples_per_second': 266.983, 'eval_steps_per_second': 16.837, 'epoch': 0.32}
{'loss': 0.7846, 'grad_norm': 0.07669521123170853, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7829433083534241, 'eval_runtime': 3.7338, 'eval_samples_per_second': 267.556, 'eval_steps_per_second': 16.873, 'epoch': 0.36}
{'loss': 0.774, 'grad_norm': 0.0721028745174408, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.771898090839386, 'eval_runtime': 3.7391, 'eval_samples_per_second': 267.18, 'eval_steps_per_second': 16.849, 'epoch': 0.4}
{'loss': 0.7483, 'grad_norm': 0.06828872114419937, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7606884241104126, 'eval_runtime': 3.7352, 'eval_samples_per_second': 267.459, 'eval_steps_per_second': 16.867, 'epoch': 0.44}
{'loss': 0.7336, 'grad_norm': 0.07539679110050201, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7527986168861389, 'eval_runtime': 3.7352, 'eval_samples_per_second': 267.455, 'eval_steps_per_second': 16.867, 'epoch': 0.48}
{'loss': 0.7461, 'grad_norm': 0.0921689048409462, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7417806386947632, 'eval_runtime': 3.7339, 'eval_samples_per_second': 267.547, 'eval_steps_per_second': 16.872, 'epoch': 0.52}
{'loss': 0.7397, 'grad_norm': 0.07681050151586533, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7339246273040771, 'eval_runtime': 3.7343, 'eval_samples_per_second': 267.517, 'eval_steps_per_second': 16.87, 'epoch': 0.56}
{'loss': 0.746, 'grad_norm': 0.08742830902338028, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7252015471458435, 'eval_runtime': 3.7325, 'eval_samples_per_second': 267.647, 'eval_steps_per_second': 16.879, 'epoch': 0.6}
{'loss': 0.7079, 'grad_norm': 0.09941377490758896, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7200724482536316, 'eval_runtime': 3.739, 'eval_samples_per_second': 267.181, 'eval_steps_per_second': 16.849, 'epoch': 0.64}
{'loss': 0.7044, 'grad_norm': 0.09165088087320328, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7115735411643982, 'eval_runtime': 3.7507, 'eval_samples_per_second': 266.354, 'eval_steps_per_second': 16.797, 'epoch': 0.68}
{'loss': 0.6876, 'grad_norm': 0.09860684722661972, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7034181356430054, 'eval_runtime': 3.7383, 'eval_samples_per_second': 267.234, 'eval_steps_per_second': 16.853, 'epoch': 0.72}
{'loss': 0.7022, 'grad_norm': 0.10530536621809006, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6993080973625183, 'eval_runtime': 3.7423, 'eval_samples_per_second': 266.947, 'eval_steps_per_second': 16.834, 'epoch': 0.76}
{'loss': 0.6854, 'grad_norm': 0.11549386382102966, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6930469274520874, 'eval_runtime': 3.766, 'eval_samples_per_second': 265.272, 'eval_steps_per_second': 16.729, 'epoch': 0.8}
{'loss': 0.6925, 'grad_norm': 0.15058405697345734, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6886305809020996, 'eval_runtime': 3.7689, 'eval_samples_per_second': 265.067, 'eval_steps_per_second': 16.716, 'epoch': 0.84}
{'loss': 0.6693, 'grad_norm': 0.10818473994731903, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6851673126220703, 'eval_runtime': 3.7552, 'eval_samples_per_second': 266.03, 'eval_steps_per_second': 16.777, 'epoch': 0.88}
{'loss': 0.6844, 'grad_norm': 0.12289910763502121, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.682292640209198, 'eval_runtime': 3.7766, 'eval_samples_per_second': 264.521, 'eval_steps_per_second': 16.681, 'epoch': 0.92}
{'loss': 0.6664, 'grad_norm': 0.11438076943159103, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6798470616340637, 'eval_runtime': 3.7746, 'eval_samples_per_second': 264.662, 'eval_steps_per_second': 16.69, 'epoch': 0.96}
{'loss': 0.6811, 'grad_norm': 0.13548417389392853, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6787642240524292, 'eval_runtime': 3.7718, 'eval_samples_per_second': 264.858, 'eval_steps_per_second': 16.703, 'epoch': 1.0}
{'train_runtime': 284.6162, 'train_samples_per_second': 35.128, 'train_steps_per_second': 2.196, 'train_loss': 0.9941076354980469, 'epoch': 1.0}
train_results:  {'eval_loss': [3.647265672683716, 1.4304592609405518, 1.0439532995224, 0.8883240222930908, 0.8469627499580383, 0.8286514282226562, 0.8143196702003479, 0.798879086971283, 0.7829433083534241, 0.771898090839386, 0.7606884241104126, 0.7527986168861389, 0.7417806386947632, 0.7339246273040771, 0.7252015471458435, 0.7200724482536316, 0.7115735411643982, 0.7034181356430054, 0.6993080973625183, 0.6930469274520874, 0.6886305809020996, 0.6851673126220703, 0.682292640209198, 0.6798470616340637, 0.6787642240524292], 'performance': [0.56, 0.45]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:42,  1.03s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 19.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 19.41it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 22.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 24.58it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 27.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.31it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.45]
current iteration observed (possibly low-fid or predicted) performance:  1.4889425039291382
current iteration best possible performance (full train run):  0.6825000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688, 1.4820952415466309, 1.4946320056915283, 1.4916050434112549, 1.4891393184661865, 1.4889425039291382]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9256 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.7787604296081173
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.9887529637212364
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 1.0113914347432016
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 1.1273838026921719
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 1.1504652821435966
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0233, dtype=torch.float64), 0, tensor(0.1380, dtype=torch.float64), 0, 0, tensor(0.5923, dtype=torch.float64), tensor(0.1396, dtype=torch.float64), 0, tensor(0.1068, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 0.035758179398772835, 1.4800000190734899, 0]
normalized proposed parameters for next round by BO: [tensor(0.0233, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1380, dtype=torch.float64), tensor(8.7698e-18, dtype=torch.float64), tensor(2.3448e-18, dtype=torch.float64), tensor(0.5923, dtype=torch.float64), tensor(0.1396, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1068, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3576, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.023
  gsm8k: 0
  rowan_hellaswag: 0.138
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.592
  wikitext: 0.14
  mmlu: 0
  arc_challenge: 0.107

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.035758179398772835,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734899,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.035758179398772835
lora alpha:  1.4800000190734899
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:40,  1.62s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.52it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  8.14it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.49it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.36it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.85it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.24it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.73it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.26it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.4068, 'grad_norm': 0.4005146026611328, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.561896324157715, 'eval_runtime': 8.5886, 'eval_samples_per_second': 116.316, 'eval_steps_per_second': 7.335, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:31,  1.08it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.79it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:21,  3.42it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:05,  8.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.88it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 11.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 13.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:00, 15.21it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 16.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.49it/s]
Evaluation performance at step 50: 0.55
{'loss': 2.6778, 'grad_norm': 0.24189722537994385, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 2.0187554359436035, 'eval_runtime': 8.6214, 'eval_samples_per_second': 115.874, 'eval_steps_per_second': 7.307, 'epoch': 0.08}
{'loss': 1.8505, 'grad_norm': 0.10536717623472214, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5581029653549194, 'eval_runtime': 8.7021, 'eval_samples_per_second': 114.8, 'eval_steps_per_second': 7.24, 'epoch': 0.12}
{'loss': 1.3667, 'grad_norm': 0.0844196006655693, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4159479141235352, 'eval_runtime': 8.7296, 'eval_samples_per_second': 114.438, 'eval_steps_per_second': 7.217, 'epoch': 0.16}
{'loss': 1.3789, 'grad_norm': 0.07312100380659103, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.3468546867370605, 'eval_runtime': 8.7539, 'eval_samples_per_second': 114.121, 'eval_steps_per_second': 7.197, 'epoch': 0.2}
{'loss': 1.3579, 'grad_norm': 0.0843876525759697, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.3160443305969238, 'eval_runtime': 8.7626, 'eval_samples_per_second': 114.008, 'eval_steps_per_second': 7.19, 'epoch': 0.24}
{'loss': 1.3454, 'grad_norm': 0.0822901576757431, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2949705123901367, 'eval_runtime': 8.7596, 'eval_samples_per_second': 114.046, 'eval_steps_per_second': 7.192, 'epoch': 0.28}
{'loss': 1.2449, 'grad_norm': 0.07943645864725113, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2755695581436157, 'eval_runtime': 8.7671, 'eval_samples_per_second': 113.949, 'eval_steps_per_second': 7.186, 'epoch': 0.32}
{'loss': 1.2968, 'grad_norm': 0.11067903786897659, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2627767324447632, 'eval_runtime': 8.7707, 'eval_samples_per_second': 113.902, 'eval_steps_per_second': 7.183, 'epoch': 0.36}
{'loss': 1.2513, 'grad_norm': 0.0908423662185669, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2494570016860962, 'eval_runtime': 8.7796, 'eval_samples_per_second': 113.787, 'eval_steps_per_second': 7.176, 'epoch': 0.4}
{'loss': 1.2307, 'grad_norm': 0.10541349649429321, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.241004228591919, 'eval_runtime': 8.7898, 'eval_samples_per_second': 113.654, 'eval_steps_per_second': 7.167, 'epoch': 0.44}
{'loss': 1.3659, 'grad_norm': 0.07762854546308517, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2322015762329102, 'eval_runtime': 8.8266, 'eval_samples_per_second': 113.181, 'eval_steps_per_second': 7.138, 'epoch': 0.48}
{'loss': 1.2716, 'grad_norm': 0.07406022399663925, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2255243062973022, 'eval_runtime': 8.7578, 'eval_samples_per_second': 114.069, 'eval_steps_per_second': 7.194, 'epoch': 0.52}
{'loss': 1.19, 'grad_norm': 0.08683052659034729, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2167614698410034, 'eval_runtime': 8.7699, 'eval_samples_per_second': 113.912, 'eval_steps_per_second': 7.184, 'epoch': 0.56}
{'loss': 1.2754, 'grad_norm': 0.10922002792358398, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2108242511749268, 'eval_runtime': 8.7341, 'eval_samples_per_second': 114.379, 'eval_steps_per_second': 7.213, 'epoch': 0.6}
{'loss': 1.2253, 'grad_norm': 0.07718349993228912, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2052645683288574, 'eval_runtime': 8.7359, 'eval_samples_per_second': 114.356, 'eval_steps_per_second': 7.212, 'epoch': 0.64}
{'loss': 1.2634, 'grad_norm': 0.09485848993062973, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.201413869857788, 'eval_runtime': 8.7357, 'eval_samples_per_second': 114.359, 'eval_steps_per_second': 7.212, 'epoch': 0.68}
{'loss': 1.2356, 'grad_norm': 0.09779984503984451, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1971832513809204, 'eval_runtime': 8.7335, 'eval_samples_per_second': 114.387, 'eval_steps_per_second': 7.214, 'epoch': 0.72}
{'loss': 1.2823, 'grad_norm': 0.08096291869878769, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1929928064346313, 'eval_runtime': 8.7378, 'eval_samples_per_second': 114.331, 'eval_steps_per_second': 7.21, 'epoch': 0.76}
{'loss': 1.2716, 'grad_norm': 0.09157509356737137, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.190373420715332, 'eval_runtime': 8.7348, 'eval_samples_per_second': 114.37, 'eval_steps_per_second': 7.212, 'epoch': 0.8}
{'loss': 1.241, 'grad_norm': 0.10675275325775146, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1866481304168701, 'eval_runtime': 8.74, 'eval_samples_per_second': 114.303, 'eval_steps_per_second': 7.208, 'epoch': 0.84}
{'loss': 1.1684, 'grad_norm': 0.08694370090961456, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1838090419769287, 'eval_runtime': 8.7317, 'eval_samples_per_second': 114.411, 'eval_steps_per_second': 7.215, 'epoch': 0.88}
{'loss': 1.1422, 'grad_norm': 0.1290428191423416, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1822620630264282, 'eval_runtime': 8.7324, 'eval_samples_per_second': 114.401, 'eval_steps_per_second': 7.215, 'epoch': 0.92}
{'loss': 1.1806, 'grad_norm': 0.09580184519290924, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1806479692459106, 'eval_runtime': 8.7226, 'eval_samples_per_second': 114.53, 'eval_steps_per_second': 7.223, 'epoch': 0.96}
{'loss': 1.2496, 'grad_norm': 0.09049271792173386, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1800987720489502, 'eval_runtime': 8.7228, 'eval_samples_per_second': 114.528, 'eval_steps_per_second': 7.222, 'epoch': 1.0}
{'train_runtime': 525.9574, 'train_samples_per_second': 19.009, 'train_steps_per_second': 1.188, 'train_loss': 1.470819680786133, 'epoch': 1.0}
train_results:  {'eval_loss': [3.561896324157715, 2.0187554359436035, 1.5581029653549194, 1.4159479141235352, 1.3468546867370605, 1.3160443305969238, 1.2949705123901367, 1.2755695581436157, 1.2627767324447632, 1.2494570016860962, 1.241004228591919, 1.2322015762329102, 1.2255243062973022, 1.2167614698410034, 1.2108242511749268, 1.2052645683288574, 1.201413869857788, 1.1971832513809204, 1.1929928064346313, 1.190373420715332, 1.1866481304168701, 1.1838090419769287, 1.1822620630264282, 1.1806479692459106, 1.1800987720489502], 'performance': [0.55, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:56,  1.74it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 19.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 14.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 18.72it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 22.79it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.01it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.4912524223327637
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688, 1.4820952415466309, 1.4946320056915283, 1.4916050434112549, 1.4891393184661865, 1.4889425039291382, 1.4912524223327637]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4930 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 1.1236063504692049
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.9995732451890893
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.7674241323693355
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 1.097409682206766
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.4281771067587734
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7941, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2059, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 2, 5.344144507667857e-19, 32.6885954376126, 0]
normalized proposed parameters for next round by BO: [tensor(0.7941, dtype=torch.float64), tensor(5.0802e-18, dtype=torch.float64), tensor(1.0491e-17, dtype=torch.float64), tensor(3.2246e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2059, dtype=torch.float64), tensor(1.6385e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(5.3441e-18, dtype=torch.float64), tensor(0.6810, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.794
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.206
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (5.344144507667857e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (32.6885954376126,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  5.344144507667857e-19
lora alpha:  32.6885954376126
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,507,328 || all params: 8,031,768,576 || trainable%: 0.0188
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:58,  1.69it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.96it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.79it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 16.64it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 15.94it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 17.18it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 14.95it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 15.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 19.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 20.48it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 18.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 17.46it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 16.91it/s]
Evaluation performance at step 25: 0.5
{'loss': 3.304, 'grad_norm': 12.900032043457031, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.5}
{'eval_loss': 1.3193047046661377, 'eval_runtime': 5.5168, 'eval_samples_per_second': 181.084, 'eval_steps_per_second': 11.42, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:46,  2.13it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.46it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 18.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 19.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 19.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 21.10it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 21.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 22.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:03<00:01, 22.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:01, 17.99it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 18.77it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 16.44it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.47it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.0387, 'grad_norm': 2.580946207046509, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 0.9084861278533936, 'eval_runtime': 4.8646, 'eval_samples_per_second': 205.362, 'eval_steps_per_second': 12.951, 'epoch': 0.08}
{'loss': 0.8906, 'grad_norm': 1.367695927619934, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8771967887878418, 'eval_runtime': 4.8595, 'eval_samples_per_second': 205.578, 'eval_steps_per_second': 12.964, 'epoch': 0.12}
{'loss': 0.8816, 'grad_norm': 1.3199704885482788, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8575795292854309, 'eval_runtime': 4.9061, 'eval_samples_per_second': 203.624, 'eval_steps_per_second': 12.841, 'epoch': 0.16}
{'loss': 0.8466, 'grad_norm': 1.1514134407043457, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8406038284301758, 'eval_runtime': 4.8959, 'eval_samples_per_second': 204.05, 'eval_steps_per_second': 12.868, 'epoch': 0.2}
{'loss': 0.8399, 'grad_norm': 1.1418887376785278, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8276594281196594, 'eval_runtime': 4.9161, 'eval_samples_per_second': 203.21, 'eval_steps_per_second': 12.815, 'epoch': 0.24}
{'loss': 0.8242, 'grad_norm': 1.384989857673645, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8114900588989258, 'eval_runtime': 4.9211, 'eval_samples_per_second': 203.003, 'eval_steps_per_second': 12.802, 'epoch': 0.28}
{'loss': 0.8069, 'grad_norm': 1.1575850248336792, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8012625575065613, 'eval_runtime': 4.9311, 'eval_samples_per_second': 202.592, 'eval_steps_per_second': 12.776, 'epoch': 0.32}
{'loss': 0.8142, 'grad_norm': 1.2241065502166748, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7901131510734558, 'eval_runtime': 4.9051, 'eval_samples_per_second': 203.664, 'eval_steps_per_second': 12.844, 'epoch': 0.36}
{'loss': 0.796, 'grad_norm': 1.3383426666259766, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7797613143920898, 'eval_runtime': 4.8731, 'eval_samples_per_second': 205.003, 'eval_steps_per_second': 12.928, 'epoch': 0.4}
{'loss': 0.8087, 'grad_norm': 1.2504631280899048, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7683364748954773, 'eval_runtime': 4.8817, 'eval_samples_per_second': 204.641, 'eval_steps_per_second': 12.905, 'epoch': 0.44}
{'loss': 0.7943, 'grad_norm': 1.2388767004013062, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7601662874221802, 'eval_runtime': 4.8723, 'eval_samples_per_second': 205.036, 'eval_steps_per_second': 12.93, 'epoch': 0.48}
{'loss': 0.8007, 'grad_norm': 1.2806583642959595, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7551383972167969, 'eval_runtime': 4.8777, 'eval_samples_per_second': 204.809, 'eval_steps_per_second': 12.916, 'epoch': 0.52}
{'loss': 0.7899, 'grad_norm': 1.106935739517212, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7465164065361023, 'eval_runtime': 4.8692, 'eval_samples_per_second': 205.168, 'eval_steps_per_second': 12.939, 'epoch': 0.56}
{'loss': 0.7861, 'grad_norm': 1.4708198308944702, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7415852546691895, 'eval_runtime': 4.8726, 'eval_samples_per_second': 205.024, 'eval_steps_per_second': 12.929, 'epoch': 0.6}
{'loss': 0.7851, 'grad_norm': 1.3858273029327393, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.735399067401886, 'eval_runtime': 4.8682, 'eval_samples_per_second': 205.209, 'eval_steps_per_second': 12.941, 'epoch': 0.64}
{'loss': 0.7687, 'grad_norm': 1.4838043451309204, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7271910905838013, 'eval_runtime': 4.8673, 'eval_samples_per_second': 205.246, 'eval_steps_per_second': 12.943, 'epoch': 0.68}
{'loss': 0.7604, 'grad_norm': 1.2465547323226929, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7193242907524109, 'eval_runtime': 4.8745, 'eval_samples_per_second': 204.943, 'eval_steps_per_second': 12.924, 'epoch': 0.72}
{'loss': 0.7792, 'grad_norm': 1.2034568786621094, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7186276912689209, 'eval_runtime': 4.8897, 'eval_samples_per_second': 204.307, 'eval_steps_per_second': 12.884, 'epoch': 0.76}
{'loss': 0.7737, 'grad_norm': 1.1720134019851685, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7125014662742615, 'eval_runtime': 4.8737, 'eval_samples_per_second': 204.976, 'eval_steps_per_second': 12.926, 'epoch': 0.8}
{'loss': 0.7474, 'grad_norm': 1.4625974893569946, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.706161379814148, 'eval_runtime': 4.8795, 'eval_samples_per_second': 204.736, 'eval_steps_per_second': 12.911, 'epoch': 0.84}
{'loss': 0.7459, 'grad_norm': 1.4450843334197998, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7047730088233948, 'eval_runtime': 4.8803, 'eval_samples_per_second': 204.701, 'eval_steps_per_second': 12.909, 'epoch': 0.88}
{'loss': 0.7583, 'grad_norm': 1.4133250713348389, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7006910443305969, 'eval_runtime': 4.8721, 'eval_samples_per_second': 205.045, 'eval_steps_per_second': 12.931, 'epoch': 0.92}
{'loss': 0.7455, 'grad_norm': 1.3520512580871582, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6980675458908081, 'eval_runtime': 4.8667, 'eval_samples_per_second': 205.274, 'eval_steps_per_second': 12.945, 'epoch': 0.96}
{'loss': 0.7518, 'grad_norm': 1.2067646980285645, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6976984739303589, 'eval_runtime': 4.8604, 'eval_samples_per_second': 205.541, 'eval_steps_per_second': 12.962, 'epoch': 1.0}
{'train_runtime': 341.9394, 'train_samples_per_second': 29.242, 'train_steps_per_second': 1.828, 'train_loss': 0.9055330078125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3193047046661377, 0.9084861278533936, 0.8771967887878418, 0.8575795292854309, 0.8406038284301758, 0.8276594281196594, 0.8114900588989258, 0.8012625575065613, 0.7901131510734558, 0.7797613143920898, 0.7683364748954773, 0.7601662874221802, 0.7551383972167969, 0.7465164065361023, 0.7415852546691895, 0.735399067401886, 0.7271910905838013, 0.7193242907524109, 0.7186276912689209, 0.7125014662742615, 0.706161379814148, 0.7047730088233948, 0.7006910443305969, 0.6980675458908081, 0.6976984739303589], 'performance': [0.5, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:55,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:04, 20.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 26.44it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 28.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 22.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 25.25it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 30.10it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 26.70it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.5, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2417174577713013
current iteration best possible performance (full train run):  0.672
max performance so far:  0.9345000000000001
BO observations:  [1.4315956830978394, 1.475605845451355, 1.4680230617523193, 1.4795663356781006, 1.4879405498504639, 1.4887475967407227, 1.4820997714996338, 1.4879324436187744, 1.4614763259887695, 1.4583327770233154, 1.4840221405029297, 1.4821724891662598, 1.491274118423462, 1.4915637969970703, 1.4815982580184937, 1.4881203174591064, 1.4866738319396973, 1.489715337753296, 1.4896281957626343, 1.4902009963989258, 1.4891033172607422, 1.4934120178222656, 1.4869228601455688, 1.4820952415466309, 1.4946320056915283, 1.4916050434112549, 1.4891393184661865, 1.4889425039291382, 1.4912524223327637, 1.2417174577713013]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9544 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.7089518015212349
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.7113381906825178
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.6864643136120276
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.7988112810315507
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.829208748698482
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.029121970096282836, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.5998e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.5024e-16, dtype=torch.float64), tensor(3.1961e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2912, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.7665, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/truthfulqa_gen/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 20, 1, 1, 1, 1, 1, 18, 0.05137445104835325, 2, 1]
Checking history sample input_X_between_0_1:  [0.16708828564359726, 0.09336634160940684, 0.10350512196589488, 0.15398330016052955, 0.1962525842274133, 0.03591537560460389, 0.22854278857579274, 0.020177596111769884, 0.0011686061009916141, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.5137445104835324, 0.041666666666666664, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 22, 1, 1, 0, 0, 1, 9, 0.09530863992118319, 22, 1]
Checking history sample input_X_between_0_1:  [0.03247234343008318, 0.24245487089243928, 0.061357950226574434, 0.04484089885610428, 0.29926759724194274, 0.06529999392250918, 0.1187890182417492, 0.06019728484402145, 0.07532004234457618, 0.6875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0703125, 0.9530863992118318, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 7, 0, 0, 0, 0, 1, 45, 0.012049704078718804, 22, 1]
Checking history sample input_X_between_0_1:  [0.21366992907700472, 0.11104122481199606, 0.002660457907293051, 0.2880264070705684, 0.04526911953486935, 0.12731493561980853, 0.03301240364606098, 0.033945532741917354, 0.14505998959048166, 0.21875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3515625, 0.12049704078718804, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 2, 1, 1, 1, 0, 0, 32, 0.03789572912213354, 26, 1]
Checking history sample input_X_between_0_1:  [0.24030658300564717, 0.015362440686023642, 0.05270153812816802, 0.02482836017022575, 0.049799924873618534, 0.08868214077907971, 0.2758786796759193, 0.15337756914576392, 0.09906276353555413, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.3789572912213354, 0.5416666666666666, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 8, 0, 0, 1, 1, 1, 57, 0.05639372568359048, 47, 0]
Checking history sample input_X_between_0_1:  [0.06805041369102573, 0.35045189495637835, 0.0690395640578839, 0.10112132696025768, 0.027676403843167306, 0.022332158667682574, 0.34878042342815946, 0.01112530002940438, 0.0014225143660406552, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4453125, 0.5639372568359048, 0.9791666666666666, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 18, 1, 1, 0, 0, 1, 112, 0.0011300351648876107, 2, 1]
Checking history sample input_X_between_0_1:  [0.37172400144744944, 0.1459238934133121, 0.008808550797885647, 0.002555032774535197, 0.19135215067165537, 0.06798090925258732, 0.006039363021493691, 0.11811063259528111, 0.08750546602580027, 0.5625, 1.0, 1.0, 0.0, 0.0, 1.0, 0.875, 0.011300351648876106, 0.041666666666666664, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 11, 0, 1, 1, 0, 1, 121, 0.04409228366491266, 38, 0]
Checking history sample input_X_between_0_1:  [0.07852170628622454, 0.06029112522468753, 0.11809118966227225, 0.027842471673524886, 0.1411958700188293, 0.07696136185529433, 0.056636940165570304, 0.09016735384699294, 0.3502919812666037, 0.34375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9453125, 0.4409228366491266, 0.7916666666666666, 0.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 19, 1, 0, 0, 1, 0, 20, 0.057419890903339765, 17, 1]
Checking history sample input_X_between_0_1:  [0.10775192425680036, 0.12045634241079212, 0.10464767038398051, 0.06995336417634229, 0.09253145036296805, 0.10519007876181581, 0.01843039754005879, 0.13092246788273074, 0.25011630422451153, 0.59375, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15625, 0.5741989090333977, 0.3541666666666667, 1.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 13, 0, 1, 0, 0, 1, 54, 0.07044921211215552, 48, 0]
Checking history sample input_X_between_0_1:  [0.042071939927891704, 0.11780787387695683, 0.1653426058719347, 0.007708324649593241, 0.2661704338934174, 0.048124532164944306, 0.08775672948923643, 0.2592072487941832, 0.005810311331842126, 0.40625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.421875, 0.7044921211215551, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 9, 1, 0, 1, 1, 0, 17, 0.07776680881547844, 40, 0]
Checking history sample input_X_between_0_1:  [0.15051693664423413, 0.00712249071600852, 0.07021598097833169, 0.24086614709201448, 0.03020264841808187, 0.18414495201728312, 0.10161716030712101, 0.12034390232341542, 0.09496978150350989, 0.28125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1328125, 0.7776680881547844, 0.8333333333333334, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 1, 0, 0, 0, 1, 0, 60, 0.001514616808966751, 14, 1]
Checking history sample input_X_between_0_1:  [0.09006538983671292, 0.19758455993964805, 0.08856073034818206, 0.04362538903651358, 0.00590724587402571, 0.42167645561181427, 0.00125128591584721, 0.06804538970466265, 0.08328355373259344, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.46875, 0.015146168089667511, 0.2916666666666667, 1.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 18, 1, 1, 1, 0, 1, 45, 0.001287895623877422, 34, 1]
Checking history sample input_X_between_0_1:  [0.006498192815382561, 0.16469283153458797, 0.12116125701717094, 0.3527199818139275, 0.08932851012704637, 0.20183373522295348, 0.006725746078318013, 0.015493009322599102, 0.04154673606801424, 0.5625, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3515625, 0.01287895623877422, 0.7083333333333334, 1.0]
Checking history sample performance at 625 steps:  0.67
Checking history sample input_X:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 8, 0, 0, 0, 1, 1, 51, 0.08835721159033366, 35, 0]
Checking history sample input_X_between_0_1:  [0.07644726452305764, 0.21313477232874148, 0.1479305506892248, 0.10962331536159223, 0.02230079581746856, 0.21185968233900423, 0.10787252528015752, 0.04896868695350478, 0.06186240670724876, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3984375, 0.8835721159033366, 0.7291666666666666, 0.0]
Checking history sample performance at 625 steps:  0.55
Checking history sample input_X:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 2, 0, 0, 0, 1, 1, 28, 0.09450434861769766, 16, 1]
Checking history sample input_X_between_0_1:  [0.10191555102343511, 0.0022694228942360907, 0.032743236883600535, 0.0681651514323016, 0.19710984250277971, 0.042715961789089325, 0.17692665809257513, 0.20590208703878002, 0.17225208834320246, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21875, 0.9450434861769765, 0.3333333333333333, 1.0]
Checking history sample performance at 625 steps:  0.51
Checking history sample input_X:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 2, 0, 0, 0, 1, 0, 27, 0.08742966606550949, 14, 0]
Checking history sample input_X_between_0_1:  [0.03163230114564802, 0.09541901023778454, 0.1132788789181554, 0.02456255406723257, 0.18297581700095394, 0.15162537275614957, 0.25777252037484716, 0.05751798525101943, 0.08521556024820945, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2109375, 0.8742966606550948, 0.2916666666666667, 0.0]
Checking history sample performance at 625 steps:  0.58
Checking history sample input_X:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 30, 0, 0, 0, 1, 0, 18, 0.07914275308569024, 23, 0]
Checking history sample input_X_between_0_1:  [0.0343443968707452, 0.1126536871940594, 0.00778551389778648, 0.04064604413024889, 0.11027449623713549, 0.09332447420838605, 0.1900206547159145, 0.15041098406844958, 0.2605397486772744, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.140625, 0.7914275308569024, 0.4791666666666667, 0.0]
Checking history sample performance at 625 steps:  0.66
Checking history sample input_X:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 4, 1, 1, 1, 1, 1, 126, 0.005789639303569194, 25, 1]
Checking history sample input_X_between_0_1:  [0.05062795580615905, 0.2921762115292695, 0.00920199338536277, 0.10399583096796572, 0.02717573171862747, 0.013395393627082716, 0.07299551322547251, 0.11385994195406972, 0.3165714277859905, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.05789639303569194, 0.5208333333333334, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 9, 1, 1, 1, 1, 1, 31, 0.03322680456132531, 22, 1]
Checking history sample input_X_between_0_1:  [0.18092795506945925, 0.0458284284940654, 0.02196946628276251, 0.12632118418208813, 0.2028614710003813, 0.06494821741102745, 0.10428933756082519, 0.20892148689294232, 0.043932453106448444, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2421875, 0.33226804561325307, 0.4583333333333333, 1.0]
Checking history sample performance at 625 steps:  0.48
Checking history sample input_X:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 15, 0, 1, 0, 1, 1, 49, 0.0008783405064032635, 29, 1]
Checking history sample input_X_between_0_1:  [0.062129858492421114, 0.07013617512779625, 0.07027442057829313, 0.16412400424606877, 0.010923480876833445, 0.43083791296199614, 0.025067918132844685, 0.08982066812011433, 0.07668556146363208, 0.46875, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3828125, 0.008783405064032634, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.69
Checking history sample input_X:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 1, 0, 1, 1, 0, 1, 106, 0.08696702158391928, 5, 0]
Checking history sample input_X_between_0_1:  [0.044464065229655764, 0.154173533077815, 0.19740961674225502, 0.016537166741367453, 0.0926634738746952, 0.056821986920930496, 0.051960320120485785, 0.09778422050749534, 0.28818561678529986, 0.03125, 0.0, 1.0, 1.0, 0.0, 1.0, 0.828125, 0.8696702158391928, 0.10416666666666667, 0.0]
Checking history sample performance at 625 steps:  0.46
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3578 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.7832053610253471
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.7851656893104026
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.7850647982096228
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.7732329629714589
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.7849718788372212
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [0, tensor(0.1052, dtype=torch.float64), tensor(0.0540, dtype=torch.float64), tensor(0.0802, dtype=torch.float64), 0, tensor(0.4351, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), tensor(0.0267, dtype=torch.float64), tensor(0.2083, dtype=torch.float64), 32, 0, 0, 0, 1, 0, 114, 8.868754143115545e-20, 44.250211883418324, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0., dtype=torch.float64), tensor(0.1052, dtype=torch.float64), tensor(0.0540, dtype=torch.float64), tensor(0.0802, dtype=torch.float64), tensor(2.5258e-18, dtype=torch.float64), tensor(0.4351, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), tensor(0.0267, dtype=torch.float64), tensor(0.2083, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8888, dtype=torch.float64), tensor(8.8688e-19, dtype=torch.float64), tensor(0.9219, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.105
  rowan_hellaswag: 0.054
  sciq: 0.08
  triviaqa: 0
  truthfulqa_gen: 0.435
  wikitext: 0.09
  mmlu: 0.027
  arc_challenge: 0.208

LoRA Parameters:
  lora_r: (114,)
  lora_dropout: (8.868754143115545e-20,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (44.250211883418324,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  114
lora dropout:  8.868754143115545e-20
lora alpha:  44.250211883418324
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 67,239,936 || all params: 8,097,501,184 || trainable%: 0.8304
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:39,  2.50it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:06, 13.04it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.02it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:05, 13.38it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 15.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 14.59it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 15.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.26it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 17.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.49it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 18.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 18.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.57it/s]
Evaluation performance at step 25: 0.57
{'loss': 3.0508, 'grad_norm': 0.8209965229034424, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 1.6464463472366333, 'eval_runtime': 8.2829, 'eval_samples_per_second': 120.61, 'eval_steps_per_second': 7.606, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:41,  2.37it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 12.35it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 16.69it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 18.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 20.21it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:02, 21.06it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 23.22it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 19.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 21.41it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:03<00:01, 21.11it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 22.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 21.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 20.67it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.39it/s]
Evaluation performance at step 50: 0.69
{'loss': 1.3571, 'grad_norm': 0.914387583732605, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.69}
{'eval_loss': 1.1689096689224243, 'eval_runtime': 8.3344, 'eval_samples_per_second': 119.865, 'eval_steps_per_second': 7.559, 'epoch': 0.08}
{'loss': 1.0838, 'grad_norm': 0.2760458290576935, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0946118831634521, 'eval_runtime': 8.3868, 'eval_samples_per_second': 119.116, 'eval_steps_per_second': 7.512, 'epoch': 0.12}
{'loss': 1.0642, 'grad_norm': 0.2528485059738159, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0547375679016113, 'eval_runtime': 8.4248, 'eval_samples_per_second': 118.578, 'eval_steps_per_second': 7.478, 'epoch': 0.16}
{'loss': 1.1117, 'grad_norm': 0.32853516936302185, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0243455171585083, 'eval_runtime': 8.4129, 'eval_samples_per_second': 118.747, 'eval_steps_per_second': 7.489, 'epoch': 0.2}
{'loss': 1.0866, 'grad_norm': 0.22072464227676392, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0089266300201416, 'eval_runtime': 8.434, 'eval_samples_per_second': 118.45, 'eval_steps_per_second': 7.47, 'epoch': 0.24}
{'loss': 0.9954, 'grad_norm': 0.2503208816051483, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9863033294677734, 'eval_runtime': 8.4194, 'eval_samples_per_second': 118.655, 'eval_steps_per_second': 7.483, 'epoch': 0.28}
{'loss': 0.9805, 'grad_norm': 0.3172454237937927, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9713627099990845, 'eval_runtime': 8.422, 'eval_samples_per_second': 118.618, 'eval_steps_per_second': 7.48, 'epoch': 0.32}
{'loss': 0.9577, 'grad_norm': 0.24823175370693207, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9533795714378357, 'eval_runtime': 8.4279, 'eval_samples_per_second': 118.535, 'eval_steps_per_second': 7.475, 'epoch': 0.36}
{'loss': 0.9632, 'grad_norm': 0.2540891170501709, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9371564984321594, 'eval_runtime': 8.4359, 'eval_samples_per_second': 118.422, 'eval_steps_per_second': 7.468, 'epoch': 0.4}
{'loss': 0.9674, 'grad_norm': 0.36414623260498047, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9234790205955505, 'eval_runtime': 8.429, 'eval_samples_per_second': 118.519, 'eval_steps_per_second': 7.474, 'epoch': 0.44}
{'loss': 0.9718, 'grad_norm': 0.26163068413734436, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9085342884063721, 'eval_runtime': 8.4176, 'eval_samples_per_second': 118.679, 'eval_steps_per_second': 7.484, 'epoch': 0.48}
{'loss': 1.0088, 'grad_norm': 0.3541780114173889, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8904419541358948, 'eval_runtime': 8.4343, 'eval_samples_per_second': 118.445, 'eval_steps_per_second': 7.469, 'epoch': 0.52}
{'loss': 0.9655, 'grad_norm': 0.3254762589931488, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8782371282577515, 'eval_runtime': 8.4289, 'eval_samples_per_second': 118.52, 'eval_steps_per_second': 7.474, 'epoch': 0.56}
{'loss': 0.9108, 'grad_norm': 0.3905855119228363, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8663678765296936, 'eval_runtime': 8.4567, 'eval_samples_per_second': 118.132, 'eval_steps_per_second': 7.45, 'epoch': 0.6}
{'loss': 0.9191, 'grad_norm': 0.28091496229171753, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8513262867927551, 'eval_runtime': 8.4072, 'eval_samples_per_second': 118.826, 'eval_steps_per_second': 7.494, 'epoch': 0.64}
{'loss': 0.9372, 'grad_norm': 0.3263140916824341, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.840718686580658, 'eval_runtime': 8.3849, 'eval_samples_per_second': 119.142, 'eval_steps_per_second': 7.513, 'epoch': 0.68}
{'loss': 0.8824, 'grad_norm': 0.34776726365089417, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8281182646751404, 'eval_runtime': 8.3769, 'eval_samples_per_second': 119.257, 'eval_steps_per_second': 7.521, 'epoch': 0.72}
{'loss': 0.8894, 'grad_norm': 0.3503296673297882, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8179853558540344, 'eval_runtime': 8.3679, 'eval_samples_per_second': 119.384, 'eval_steps_per_second': 7.529, 'epoch': 0.76}
{'loss': 0.9473, 'grad_norm': 0.3958483040332794, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8084117770195007, 'eval_runtime': 8.3708, 'eval_samples_per_second': 119.343, 'eval_steps_per_second': 7.526, 'epoch': 0.8}
{'loss': 0.8892, 'grad_norm': 0.3637392222881317, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7993536591529846, 'eval_runtime': 8.3709, 'eval_samples_per_second': 119.342, 'eval_steps_per_second': 7.526, 'epoch': 0.84}
{'loss': 0.9358, 'grad_norm': 0.420409232378006, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7949042916297913, 'eval_runtime': 8.3778, 'eval_samples_per_second': 119.244, 'eval_steps_per_second': 7.52, 'epoch': 0.88}
{'loss': 0.8869, 'grad_norm': 0.29052937030792236, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7911540865898132, 'eval_runtime': 8.3873, 'eval_samples_per_second': 119.109, 'eval_steps_per_second': 7.511, 'epoch': 0.92}
{'loss': 0.8004, 'grad_norm': 0.4196775257587433, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7878970503807068, 'eval_runtime': 8.4358, 'eval_samples_per_second': 118.424, 'eval_steps_per_second': 7.468, 'epoch': 0.96}
{'loss': 0.8333, 'grad_norm': 0.28605136275291443, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7874770164489746, 'eval_runtime': 8.435, 'eval_samples_per_second': 118.435, 'eval_steps_per_second': 7.469, 'epoch': 1.0}
{'train_runtime': 499.5804, 'train_samples_per_second': 20.011, 'train_steps_per_second': 1.251, 'train_loss': 1.0558588104248048, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6464463472366333, 1.1689096689224243, 1.0946118831634521, 1.0547375679016113, 1.0243455171585083, 1.0089266300201416, 0.9863033294677734, 0.9713627099990845, 0.9533795714378357, 0.9371564984321594, 0.9234790205955505, 0.9085342884063721, 0.8904419541358948, 0.8782371282577515, 0.8663678765296936, 0.8513262867927551, 0.840718686580658, 0.8281182646751404, 0.8179853558540344, 0.8084117770195007, 0.7993536591529846, 0.7949042916297913, 0.7911540865898132, 0.7878970503807068, 0.7874770164489746], 'performance': [0.57, 0.69]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:56,  1.74it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 18.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 27.06it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 29.42it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 33.16it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 34.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 38.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 32.21it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.69]
current iteration observed (possibly low-fid or predicted) performance:  1.4319740533828735
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.6930000000000001
BO observations:  [1.4319740533828735]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6472 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.4121916634692742
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 1.3628984043544925
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.415380193993967
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.4037977271713575
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.3413813357163198
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7106e-17, dtype=torch.float64), tensor(6.4699e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.4226e-18, dtype=torch.float64), tensor(4.0406e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 1.0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:48,  1.10s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.77it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 14.18it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.47it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.68it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.25it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 17.50it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 18.43it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 13.42it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 14.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.75it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.4345, 'grad_norm': 0.8715059757232666, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 1.194244146347046, 'eval_runtime': 3.3384, 'eval_samples_per_second': 299.545, 'eval_steps_per_second': 18.871, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:50,  1.97it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.58it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.12it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:20,  3.66it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:12,  5.39it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  7.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:03, 11.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:02, 13.51it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:01, 14.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 16.22it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 18.28it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 19.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.23it/s]
Evaluation performance at step 50: 0.59
{'loss': 0.869, 'grad_norm': 0.4661425054073334, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 0.7544748783111572, 'eval_runtime': 3.3347, 'eval_samples_per_second': 299.876, 'eval_steps_per_second': 18.892, 'epoch': 0.08}
{'loss': 0.7028, 'grad_norm': 0.38970041275024414, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.6592220664024353, 'eval_runtime': 3.328, 'eval_samples_per_second': 300.476, 'eval_steps_per_second': 18.93, 'epoch': 0.12}
{'loss': 0.6291, 'grad_norm': 0.33517131209373474, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.5765259861946106, 'eval_runtime': 3.3473, 'eval_samples_per_second': 298.745, 'eval_steps_per_second': 18.821, 'epoch': 0.16}
{'loss': 0.5296, 'grad_norm': 0.3038932681083679, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.49953797459602356, 'eval_runtime': 3.3515, 'eval_samples_per_second': 298.371, 'eval_steps_per_second': 18.797, 'epoch': 0.2}
{'loss': 0.4557, 'grad_norm': 0.34761351346969604, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.4173305630683899, 'eval_runtime': 3.3463, 'eval_samples_per_second': 298.84, 'eval_steps_per_second': 18.827, 'epoch': 0.24}
{'loss': 0.42, 'grad_norm': 0.3547000288963318, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.3612334132194519, 'eval_runtime': 3.3419, 'eval_samples_per_second': 299.232, 'eval_steps_per_second': 18.852, 'epoch': 0.28}
{'loss': 0.3396, 'grad_norm': 0.4158455729484558, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.3170815110206604, 'eval_runtime': 3.3696, 'eval_samples_per_second': 296.768, 'eval_steps_per_second': 18.696, 'epoch': 0.32}
{'loss': 0.317, 'grad_norm': 0.3871840238571167, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.28415074944496155, 'eval_runtime': 3.3644, 'eval_samples_per_second': 297.226, 'eval_steps_per_second': 18.725, 'epoch': 0.36}
{'loss': 0.2696, 'grad_norm': 0.31247836351394653, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.24272382259368896, 'eval_runtime': 3.3847, 'eval_samples_per_second': 295.447, 'eval_steps_per_second': 18.613, 'epoch': 0.4}
{'loss': 0.2472, 'grad_norm': 0.24616888165473938, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.22349600493907928, 'eval_runtime': 3.3755, 'eval_samples_per_second': 296.251, 'eval_steps_per_second': 18.664, 'epoch': 0.44}
{'loss': 0.2223, 'grad_norm': 0.31517651677131653, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.20109453797340393, 'eval_runtime': 3.3736, 'eval_samples_per_second': 296.417, 'eval_steps_per_second': 18.674, 'epoch': 0.48}
{'loss': 0.1918, 'grad_norm': 0.3120644986629486, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.1883200854063034, 'eval_runtime': 3.3715, 'eval_samples_per_second': 296.603, 'eval_steps_per_second': 18.686, 'epoch': 0.52}
{'loss': 0.2002, 'grad_norm': 0.27402520179748535, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.17608660459518433, 'eval_runtime': 3.3762, 'eval_samples_per_second': 296.19, 'eval_steps_per_second': 18.66, 'epoch': 0.56}
{'loss': 0.1935, 'grad_norm': 0.255909264087677, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.16658073663711548, 'eval_runtime': 3.3686, 'eval_samples_per_second': 296.857, 'eval_steps_per_second': 18.702, 'epoch': 0.6}
{'loss': 0.1669, 'grad_norm': 0.19890354573726654, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.15655255317687988, 'eval_runtime': 3.3911, 'eval_samples_per_second': 294.89, 'eval_steps_per_second': 18.578, 'epoch': 0.64}
{'loss': 0.16, 'grad_norm': 0.20085324347019196, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.15185730159282684, 'eval_runtime': 3.3729, 'eval_samples_per_second': 296.482, 'eval_steps_per_second': 18.678, 'epoch': 0.68}
{'loss': 0.1591, 'grad_norm': 0.21954111754894257, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.14818353950977325, 'eval_runtime': 3.3749, 'eval_samples_per_second': 296.308, 'eval_steps_per_second': 18.667, 'epoch': 0.72}
{'loss': 0.1554, 'grad_norm': 0.1866571009159088, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.14570865035057068, 'eval_runtime': 3.3768, 'eval_samples_per_second': 296.142, 'eval_steps_per_second': 18.657, 'epoch': 0.76}
{'loss': 0.1475, 'grad_norm': 0.1488746702671051, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.14333142340183258, 'eval_runtime': 3.3737, 'eval_samples_per_second': 296.408, 'eval_steps_per_second': 18.674, 'epoch': 0.8}
{'loss': 0.1472, 'grad_norm': 0.17144054174423218, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.1428230255842209, 'eval_runtime': 3.3743, 'eval_samples_per_second': 296.359, 'eval_steps_per_second': 18.671, 'epoch': 0.84}
{'loss': 0.1462, 'grad_norm': 0.2672843039035797, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.14042147994041443, 'eval_runtime': 3.3751, 'eval_samples_per_second': 296.289, 'eval_steps_per_second': 18.666, 'epoch': 0.88}
{'loss': 0.1424, 'grad_norm': 0.1796800196170807, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.13862918317317963, 'eval_runtime': 3.3702, 'eval_samples_per_second': 296.72, 'eval_steps_per_second': 18.693, 'epoch': 0.92}
{'loss': 0.1438, 'grad_norm': 0.13958746194839478, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.13764554262161255, 'eval_runtime': 3.371, 'eval_samples_per_second': 296.645, 'eval_steps_per_second': 18.689, 'epoch': 0.96}
{'loss': 0.1409, 'grad_norm': 0.1400379240512848, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.1372704654932022, 'eval_runtime': 3.3968, 'eval_samples_per_second': 294.392, 'eval_steps_per_second': 18.547, 'epoch': 1.0}
{'train_runtime': 256.9243, 'train_samples_per_second': 38.922, 'train_steps_per_second': 2.433, 'train_loss': 0.4212610137939453, 'epoch': 1.0}
train_results:  {'eval_loss': [1.194244146347046, 0.7544748783111572, 0.6592220664024353, 0.5765259861946106, 0.49953797459602356, 0.4173305630683899, 0.3612334132194519, 0.3170815110206604, 0.28415074944496155, 0.24272382259368896, 0.22349600493907928, 0.20109453797340393, 0.1883200854063034, 0.17608660459518433, 0.16658073663711548, 0.15655255317687988, 0.15185730159282684, 0.14818353950977325, 0.14570865035057068, 0.14333142340183258, 0.1428230255842209, 0.14042147994041443, 0.13862918317317963, 0.13764554262161255, 0.1372704654932022], 'performance': [0.48, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:53,  1.84it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 20.78it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 27.25it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:01, 26.88it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 29.62it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 33.38it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 39.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 32.00it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.4756853580474854
current iteration best possible performance (full train run):  0.8925
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9003 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.3644157464394477
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 1.263001909200406
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.3550268194548274
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 1.3379970872872857
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 1.13060660305929
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.4540, dtype=torch.float64), 0, 0, tensor(0.5460, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 3.469446951953639e-19, 47.99999999999996, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4540, dtype=torch.float64), tensor(1.1046e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5460, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.5595e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.4694e-18, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.454
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.546
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.469446951953639e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (47.99999999999996,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  3.469446951953639e-19
lora alpha:  47.99999999999996
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:58,  1.70it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:08, 10.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.56it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:19,  3.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:11,  5.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:07,  7.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:04, 10.32it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 10.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:02, 12.15it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 15.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 14.64it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 15.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.67it/s]
Evaluation performance at step 25: 0.48
{'loss': 3.4428, 'grad_norm': 0.723587691783905, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.48}
{'eval_loss': 2.1040942668914795, 'eval_runtime': 9.455, 'eval_samples_per_second': 105.658, 'eval_steps_per_second': 6.663, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:52,  1.89it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09, 10.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 16.66it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 16.51it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 16.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.79it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 14.60it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 15.57it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 16.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 18.09it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 19.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 20.43it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.28it/s]
Evaluation performance at step 50: 0.46
{'loss': 1.7334, 'grad_norm': 0.3362307846546173, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.46}
{'eval_loss': 1.6059173345565796, 'eval_runtime': 9.515, 'eval_samples_per_second': 104.992, 'eval_steps_per_second': 6.621, 'epoch': 0.08}
{'loss': 1.5294, 'grad_norm': 0.3130951523780823, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5408843755722046, 'eval_runtime': 9.5378, 'eval_samples_per_second': 104.741, 'eval_steps_per_second': 6.605, 'epoch': 0.12}
{'loss': 1.4839, 'grad_norm': 0.22809311747550964, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5148478746414185, 'eval_runtime': 9.5883, 'eval_samples_per_second': 104.19, 'eval_steps_per_second': 6.571, 'epoch': 0.16}
{'loss': 1.4765, 'grad_norm': 0.2613281309604645, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4964100122451782, 'eval_runtime': 9.6163, 'eval_samples_per_second': 103.886, 'eval_steps_per_second': 6.551, 'epoch': 0.2}
{'loss': 1.4546, 'grad_norm': 0.22672836482524872, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4828298091888428, 'eval_runtime': 9.6437, 'eval_samples_per_second': 103.591, 'eval_steps_per_second': 6.533, 'epoch': 0.24}
{'loss': 1.4198, 'grad_norm': 0.26072150468826294, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4659581184387207, 'eval_runtime': 9.6183, 'eval_samples_per_second': 103.865, 'eval_steps_per_second': 6.55, 'epoch': 0.28}
{'loss': 1.427, 'grad_norm': 0.24828816950321198, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4529988765716553, 'eval_runtime': 9.6252, 'eval_samples_per_second': 103.79, 'eval_steps_per_second': 6.545, 'epoch': 0.32}
{'loss': 1.4165, 'grad_norm': 0.21511514484882355, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4423502683639526, 'eval_runtime': 9.6019, 'eval_samples_per_second': 104.042, 'eval_steps_per_second': 6.561, 'epoch': 0.36}
{'loss': 1.4061, 'grad_norm': 0.22622050344944, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4205867052078247, 'eval_runtime': 9.5773, 'eval_samples_per_second': 104.309, 'eval_steps_per_second': 6.578, 'epoch': 0.4}
{'loss': 1.3975, 'grad_norm': 0.24205932021141052, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4038368463516235, 'eval_runtime': 9.5671, 'eval_samples_per_second': 104.421, 'eval_steps_per_second': 6.585, 'epoch': 0.44}
{'loss': 1.3548, 'grad_norm': 0.23792555928230286, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.393982172012329, 'eval_runtime': 9.5681, 'eval_samples_per_second': 104.409, 'eval_steps_per_second': 6.584, 'epoch': 0.48}
{'loss': 1.4173, 'grad_norm': 0.27247416973114014, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.3827323913574219, 'eval_runtime': 9.5845, 'eval_samples_per_second': 104.23, 'eval_steps_per_second': 6.573, 'epoch': 0.52}
{'loss': 1.3987, 'grad_norm': 0.265580952167511, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.3709588050842285, 'eval_runtime': 9.5583, 'eval_samples_per_second': 104.517, 'eval_steps_per_second': 6.591, 'epoch': 0.56}
{'loss': 1.3536, 'grad_norm': 0.208332821726799, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.3600687980651855, 'eval_runtime': 9.566, 'eval_samples_per_second': 104.432, 'eval_steps_per_second': 6.586, 'epoch': 0.6}
{'loss': 1.396, 'grad_norm': 0.2455824464559555, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.34787917137146, 'eval_runtime': 9.572, 'eval_samples_per_second': 104.367, 'eval_steps_per_second': 6.582, 'epoch': 0.64}
{'loss': 1.3325, 'grad_norm': 0.2708888351917267, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3398057222366333, 'eval_runtime': 9.5654, 'eval_samples_per_second': 104.438, 'eval_steps_per_second': 6.586, 'epoch': 0.68}
{'loss': 1.3365, 'grad_norm': 0.2351377010345459, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3318780660629272, 'eval_runtime': 9.5495, 'eval_samples_per_second': 104.613, 'eval_steps_per_second': 6.597, 'epoch': 0.72}
{'loss': 1.3652, 'grad_norm': 0.2996492385864258, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.32381010055542, 'eval_runtime': 9.5616, 'eval_samples_per_second': 104.48, 'eval_steps_per_second': 6.589, 'epoch': 0.76}
{'loss': 1.3479, 'grad_norm': 0.3056581914424896, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.3177410364151, 'eval_runtime': 9.5569, 'eval_samples_per_second': 104.531, 'eval_steps_per_second': 6.592, 'epoch': 0.8}
{'loss': 1.3579, 'grad_norm': 0.27194297313690186, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3110319375991821, 'eval_runtime': 9.6309, 'eval_samples_per_second': 103.729, 'eval_steps_per_second': 6.541, 'epoch': 0.84}
{'loss': 1.3219, 'grad_norm': 0.24758975207805634, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3071318864822388, 'eval_runtime': 9.6472, 'eval_samples_per_second': 103.553, 'eval_steps_per_second': 6.53, 'epoch': 0.88}
{'loss': 1.294, 'grad_norm': 0.227447971701622, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.30367910861969, 'eval_runtime': 9.6287, 'eval_samples_per_second': 103.752, 'eval_steps_per_second': 6.543, 'epoch': 0.92}
{'loss': 1.3477, 'grad_norm': 0.22248375415802002, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3014883995056152, 'eval_runtime': 9.6372, 'eval_samples_per_second': 103.661, 'eval_steps_per_second': 6.537, 'epoch': 0.96}
{'loss': 1.2998, 'grad_norm': 0.3011561334133148, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3000898361206055, 'eval_runtime': 9.6511, 'eval_samples_per_second': 103.512, 'eval_steps_per_second': 6.528, 'epoch': 1.0}
{'train_runtime': 543.9082, 'train_samples_per_second': 18.384, 'train_steps_per_second': 1.149, 'train_loss': 1.4844530700683594, 'epoch': 1.0}
train_results:  {'eval_loss': [2.1040942668914795, 1.6059173345565796, 1.5408843755722046, 1.5148478746414185, 1.4964100122451782, 1.4828298091888428, 1.4659581184387207, 1.4529988765716553, 1.4423502683639526, 1.4205867052078247, 1.4038368463516235, 1.393982172012329, 1.3827323913574219, 1.3709588050842285, 1.3600687980651855, 1.34787917137146, 1.3398057222366333, 1.3318780660629272, 1.32381010055542, 1.3177410364151, 1.3110319375991821, 1.3071318864822388, 1.30367910861969, 1.3014883995056152, 1.3000898361206055], 'performance': [0.48, 0.46]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:19,  1.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 16.06it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.10it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 21.03it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 21.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 25.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.64it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.48, 0.46]
current iteration observed (possibly low-fid or predicted) performance:  1.4684956073760986
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8241 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.9881389470940706
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.1648331260207665
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 0.9988567747120455
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.140858705724781
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.2067482528251068
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.4426, dtype=torch.float64), 0, tensor(0.5574, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 0, 128, 5.898059818321145e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(5.3498e-17, dtype=torch.float64), tensor(1.9179e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9020e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4426, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5574, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.8981e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.443
  wikitext: 0
  mmlu: 0.557
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.898059818321145e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  5.898059818321145e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:45,  2.17it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.49it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.85it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:24,  3.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:14,  4.53it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.19it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.35it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:12<00:11,  3.60it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:13<00:07,  4.83it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:13<00:04,  6.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:18<00:05,  3.31it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:19<00:02,  3.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:20<00:00,  5.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:20<00:00,  4.98it/s]
Evaluation performance at step 25: 0.54
{'loss': 2.7286, 'grad_norm': 0.784761369228363, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 1.4194847345352173, 'eval_runtime': 8.8734, 'eval_samples_per_second': 112.584, 'eval_steps_per_second': 7.1, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:23,  4.48s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:08<01:18,  1.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:34,  2.44it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:10<00:19,  3.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:10<00:12,  5.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:11<00:09,  6.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:11<00:06,  8.19it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:16<00:11,  3.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:16<00:06,  5.13it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:17<00:04,  6.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:17<00:02,  7.74it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:18<00:01,  9.15it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:18<00:00,  9.96it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18<00:00,  5.31it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.2975, 'grad_norm': 0.47974082827568054, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.1593677997589111, 'eval_runtime': 8.821, 'eval_samples_per_second': 113.252, 'eval_steps_per_second': 7.142, 'epoch': 0.08}
{'loss': 1.1277, 'grad_norm': 0.30362799763679504, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1064311265945435, 'eval_runtime': 8.9156, 'eval_samples_per_second': 112.051, 'eval_steps_per_second': 7.066, 'epoch': 0.12}
{'loss': 1.0772, 'grad_norm': 0.2577116787433624, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0872056484222412, 'eval_runtime': 8.899, 'eval_samples_per_second': 112.26, 'eval_steps_per_second': 7.079, 'epoch': 0.16}
{'loss': 1.1369, 'grad_norm': 0.25585123896598816, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0686994791030884, 'eval_runtime': 8.9287, 'eval_samples_per_second': 111.887, 'eval_steps_per_second': 7.056, 'epoch': 0.2}
{'loss': 1.0422, 'grad_norm': 0.2575291395187378, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0491052865982056, 'eval_runtime': 8.9374, 'eval_samples_per_second': 111.778, 'eval_steps_per_second': 7.049, 'epoch': 0.24}
{'loss': 1.069, 'grad_norm': 0.2739298641681671, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.03248131275177, 'eval_runtime': 8.9413, 'eval_samples_per_second': 111.729, 'eval_steps_per_second': 7.046, 'epoch': 0.28}
{'loss': 1.0603, 'grad_norm': 0.24985283613204956, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0140975713729858, 'eval_runtime': 8.9552, 'eval_samples_per_second': 111.555, 'eval_steps_per_second': 7.035, 'epoch': 0.32}
{'loss': 1.0553, 'grad_norm': 0.3058071434497833, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.999961793422699, 'eval_runtime': 8.9687, 'eval_samples_per_second': 111.388, 'eval_steps_per_second': 7.024, 'epoch': 0.36}
{'loss': 1.1098, 'grad_norm': 0.31459158658981323, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9773849844932556, 'eval_runtime': 8.9631, 'eval_samples_per_second': 111.457, 'eval_steps_per_second': 7.029, 'epoch': 0.4}
{'loss': 0.9928, 'grad_norm': 0.25939345359802246, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.960979700088501, 'eval_runtime': 8.9548, 'eval_samples_per_second': 111.56, 'eval_steps_per_second': 7.035, 'epoch': 0.44}
{'loss': 1.0299, 'grad_norm': 0.2801951766014099, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9395464658737183, 'eval_runtime': 8.9523, 'eval_samples_per_second': 111.591, 'eval_steps_per_second': 7.037, 'epoch': 0.48}
{'loss': 0.9534, 'grad_norm': 0.3008354902267456, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9277295470237732, 'eval_runtime': 8.952, 'eval_samples_per_second': 111.595, 'eval_steps_per_second': 7.038, 'epoch': 0.52}
{'loss': 0.9912, 'grad_norm': 0.32581809163093567, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9121654629707336, 'eval_runtime': 8.9445, 'eval_samples_per_second': 111.689, 'eval_steps_per_second': 7.043, 'epoch': 0.56}
{'loss': 0.9517, 'grad_norm': 0.3671553432941437, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.898653507232666, 'eval_runtime': 8.9478, 'eval_samples_per_second': 111.647, 'eval_steps_per_second': 7.041, 'epoch': 0.6}
{'loss': 0.9597, 'grad_norm': 0.366378515958786, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8861696124076843, 'eval_runtime': 8.8984, 'eval_samples_per_second': 112.268, 'eval_steps_per_second': 7.08, 'epoch': 0.64}
{'loss': 1.0065, 'grad_norm': 0.41968435049057007, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8721328377723694, 'eval_runtime': 8.8848, 'eval_samples_per_second': 112.439, 'eval_steps_per_second': 7.091, 'epoch': 0.68}
{'loss': 0.8737, 'grad_norm': 0.24258165061473846, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8563985228538513, 'eval_runtime': 8.8836, 'eval_samples_per_second': 112.454, 'eval_steps_per_second': 7.092, 'epoch': 0.72}
{'loss': 0.9193, 'grad_norm': 0.272256463766098, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8453183174133301, 'eval_runtime': 8.8947, 'eval_samples_per_second': 112.314, 'eval_steps_per_second': 7.083, 'epoch': 0.76}
{'loss': 0.8853, 'grad_norm': 0.37082362174987793, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8366277813911438, 'eval_runtime': 8.8948, 'eval_samples_per_second': 112.313, 'eval_steps_per_second': 7.083, 'epoch': 0.8}
{'loss': 0.9411, 'grad_norm': 0.2894267737865448, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8295865654945374, 'eval_runtime': 8.9105, 'eval_samples_per_second': 112.115, 'eval_steps_per_second': 7.07, 'epoch': 0.84}
{'loss': 0.9112, 'grad_norm': 0.287906676530838, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8227117657661438, 'eval_runtime': 8.9919, 'eval_samples_per_second': 111.1, 'eval_steps_per_second': 7.006, 'epoch': 0.88}
{'loss': 0.9239, 'grad_norm': 0.419770747423172, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8156492710113525, 'eval_runtime': 8.9728, 'eval_samples_per_second': 111.337, 'eval_steps_per_second': 7.021, 'epoch': 0.92}
{'loss': 0.9042, 'grad_norm': 0.26011940836906433, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8132054209709167, 'eval_runtime': 9.0073, 'eval_samples_per_second': 110.91, 'eval_steps_per_second': 6.994, 'epoch': 0.96}
{'loss': 0.8474, 'grad_norm': 0.2858172357082367, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8117875456809998, 'eval_runtime': 9.0308, 'eval_samples_per_second': 110.621, 'eval_steps_per_second': 6.976, 'epoch': 1.0}
{'train_runtime': 570.0219, 'train_samples_per_second': 17.541, 'train_steps_per_second': 1.096, 'train_loss': 1.071825735473633, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4194847345352173, 1.1593677997589111, 1.1064311265945435, 1.0872056484222412, 1.0686994791030884, 1.0491052865982056, 1.03248131275177, 1.0140975713729858, 0.999961793422699, 0.9773849844932556, 0.960979700088501, 0.9395464658737183, 0.9277295470237732, 0.9121654629707336, 0.898653507232666, 0.8861696124076843, 0.8721328377723694, 0.8563985228538513, 0.8453183174133301, 0.8366277813911438, 0.8295865654945374, 0.8227117657661438, 0.8156492710113525, 0.8132054209709167, 0.8117875456809998], 'performance': [0.54, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:32,  1.54s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.68it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 15.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 19.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 21.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 23.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 27.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.26it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.4780199527740479
current iteration best possible performance (full train run):  0.7875000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0738 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.3009622121230004
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 1.2635739041923695
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.229428390199688
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.8757275352137726
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 1.1678698863107835
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.1147, dtype=torch.float64), tensor(0.8853, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1147, dtype=torch.float64), tensor(0.8853, dtype=torch.float64), tensor(7.9267e-17, dtype=torch.float64), tensor(1.5206e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.115
  wikitext: 0.885
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:47,  1.69s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:17,  5.17it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.32it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.73it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.83it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.10it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 11.91it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  9.73it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.71it/s]
Evaluation performance at step 25: 0.54
{'loss': 3.4157, 'grad_norm': 0.11087062209844589, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.282874584197998, 'eval_runtime': 8.6012, 'eval_samples_per_second': 116.146, 'eval_steps_per_second': 7.325, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.70it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:09,  9.89it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 16.29it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.28it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 19.16it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 20.07it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 19.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 21.75it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 20.54it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 19.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 19.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 17.88it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.44it/s]
Evaluation performance at step 50: 0.6
{'loss': 2.844, 'grad_norm': 0.19986282289028168, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 2.382877826690674, 'eval_runtime': 8.5827, 'eval_samples_per_second': 116.397, 'eval_steps_per_second': 7.34, 'epoch': 0.08}
{'loss': 2.255, 'grad_norm': 0.09385717660188675, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.1613826751708984, 'eval_runtime': 8.6145, 'eval_samples_per_second': 115.968, 'eval_steps_per_second': 7.313, 'epoch': 0.12}
{'loss': 2.1406, 'grad_norm': 0.09057259559631348, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.100109815597534, 'eval_runtime': 8.6102, 'eval_samples_per_second': 116.025, 'eval_steps_per_second': 7.317, 'epoch': 0.16}
{'loss': 2.077, 'grad_norm': 0.10115280747413635, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.0717482566833496, 'eval_runtime': 8.6333, 'eval_samples_per_second': 115.715, 'eval_steps_per_second': 7.297, 'epoch': 0.2}
{'loss': 2.0919, 'grad_norm': 0.09391231089830399, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.0506629943847656, 'eval_runtime': 8.6288, 'eval_samples_per_second': 115.776, 'eval_steps_per_second': 7.301, 'epoch': 0.24}
{'loss': 2.1177, 'grad_norm': 0.10208204388618469, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.034993886947632, 'eval_runtime': 8.6361, 'eval_samples_per_second': 115.677, 'eval_steps_per_second': 7.295, 'epoch': 0.28}
{'loss': 2.0447, 'grad_norm': 0.1815520077943802, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.021793842315674, 'eval_runtime': 8.6341, 'eval_samples_per_second': 115.704, 'eval_steps_per_second': 7.297, 'epoch': 0.32}
{'loss': 2.0667, 'grad_norm': 0.13343429565429688, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.015374183654785, 'eval_runtime': 8.6495, 'eval_samples_per_second': 115.499, 'eval_steps_per_second': 7.284, 'epoch': 0.36}
{'loss': 2.0795, 'grad_norm': 0.12215523421764374, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0041379928588867, 'eval_runtime': 8.6558, 'eval_samples_per_second': 115.414, 'eval_steps_per_second': 7.278, 'epoch': 0.4}
{'loss': 1.9795, 'grad_norm': 0.1339951902627945, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0008366107940674, 'eval_runtime': 8.6507, 'eval_samples_per_second': 115.482, 'eval_steps_per_second': 7.283, 'epoch': 0.44}
{'loss': 2.0246, 'grad_norm': 0.11725778132677078, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.9957208633422852, 'eval_runtime': 8.641, 'eval_samples_per_second': 115.612, 'eval_steps_per_second': 7.291, 'epoch': 0.48}
{'loss': 1.906, 'grad_norm': 0.17197081446647644, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.9974315166473389, 'eval_runtime': 8.6446, 'eval_samples_per_second': 115.563, 'eval_steps_per_second': 7.288, 'epoch': 0.52}
{'loss': 1.9698, 'grad_norm': 0.10977363586425781, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.9855740070343018, 'eval_runtime': 8.6457, 'eval_samples_per_second': 115.548, 'eval_steps_per_second': 7.287, 'epoch': 0.56}
{'loss': 2.0782, 'grad_norm': 0.3127085268497467, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.9822672605514526, 'eval_runtime': 8.6439, 'eval_samples_per_second': 115.573, 'eval_steps_per_second': 7.288, 'epoch': 0.6}
{'loss': 1.8888, 'grad_norm': 0.17798662185668945, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.978918194770813, 'eval_runtime': 8.6885, 'eval_samples_per_second': 114.98, 'eval_steps_per_second': 7.251, 'epoch': 0.64}
{'loss': 1.9824, 'grad_norm': 0.09304475784301758, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.9758275747299194, 'eval_runtime': 8.7242, 'eval_samples_per_second': 114.509, 'eval_steps_per_second': 7.221, 'epoch': 0.68}
{'loss': 1.9713, 'grad_norm': 0.10180631279945374, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.9733670949935913, 'eval_runtime': 8.7058, 'eval_samples_per_second': 114.752, 'eval_steps_per_second': 7.237, 'epoch': 0.72}
{'loss': 1.8538, 'grad_norm': 0.11013134568929672, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.9700863361358643, 'eval_runtime': 8.732, 'eval_samples_per_second': 114.406, 'eval_steps_per_second': 7.215, 'epoch': 0.76}
{'loss': 1.9073, 'grad_norm': 0.14324766397476196, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.96894109249115, 'eval_runtime': 8.7326, 'eval_samples_per_second': 114.399, 'eval_steps_per_second': 7.214, 'epoch': 0.8}
{'loss': 1.9213, 'grad_norm': 0.08616749197244644, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.9671895503997803, 'eval_runtime': 8.7674, 'eval_samples_per_second': 113.945, 'eval_steps_per_second': 7.186, 'epoch': 0.84}
{'loss': 2.0254, 'grad_norm': 0.11590445786714554, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.9650981426239014, 'eval_runtime': 8.7567, 'eval_samples_per_second': 114.084, 'eval_steps_per_second': 7.194, 'epoch': 0.88}
{'loss': 1.9104, 'grad_norm': 0.13047072291374207, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.9637364149093628, 'eval_runtime': 8.7684, 'eval_samples_per_second': 113.932, 'eval_steps_per_second': 7.185, 'epoch': 0.92}
{'loss': 1.954, 'grad_norm': 0.10697814077138901, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.9630919694900513, 'eval_runtime': 8.7615, 'eval_samples_per_second': 114.022, 'eval_steps_per_second': 7.191, 'epoch': 0.96}
{'loss': 2.0093, 'grad_norm': 0.18550486862659454, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.9626606702804565, 'eval_runtime': 8.7587, 'eval_samples_per_second': 114.058, 'eval_steps_per_second': 7.193, 'epoch': 1.0}
{'train_runtime': 510.0815, 'train_samples_per_second': 19.603, 'train_steps_per_second': 1.225, 'train_loss': 2.1005971313476564, 'epoch': 1.0}
train_results:  {'eval_loss': [3.282874584197998, 2.382877826690674, 2.1613826751708984, 2.100109815597534, 2.0717482566833496, 2.0506629943847656, 2.034993886947632, 2.021793842315674, 2.015374183654785, 2.0041379928588867, 2.0008366107940674, 1.9957208633422852, 1.9974315166473389, 1.9855740070343018, 1.9822672605514526, 1.978918194770813, 1.9758275747299194, 1.9733670949935913, 1.9700863361358643, 1.96894109249115, 1.9671895503997803, 1.9650981426239014, 1.9637364149093628, 1.9630919694900513, 1.9626606702804565], 'performance': [0.54, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:26,  1.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.37it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:16,  4.17it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:13<00:15,  3.33it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:13<00:06,  5.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:02,  7.02it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:15<00:00,  9.29it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15<00:00,  6.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.488027811050415
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.5006 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.3120004239867824
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.98613850737689
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 1.3002563943544745
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.7603104765660189
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.1164637347935134
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 5.551115123125783e-17, 1.4800000190734908, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5756e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.7447e-16, dtype=torch.float64), tensor(5.0895e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.5511e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 1.0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.551115123125783e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734908,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  5.551115123125783e-17
lora alpha:  1.4800000190734908
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:16<27:42, 16.80s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:24<03:20,  2.21s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:28<01:45,  1.28s/it]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:36<01:24,  1.12s/it]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:40<00:59,  1.12it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:45<00:47,  1.23it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:54<00:45,  1.11it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [01:02<00:40,  1.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [01:08<00:30,  1.16it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [01:11<00:19,  1.39it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [01:24<00:18,  1.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [01:31<00:10,  1.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:37<00:02,  1.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:37<00:00,  1.03it/s]
Evaluation performance at step 25: 0.55
{'loss': 5.318, 'grad_norm': 0.5636183619499207, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.7387592792510986, 'eval_runtime': 13.1889, 'eval_samples_per_second': 75.821, 'eval_steps_per_second': 4.777, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:10<16:53, 10.24s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:16<02:21,  1.55s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:20<01:19,  1.04it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:25<01:01,  1.22it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:30<00:49,  1.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:34<00:38,  1.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:40<00:34,  1.48it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:46<00:29,  1.44it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:50<00:21,  1.61it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:54<00:16,  1.67it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [01:03<00:14,  1.32it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [01:11<00:09,  1.22it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:17<00:02,  1.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:17<00:00,  1.29it/s]
Evaluation performance at step 50: 0.5
{'loss': 2.2864, 'grad_norm': 0.4620238244533539, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.5}
{'eval_loss': 1.2602046728134155, 'eval_runtime': 13.4443, 'eval_samples_per_second': 74.381, 'eval_steps_per_second': 4.686, 'epoch': 0.08}
{'loss': 1.0585, 'grad_norm': 0.10637518763542175, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9082270264625549, 'eval_runtime': 14.0949, 'eval_samples_per_second': 70.947, 'eval_steps_per_second': 4.47, 'epoch': 0.12}
{'loss': 0.8472, 'grad_norm': 0.12318533658981323, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7524675130844116, 'eval_runtime': 13.1773, 'eval_samples_per_second': 75.888, 'eval_steps_per_second': 4.781, 'epoch': 0.16}
{'loss': 0.7599, 'grad_norm': 0.05014056712388992, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7331357002258301, 'eval_runtime': 10.5517, 'eval_samples_per_second': 94.771, 'eval_steps_per_second': 5.971, 'epoch': 0.2}
{'loss': 0.7672, 'grad_norm': 0.0586470328271389, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7240105271339417, 'eval_runtime': 12.5557, 'eval_samples_per_second': 79.645, 'eval_steps_per_second': 5.018, 'epoch': 0.24}
{'loss': 0.7082, 'grad_norm': 0.054918404668569565, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7183908224105835, 'eval_runtime': 11.7869, 'eval_samples_per_second': 84.84, 'eval_steps_per_second': 5.345, 'epoch': 0.28}
{'loss': 0.7587, 'grad_norm': 0.0575297512114048, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7145659923553467, 'eval_runtime': 14.2107, 'eval_samples_per_second': 70.369, 'eval_steps_per_second': 4.433, 'epoch': 0.32}
{'loss': 0.7302, 'grad_norm': 0.0626683235168457, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7126519680023193, 'eval_runtime': 10.8065, 'eval_samples_per_second': 92.537, 'eval_steps_per_second': 5.83, 'epoch': 0.36}
{'loss': 0.7496, 'grad_norm': 0.05034424737095833, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7101975679397583, 'eval_runtime': 13.1284, 'eval_samples_per_second': 76.171, 'eval_steps_per_second': 4.799, 'epoch': 0.4}
{'loss': 0.7324, 'grad_norm': 0.0771903395652771, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7085505723953247, 'eval_runtime': 12.4506, 'eval_samples_per_second': 80.318, 'eval_steps_per_second': 5.06, 'epoch': 0.44}
{'loss': 0.7203, 'grad_norm': 0.056647688150405884, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.705766499042511, 'eval_runtime': 7.2395, 'eval_samples_per_second': 138.131, 'eval_steps_per_second': 8.702, 'epoch': 0.48}
{'loss': 0.726, 'grad_norm': 0.06044269725680351, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7043516635894775, 'eval_runtime': 13.6236, 'eval_samples_per_second': 73.402, 'eval_steps_per_second': 4.624, 'epoch': 0.52}
{'loss': 0.714, 'grad_norm': 0.07376138865947723, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7016635537147522, 'eval_runtime': 12.8258, 'eval_samples_per_second': 77.968, 'eval_steps_per_second': 4.912, 'epoch': 0.56}
{'loss': 0.7188, 'grad_norm': 0.06656024605035782, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7006065249443054, 'eval_runtime': 12.332, 'eval_samples_per_second': 81.09, 'eval_steps_per_second': 5.109, 'epoch': 0.6}
‚è∞ Max training time of 1000 seconds reached. Stopping.
{'train_runtime': 1000.4047, 'train_samples_per_second': 9.996, 'train_steps_per_second': 0.625, 'train_loss': 1.1605714639851465, 'epoch': 0.62}
train_results:  {'eval_loss': [3.7387592792510986, 1.2602046728134155, 0.9082270264625549, 0.7524675130844116, 0.7331357002258301, 0.7240105271339417, 0.7183908224105835, 0.7145659923553467, 0.7126519680023193, 0.7101975679397583, 0.7085505723953247, 0.705766499042511, 0.7043516635894775, 0.7016635537147522, 0.7006065249443054], 'performance': [0.55, 0.5]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:20,  5.66s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:17<01:17,  1.07it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:25<00:46,  1.44it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:32<00:29,  1.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:38<00:17,  1.98it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:46<00:09,  1.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:51<00:01,  2.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:51<00:00,  1.94it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.5]
current iteration observed (possibly low-fid or predicted) performance:  1.4894435405731201
current iteration best possible performance (full train run):  0.462
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5553 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.2091555442704678
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 0.8108689581657058
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.0382842716003415
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 0.9071372141889398
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.0516404273663034
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.8788, dtype=torch.float64), 0, 0, 0, tensor(0.1212, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 2.7402852908514988e-17, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1.3623e-17, dtype=torch.float64), tensor(0.8788, dtype=torch.float64), tensor(6.0272e-17, dtype=torch.float64), tensor(2.4507e-16, dtype=torch.float64), tensor(1.2239e-16, dtype=torch.float64), tensor(0.1212, dtype=torch.float64), tensor(8.4225e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.7403e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.879
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.121
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.7402852908514988e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  2.7402852908514988e-17
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:27,  2.09s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:25,  3.51it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:13,  6.10it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:11,  6.79it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  8.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:06,  8.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.43it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:04,  8.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.44it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 10.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  8.39it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.82it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.76it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  7.97it/s]
Evaluation performance at step 25: 0.56
{'loss': 2.6053, 'grad_norm': 0.2407596856355667, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 2.0852863788604736, 'eval_runtime': 9.78, 'eval_samples_per_second': 102.148, 'eval_steps_per_second': 6.442, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:22,  1.44s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.95it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.88it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:11,  6.60it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:08,  8.05it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  9.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.74it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.38it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.56it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.47it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 12.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 12.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 13.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.20it/s]
Evaluation performance at step 50: 0.55
{'loss': 1.5723, 'grad_norm': 0.16646063327789307, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 1.2045105695724487, 'eval_runtime': 9.7219, 'eval_samples_per_second': 102.758, 'eval_steps_per_second': 6.48, 'epoch': 0.08}
{'loss': 1.0333, 'grad_norm': 0.0490780808031559, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9555560350418091, 'eval_runtime': 9.8461, 'eval_samples_per_second': 101.461, 'eval_steps_per_second': 6.398, 'epoch': 0.12}
{'loss': 0.9266, 'grad_norm': 0.06939149647951126, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8949277400970459, 'eval_runtime': 9.8884, 'eval_samples_per_second': 101.027, 'eval_steps_per_second': 6.371, 'epoch': 0.16}
{'loss': 0.8886, 'grad_norm': 0.0417330376803875, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8701633810997009, 'eval_runtime': 9.8665, 'eval_samples_per_second': 101.252, 'eval_steps_per_second': 6.385, 'epoch': 0.2}
{'loss': 0.8723, 'grad_norm': 0.0388769768178463, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8621509075164795, 'eval_runtime': 9.9155, 'eval_samples_per_second': 100.752, 'eval_steps_per_second': 6.354, 'epoch': 0.24}
{'loss': 0.8596, 'grad_norm': 0.04004707559943199, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8552249670028687, 'eval_runtime': 9.8358, 'eval_samples_per_second': 101.568, 'eval_steps_per_second': 6.405, 'epoch': 0.28}
{'loss': 0.8731, 'grad_norm': 0.0482606515288353, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8513909578323364, 'eval_runtime': 9.8498, 'eval_samples_per_second': 101.424, 'eval_steps_per_second': 6.396, 'epoch': 0.32}
{'loss': 0.8599, 'grad_norm': 0.04343508556485176, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8463771939277649, 'eval_runtime': 9.8436, 'eval_samples_per_second': 101.487, 'eval_steps_per_second': 6.4, 'epoch': 0.36}
{'loss': 0.8259, 'grad_norm': 0.03981135040521622, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8423541784286499, 'eval_runtime': 9.8038, 'eval_samples_per_second': 101.9, 'eval_steps_per_second': 6.426, 'epoch': 0.4}
{'loss': 0.8357, 'grad_norm': 0.04198838770389557, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8388653993606567, 'eval_runtime': 9.7664, 'eval_samples_per_second': 102.29, 'eval_steps_per_second': 6.451, 'epoch': 0.44}
{'loss': 0.8525, 'grad_norm': 0.04298572242259979, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8363186717033386, 'eval_runtime': 9.8428, 'eval_samples_per_second': 101.495, 'eval_steps_per_second': 6.401, 'epoch': 0.48}
{'loss': 0.8347, 'grad_norm': 0.04593459889292717, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8331776857376099, 'eval_runtime': 9.8439, 'eval_samples_per_second': 101.484, 'eval_steps_per_second': 6.4, 'epoch': 0.52}
{'loss': 0.8359, 'grad_norm': 0.04553057998418808, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8308570981025696, 'eval_runtime': 9.8486, 'eval_samples_per_second': 101.436, 'eval_steps_per_second': 6.397, 'epoch': 0.56}
{'loss': 0.8462, 'grad_norm': 0.051323626190423965, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8284504413604736, 'eval_runtime': 9.8727, 'eval_samples_per_second': 101.188, 'eval_steps_per_second': 6.381, 'epoch': 0.6}
{'loss': 0.8439, 'grad_norm': 0.04714914411306381, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8266993761062622, 'eval_runtime': 9.8137, 'eval_samples_per_second': 101.796, 'eval_steps_per_second': 6.42, 'epoch': 0.64}
{'loss': 0.8053, 'grad_norm': 0.04700300842523575, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8238916397094727, 'eval_runtime': 9.7902, 'eval_samples_per_second': 102.041, 'eval_steps_per_second': 6.435, 'epoch': 0.68}
{'loss': 0.8718, 'grad_norm': 0.04474431276321411, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.822709321975708, 'eval_runtime': 9.7986, 'eval_samples_per_second': 101.954, 'eval_steps_per_second': 6.43, 'epoch': 0.72}
{'loss': 0.8448, 'grad_norm': 0.045481424778699875, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8212003111839294, 'eval_runtime': 9.7903, 'eval_samples_per_second': 102.04, 'eval_steps_per_second': 6.435, 'epoch': 0.76}
{'loss': 0.8388, 'grad_norm': 0.06411048024892807, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8197849988937378, 'eval_runtime': 9.7964, 'eval_samples_per_second': 101.976, 'eval_steps_per_second': 6.431, 'epoch': 0.8}
{'loss': 0.8383, 'grad_norm': 0.05626566335558891, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8185868859291077, 'eval_runtime': 9.791, 'eval_samples_per_second': 102.032, 'eval_steps_per_second': 6.434, 'epoch': 0.84}
{'loss': 0.8294, 'grad_norm': 0.045601025223731995, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8177432417869568, 'eval_runtime': 9.7759, 'eval_samples_per_second': 102.19, 'eval_steps_per_second': 6.444, 'epoch': 0.88}
{'loss': 0.8396, 'grad_norm': 0.04603772237896919, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8168213367462158, 'eval_runtime': 9.7745, 'eval_samples_per_second': 102.205, 'eval_steps_per_second': 6.445, 'epoch': 0.92}
{'loss': 0.8324, 'grad_norm': 0.046801842749118805, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8163814544677734, 'eval_runtime': 9.7659, 'eval_samples_per_second': 102.295, 'eval_steps_per_second': 6.451, 'epoch': 0.96}
{'loss': 0.8344, 'grad_norm': 0.050979722291231155, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8161984086036682, 'eval_runtime': 9.7747, 'eval_samples_per_second': 102.202, 'eval_steps_per_second': 6.445, 'epoch': 1.0}
{'train_runtime': 593.3772, 'train_samples_per_second': 16.851, 'train_steps_per_second': 1.053, 'train_loss': 0.9560234588623047, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0852863788604736, 1.2045105695724487, 0.9555560350418091, 0.8949277400970459, 0.8701633810997009, 0.8621509075164795, 0.8552249670028687, 0.8513909578323364, 0.8463771939277649, 0.8423541784286499, 0.8388653993606567, 0.8363186717033386, 0.8331776857376099, 0.8308570981025696, 0.8284504413604736, 0.8266993761062622, 0.8238916397094727, 0.822709321975708, 0.8212003111839294, 0.8197849988937378, 0.8185868859291077, 0.8177432417869568, 0.8168213367462158, 0.8163814544677734, 0.8161984086036682], 'performance': [0.56, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:07,  1.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:08, 10.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.33it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 12.77it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 15.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 20.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.70it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.481907844543457
current iteration best possible performance (full train run):  0.5145
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0052 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.1906701104959903
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 1.0879939151682079
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.1933714865814218
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 1.1068657483623299
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.6664290564696376
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.8028, dtype=torch.float64), 0, tensor(0.1972, dtype=torch.float64), 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(1.1776e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.6862e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8028, dtype=torch.float64), tensor(1.0877e-16, dtype=torch.float64), tensor(0.1972, dtype=torch.float64), tensor(6.0763e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.803
  wikitext: 0
  mmlu: 0.197
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:26,  2.09s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:25,  3.50it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:13,  6.09it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:11,  6.72it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  8.00it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:06,  8.68it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.32it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:04,  8.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.41it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 10.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  8.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.86it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.79it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  7.97it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.4469, 'grad_norm': 0.39959245920181274, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.169999599456787, 'eval_runtime': 7.7955, 'eval_samples_per_second': 128.152, 'eval_steps_per_second': 8.082, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:58,  1.70it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 10.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:07,  9.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:05,  8.65it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04,  9.59it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:12<00:10,  3.21it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:12<00:06,  4.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:13<00:03,  5.21it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:14<00:02,  5.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:15<00:00,  7.00it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15<00:00,  6.61it/s]
Evaluation performance at step 50: 0.48
{'loss': 2.107, 'grad_norm': 0.2623634934425354, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.48}
{'eval_loss': 1.4010565280914307, 'eval_runtime': 7.8044, 'eval_samples_per_second': 128.004, 'eval_steps_per_second': 8.072, 'epoch': 0.08}
{'loss': 1.1743, 'grad_norm': 0.08685778826475143, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1481757164001465, 'eval_runtime': 7.8339, 'eval_samples_per_second': 127.522, 'eval_steps_per_second': 8.042, 'epoch': 0.12}
{'loss': 1.0619, 'grad_norm': 0.060749296098947525, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0167440176010132, 'eval_runtime': 7.8569, 'eval_samples_per_second': 127.149, 'eval_steps_per_second': 8.018, 'epoch': 0.16}
{'loss': 1.0093, 'grad_norm': 0.06604954600334167, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9899524450302124, 'eval_runtime': 7.8739, 'eval_samples_per_second': 126.874, 'eval_steps_per_second': 8.001, 'epoch': 0.2}
{'loss': 0.9637, 'grad_norm': 0.060165103524923325, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9670858979225159, 'eval_runtime': 7.8854, 'eval_samples_per_second': 126.69, 'eval_steps_per_second': 7.989, 'epoch': 0.24}
{'loss': 0.9591, 'grad_norm': 0.06476480513811111, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9443888664245605, 'eval_runtime': 7.9134, 'eval_samples_per_second': 126.242, 'eval_steps_per_second': 7.961, 'epoch': 0.28}
{'loss': 0.8823, 'grad_norm': 0.06685388833284378, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9238821268081665, 'eval_runtime': 7.8998, 'eval_samples_per_second': 126.459, 'eval_steps_per_second': 7.975, 'epoch': 0.32}
{'loss': 0.8928, 'grad_norm': 0.06268959492444992, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8999385237693787, 'eval_runtime': 7.9011, 'eval_samples_per_second': 126.438, 'eval_steps_per_second': 7.974, 'epoch': 0.36}
{'loss': 0.8724, 'grad_norm': 0.08293210715055466, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8735918402671814, 'eval_runtime': 7.9231, 'eval_samples_per_second': 126.087, 'eval_steps_per_second': 7.951, 'epoch': 0.4}
{'loss': 0.8446, 'grad_norm': 0.08933334797620773, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8458920121192932, 'eval_runtime': 7.893, 'eval_samples_per_second': 126.568, 'eval_steps_per_second': 7.982, 'epoch': 0.44}
{'loss': 0.8874, 'grad_norm': 0.08035153150558472, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8166995644569397, 'eval_runtime': 7.9093, 'eval_samples_per_second': 126.306, 'eval_steps_per_second': 7.965, 'epoch': 0.48}
{'loss': 0.7925, 'grad_norm': 0.08815934509038925, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.790638267993927, 'eval_runtime': 7.9099, 'eval_samples_per_second': 126.298, 'eval_steps_per_second': 7.965, 'epoch': 0.52}
{'loss': 0.7249, 'grad_norm': 0.13752125203609467, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7587572932243347, 'eval_runtime': 7.913, 'eval_samples_per_second': 126.247, 'eval_steps_per_second': 7.962, 'epoch': 0.56}
{'loss': 0.6901, 'grad_norm': 0.07812360674142838, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7329230904579163, 'eval_runtime': 7.8945, 'eval_samples_per_second': 126.543, 'eval_steps_per_second': 7.98, 'epoch': 0.6}
{'loss': 0.6548, 'grad_norm': 0.1029905304312706, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7041890621185303, 'eval_runtime': 7.8998, 'eval_samples_per_second': 126.46, 'eval_steps_per_second': 7.975, 'epoch': 0.64}
{'loss': 0.6573, 'grad_norm': 0.1318175196647644, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6772792339324951, 'eval_runtime': 7.8838, 'eval_samples_per_second': 126.716, 'eval_steps_per_second': 7.991, 'epoch': 0.68}
{'loss': 0.6354, 'grad_norm': 0.14389760792255402, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.660785973072052, 'eval_runtime': 7.8722, 'eval_samples_per_second': 126.903, 'eval_steps_per_second': 8.003, 'epoch': 0.72}
{'loss': 0.6731, 'grad_norm': 0.10217440873384476, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.642083466053009, 'eval_runtime': 7.843, 'eval_samples_per_second': 127.375, 'eval_steps_per_second': 8.033, 'epoch': 0.76}
{'loss': 0.5795, 'grad_norm': 0.1291084736585617, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6261106133460999, 'eval_runtime': 7.8463, 'eval_samples_per_second': 127.32, 'eval_steps_per_second': 8.029, 'epoch': 0.8}
{'loss': 0.6328, 'grad_norm': 0.11784558743238449, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6117599010467529, 'eval_runtime': 7.844, 'eval_samples_per_second': 127.359, 'eval_steps_per_second': 8.032, 'epoch': 0.84}
{'loss': 0.5425, 'grad_norm': 0.12384762614965439, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6013671159744263, 'eval_runtime': 7.8848, 'eval_samples_per_second': 126.699, 'eval_steps_per_second': 7.99, 'epoch': 0.88}
{'loss': 0.6372, 'grad_norm': 0.11300766468048096, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5946223139762878, 'eval_runtime': 7.856, 'eval_samples_per_second': 127.165, 'eval_steps_per_second': 8.019, 'epoch': 0.92}
{'loss': 0.489, 'grad_norm': 0.1195969209074974, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5894953012466431, 'eval_runtime': 7.8505, 'eval_samples_per_second': 127.254, 'eval_steps_per_second': 8.025, 'epoch': 0.96}
{'loss': 0.5842, 'grad_norm': 0.16258499026298523, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5874178409576416, 'eval_runtime': 7.8638, 'eval_samples_per_second': 127.037, 'eval_steps_per_second': 8.011, 'epoch': 1.0}
{'train_runtime': 504.0083, 'train_samples_per_second': 19.839, 'train_steps_per_second': 1.24, 'train_loss': 0.9758017166137696, 'epoch': 1.0}
train_results:  {'eval_loss': [3.169999599456787, 1.4010565280914307, 1.1481757164001465, 1.0167440176010132, 0.9899524450302124, 0.9670858979225159, 0.9443888664245605, 0.9238821268081665, 0.8999385237693787, 0.8735918402671814, 0.8458920121192932, 0.8166995644569397, 0.790638267993927, 0.7587572932243347, 0.7329230904579163, 0.7041890621185303, 0.6772792339324951, 0.660785973072052, 0.642083466053009, 0.6261106133460999, 0.6117599010467529, 0.6013671159744263, 0.5946223139762878, 0.5894953012466431, 0.5874178409576416], 'performance': [0.54, 0.48]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:24,  1.18it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 19.24it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 13.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:06,  5.61it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  7.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 11.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.05it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.48]
current iteration observed (possibly low-fid or predicted) performance:  1.489302635192871
current iteration best possible performance (full train run):  0.672
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8174 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.7122517422634813
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.9286584450045829
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.1951350125249633
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.2689686047804665
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.229643049333021
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 32, 0, 0, 0, 0, 1, 128, 3.6107514858430517e-17, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(2.6710e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8050e-16, dtype=torch.float64), tensor(9.2121e-17, dtype=torch.float64), tensor(4.5417e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.6108e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 1.0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.6107514858430517e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  3.6107514858430517e-17
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:26,  1.48s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:15,  5.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  9.06it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:07,  9.85it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.61it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:04, 12.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 13.16it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 13.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.56it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 15.58it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 10.38it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01, 10.68it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 12.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.42it/s]
Evaluation performance at step 25: 0.55
{'loss': 3.4741, 'grad_norm': 0.08955937623977661, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.0760672092437744, 'eval_runtime': 9.0771, 'eval_samples_per_second': 110.168, 'eval_steps_per_second': 6.941, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:46,  4.11s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:36,  2.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:18,  4.55it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:11,  6.62it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:07,  8.50it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:05, 10.02it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:04, 11.17it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:04, 10.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:02, 12.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 12.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  9.30it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  9.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 11.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.36it/s]
Evaluation performance at step 50: 0.56
{'loss': 2.2621, 'grad_norm': 0.02745206467807293, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.8221567869186401, 'eval_runtime': 9.0638, 'eval_samples_per_second': 110.329, 'eval_steps_per_second': 6.951, 'epoch': 0.08}
{'loss': 1.7275, 'grad_norm': 0.027442917227745056, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6407899856567383, 'eval_runtime': 9.1123, 'eval_samples_per_second': 109.741, 'eval_steps_per_second': 6.914, 'epoch': 0.12}
{'loss': 1.6169, 'grad_norm': 0.03638271242380142, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5660827159881592, 'eval_runtime': 9.1922, 'eval_samples_per_second': 108.787, 'eval_steps_per_second': 6.854, 'epoch': 0.16}
{'loss': 1.5295, 'grad_norm': 0.03685601055622101, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4776010513305664, 'eval_runtime': 9.1795, 'eval_samples_per_second': 108.939, 'eval_steps_per_second': 6.863, 'epoch': 0.2}
{'loss': 1.4713, 'grad_norm': 0.026046667248010635, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.4100937843322754, 'eval_runtime': 9.2559, 'eval_samples_per_second': 108.039, 'eval_steps_per_second': 6.806, 'epoch': 0.24}
{'loss': 1.4573, 'grad_norm': 0.024547291919589043, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.3858486413955688, 'eval_runtime': 9.2516, 'eval_samples_per_second': 108.09, 'eval_steps_per_second': 6.81, 'epoch': 0.28}
{'loss': 1.3872, 'grad_norm': 0.027682961896061897, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.366089940071106, 'eval_runtime': 9.2853, 'eval_samples_per_second': 107.697, 'eval_steps_per_second': 6.785, 'epoch': 0.32}
{'loss': 1.3673, 'grad_norm': 0.026527125388383865, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.347231149673462, 'eval_runtime': 9.3061, 'eval_samples_per_second': 107.456, 'eval_steps_per_second': 6.77, 'epoch': 0.36}
{'loss': 1.4177, 'grad_norm': 0.026748627424240112, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3257282972335815, 'eval_runtime': 9.2718, 'eval_samples_per_second': 107.854, 'eval_steps_per_second': 6.795, 'epoch': 0.4}
{'loss': 1.3799, 'grad_norm': 0.0332673154771328, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2963513135910034, 'eval_runtime': 9.2748, 'eval_samples_per_second': 107.819, 'eval_steps_per_second': 6.793, 'epoch': 0.44}
{'loss': 1.2958, 'grad_norm': 0.03697655349969864, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2563483715057373, 'eval_runtime': 9.2863, 'eval_samples_per_second': 107.685, 'eval_steps_per_second': 6.784, 'epoch': 0.48}
{'loss': 1.2905, 'grad_norm': 0.026605984196066856, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2359328269958496, 'eval_runtime': 9.2707, 'eval_samples_per_second': 107.867, 'eval_steps_per_second': 6.796, 'epoch': 0.52}
{'loss': 1.2804, 'grad_norm': 0.03274263069033623, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2196909189224243, 'eval_runtime': 9.2805, 'eval_samples_per_second': 107.752, 'eval_steps_per_second': 6.788, 'epoch': 0.56}
{'loss': 1.2336, 'grad_norm': 0.034850116819143295, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2046828269958496, 'eval_runtime': 9.2932, 'eval_samples_per_second': 107.605, 'eval_steps_per_second': 6.779, 'epoch': 0.6}
{'loss': 1.2253, 'grad_norm': 0.03669010102748871, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1905829906463623, 'eval_runtime': 9.2687, 'eval_samples_per_second': 107.89, 'eval_steps_per_second': 6.797, 'epoch': 0.64}
{'loss': 1.182, 'grad_norm': 0.028774583712220192, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.184605598449707, 'eval_runtime': 9.2576, 'eval_samples_per_second': 108.019, 'eval_steps_per_second': 6.805, 'epoch': 0.68}
{'loss': 1.2047, 'grad_norm': 0.028189608827233315, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.180983543395996, 'eval_runtime': 9.305, 'eval_samples_per_second': 107.469, 'eval_steps_per_second': 6.771, 'epoch': 0.72}
{'loss': 1.2289, 'grad_norm': 0.034753136336803436, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1781282424926758, 'eval_runtime': 9.2912, 'eval_samples_per_second': 107.629, 'eval_steps_per_second': 6.781, 'epoch': 0.76}
{'loss': 1.2489, 'grad_norm': 0.034673430025577545, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1761481761932373, 'eval_runtime': 9.2907, 'eval_samples_per_second': 107.634, 'eval_steps_per_second': 6.781, 'epoch': 0.8}
{'loss': 1.2292, 'grad_norm': 0.032202720642089844, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1741081476211548, 'eval_runtime': 9.3104, 'eval_samples_per_second': 107.407, 'eval_steps_per_second': 6.767, 'epoch': 0.84}
{'loss': 1.2042, 'grad_norm': 0.033413276076316833, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1726042032241821, 'eval_runtime': 9.2787, 'eval_samples_per_second': 107.773, 'eval_steps_per_second': 6.79, 'epoch': 0.88}
{'loss': 1.2131, 'grad_norm': 0.0320965051651001, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1714389324188232, 'eval_runtime': 9.2746, 'eval_samples_per_second': 107.822, 'eval_steps_per_second': 6.793, 'epoch': 0.92}
{'loss': 1.2493, 'grad_norm': 0.029146356508135796, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1707478761672974, 'eval_runtime': 9.2683, 'eval_samples_per_second': 107.895, 'eval_steps_per_second': 6.797, 'epoch': 0.96}
{'loss': 1.2018, 'grad_norm': 0.038173750042915344, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1704038381576538, 'eval_runtime': 9.2748, 'eval_samples_per_second': 107.819, 'eval_steps_per_second': 6.793, 'epoch': 1.0}
{'train_runtime': 537.8118, 'train_samples_per_second': 18.594, 'train_steps_per_second': 1.162, 'train_loss': 1.4551399444580078, 'epoch': 1.0}
train_results:  {'eval_loss': [3.0760672092437744, 1.8221567869186401, 1.6407899856567383, 1.5660827159881592, 1.4776010513305664, 1.4100937843322754, 1.3858486413955688, 1.366089940071106, 1.347231149673462, 1.3257282972335815, 1.2963513135910034, 1.2563483715057373, 1.2359328269958496, 1.2196909189224243, 1.2046828269958496, 1.1905829906463623, 1.184605598449707, 1.180983543395996, 1.1781282424926758, 1.1761481761932373, 1.1741081476211548, 1.1726042032241821, 1.1714389324188232, 1.1707478761672974, 1.1704038381576538], 'performance': [0.55, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:50,  1.12s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 13.18it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 13.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:03,  6.31it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  8.76it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.36it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.461254358291626
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5158 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 1.2051722435229346
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9888554685361137
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.162962294192965
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7388981519622433
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 1.1378151737047908
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [0, 0, 0, tensor(0.6029, dtype=torch.float64), 0, 0, tensor(0.3971, dtype=torch.float64), 0, 0, 32, 0, 0, 0, 0, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.1377e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6029, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.3757e-17, dtype=torch.float64), tensor(0.3971, dtype=torch.float64), tensor(4.3464e-17, dtype=torch.float64), tensor(8.8664e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.603
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.397
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:09<15:08,  9.18s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:12<01:44,  1.15s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:14<00:52,  1.57it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:17<00:39,  1.90it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:21<00:32,  2.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:24<00:27,  2.14it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:29<00:25,  2.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:32<00:21,  2.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:35<00:14,  2.41it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:37<00:09,  2.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:41<00:08,  2.31it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:45<00:04,  2.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:48<00:01,  2.37it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.06it/s]
Evaluation performance at step 25: 0.55
{'loss': 3.5612, 'grad_norm': 0.3330984115600586, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 2.236999750137329, 'eval_runtime': 8.9071, 'eval_samples_per_second': 112.157, 'eval_steps_per_second': 7.073, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:19,  2.62s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:45,  2.00it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:07<00:29,  2.83it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:32,  2.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:13<00:24,  2.72it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:14<00:17,  3.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:16<00:14,  3.63it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:19<00:12,  3.42it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:21<00:09,  3.51it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:24<00:08,  3.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:26<00:05,  3.29it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:29<00:03,  3.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:31<00:00,  3.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.13it/s]
Evaluation performance at step 50: 0.56
{'loss': 1.9126, 'grad_norm': 0.24863457679748535, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.698997139930725, 'eval_runtime': 9.2897, 'eval_samples_per_second': 107.538, 'eval_steps_per_second': 6.782, 'epoch': 0.08}
{'loss': 1.6066, 'grad_norm': 0.3156512975692749, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.549689769744873, 'eval_runtime': 9.643, 'eval_samples_per_second': 103.598, 'eval_steps_per_second': 6.533, 'epoch': 0.12}
{'loss': 1.4469, 'grad_norm': 0.3444536030292511, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.4432518482208252, 'eval_runtime': 9.4031, 'eval_samples_per_second': 106.241, 'eval_steps_per_second': 6.7, 'epoch': 0.16}
{'loss': 1.4577, 'grad_norm': 0.25154566764831543, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.351850152015686, 'eval_runtime': 9.614, 'eval_samples_per_second': 103.911, 'eval_steps_per_second': 6.553, 'epoch': 0.2}
{'loss': 1.4051, 'grad_norm': 0.2441353052854538, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.298224925994873, 'eval_runtime': 9.5302, 'eval_samples_per_second': 104.824, 'eval_steps_per_second': 6.611, 'epoch': 0.24}
{'loss': 1.4217, 'grad_norm': 0.2814604640007019, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.2749474048614502, 'eval_runtime': 9.3559, 'eval_samples_per_second': 106.778, 'eval_steps_per_second': 6.734, 'epoch': 0.28}
{'loss': 1.3346, 'grad_norm': 0.2510632872581482, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.2606099843978882, 'eval_runtime': 9.0302, 'eval_samples_per_second': 110.628, 'eval_steps_per_second': 6.977, 'epoch': 0.32}
{'loss': 1.2875, 'grad_norm': 0.22854211926460266, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.2485651969909668, 'eval_runtime': 9.3905, 'eval_samples_per_second': 106.384, 'eval_steps_per_second': 6.709, 'epoch': 0.36}
{'loss': 1.3713, 'grad_norm': 0.21689407527446747, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.2336704730987549, 'eval_runtime': 9.2403, 'eval_samples_per_second': 108.114, 'eval_steps_per_second': 6.818, 'epoch': 0.4}
{'loss': 1.2551, 'grad_norm': 0.3311136066913605, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.2187203168869019, 'eval_runtime': 9.6183, 'eval_samples_per_second': 103.865, 'eval_steps_per_second': 6.55, 'epoch': 0.44}
{'loss': 1.3219, 'grad_norm': 0.26985010504722595, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.2036408185958862, 'eval_runtime': 9.8703, 'eval_samples_per_second': 101.212, 'eval_steps_per_second': 6.383, 'epoch': 0.48}
{'loss': 1.2225, 'grad_norm': 0.23374471068382263, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.1982289552688599, 'eval_runtime': 9.1768, 'eval_samples_per_second': 108.861, 'eval_steps_per_second': 6.865, 'epoch': 0.52}
{'loss': 1.2271, 'grad_norm': 0.23996233940124512, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.1865990161895752, 'eval_runtime': 9.0576, 'eval_samples_per_second': 110.294, 'eval_steps_per_second': 6.956, 'epoch': 0.56}
{'loss': 1.3, 'grad_norm': 0.3617055118083954, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.177769422531128, 'eval_runtime': 8.616, 'eval_samples_per_second': 115.948, 'eval_steps_per_second': 7.312, 'epoch': 0.6}
{'loss': 1.3074, 'grad_norm': 0.2142048329114914, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1709011793136597, 'eval_runtime': 8.6178, 'eval_samples_per_second': 115.923, 'eval_steps_per_second': 7.31, 'epoch': 0.64}
{'loss': 1.3438, 'grad_norm': 0.3796643912792206, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.159603238105774, 'eval_runtime': 9.5749, 'eval_samples_per_second': 104.336, 'eval_steps_per_second': 6.58, 'epoch': 0.68}
{'loss': 1.2807, 'grad_norm': 0.25659704208374023, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.1542184352874756, 'eval_runtime': 9.4493, 'eval_samples_per_second': 105.722, 'eval_steps_per_second': 6.667, 'epoch': 0.72}
{'loss': 1.2597, 'grad_norm': 0.33756741881370544, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.1481504440307617, 'eval_runtime': 9.1708, 'eval_samples_per_second': 108.933, 'eval_steps_per_second': 6.87, 'epoch': 0.76}
{'loss': 1.1978, 'grad_norm': 0.2587280869483948, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.140138864517212, 'eval_runtime': 10.1164, 'eval_samples_per_second': 98.751, 'eval_steps_per_second': 6.228, 'epoch': 0.8}
{'loss': 1.3177, 'grad_norm': 0.2756775915622711, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.1365885734558105, 'eval_runtime': 9.1084, 'eval_samples_per_second': 109.679, 'eval_steps_per_second': 6.917, 'epoch': 0.84}
{'loss': 1.2971, 'grad_norm': 0.25553300976753235, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1331453323364258, 'eval_runtime': 9.1455, 'eval_samples_per_second': 109.233, 'eval_steps_per_second': 6.889, 'epoch': 0.88}
{'loss': 1.2511, 'grad_norm': 0.2740749418735504, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1283161640167236, 'eval_runtime': 9.594, 'eval_samples_per_second': 104.127, 'eval_steps_per_second': 6.567, 'epoch': 0.92}
{'loss': 1.1989, 'grad_norm': 0.2472667396068573, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1251285076141357, 'eval_runtime': 8.9675, 'eval_samples_per_second': 111.402, 'eval_steps_per_second': 7.025, 'epoch': 0.96}
{'loss': 1.2079, 'grad_norm': 0.2947847545146942, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1245256662368774, 'eval_runtime': 9.3926, 'eval_samples_per_second': 106.36, 'eval_steps_per_second': 6.707, 'epoch': 1.0}
{'train_runtime': 834.5282, 'train_samples_per_second': 11.982, 'train_steps_per_second': 0.749, 'train_loss': 1.4317574981689454, 'epoch': 1.0}
train_results:  {'eval_loss': [2.236999750137329, 1.698997139930725, 1.549689769744873, 1.4432518482208252, 1.351850152015686, 1.298224925994873, 1.2749474048614502, 1.2606099843978882, 1.2485651969909668, 1.2336704730987549, 1.2187203168869019, 1.2036408185958862, 1.1982289552688599, 1.1865990161895752, 1.177769422531128, 1.1709011793136597, 1.159603238105774, 1.1542184352874756, 1.1481504440307617, 1.140138864517212, 1.1365885734558105, 1.1331453323364258, 1.1283161640167236, 1.1251285076141357, 1.1245256662368774], 'performance': [0.55, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:11,  2.54s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:30<02:27,  1.78s/it]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:33<00:57,  1.16it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [01:01<01:03,  1.25s/it]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [01:03<00:28,  1.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [01:21<00:17,  1.06it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:23<00:01,  1.53it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:23<00:00,  1.20it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.4595184326171875
current iteration best possible performance (full train run):  0.441
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1272 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.9454464503410738
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.8592214717261193
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.9648719288567996
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.8116055072134639
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 1.073387061163557
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.5431, dtype=torch.float64), 0, 0, tensor(0.4569, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 5.689893001203928e-17, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.6777e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.8385e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5431, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3168e-16, dtype=torch.float64), tensor(0.4569, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(5.6899e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.543
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.457

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.689893001203928e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  5.689893001203928e-17
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:46,  1.68s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.35it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.55it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.33it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.90it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.05it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 11.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.96it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.26it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.39it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.69it/s]
Evaluation performance at step 25: 0.52
{'loss': 4.2492, 'grad_norm': 0.4394215941429138, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.52}
{'eval_loss': 3.2297751903533936, 'eval_runtime': 5.9792, 'eval_samples_per_second': 167.078, 'eval_steps_per_second': 10.536, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:39,  2.54it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.93it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.90it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:20,  3.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  5.12it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  7.00it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.02it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:02, 12.64it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 13.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 13.87it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 15.29it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 15.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.24it/s]
Evaluation performance at step 50: 0.57
{'loss': 2.134, 'grad_norm': 0.31490734219551086, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 1.3462578058242798, 'eval_runtime': 5.9913, 'eval_samples_per_second': 166.742, 'eval_steps_per_second': 10.515, 'epoch': 0.08}
{'loss': 1.0842, 'grad_norm': 0.06246219575405121, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9889904260635376, 'eval_runtime': 6.0094, 'eval_samples_per_second': 166.241, 'eval_steps_per_second': 10.484, 'epoch': 0.12}
{'loss': 0.9386, 'grad_norm': 0.11638146638870239, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8793590664863586, 'eval_runtime': 6.0439, 'eval_samples_per_second': 165.29, 'eval_steps_per_second': 10.424, 'epoch': 0.16}
{'loss': 0.835, 'grad_norm': 0.06153003126382828, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8444883823394775, 'eval_runtime': 6.0817, 'eval_samples_per_second': 164.264, 'eval_steps_per_second': 10.359, 'epoch': 0.2}
{'loss': 0.8354, 'grad_norm': 0.0483526736497879, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8303935527801514, 'eval_runtime': 6.0792, 'eval_samples_per_second': 164.33, 'eval_steps_per_second': 10.363, 'epoch': 0.24}
{'loss': 0.8157, 'grad_norm': 0.05618268996477127, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8188208937644958, 'eval_runtime': 6.0865, 'eval_samples_per_second': 164.134, 'eval_steps_per_second': 10.351, 'epoch': 0.28}
{'loss': 0.817, 'grad_norm': 0.055202145129442215, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8118361234664917, 'eval_runtime': 6.0883, 'eval_samples_per_second': 164.085, 'eval_steps_per_second': 10.348, 'epoch': 0.32}
{'loss': 0.7861, 'grad_norm': 0.0712444931268692, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8028691411018372, 'eval_runtime': 6.1241, 'eval_samples_per_second': 163.126, 'eval_steps_per_second': 10.287, 'epoch': 0.36}
{'loss': 0.799, 'grad_norm': 0.05370838940143585, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7958259582519531, 'eval_runtime': 6.1335, 'eval_samples_per_second': 162.876, 'eval_steps_per_second': 10.271, 'epoch': 0.4}
{'loss': 0.7929, 'grad_norm': 0.07037100940942764, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7881227135658264, 'eval_runtime': 6.3031, 'eval_samples_per_second': 158.493, 'eval_steps_per_second': 9.995, 'epoch': 0.44}
{'loss': 0.7893, 'grad_norm': 0.057737771421670914, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7819615602493286, 'eval_runtime': 6.3386, 'eval_samples_per_second': 157.605, 'eval_steps_per_second': 9.939, 'epoch': 0.48}
{'loss': 0.8043, 'grad_norm': 0.06722169369459152, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7778823971748352, 'eval_runtime': 6.4494, 'eval_samples_per_second': 154.897, 'eval_steps_per_second': 9.768, 'epoch': 0.52}
{'loss': 0.7748, 'grad_norm': 0.07568898797035217, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7723070979118347, 'eval_runtime': 6.4073, 'eval_samples_per_second': 155.916, 'eval_steps_per_second': 9.833, 'epoch': 0.56}
{'loss': 0.7743, 'grad_norm': 0.06374982744455338, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7675498127937317, 'eval_runtime': 6.4509, 'eval_samples_per_second': 154.863, 'eval_steps_per_second': 9.766, 'epoch': 0.6}
{'loss': 0.7813, 'grad_norm': 0.08335129916667938, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7636297941207886, 'eval_runtime': 6.4577, 'eval_samples_per_second': 154.7, 'eval_steps_per_second': 9.756, 'epoch': 0.64}
{'loss': 0.7767, 'grad_norm': 0.08088257163763046, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.759560227394104, 'eval_runtime': 6.4011, 'eval_samples_per_second': 156.066, 'eval_steps_per_second': 9.842, 'epoch': 0.68}
{'loss': 0.7513, 'grad_norm': 0.07756219059228897, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7560797929763794, 'eval_runtime': 6.5224, 'eval_samples_per_second': 153.165, 'eval_steps_per_second': 9.659, 'epoch': 0.72}
{'loss': 0.7627, 'grad_norm': 0.07230997830629349, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7526417374610901, 'eval_runtime': 6.3564, 'eval_samples_per_second': 157.166, 'eval_steps_per_second': 9.911, 'epoch': 0.76}
{'loss': 0.7604, 'grad_norm': 0.08801195025444031, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.749025285243988, 'eval_runtime': 6.4734, 'eval_samples_per_second': 154.323, 'eval_steps_per_second': 9.732, 'epoch': 0.8}
{'loss': 0.7516, 'grad_norm': 0.0777549222111702, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7467307448387146, 'eval_runtime': 6.1549, 'eval_samples_per_second': 162.309, 'eval_steps_per_second': 10.236, 'epoch': 0.84}
{'loss': 0.7476, 'grad_norm': 0.07685074210166931, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7438814640045166, 'eval_runtime': 6.1351, 'eval_samples_per_second': 162.835, 'eval_steps_per_second': 10.269, 'epoch': 0.88}
{'loss': 0.7628, 'grad_norm': 0.08355453610420227, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7418990135192871, 'eval_runtime': 6.2379, 'eval_samples_per_second': 160.151, 'eval_steps_per_second': 10.1, 'epoch': 0.92}
{'loss': 0.7385, 'grad_norm': 0.08583465218544006, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.740974485874176, 'eval_runtime': 14.1231, 'eval_samples_per_second': 70.735, 'eval_steps_per_second': 4.461, 'epoch': 0.96}
{'loss': 0.7498, 'grad_norm': 0.07972480356693268, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7405558228492737, 'eval_runtime': 12.3006, 'eval_samples_per_second': 81.216, 'eval_steps_per_second': 5.122, 'epoch': 1.0}
{'train_runtime': 448.9329, 'train_samples_per_second': 22.273, 'train_steps_per_second': 1.392, 'train_loss': 0.9924960266113281, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2297751903533936, 1.3462578058242798, 0.9889904260635376, 0.8793590664863586, 0.8444883823394775, 0.8303935527801514, 0.8188208937644958, 0.8118361234664917, 0.8028691411018372, 0.7958259582519531, 0.7881227135658264, 0.7819615602493286, 0.7778823971748352, 0.7723070979118347, 0.7675498127937317, 0.7636297941207886, 0.759560227394104, 0.7560797929763794, 0.7526417374610901, 0.749025285243988, 0.7467307448387146, 0.7438814640045166, 0.7418990135192871, 0.740974485874176, 0.7405558228492737], 'performance': [0.52, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:29<48:44, 29.54s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:33<02:00,  1.46s/it]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:37<00:51,  1.31it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [01:04<01:00,  1.18s/it]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [01:08<00:28,  1.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [01:11<00:11,  1.68it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [01:14<00:01,  2.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:14<00:00,  1.35it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.52, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.4905487298965454
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.2004 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 1.178313274746209
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.9181384193056286
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.9728898081228758
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 1.0651311004341628
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 1.1388955413738042
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1798, dtype=torch.float64), 0, tensor(0.8202, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 0, 1, 128, 0.0, 1.4800000190734863, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.3247e-16, dtype=torch.float64), tensor(2.8657e-16, dtype=torch.float64), tensor(0.1798, dtype=torch.float64), tensor(1.2209e-16, dtype=torch.float64), tensor(0.8202, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3591e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.18
  triviaqa: 0
  truthfulqa_gen: 0.82
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:15,  2.58s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:31,  2.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:04<00:17,  4.73it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:14,  5.33it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:10,  6.39it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:08,  7.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:06,  7.47it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:09<00:05,  7.41it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:10<00:04,  8.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:11<00:03,  8.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:13<00:03,  6.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:15<00:01,  5.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:15<00:00,  6.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15<00:00,  6.33it/s]
Evaluation performance at step 25: 0.53
{'loss': 5.0045, 'grad_norm': 0.2723817825317383, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.53}
{'eval_loss': 3.441582679748535, 'eval_runtime': 3.7875, 'eval_samples_per_second': 263.764, 'eval_steps_per_second': 16.634, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:06,  1.48it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.94it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 10.47it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:08<00:31,  2.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:09<00:20,  3.34it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:09<00:12,  4.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:11<00:09,  5.27it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:11<00:06,  6.37it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:12<00:04,  7.88it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:12<00:03,  8.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:02,  7.78it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:15<00:01,  7.68it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:16<00:00,  8.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16<00:00,  6.23it/s]
Evaluation performance at step 50: 0.55
{'loss': 2.1722, 'grad_norm': 0.08207214623689651, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 1.4675371646881104, 'eval_runtime': 3.8071, 'eval_samples_per_second': 262.406, 'eval_steps_per_second': 16.548, 'epoch': 0.08}
{'loss': 1.358, 'grad_norm': 0.06995663046836853, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.193084478378296, 'eval_runtime': 3.7984, 'eval_samples_per_second': 263.002, 'eval_steps_per_second': 16.586, 'epoch': 0.12}
{'loss': 1.1795, 'grad_norm': 0.04784005135297775, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1084010601043701, 'eval_runtime': 3.8092, 'eval_samples_per_second': 262.259, 'eval_steps_per_second': 16.539, 'epoch': 0.16}
{'loss': 1.0774, 'grad_norm': 0.05229217931628227, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0591894388198853, 'eval_runtime': 3.7988, 'eval_samples_per_second': 262.981, 'eval_steps_per_second': 16.584, 'epoch': 0.2}
{'loss': 1.0405, 'grad_norm': 0.05760757625102997, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0070090293884277, 'eval_runtime': 3.8625, 'eval_samples_per_second': 258.641, 'eval_steps_per_second': 16.311, 'epoch': 0.24}
{'loss': 0.9688, 'grad_norm': 0.06119502708315849, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9421967267990112, 'eval_runtime': 3.8057, 'eval_samples_per_second': 262.498, 'eval_steps_per_second': 16.554, 'epoch': 0.28}
{'loss': 0.887, 'grad_norm': 0.08067942410707474, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8276770710945129, 'eval_runtime': 3.8317, 'eval_samples_per_second': 260.723, 'eval_steps_per_second': 16.442, 'epoch': 0.32}
{'loss': 0.7696, 'grad_norm': 0.07183079421520233, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.72798752784729, 'eval_runtime': 3.8221, 'eval_samples_per_second': 261.374, 'eval_steps_per_second': 16.483, 'epoch': 0.36}
{'loss': 0.6911, 'grad_norm': 0.0851278305053711, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6427706480026245, 'eval_runtime': 3.8231, 'eval_samples_per_second': 261.303, 'eval_steps_per_second': 16.479, 'epoch': 0.4}
{'loss': 0.6335, 'grad_norm': 0.0957733765244484, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5731302499771118, 'eval_runtime': 3.8167, 'eval_samples_per_second': 261.748, 'eval_steps_per_second': 16.507, 'epoch': 0.44}
{'loss': 0.568, 'grad_norm': 0.10404293984174728, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5344755053520203, 'eval_runtime': 3.8119, 'eval_samples_per_second': 262.073, 'eval_steps_per_second': 16.527, 'epoch': 0.48}
{'loss': 0.5588, 'grad_norm': 0.1321217119693756, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5049974322319031, 'eval_runtime': 3.8168, 'eval_samples_per_second': 261.738, 'eval_steps_per_second': 16.506, 'epoch': 0.52}
{'loss': 0.488, 'grad_norm': 0.11454617977142334, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.46095284819602966, 'eval_runtime': 3.811, 'eval_samples_per_second': 262.138, 'eval_steps_per_second': 16.531, 'epoch': 0.56}
{'loss': 0.4678, 'grad_norm': 0.15121248364448547, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4264190196990967, 'eval_runtime': 3.8171, 'eval_samples_per_second': 261.717, 'eval_steps_per_second': 16.505, 'epoch': 0.6}
{'loss': 0.4613, 'grad_norm': 0.1198592558503151, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4001634120941162, 'eval_runtime': 3.8201, 'eval_samples_per_second': 261.512, 'eval_steps_per_second': 16.492, 'epoch': 0.64}
{'loss': 0.4032, 'grad_norm': 0.10921436548233032, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.37390831112861633, 'eval_runtime': 3.8284, 'eval_samples_per_second': 260.945, 'eval_steps_per_second': 16.456, 'epoch': 0.68}
{'loss': 0.3933, 'grad_norm': 0.14990754425525665, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.35355237126350403, 'eval_runtime': 3.8115, 'eval_samples_per_second': 262.101, 'eval_steps_per_second': 16.529, 'epoch': 0.72}
{'loss': 0.3873, 'grad_norm': 0.1168893352150917, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.33603423833847046, 'eval_runtime': 3.8328, 'eval_samples_per_second': 260.647, 'eval_steps_per_second': 16.437, 'epoch': 0.76}
{'loss': 0.3759, 'grad_norm': 0.10167746990919113, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3224659562110901, 'eval_runtime': 3.8117, 'eval_samples_per_second': 262.088, 'eval_steps_per_second': 16.528, 'epoch': 0.8}
{'loss': 0.3625, 'grad_norm': 0.1255200356245041, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.31016263365745544, 'eval_runtime': 3.7995, 'eval_samples_per_second': 262.931, 'eval_steps_per_second': 16.581, 'epoch': 0.84}
{'loss': 0.362, 'grad_norm': 0.1356492042541504, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3003096878528595, 'eval_runtime': 3.8361, 'eval_samples_per_second': 260.423, 'eval_steps_per_second': 16.423, 'epoch': 0.88}
{'loss': 0.3521, 'grad_norm': 0.13059645891189575, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2932184338569641, 'eval_runtime': 3.7898, 'eval_samples_per_second': 263.602, 'eval_steps_per_second': 16.624, 'epoch': 0.92}
{'loss': 0.3359, 'grad_norm': 0.14529506862163544, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.28897473216056824, 'eval_runtime': 3.7722, 'eval_samples_per_second': 264.831, 'eval_steps_per_second': 16.701, 'epoch': 0.96}
{'loss': 0.3141, 'grad_norm': 0.1636623591184616, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2872634530067444, 'eval_runtime': 3.7673, 'eval_samples_per_second': 265.174, 'eval_steps_per_second': 16.723, 'epoch': 1.0}
{'train_runtime': 319.3831, 'train_samples_per_second': 31.307, 'train_steps_per_second': 1.957, 'train_loss': 0.8644986495971679, 'epoch': 1.0}
train_results:  {'eval_loss': [3.441582679748535, 1.4675371646881104, 1.193084478378296, 1.1084010601043701, 1.0591894388198853, 1.0070090293884277, 0.9421967267990112, 0.8276770710945129, 0.72798752784729, 0.6427706480026245, 0.5731302499771118, 0.5344755053520203, 0.5049974322319031, 0.46095284819602966, 0.4264190196990967, 0.4001634120941162, 0.37390831112861633, 0.35355237126350403, 0.33603423833847046, 0.3224659562110901, 0.31016263365745544, 0.3003096878528595, 0.2932184338569641, 0.28897473216056824, 0.2872634530067444], 'performance': [0.53, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:39,  5.85s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:23,  3.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:09,  6.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:05,  9.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:02, 12.87it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 15.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 18.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.22it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.53, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.4822300672531128
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3097 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 1.1607577585600148
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 1.2150551631984727
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 1.133251930239832
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.998268856899875
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 1.1848869732232816
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.3298, dtype=torch.float64), 0, 0, tensor(0.6699, dtype=torch.float64), 32, 0, 1, 0, 1, 1, 128, 4.938540895671435e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(4.8793e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(1.3568e-16, dtype=torch.float64), tensor(0.3298, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6699, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(4.9385e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.33
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.67

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.938540895671435e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  4.938540895671435e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:00,  1.82s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  4.02it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.21it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.85it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.30it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.08it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.64it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04,  9.97it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.78it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.28it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.80it/s]
Evaluation performance at step 25: 0.55
{'loss': 3.9594, 'grad_norm': 0.3103677034378052, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 2.829458475112915, 'eval_runtime': 6.8019, 'eval_samples_per_second': 146.871, 'eval_steps_per_second': 9.262, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:22,  1.21it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.95it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.20it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:14,  4.49it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.14it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:12<00:15,  3.23it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:13<00:10,  4.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:13<00:06,  5.75it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:14<00:03,  6.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:02,  8.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:15<00:01,  9.86it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:15<00:00, 10.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15<00:00,  6.40it/s]
Evaluation performance at step 50: 0.57
{'loss': 1.8323, 'grad_norm': 0.23919139802455902, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 1.1519320011138916, 'eval_runtime': 6.7742, 'eval_samples_per_second': 147.472, 'eval_steps_per_second': 9.3, 'epoch': 0.08}
{'loss': 1.0493, 'grad_norm': 0.07642568647861481, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9420340657234192, 'eval_runtime': 6.8478, 'eval_samples_per_second': 145.885, 'eval_steps_per_second': 9.2, 'epoch': 0.12}
{'loss': 0.8896, 'grad_norm': 0.053240206092596054, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8398334383964539, 'eval_runtime': 6.8277, 'eval_samples_per_second': 146.315, 'eval_steps_per_second': 9.227, 'epoch': 0.16}
{'loss': 0.8477, 'grad_norm': 0.042772360146045685, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8210800886154175, 'eval_runtime': 6.8312, 'eval_samples_per_second': 146.24, 'eval_steps_per_second': 9.222, 'epoch': 0.2}
{'loss': 0.812, 'grad_norm': 0.04231863468885422, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8064860105514526, 'eval_runtime': 6.8398, 'eval_samples_per_second': 146.057, 'eval_steps_per_second': 9.211, 'epoch': 0.24}
{'loss': 0.8316, 'grad_norm': 0.04471304640173912, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7957794070243835, 'eval_runtime': 6.843, 'eval_samples_per_second': 145.988, 'eval_steps_per_second': 9.206, 'epoch': 0.28}
{'loss': 0.7841, 'grad_norm': 0.04336300492286682, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7820563912391663, 'eval_runtime': 6.8486, 'eval_samples_per_second': 145.869, 'eval_steps_per_second': 9.199, 'epoch': 0.32}
{'loss': 0.8022, 'grad_norm': 0.04624820128083229, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7694823741912842, 'eval_runtime': 6.8781, 'eval_samples_per_second': 145.244, 'eval_steps_per_second': 9.16, 'epoch': 0.36}
{'loss': 0.7908, 'grad_norm': 0.04854901507496834, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7574999332427979, 'eval_runtime': 6.9152, 'eval_samples_per_second': 144.465, 'eval_steps_per_second': 9.11, 'epoch': 0.4}
{'loss': 0.7816, 'grad_norm': 0.05916356295347214, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7433781623840332, 'eval_runtime': 6.888, 'eval_samples_per_second': 145.034, 'eval_steps_per_second': 9.146, 'epoch': 0.44}
{'loss': 0.7634, 'grad_norm': 0.06843415647745132, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7306211590766907, 'eval_runtime': 6.8968, 'eval_samples_per_second': 144.85, 'eval_steps_per_second': 9.135, 'epoch': 0.48}
{'loss': 0.7479, 'grad_norm': 0.056845784187316895, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7169570922851562, 'eval_runtime': 6.8992, 'eval_samples_per_second': 144.799, 'eval_steps_per_second': 9.131, 'epoch': 0.52}
{'loss': 0.7221, 'grad_norm': 0.06704054772853851, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7008544206619263, 'eval_runtime': 6.9072, 'eval_samples_per_second': 144.633, 'eval_steps_per_second': 9.121, 'epoch': 0.56}
{'loss': 0.7122, 'grad_norm': 0.06923478841781616, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6832066178321838, 'eval_runtime': 6.8923, 'eval_samples_per_second': 144.944, 'eval_steps_per_second': 9.141, 'epoch': 0.6}
{'loss': 0.698, 'grad_norm': 0.07081078737974167, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6661460995674133, 'eval_runtime': 6.9142, 'eval_samples_per_second': 144.485, 'eval_steps_per_second': 9.112, 'epoch': 0.64}
{'loss': 0.6988, 'grad_norm': 0.07884269207715988, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6490090489387512, 'eval_runtime': 6.8936, 'eval_samples_per_second': 144.917, 'eval_steps_per_second': 9.139, 'epoch': 0.68}
{'loss': 0.6792, 'grad_norm': 0.09110318869352341, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6332283020019531, 'eval_runtime': 6.8804, 'eval_samples_per_second': 145.195, 'eval_steps_per_second': 9.156, 'epoch': 0.72}
{'loss': 0.659, 'grad_norm': 0.10294762998819351, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.619386613368988, 'eval_runtime': 6.8958, 'eval_samples_per_second': 144.871, 'eval_steps_per_second': 9.136, 'epoch': 0.76}
{'loss': 0.6227, 'grad_norm': 0.08893023431301117, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6026034951210022, 'eval_runtime': 6.9219, 'eval_samples_per_second': 144.325, 'eval_steps_per_second': 9.102, 'epoch': 0.8}
{'loss': 0.6335, 'grad_norm': 0.11054515093564987, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5907891988754272, 'eval_runtime': 6.9075, 'eval_samples_per_second': 144.625, 'eval_steps_per_second': 9.121, 'epoch': 0.84}
{'loss': 0.6333, 'grad_norm': 0.09627719968557358, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5793647170066833, 'eval_runtime': 6.9015, 'eval_samples_per_second': 144.751, 'eval_steps_per_second': 9.128, 'epoch': 0.88}
{'loss': 0.6196, 'grad_norm': 0.10265417397022247, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.570870041847229, 'eval_runtime': 6.8941, 'eval_samples_per_second': 144.906, 'eval_steps_per_second': 9.138, 'epoch': 0.92}
{'loss': 0.5995, 'grad_norm': 0.12719500064849854, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5641784071922302, 'eval_runtime': 6.9263, 'eval_samples_per_second': 144.232, 'eval_steps_per_second': 9.096, 'epoch': 0.96}
{'loss': 0.5851, 'grad_norm': 0.12476319819688797, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5620088577270508, 'eval_runtime': 6.9159, 'eval_samples_per_second': 144.45, 'eval_steps_per_second': 9.109, 'epoch': 1.0}
{'train_runtime': 449.5529, 'train_samples_per_second': 22.238, 'train_steps_per_second': 1.39, 'train_loss': 0.9101993392944336, 'epoch': 1.0}
train_results:  {'eval_loss': [2.829458475112915, 1.1519320011138916, 0.9420340657234192, 0.8398334383964539, 0.8210800886154175, 0.8064860105514526, 0.7957794070243835, 0.7820563912391663, 0.7694823741912842, 0.7574999332427979, 0.7433781623840332, 0.7306211590766907, 0.7169570922851562, 0.7008544206619263, 0.6832066178321838, 0.6661460995674133, 0.6490090489387512, 0.6332283020019531, 0.619386613368988, 0.6026034951210022, 0.5907891988754272, 0.5793647170066833, 0.570870041847229, 0.5641784071922302, 0.5620088577270508], 'performance': [0.55, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:40,  1.63s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.80it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:09<00:11,  4.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:10<00:05,  5.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  8.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 11.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.34it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.486533284187317
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1901 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 1.1107114561718143
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.9721290908842248
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.5551522058277082
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.9786550224723932
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 1.1350507668333618
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.4871, dtype=torch.float64), tensor(0.5129, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.0, 1.4800000190750122, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.5850e-14, dtype=torch.float64), tensor(3.9922e-14, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4871, dtype=torch.float64), tensor(0.5129, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.8586e-14, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.487
  wikitext: 0.513
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190750122,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190750122
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:36,  2.18s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:27,  3.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:14,  5.66it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:11,  6.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  7.60it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:07,  8.23it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.82it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03,  8.81it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02,  9.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  7.84it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:12<00:01,  7.36it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00,  8.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.46it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.0812, 'grad_norm': 0.21189744770526886, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.3126885890960693, 'eval_runtime': 8.3275, 'eval_samples_per_second': 119.963, 'eval_steps_per_second': 7.565, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:54,  1.16s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.35it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.61it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 11.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 11.55it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 12.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:03, 12.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 13.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 15.26it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 14.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 15.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 15.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 14.53it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.04it/s]
Evaluation performance at step 50: 0.55
{'loss': 2.499, 'grad_norm': 0.2239237129688263, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.55}
{'eval_loss': 1.9254716634750366, 'eval_runtime': 8.2636, 'eval_samples_per_second': 120.891, 'eval_steps_per_second': 7.624, 'epoch': 0.08}
{'loss': 1.8908, 'grad_norm': 0.13071900606155396, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.6769351959228516, 'eval_runtime': 8.2706, 'eval_samples_per_second': 120.79, 'eval_steps_per_second': 7.617, 'epoch': 0.12}
{'loss': 1.6805, 'grad_norm': 0.06924904137849808, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.5393929481506348, 'eval_runtime': 8.258, 'eval_samples_per_second': 120.973, 'eval_steps_per_second': 7.629, 'epoch': 0.16}
{'loss': 1.5948, 'grad_norm': 0.17583665251731873, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.4538283348083496, 'eval_runtime': 8.2737, 'eval_samples_per_second': 120.744, 'eval_steps_per_second': 7.615, 'epoch': 0.2}
{'loss': 1.4819, 'grad_norm': 0.08252575248479843, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.426732063293457, 'eval_runtime': 8.2763, 'eval_samples_per_second': 120.706, 'eval_steps_per_second': 7.612, 'epoch': 0.24}
{'loss': 1.3895, 'grad_norm': 0.15979352593421936, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.4049209356307983, 'eval_runtime': 8.2869, 'eval_samples_per_second': 120.551, 'eval_steps_per_second': 7.602, 'epoch': 0.28}
{'loss': 1.4954, 'grad_norm': 0.1604214459657669, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.3898590803146362, 'eval_runtime': 8.2966, 'eval_samples_per_second': 120.411, 'eval_steps_per_second': 7.593, 'epoch': 0.32}
{'loss': 1.4332, 'grad_norm': 0.08886853605508804, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.3743289709091187, 'eval_runtime': 8.2961, 'eval_samples_per_second': 120.417, 'eval_steps_per_second': 7.594, 'epoch': 0.36}
{'loss': 1.4277, 'grad_norm': 0.09801097959280014, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.3537607192993164, 'eval_runtime': 8.3065, 'eval_samples_per_second': 120.267, 'eval_steps_per_second': 7.584, 'epoch': 0.4}
{'loss': 1.3868, 'grad_norm': 0.10752839595079422, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.3329154253005981, 'eval_runtime': 8.3216, 'eval_samples_per_second': 120.049, 'eval_steps_per_second': 7.571, 'epoch': 0.44}
{'loss': 1.3482, 'grad_norm': 0.13395796716213226, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.316124439239502, 'eval_runtime': 8.3246, 'eval_samples_per_second': 120.005, 'eval_steps_per_second': 7.568, 'epoch': 0.48}
{'loss': 1.2723, 'grad_norm': 0.08380632847547531, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.2976641654968262, 'eval_runtime': 8.3588, 'eval_samples_per_second': 119.515, 'eval_steps_per_second': 7.537, 'epoch': 0.52}
{'loss': 1.3912, 'grad_norm': 0.0943574458360672, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.2770917415618896, 'eval_runtime': 8.3778, 'eval_samples_per_second': 119.244, 'eval_steps_per_second': 7.52, 'epoch': 0.56}
{'loss': 1.3752, 'grad_norm': 0.09904514998197556, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.2622084617614746, 'eval_runtime': 8.3428, 'eval_samples_per_second': 119.743, 'eval_steps_per_second': 7.551, 'epoch': 0.6}
{'loss': 1.3361, 'grad_norm': 0.1399254947900772, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.2431247234344482, 'eval_runtime': 8.3512, 'eval_samples_per_second': 119.624, 'eval_steps_per_second': 7.544, 'epoch': 0.64}
{'loss': 1.3843, 'grad_norm': 0.10619500279426575, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.2241142988204956, 'eval_runtime': 8.3542, 'eval_samples_per_second': 119.58, 'eval_steps_per_second': 7.541, 'epoch': 0.68}
{'loss': 1.2622, 'grad_norm': 0.11441054195165634, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.2054903507232666, 'eval_runtime': 8.3528, 'eval_samples_per_second': 119.6, 'eval_steps_per_second': 7.542, 'epoch': 0.72}
{'loss': 1.1914, 'grad_norm': 0.10125121474266052, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.192234992980957, 'eval_runtime': 8.3515, 'eval_samples_per_second': 119.62, 'eval_steps_per_second': 7.544, 'epoch': 0.76}
{'loss': 1.426, 'grad_norm': 0.1629064381122589, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.1814602613449097, 'eval_runtime': 8.3757, 'eval_samples_per_second': 119.274, 'eval_steps_per_second': 7.522, 'epoch': 0.8}
{'loss': 1.17, 'grad_norm': 0.16839416325092316, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.169406533241272, 'eval_runtime': 8.3563, 'eval_samples_per_second': 119.551, 'eval_steps_per_second': 7.539, 'epoch': 0.84}
{'loss': 1.318, 'grad_norm': 0.09982865303754807, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.1605844497680664, 'eval_runtime': 8.3472, 'eval_samples_per_second': 119.681, 'eval_steps_per_second': 7.547, 'epoch': 0.88}
{'loss': 1.2188, 'grad_norm': 0.17956236004829407, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.1536973714828491, 'eval_runtime': 8.3478, 'eval_samples_per_second': 119.672, 'eval_steps_per_second': 7.547, 'epoch': 0.92}
{'loss': 1.2233, 'grad_norm': 0.17142578959465027, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.1517679691314697, 'eval_runtime': 8.3516, 'eval_samples_per_second': 119.618, 'eval_steps_per_second': 7.543, 'epoch': 0.96}
{'loss': 1.1543, 'grad_norm': 0.1361769735813141, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.1497734785079956, 'eval_runtime': 8.3503, 'eval_samples_per_second': 119.636, 'eval_steps_per_second': 7.545, 'epoch': 1.0}
{'train_runtime': 506.1548, 'train_samples_per_second': 19.755, 'train_steps_per_second': 1.235, 'train_loss': 1.537286880493164, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3126885890960693, 1.9254716634750366, 1.6769351959228516, 1.5393929481506348, 1.4538283348083496, 1.426732063293457, 1.4049209356307983, 1.3898590803146362, 1.3743289709091187, 1.3537607192993164, 1.3329154253005981, 1.316124439239502, 1.2976641654968262, 1.2770917415618896, 1.2622084617614746, 1.2431247234344482, 1.2241142988204956, 1.2054903507232666, 1.192234992980957, 1.1814602613449097, 1.169406533241272, 1.1605844497680664, 1.1536973714828491, 1.1517679691314697, 1.1497734785079956], 'performance': [0.54, 0.55]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:10,  5.56s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:11<00:46,  1.78it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:11<00:17,  3.79it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:17<00:15,  3.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:11,  3.16it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:28<00:06,  3.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:28<00:00,  4.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:28<00:00,  3.50it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.55]
current iteration observed (possibly low-fid or predicted) performance:  1.489725112915039
current iteration best possible performance (full train run):  0.6825000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7155 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.7102190317453612
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 1.1825838878567672
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 1.0668502083054905
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 1.023995937765228
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 1.1416469183053304
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0645, dtype=torch.float64), 0, 0, tensor(0.3646, dtype=torch.float64), 0, tensor(0.5709, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 1.0408340855860846e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0645, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3646, dtype=torch.float64), tensor(3.3143e-16, dtype=torch.float64), tensor(0.5709, dtype=torch.float64), tensor(2.9674e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0408e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.064
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.365
  triviaqa: 0
  truthfulqa_gen: 0.571
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.0408340855860846e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  1.0408340855860846e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:58,  1.80s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:22,  4.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:11,  7.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:09,  7.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  9.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:05, 10.20it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.65it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 11.02it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.70it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.37it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.11it/s]
Evaluation performance at step 25: 0.55
{'loss': 5.0976, 'grad_norm': 0.5381615161895752, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.6535580158233643, 'eval_runtime': 3.927, 'eval_samples_per_second': 254.395, 'eval_steps_per_second': 16.043, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:42,  1.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:13,  6.87it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.40it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:05, 13.39it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 15.04it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 12.48it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 14.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.99it/s]
Evaluation performance at step 50: 0.54
{'loss': 2.3573, 'grad_norm': 0.43922680616378784, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.54}
{'eval_loss': 1.4717527627944946, 'eval_runtime': 3.9296, 'eval_samples_per_second': 254.223, 'eval_steps_per_second': 16.032, 'epoch': 0.08}
{'loss': 1.198, 'grad_norm': 0.09542863070964813, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.047505259513855, 'eval_runtime': 3.9326, 'eval_samples_per_second': 254.027, 'eval_steps_per_second': 16.02, 'epoch': 0.12}
{'loss': 0.9741, 'grad_norm': 0.15894095599651337, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8825293779373169, 'eval_runtime': 3.9376, 'eval_samples_per_second': 253.711, 'eval_steps_per_second': 16.0, 'epoch': 0.16}
{'loss': 0.8166, 'grad_norm': 0.08510376513004303, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8347782492637634, 'eval_runtime': 3.9536, 'eval_samples_per_second': 252.678, 'eval_steps_per_second': 15.935, 'epoch': 0.2}
{'loss': 0.8113, 'grad_norm': 0.06417572498321533, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8172631859779358, 'eval_runtime': 3.9369, 'eval_samples_per_second': 253.755, 'eval_steps_per_second': 16.003, 'epoch': 0.24}
{'loss': 0.8264, 'grad_norm': 0.06809128820896149, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8009061217308044, 'eval_runtime': 3.9303, 'eval_samples_per_second': 254.176, 'eval_steps_per_second': 16.029, 'epoch': 0.28}
{'loss': 0.8104, 'grad_norm': 0.05971450358629227, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7923530340194702, 'eval_runtime': 3.9306, 'eval_samples_per_second': 254.16, 'eval_steps_per_second': 16.028, 'epoch': 0.32}
{'loss': 0.8161, 'grad_norm': 0.08716403692960739, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7824528813362122, 'eval_runtime': 3.9303, 'eval_samples_per_second': 254.178, 'eval_steps_per_second': 16.029, 'epoch': 0.36}
{'loss': 0.8095, 'grad_norm': 0.07881064713001251, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7741519212722778, 'eval_runtime': 3.9532, 'eval_samples_per_second': 252.704, 'eval_steps_per_second': 15.936, 'epoch': 0.4}
{'loss': 0.8064, 'grad_norm': 0.06850960105657578, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7653718590736389, 'eval_runtime': 3.9391, 'eval_samples_per_second': 253.614, 'eval_steps_per_second': 15.994, 'epoch': 0.44}
{'loss': 0.7741, 'grad_norm': 0.08666522800922394, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7609007954597473, 'eval_runtime': 3.937, 'eval_samples_per_second': 253.748, 'eval_steps_per_second': 16.002, 'epoch': 0.48}
{'loss': 0.7758, 'grad_norm': 0.08562065660953522, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7549543976783752, 'eval_runtime': 3.9449, 'eval_samples_per_second': 253.236, 'eval_steps_per_second': 15.97, 'epoch': 0.52}
{'loss': 0.749, 'grad_norm': 0.08015444129705429, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7489018440246582, 'eval_runtime': 3.9402, 'eval_samples_per_second': 253.537, 'eval_steps_per_second': 15.989, 'epoch': 0.56}
{'loss': 0.7666, 'grad_norm': 0.09143206477165222, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7442762851715088, 'eval_runtime': 3.9474, 'eval_samples_per_second': 253.076, 'eval_steps_per_second': 15.96, 'epoch': 0.6}
{'loss': 0.7521, 'grad_norm': 0.11379434168338776, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.738691508769989, 'eval_runtime': 3.9463, 'eval_samples_per_second': 253.151, 'eval_steps_per_second': 15.964, 'epoch': 0.64}
{'loss': 0.7744, 'grad_norm': 0.08630046993494034, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7348029613494873, 'eval_runtime': 3.9465, 'eval_samples_per_second': 253.134, 'eval_steps_per_second': 15.963, 'epoch': 0.68}
{'loss': 0.7354, 'grad_norm': 0.0858299508690834, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7299055457115173, 'eval_runtime': 3.9478, 'eval_samples_per_second': 253.049, 'eval_steps_per_second': 15.958, 'epoch': 0.72}
{'loss': 0.7401, 'grad_norm': 0.10725978761911392, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7256370186805725, 'eval_runtime': 3.9453, 'eval_samples_per_second': 253.212, 'eval_steps_per_second': 15.968, 'epoch': 0.76}
{'loss': 0.7511, 'grad_norm': 0.09398207068443298, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7226573824882507, 'eval_runtime': 3.9455, 'eval_samples_per_second': 253.199, 'eval_steps_per_second': 15.968, 'epoch': 0.8}
{'loss': 0.733, 'grad_norm': 0.12510530650615692, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7201662063598633, 'eval_runtime': 3.9515, 'eval_samples_per_second': 252.813, 'eval_steps_per_second': 15.943, 'epoch': 0.84}
{'loss': 0.7367, 'grad_norm': 0.08781972527503967, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7173641920089722, 'eval_runtime': 3.9622, 'eval_samples_per_second': 252.135, 'eval_steps_per_second': 15.9, 'epoch': 0.88}
{'loss': 0.7557, 'grad_norm': 0.09190841019153595, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7151979804039001, 'eval_runtime': 3.9595, 'eval_samples_per_second': 252.302, 'eval_steps_per_second': 15.911, 'epoch': 0.92}
{'loss': 0.718, 'grad_norm': 0.11048077791929245, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7142361998558044, 'eval_runtime': 3.9641, 'eval_samples_per_second': 252.013, 'eval_steps_per_second': 15.893, 'epoch': 0.96}
{'loss': 0.7455, 'grad_norm': 0.11115529388189316, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7133496403694153, 'eval_runtime': 3.9615, 'eval_samples_per_second': 252.178, 'eval_steps_per_second': 15.903, 'epoch': 1.0}
{'train_runtime': 294.0938, 'train_samples_per_second': 33.996, 'train_steps_per_second': 2.125, 'train_loss': 1.0332417022705078, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6535580158233643, 1.4717527627944946, 1.047505259513855, 0.8825293779373169, 0.8347782492637634, 0.8172631859779358, 0.8009061217308044, 0.7923530340194702, 0.7824528813362122, 0.7741519212722778, 0.7653718590736389, 0.7609007954597473, 0.7549543976783752, 0.7489018440246582, 0.7442762851715088, 0.738691508769989, 0.7348029613494873, 0.7299055457115173, 0.7256370186805725, 0.7226573824882507, 0.7201662063598633, 0.7173641920089722, 0.7151979804039001, 0.7142361998558044, 0.7133496403694153], 'performance': [0.55, 0.54]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:03,  1.57it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 16.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 24.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 24.03it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 25.59it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 27.59it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 24.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.54]
current iteration observed (possibly low-fid or predicted) performance:  1.4922642707824707
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3721 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 1.0815926914665561
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.9097406546460602
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 1.1669091439033192
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 1.068573838185673
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.6367270250252927
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0545, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6075, dtype=torch.float64), 0, 0, tensor(0.3380, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 0.07831912402982244, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0545, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5175e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.6349e-16, dtype=torch.float64), tensor(0.6075, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3380, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7832, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.054
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.607
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.338

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07831912402982244,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.07831912402982244
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Using the latest cached version of the dataset since truthful_qa couldn't be found on the Hugging Face Hub
WARNING:datasets.load:Using the latest cached version of the dataset since truthful_qa couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'generation' at /home/alfred/.cache/huggingface/datasets/truthful_qa/generation/0.0.0/741b8276f2d1982aa3d5b832d3ee81ed3b896490 (last modified on Fri Dec 12 10:26:11 2025).
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'generation' at /home/alfred/.cache/huggingface/datasets/truthful_qa/generation/0.0.0/741b8276f2d1982aa3d5b832d3ee81ed3b896490 (last modified on Fri Dec 12 10:26:11 2025).
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:41,  1.63s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.48it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.73it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.52it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.31it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.16it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.51it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.95it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.4906, 'grad_norm': 0.556962251663208, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.3595831394195557, 'eval_runtime': 6.4195, 'eval_samples_per_second': 155.619, 'eval_steps_per_second': 9.814, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:24,  1.17it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:21,  3.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.97it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  6.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.49it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.97it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:02, 12.61it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 13.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 14.10it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 15.48it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 15.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.79it/s]
Evaluation performance at step 50: 0.54
{'loss': 2.2171, 'grad_norm': 0.34092652797698975, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.54}
{'eval_loss': 1.430511236190796, 'eval_runtime': 5.9847, 'eval_samples_per_second': 166.925, 'eval_steps_per_second': 10.527, 'epoch': 0.08}
{'loss': 1.1677, 'grad_norm': 0.07022635638713837, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0169342756271362, 'eval_runtime': 6.0601, 'eval_samples_per_second': 164.849, 'eval_steps_per_second': 10.396, 'epoch': 0.12}
{'loss': 0.9523, 'grad_norm': 0.11654984205961227, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8988866806030273, 'eval_runtime': 6.0505, 'eval_samples_per_second': 165.111, 'eval_steps_per_second': 10.412, 'epoch': 0.16}
{'loss': 0.8733, 'grad_norm': 0.05733577534556389, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8591763973236084, 'eval_runtime': 6.0575, 'eval_samples_per_second': 164.92, 'eval_steps_per_second': 10.4, 'epoch': 0.2}
{'loss': 0.8499, 'grad_norm': 0.058612048625946045, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8433262705802917, 'eval_runtime': 6.043, 'eval_samples_per_second': 165.314, 'eval_steps_per_second': 10.425, 'epoch': 0.24}
{'loss': 0.8382, 'grad_norm': 0.07337537407875061, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8316412568092346, 'eval_runtime': 6.0405, 'eval_samples_per_second': 165.384, 'eval_steps_per_second': 10.43, 'epoch': 0.28}
{'loss': 0.8203, 'grad_norm': 0.06386838853359222, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8209492564201355, 'eval_runtime': 6.0637, 'eval_samples_per_second': 164.751, 'eval_steps_per_second': 10.39, 'epoch': 0.32}
{'loss': 0.8256, 'grad_norm': 0.05744841694831848, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8135898113250732, 'eval_runtime': 6.0466, 'eval_samples_per_second': 165.216, 'eval_steps_per_second': 10.419, 'epoch': 0.36}
{'loss': 0.7986, 'grad_norm': 0.07401547580957413, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8048438429832458, 'eval_runtime': 6.052, 'eval_samples_per_second': 165.069, 'eval_steps_per_second': 10.41, 'epoch': 0.4}
{'loss': 0.7897, 'grad_norm': 0.06223471835255623, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7977997660636902, 'eval_runtime': 6.0622, 'eval_samples_per_second': 164.791, 'eval_steps_per_second': 10.392, 'epoch': 0.44}
{'loss': 0.7776, 'grad_norm': 0.06886189430952072, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7918170690536499, 'eval_runtime': 6.0702, 'eval_samples_per_second': 164.574, 'eval_steps_per_second': 10.379, 'epoch': 0.48}
{'loss': 0.7847, 'grad_norm': 0.06732934713363647, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7861765623092651, 'eval_runtime': 6.0737, 'eval_samples_per_second': 164.48, 'eval_steps_per_second': 10.373, 'epoch': 0.52}
{'loss': 0.7757, 'grad_norm': 0.08378017693758011, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7798441648483276, 'eval_runtime': 6.0642, 'eval_samples_per_second': 164.737, 'eval_steps_per_second': 10.389, 'epoch': 0.56}
{'loss': 0.8, 'grad_norm': 0.07007088512182236, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7740938663482666, 'eval_runtime': 6.0562, 'eval_samples_per_second': 164.955, 'eval_steps_per_second': 10.403, 'epoch': 0.6}
{'loss': 0.7602, 'grad_norm': 0.07656889408826828, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.771385133266449, 'eval_runtime': 6.0201, 'eval_samples_per_second': 165.943, 'eval_steps_per_second': 10.465, 'epoch': 0.64}
{'loss': 0.7853, 'grad_norm': 0.08828963339328766, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7665692567825317, 'eval_runtime': 6.0173, 'eval_samples_per_second': 166.02, 'eval_steps_per_second': 10.47, 'epoch': 0.68}
{'loss': 0.7566, 'grad_norm': 0.09969817101955414, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7631606459617615, 'eval_runtime': 6.0231, 'eval_samples_per_second': 165.86, 'eval_steps_per_second': 10.46, 'epoch': 0.72}
{'loss': 0.7372, 'grad_norm': 0.07564505189657211, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.758231520652771, 'eval_runtime': 6.0259, 'eval_samples_per_second': 165.784, 'eval_steps_per_second': 10.455, 'epoch': 0.76}
{'loss': 0.7939, 'grad_norm': 0.09558999538421631, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7555166482925415, 'eval_runtime': 6.0212, 'eval_samples_per_second': 165.913, 'eval_steps_per_second': 10.463, 'epoch': 0.8}
{'loss': 0.7635, 'grad_norm': 0.0989559069275856, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7521511912345886, 'eval_runtime': 6.0179, 'eval_samples_per_second': 166.006, 'eval_steps_per_second': 10.469, 'epoch': 0.84}
{'loss': 0.7719, 'grad_norm': 0.07758230715990067, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7494823336601257, 'eval_runtime': 6.0201, 'eval_samples_per_second': 165.945, 'eval_steps_per_second': 10.465, 'epoch': 0.88}
{'loss': 0.756, 'grad_norm': 0.09190164506435394, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7471656203269958, 'eval_runtime': 6.0225, 'eval_samples_per_second': 165.877, 'eval_steps_per_second': 10.461, 'epoch': 0.92}
{'loss': 0.7551, 'grad_norm': 0.09780710935592651, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7458929419517517, 'eval_runtime': 6.0365, 'eval_samples_per_second': 165.492, 'eval_steps_per_second': 10.436, 'epoch': 0.96}
{'loss': 0.746, 'grad_norm': 0.09068725258111954, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.745254397392273, 'eval_runtime': 6.0598, 'eval_samples_per_second': 164.858, 'eval_steps_per_second': 10.396, 'epoch': 1.0}
{'train_runtime': 405.5166, 'train_samples_per_second': 24.655, 'train_steps_per_second': 1.541, 'train_loss': 1.015477081298828, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3595831394195557, 1.430511236190796, 1.0169342756271362, 0.8988866806030273, 0.8591763973236084, 0.8433262705802917, 0.8316412568092346, 0.8209492564201355, 0.8135898113250732, 0.8048438429832458, 0.7977997660636902, 0.7918170690536499, 0.7861765623092651, 0.7798441648483276, 0.7740938663482666, 0.771385133266449, 0.7665692567825317, 0.7631606459617615, 0.758231520652771, 0.7555166482925415, 0.7521511912345886, 0.7494823336601257, 0.7471656203269958, 0.7458929419517517, 0.745254397392273], 'performance': [0.55, 0.54]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:54,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.72it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.74it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:06,  8.37it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:03, 10.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 13.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 17.95it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.72it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.54]
current iteration observed (possibly low-fid or predicted) performance:  1.490051507949829
current iteration best possible performance (full train run):  0.651
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5792 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 1.1886353872923756
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 1.1236690847933308
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 1.1472295219220479
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 1.0058761296224357
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 1.153200660905021
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [tensor(0.0412, dtype=torch.float64), 0, tensor(0.4428, dtype=torch.float64), tensor(0.0400, dtype=torch.float64), 0, tensor(0.4760, dtype=torch.float64), 0, 0, 0, 32, 0, 0, 0, 0, 1, 128, 0.012362123173718615, 1.4800000190734897, 0]
normalized proposed parameters for next round by BO: [tensor(0.0412, dtype=torch.float64), tensor(1.5214e-17, dtype=torch.float64), tensor(0.4428, dtype=torch.float64), tensor(0.0400, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4760, dtype=torch.float64), tensor(8.8671e-17, dtype=torch.float64), tensor(8.8201e-17, dtype=torch.float64), tensor(1.1337e-15, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1236, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.041
  gsm8k: 0
  rowan_hellaswag: 0.443
  sciq: 0.04
  triviaqa: 0
  truthfulqa_gen: 0.476
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.012362123173718615,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (1.4800000190734897,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  0.012362123173718615
lora alpha:  1.4800000190734897
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:27,  1.49s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:16,  5.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.43it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:07,  9.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.75it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 12.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.60it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 13.52it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 14.78it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 10.18it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01, 10.61it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 11.96it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.13it/s]
Evaluation performance at step 25: 0.53
{'loss': 4.4474, 'grad_norm': 0.07879607379436493, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.53}
{'eval_loss': 4.005934238433838, 'eval_runtime': 9.3223, 'eval_samples_per_second': 107.162, 'eval_steps_per_second': 6.758, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:40,  2.46it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  9.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.43it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:21,  3.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.93it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.34it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.54it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:05,  8.53it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.33it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 11.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 12.86it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:00, 12.52it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 13.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.11it/s]
Evaluation performance at step 50: 0.56
{'loss': 3.204, 'grad_norm': 0.059219904243946075, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 2.5061042308807373, 'eval_runtime': 9.3461, 'eval_samples_per_second': 106.89, 'eval_steps_per_second': 6.741, 'epoch': 0.08}
{'loss': 2.2306, 'grad_norm': 0.04570881277322769, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.037806272506714, 'eval_runtime': 9.3916, 'eval_samples_per_second': 106.372, 'eval_steps_per_second': 6.708, 'epoch': 0.12}
{'loss': 1.9414, 'grad_norm': 0.04298019781708717, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.9221880435943604, 'eval_runtime': 9.4172, 'eval_samples_per_second': 106.083, 'eval_steps_per_second': 6.69, 'epoch': 0.16}
{'loss': 1.8462, 'grad_norm': 0.04836374893784523, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.8149406909942627, 'eval_runtime': 9.4494, 'eval_samples_per_second': 105.721, 'eval_steps_per_second': 6.667, 'epoch': 0.2}
{'loss': 1.7658, 'grad_norm': 0.04183752089738846, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7532732486724854, 'eval_runtime': 9.4398, 'eval_samples_per_second': 105.829, 'eval_steps_per_second': 6.674, 'epoch': 0.24}
{'loss': 1.6982, 'grad_norm': 0.04453809931874275, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.7185996770858765, 'eval_runtime': 9.4518, 'eval_samples_per_second': 105.694, 'eval_steps_per_second': 6.665, 'epoch': 0.28}
{'loss': 1.7263, 'grad_norm': 0.04078267887234688, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.6954580545425415, 'eval_runtime': 9.4532, 'eval_samples_per_second': 105.678, 'eval_steps_per_second': 6.664, 'epoch': 0.32}
{'loss': 1.6655, 'grad_norm': 0.0465218722820282, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.6753584146499634, 'eval_runtime': 9.456, 'eval_samples_per_second': 105.647, 'eval_steps_per_second': 6.662, 'epoch': 0.36}
{'loss': 1.6932, 'grad_norm': 0.03905222937464714, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.657671570777893, 'eval_runtime': 9.4618, 'eval_samples_per_second': 105.582, 'eval_steps_per_second': 6.658, 'epoch': 0.4}
{'loss': 1.6254, 'grad_norm': 0.044627439230680466, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.6382819414138794, 'eval_runtime': 9.4496, 'eval_samples_per_second': 105.719, 'eval_steps_per_second': 6.667, 'epoch': 0.44}
{'loss': 1.63, 'grad_norm': 0.05283838510513306, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.6167469024658203, 'eval_runtime': 9.4647, 'eval_samples_per_second': 105.55, 'eval_steps_per_second': 6.656, 'epoch': 0.48}
{'loss': 1.546, 'grad_norm': 0.0515754334628582, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.5854626893997192, 'eval_runtime': 9.4496, 'eval_samples_per_second': 105.718, 'eval_steps_per_second': 6.667, 'epoch': 0.52}
{'loss': 1.5178, 'grad_norm': 0.042142920196056366, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.551337718963623, 'eval_runtime': 9.4462, 'eval_samples_per_second': 105.757, 'eval_steps_per_second': 6.669, 'epoch': 0.56}
{'loss': 1.5328, 'grad_norm': 0.05362940952181816, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.540067195892334, 'eval_runtime': 9.4393, 'eval_samples_per_second': 105.834, 'eval_steps_per_second': 6.674, 'epoch': 0.6}
{'loss': 1.518, 'grad_norm': 0.08027556538581848, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.5265541076660156, 'eval_runtime': 9.431, 'eval_samples_per_second': 105.927, 'eval_steps_per_second': 6.68, 'epoch': 0.64}
{'loss': 1.4831, 'grad_norm': 0.05072193965315819, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.512824296951294, 'eval_runtime': 9.4289, 'eval_samples_per_second': 105.951, 'eval_steps_per_second': 6.682, 'epoch': 0.68}
{'loss': 1.4556, 'grad_norm': 0.04197941720485687, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.5032604932785034, 'eval_runtime': 9.4425, 'eval_samples_per_second': 105.799, 'eval_steps_per_second': 6.672, 'epoch': 0.72}
{'loss': 1.4975, 'grad_norm': 0.051484063267707825, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.4934579133987427, 'eval_runtime': 9.4543, 'eval_samples_per_second': 105.667, 'eval_steps_per_second': 6.664, 'epoch': 0.76}
{'loss': 1.4701, 'grad_norm': 0.04721697419881821, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.4878312349319458, 'eval_runtime': 9.4274, 'eval_samples_per_second': 105.967, 'eval_steps_per_second': 6.683, 'epoch': 0.8}
{'loss': 1.4796, 'grad_norm': 0.04214165732264519, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.4833102226257324, 'eval_runtime': 9.4278, 'eval_samples_per_second': 105.963, 'eval_steps_per_second': 6.682, 'epoch': 0.84}
{'loss': 1.4499, 'grad_norm': 0.048619821667671204, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.4809240102767944, 'eval_runtime': 9.4569, 'eval_samples_per_second': 105.638, 'eval_steps_per_second': 6.662, 'epoch': 0.88}
{'loss': 1.4313, 'grad_norm': 0.048254143446683884, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.479195475578308, 'eval_runtime': 9.4717, 'eval_samples_per_second': 105.473, 'eval_steps_per_second': 6.651, 'epoch': 0.92}
{'loss': 1.4275, 'grad_norm': 0.04680159315466881, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.477905511856079, 'eval_runtime': 9.5116, 'eval_samples_per_second': 105.03, 'eval_steps_per_second': 6.623, 'epoch': 0.96}
{'loss': 1.5161, 'grad_norm': 0.04873408377170563, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.4778329133987427, 'eval_runtime': 9.5054, 'eval_samples_per_second': 105.098, 'eval_steps_per_second': 6.628, 'epoch': 1.0}
{'train_runtime': 547.0045, 'train_samples_per_second': 18.278, 'train_steps_per_second': 1.143, 'train_loss': 1.7919682312011718, 'epoch': 1.0}
train_results:  {'eval_loss': [4.005934238433838, 2.5061042308807373, 2.037806272506714, 1.9221880435943604, 1.8149406909942627, 1.7532732486724854, 1.7185996770858765, 1.6954580545425415, 1.6753584146499634, 1.657671570777893, 1.6382819414138794, 1.6167469024658203, 1.5854626893997192, 1.551337718963623, 1.540067195892334, 1.5265541076660156, 1.512824296951294, 1.5032604932785034, 1.4934579133987427, 1.4878312349319458, 1.4833102226257324, 1.4809240102767944, 1.479195475578308, 1.477905511856079, 1.4778329133987427], 'performance': [0.53, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:19,  1.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:27,  3.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:11,  6.01it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.11it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.98it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 13.00it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 16.00it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.53, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.4731359481811523
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4057 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 1.1416950469276315
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 1.068197313744392
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 1.1574744084285027
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 1.1111880233609752
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.6998064272810934
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0311, dtype=torch.float64), 0, 0, tensor(0.6094, dtype=torch.float64), 0, tensor(0.3595, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 4.302551936337919e-17, 1.4800000190734908, 1]
normalized proposed parameters for next round by BO: [tensor(0.0311, dtype=torch.float64), tensor(3.2076e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6094, dtype=torch.float64), tensor(8.3828e-17, dtype=torch.float64), tensor(0.3595, dtype=torch.float64), tensor(2.2041e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.2076e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(4.3026e-16, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.031
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.609
  triviaqa: 0
  truthfulqa_gen: 0.359
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.302551936337919e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734908,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  4.302551936337919e-17
lora alpha:  1.4800000190734908
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:27,  2.10s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:03<00:26,  3.47it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:13,  6.08it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:11,  6.73it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:08,  8.03it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:06,  8.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:05,  8.90it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.76it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 10.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:11<00:02,  7.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:12<00:01,  6.97it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00,  7.95it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.67it/s]
Evaluation performance at step 25: 0.57
{'loss': 5.1685, 'grad_norm': 0.5782148838043213, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 3.680744171142578, 'eval_runtime': 4.4112, 'eval_samples_per_second': 226.47, 'eval_steps_per_second': 14.282, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:13,  1.35s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:17,  5.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  8.91it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:07,  9.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.52it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.25it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 12.07it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 14.47it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:01, 13.62it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 11.17it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:01,  9.56it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 10.97it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.72it/s]
Evaluation performance at step 50: 0.49
{'loss': 2.3164, 'grad_norm': 0.42913618683815, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 1.4264997243881226, 'eval_runtime': 3.684, 'eval_samples_per_second': 271.171, 'eval_steps_per_second': 17.101, 'epoch': 0.08}
{'loss': 1.1536, 'grad_norm': 0.09628818929195404, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0345958471298218, 'eval_runtime': 3.6745, 'eval_samples_per_second': 271.875, 'eval_steps_per_second': 17.145, 'epoch': 0.12}
{'loss': 0.9648, 'grad_norm': 0.12957215309143066, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8745009303092957, 'eval_runtime': 3.6872, 'eval_samples_per_second': 270.936, 'eval_steps_per_second': 17.086, 'epoch': 0.16}
{'loss': 0.8475, 'grad_norm': 0.055796440690755844, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8419146537780762, 'eval_runtime': 3.6954, 'eval_samples_per_second': 270.333, 'eval_steps_per_second': 17.048, 'epoch': 0.2}
{'loss': 0.8143, 'grad_norm': 0.07281271368265152, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8240497708320618, 'eval_runtime': 3.6866, 'eval_samples_per_second': 270.984, 'eval_steps_per_second': 17.089, 'epoch': 0.24}
{'loss': 0.8136, 'grad_norm': 0.08998994529247284, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.807454526424408, 'eval_runtime': 3.6867, 'eval_samples_per_second': 270.973, 'eval_steps_per_second': 17.088, 'epoch': 0.28}
{'loss': 0.8105, 'grad_norm': 0.07624056190252304, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.799039363861084, 'eval_runtime': 3.6953, 'eval_samples_per_second': 270.342, 'eval_steps_per_second': 17.049, 'epoch': 0.32}
{'loss': 0.8093, 'grad_norm': 0.08590245246887207, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7878122925758362, 'eval_runtime': 3.6956, 'eval_samples_per_second': 270.322, 'eval_steps_per_second': 17.047, 'epoch': 0.36}
{'loss': 0.7967, 'grad_norm': 0.07840313017368317, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7832456231117249, 'eval_runtime': 3.7015, 'eval_samples_per_second': 269.892, 'eval_steps_per_second': 17.02, 'epoch': 0.4}
{'loss': 0.7928, 'grad_norm': 0.07447658479213715, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.778850793838501, 'eval_runtime': 3.7062, 'eval_samples_per_second': 269.546, 'eval_steps_per_second': 16.998, 'epoch': 0.44}
{'loss': 0.7666, 'grad_norm': 0.09057839959859848, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7744248509407043, 'eval_runtime': 3.6986, 'eval_samples_per_second': 270.105, 'eval_steps_per_second': 17.034, 'epoch': 0.48}
{'loss': 0.7665, 'grad_norm': 0.07918189465999603, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7693403959274292, 'eval_runtime': 3.6977, 'eval_samples_per_second': 270.166, 'eval_steps_per_second': 17.038, 'epoch': 0.52}
{'loss': 0.7792, 'grad_norm': 0.10863897949457169, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7650240659713745, 'eval_runtime': 3.6925, 'eval_samples_per_second': 270.547, 'eval_steps_per_second': 17.062, 'epoch': 0.56}
{'loss': 0.7751, 'grad_norm': 0.0802827924489975, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7637458443641663, 'eval_runtime': 3.6913, 'eval_samples_per_second': 270.633, 'eval_steps_per_second': 17.067, 'epoch': 0.6}
{'loss': 0.7705, 'grad_norm': 0.10900864750146866, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7604612708091736, 'eval_runtime': 3.6936, 'eval_samples_per_second': 270.466, 'eval_steps_per_second': 17.056, 'epoch': 0.64}
{'loss': 0.7482, 'grad_norm': 0.07634004950523376, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7560614347457886, 'eval_runtime': 3.6986, 'eval_samples_per_second': 270.106, 'eval_steps_per_second': 17.034, 'epoch': 0.68}
{'loss': 0.7586, 'grad_norm': 0.08249983191490173, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7539486289024353, 'eval_runtime': 3.6885, 'eval_samples_per_second': 270.845, 'eval_steps_per_second': 17.08, 'epoch': 0.72}
{'loss': 0.7578, 'grad_norm': 0.09191588312387466, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7523926496505737, 'eval_runtime': 3.6913, 'eval_samples_per_second': 270.635, 'eval_steps_per_second': 17.067, 'epoch': 0.76}
{'loss': 0.7676, 'grad_norm': 0.07011675089597702, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7507293820381165, 'eval_runtime': 3.6963, 'eval_samples_per_second': 270.27, 'eval_steps_per_second': 17.044, 'epoch': 0.8}
{'loss': 0.7548, 'grad_norm': 0.13242174685001373, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.747649610042572, 'eval_runtime': 3.6912, 'eval_samples_per_second': 270.647, 'eval_steps_per_second': 17.068, 'epoch': 0.84}
{'loss': 0.7283, 'grad_norm': 0.09225225448608398, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7467662692070007, 'eval_runtime': 3.6909, 'eval_samples_per_second': 270.668, 'eval_steps_per_second': 17.069, 'epoch': 0.88}
{'loss': 0.7879, 'grad_norm': 0.09105824679136276, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7453560829162598, 'eval_runtime': 3.6949, 'eval_samples_per_second': 270.372, 'eval_steps_per_second': 17.051, 'epoch': 0.92}
{'loss': 0.7643, 'grad_norm': 0.08045841008424759, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7449856996536255, 'eval_runtime': 3.696, 'eval_samples_per_second': 270.294, 'eval_steps_per_second': 17.046, 'epoch': 0.96}
{'loss': 0.7416, 'grad_norm': 0.08901751041412354, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7442354559898376, 'eval_runtime': 3.6936, 'eval_samples_per_second': 270.467, 'eval_steps_per_second': 17.056, 'epoch': 1.0}
{'train_runtime': 300.7361, 'train_samples_per_second': 33.245, 'train_steps_per_second': 2.078, 'train_loss': 1.0382078369140626, 'epoch': 1.0}
train_results:  {'eval_loss': [3.680744171142578, 1.4264997243881226, 1.0345958471298218, 0.8745009303092957, 0.8419146537780762, 0.8240497708320618, 0.807454526424408, 0.799039363861084, 0.7878122925758362, 0.7832456231117249, 0.778850793838501, 0.7744248509407043, 0.7693403959274292, 0.7650240659713745, 0.7637458443641663, 0.7604612708091736, 0.7560614347457886, 0.7539486289024353, 0.7523926496505737, 0.7507293820381165, 0.747649610042572, 0.7467662692070007, 0.7453560829162598, 0.7449856996536255, 0.7442354559898376], 'performance': [0.57, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:35,  1.04it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.37it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.01it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 17.78it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 19.36it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 22.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.99it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.4908437728881836
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.7501 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 1.0237855649046597
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.9779332830104147
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 1.167025853931658
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 1.1093621426865232
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 1.1996772346311442
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0421, dtype=torch.float64), 0, 0, 0, 0, tensor(0.4134, dtype=torch.float64), tensor(0.5444, dtype=torch.float64), 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.0, 1.4800000190734912, 1]
normalized proposed parameters for next round by BO: [tensor(0.0421, dtype=torch.float64), tensor(2.5139e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.5706e-17, dtype=torch.float64), tensor(0.4134, dtype=torch.float64), tensor(0.5444, dtype=torch.float64), tensor(4.1329e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.042
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.413
  wikitext: 0.544
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734912,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734912
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<03:21,  2.04s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:24,  3.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:12,  6.46it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:10,  7.30it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:07,  8.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  9.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.19it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.59it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.45it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 11.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  9.37it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00,  9.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.74it/s]
Evaluation performance at step 25: 0.56
{'loss': 3.9758, 'grad_norm': 0.24988210201263428, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.2333765029907227, 'eval_runtime': 8.7279, 'eval_samples_per_second': 114.46, 'eval_steps_per_second': 7.218, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:50,  1.96it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.76it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 15.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 15.21it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 16.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.76it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 15.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.21it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 17.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 18.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 16.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 15.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.11it/s]
Evaluation performance at step 50: 0.56
{'loss': 2.5931, 'grad_norm': 0.17152832448482513, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 2.005025863647461, 'eval_runtime': 8.7385, 'eval_samples_per_second': 114.321, 'eval_steps_per_second': 7.209, 'epoch': 0.08}
{'loss': 1.9172, 'grad_norm': 0.17959268391132355, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.7538669109344482, 'eval_runtime': 8.7953, 'eval_samples_per_second': 113.584, 'eval_steps_per_second': 7.163, 'epoch': 0.12}
{'loss': 1.7737, 'grad_norm': 0.07997235655784607, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.6166387796401978, 'eval_runtime': 8.7988, 'eval_samples_per_second': 113.538, 'eval_steps_per_second': 7.16, 'epoch': 0.16}
{'loss': 1.7098, 'grad_norm': 0.08079905807971954, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.5477691888809204, 'eval_runtime': 8.8088, 'eval_samples_per_second': 113.41, 'eval_steps_per_second': 7.152, 'epoch': 0.2}
{'loss': 1.583, 'grad_norm': 0.32604584097862244, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.5248631238937378, 'eval_runtime': 8.8106, 'eval_samples_per_second': 113.386, 'eval_steps_per_second': 7.15, 'epoch': 0.24}
{'loss': 1.6217, 'grad_norm': 0.0839196965098381, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.506700038909912, 'eval_runtime': 8.8189, 'eval_samples_per_second': 113.279, 'eval_steps_per_second': 7.144, 'epoch': 0.28}
{'loss': 1.4625, 'grad_norm': 0.10464446246623993, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.4913604259490967, 'eval_runtime': 8.8084, 'eval_samples_per_second': 113.414, 'eval_steps_per_second': 7.152, 'epoch': 0.32}
{'loss': 1.5047, 'grad_norm': 0.1575835645198822, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.4807684421539307, 'eval_runtime': 8.8195, 'eval_samples_per_second': 113.272, 'eval_steps_per_second': 7.143, 'epoch': 0.36}
{'loss': 1.504, 'grad_norm': 0.08188650757074356, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.4592348337173462, 'eval_runtime': 8.8517, 'eval_samples_per_second': 112.859, 'eval_steps_per_second': 7.117, 'epoch': 0.4}
{'loss': 1.5147, 'grad_norm': 0.07800301164388657, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.4418983459472656, 'eval_runtime': 8.8262, 'eval_samples_per_second': 113.186, 'eval_steps_per_second': 7.138, 'epoch': 0.44}
{'loss': 1.4296, 'grad_norm': 0.09834613651037216, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.4304730892181396, 'eval_runtime': 8.8348, 'eval_samples_per_second': 113.076, 'eval_steps_per_second': 7.131, 'epoch': 0.48}
{'loss': 1.4208, 'grad_norm': 0.09889281541109085, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.4157458543777466, 'eval_runtime': 8.8594, 'eval_samples_per_second': 112.762, 'eval_steps_per_second': 7.111, 'epoch': 0.52}
{'loss': 1.4824, 'grad_norm': 0.10538749396800995, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.403364658355713, 'eval_runtime': 8.8397, 'eval_samples_per_second': 113.013, 'eval_steps_per_second': 7.127, 'epoch': 0.56}
{'loss': 1.4238, 'grad_norm': 0.08365520089864731, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.393331527709961, 'eval_runtime': 8.8157, 'eval_samples_per_second': 113.321, 'eval_steps_per_second': 7.146, 'epoch': 0.6}
{'loss': 1.3789, 'grad_norm': 0.1046450063586235, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.380493402481079, 'eval_runtime': 8.8124, 'eval_samples_per_second': 113.363, 'eval_steps_per_second': 7.149, 'epoch': 0.64}
{'loss': 1.3576, 'grad_norm': 0.13113000988960266, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.3690085411071777, 'eval_runtime': 8.8292, 'eval_samples_per_second': 113.147, 'eval_steps_per_second': 7.135, 'epoch': 0.68}
{'loss': 1.3326, 'grad_norm': 0.09573809057474136, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.3610583543777466, 'eval_runtime': 8.803, 'eval_samples_per_second': 113.485, 'eval_steps_per_second': 7.157, 'epoch': 0.72}
{'loss': 1.2564, 'grad_norm': 0.11796484887599945, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.3502658605575562, 'eval_runtime': 8.8295, 'eval_samples_per_second': 113.143, 'eval_steps_per_second': 7.135, 'epoch': 0.76}
{'loss': 1.4372, 'grad_norm': 0.11288899183273315, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.341301679611206, 'eval_runtime': 8.8607, 'eval_samples_per_second': 112.745, 'eval_steps_per_second': 7.11, 'epoch': 0.8}
{'loss': 1.4003, 'grad_norm': 0.1235809475183487, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.3345869779586792, 'eval_runtime': 8.8656, 'eval_samples_per_second': 112.683, 'eval_steps_per_second': 7.106, 'epoch': 0.84}
{'loss': 1.3065, 'grad_norm': 0.16444233059883118, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.3283458948135376, 'eval_runtime': 8.8536, 'eval_samples_per_second': 112.835, 'eval_steps_per_second': 7.116, 'epoch': 0.88}
{'loss': 1.375, 'grad_norm': 0.12003342807292938, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.3228853940963745, 'eval_runtime': 8.84, 'eval_samples_per_second': 113.009, 'eval_steps_per_second': 7.127, 'epoch': 0.92}
{'loss': 1.3377, 'grad_norm': 0.117196224629879, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.3202027082443237, 'eval_runtime': 8.8557, 'eval_samples_per_second': 112.809, 'eval_steps_per_second': 7.114, 'epoch': 0.96}
{'loss': 1.1977, 'grad_norm': 0.173920676112175, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.3186993598937988, 'eval_runtime': 8.8463, 'eval_samples_per_second': 112.929, 'eval_steps_per_second': 7.122, 'epoch': 1.0}
{'train_runtime': 516.7927, 'train_samples_per_second': 19.348, 'train_steps_per_second': 1.209, 'train_loss': 1.611860137939453, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2333765029907227, 2.005025863647461, 1.7538669109344482, 1.6166387796401978, 1.5477691888809204, 1.5248631238937378, 1.506700038909912, 1.4913604259490967, 1.4807684421539307, 1.4592348337173462, 1.4418983459472656, 1.4304730892181396, 1.4157458543777466, 1.403364658355713, 1.393331527709961, 1.380493402481079, 1.3690085411071777, 1.3610583543777466, 1.3502658605575562, 1.341301679611206, 1.3345869779586792, 1.3283458948135376, 1.3228853940963745, 1.3202027082443237, 1.3186993598937988], 'performance': [0.56, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:29,  1.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.33it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 15.40it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.52it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:06,  5.69it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:04,  4.22it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:14<00:00,  6.06it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.70it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.4874308109283447
current iteration best possible performance (full train run):  0.6825000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4418 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 1.102414507478342
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 1.179945474054477
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 1.0550647908275388
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 1.0123788251957755
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.6124949491116153
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0149, dtype=torch.float64), 0, 0, tensor(0.3990, dtype=torch.float64), 0, tensor(0.5861, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.06739935957276309, 1.4800000190734874, 0]
normalized proposed parameters for next round by BO: [tensor(0.0149, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3990, dtype=torch.float64), tensor(6.2575e-17, dtype=torch.float64), tensor(0.5861, dtype=torch.float64), tensor(3.3605e-17, dtype=torch.float64), tensor(2.6213e-17, dtype=torch.float64), tensor(2.5930e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6740, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.015
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.399
  triviaqa: 0
  truthfulqa_gen: 0.586
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06739935957276309,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734874,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.06739935957276309
lora alpha:  1.4800000190734874
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:40,  1.62s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.51it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.66it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.31it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.21it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.76it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.14it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.25it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.63it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.83it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.97it/s]
Evaluation performance at step 25: 0.54
{'loss': 5.1414, 'grad_norm': 0.5545182228088379, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.6712636947631836, 'eval_runtime': 4.9443, 'eval_samples_per_second': 202.05, 'eval_steps_per_second': 12.742, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:33,  1.05it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.20it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.74it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 13.47it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.36it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.08it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 15.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 18.89it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.04it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 13.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 14.83it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.37it/s]
Evaluation performance at step 50: 0.49
{'loss': 2.324, 'grad_norm': 0.4275190532207489, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 1.4313746690750122, 'eval_runtime': 3.6154, 'eval_samples_per_second': 276.321, 'eval_steps_per_second': 17.426, 'epoch': 0.08}
{'loss': 1.1605, 'grad_norm': 0.1086101233959198, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0266408920288086, 'eval_runtime': 3.5994, 'eval_samples_per_second': 277.546, 'eval_steps_per_second': 17.503, 'epoch': 0.12}
{'loss': 0.9285, 'grad_norm': 0.14575444161891937, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8695198893547058, 'eval_runtime': 3.6057, 'eval_samples_per_second': 277.06, 'eval_steps_per_second': 17.472, 'epoch': 0.16}
{'loss': 0.8457, 'grad_norm': 0.05856607109308243, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8377513289451599, 'eval_runtime': 3.61, 'eval_samples_per_second': 276.735, 'eval_steps_per_second': 17.452, 'epoch': 0.2}
{'loss': 0.8302, 'grad_norm': 0.06417132169008255, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8192814588546753, 'eval_runtime': 3.6222, 'eval_samples_per_second': 275.796, 'eval_steps_per_second': 17.393, 'epoch': 0.24}
{'loss': 0.8082, 'grad_norm': 0.07313350588083267, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8053567409515381, 'eval_runtime': 3.6258, 'eval_samples_per_second': 275.523, 'eval_steps_per_second': 17.375, 'epoch': 0.28}
{'loss': 0.7797, 'grad_norm': 0.07558806240558624, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7953226566314697, 'eval_runtime': 3.6277, 'eval_samples_per_second': 275.38, 'eval_steps_per_second': 17.366, 'epoch': 0.32}
{'loss': 0.7927, 'grad_norm': 0.08542133867740631, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7783759832382202, 'eval_runtime': 3.6277, 'eval_samples_per_second': 275.381, 'eval_steps_per_second': 17.366, 'epoch': 0.36}
{'loss': 0.7684, 'grad_norm': 0.06871335208415985, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7674506902694702, 'eval_runtime': 3.6481, 'eval_samples_per_second': 273.84, 'eval_steps_per_second': 17.269, 'epoch': 0.4}
{'loss': 0.7792, 'grad_norm': 0.07722730189561844, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7564712762832642, 'eval_runtime': 3.6345, 'eval_samples_per_second': 274.869, 'eval_steps_per_second': 17.334, 'epoch': 0.44}
{'loss': 0.7862, 'grad_norm': 0.08529294282197952, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7502294182777405, 'eval_runtime': 3.6309, 'eval_samples_per_second': 275.137, 'eval_steps_per_second': 17.351, 'epoch': 0.48}
{'loss': 0.723, 'grad_norm': 0.08458008617162704, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7444713711738586, 'eval_runtime': 3.6285, 'eval_samples_per_second': 275.32, 'eval_steps_per_second': 17.363, 'epoch': 0.52}
{'loss': 0.7625, 'grad_norm': 0.08582629263401031, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7372704744338989, 'eval_runtime': 3.6345, 'eval_samples_per_second': 274.865, 'eval_steps_per_second': 17.334, 'epoch': 0.56}
{'loss': 0.7407, 'grad_norm': 0.07775570452213287, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7309595942497253, 'eval_runtime': 3.6301, 'eval_samples_per_second': 275.196, 'eval_steps_per_second': 17.355, 'epoch': 0.6}
{'loss': 0.7366, 'grad_norm': 0.1155429556965828, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7259549498558044, 'eval_runtime': 3.626, 'eval_samples_per_second': 275.514, 'eval_steps_per_second': 17.375, 'epoch': 0.64}
{'loss': 0.7253, 'grad_norm': 0.11966948211193085, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7208543419837952, 'eval_runtime': 3.6287, 'eval_samples_per_second': 275.302, 'eval_steps_per_second': 17.361, 'epoch': 0.68}
{'loss': 0.7048, 'grad_norm': 0.09503108263015747, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.717975378036499, 'eval_runtime': 3.6335, 'eval_samples_per_second': 274.942, 'eval_steps_per_second': 17.339, 'epoch': 0.72}
{'loss': 0.714, 'grad_norm': 0.10197407752275467, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7127405405044556, 'eval_runtime': 3.637, 'eval_samples_per_second': 274.675, 'eval_steps_per_second': 17.322, 'epoch': 0.76}
{'loss': 0.707, 'grad_norm': 0.12092268466949463, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7081769704818726, 'eval_runtime': 3.6463, 'eval_samples_per_second': 273.975, 'eval_steps_per_second': 17.278, 'epoch': 0.8}
{'loss': 0.7129, 'grad_norm': 0.08409819006919861, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7064432501792908, 'eval_runtime': 3.6455, 'eval_samples_per_second': 274.035, 'eval_steps_per_second': 17.281, 'epoch': 0.84}
{'loss': 0.7084, 'grad_norm': 0.11180567741394043, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7032504677772522, 'eval_runtime': 3.8203, 'eval_samples_per_second': 261.5, 'eval_steps_per_second': 16.491, 'epoch': 0.88}
{'loss': 0.7062, 'grad_norm': 0.10361342877149582, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7016933560371399, 'eval_runtime': 3.659, 'eval_samples_per_second': 273.029, 'eval_steps_per_second': 17.218, 'epoch': 0.92}
{'loss': 0.6897, 'grad_norm': 0.10303913801908493, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.700304388999939, 'eval_runtime': 3.6391, 'eval_samples_per_second': 274.515, 'eval_steps_per_second': 17.312, 'epoch': 0.96}
{'loss': 0.713, 'grad_norm': 0.0997057557106018, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7002399563789368, 'eval_runtime': 3.6541, 'eval_samples_per_second': 273.392, 'eval_steps_per_second': 17.241, 'epoch': 1.0}
{'train_runtime': 280.3662, 'train_samples_per_second': 35.664, 'train_steps_per_second': 2.229, 'train_loss': 1.011553207397461, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6712636947631836, 1.4313746690750122, 1.0266408920288086, 0.8695198893547058, 0.8377513289451599, 0.8192814588546753, 0.8053567409515381, 0.7953226566314697, 0.7783759832382202, 0.7674506902694702, 0.7564712762832642, 0.7502294182777405, 0.7444713711738586, 0.7372704744338989, 0.7309595942497253, 0.7259549498558044, 0.7208543419837952, 0.717975378036499, 0.7127405405044556, 0.7081769704818726, 0.7064432501792908, 0.7032504677772522, 0.7016933560371399, 0.700304388999939, 0.7002399563789368], 'performance': [0.54, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:29,  1.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 18.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 21.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 24.64it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.03it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.492409348487854
current iteration best possible performance (full train run):  0.651
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0015 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.9346383578611535
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 1.178066888939306
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.923827560619668
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 1.1626570891211756
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 1.1164200142474925
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(0.0712, dtype=torch.float64), tensor(0.9190, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.0, 1.4800000190734885, 0]
normalized proposed parameters for next round by BO: [tensor(0.0098, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0712, dtype=torch.float64), tensor(0.9190, dtype=torch.float64), tensor(2.1570e-18, dtype=torch.float64), tensor(1.7952e-17, dtype=torch.float64), tensor(3.0259e-17, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.071
  truthfulqa_gen: 0.919
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9901
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  990
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<03:12,  1.94s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:24,  3.77it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:03<00:12,  6.68it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:10,  7.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:07,  8.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  9.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:05,  9.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04,  9.49it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.97it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 10.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  7.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  7.40it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.39it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.15it/s]
Evaluation performance at step 25: 0.55
{'loss': 4.9442, 'grad_norm': 0.39955607056617737, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.395381450653076, 'eval_runtime': 3.5451, 'eval_samples_per_second': 279.256, 'eval_steps_per_second': 17.489, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:04,  1.54it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.41it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.35it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:05, 12.91it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 14.21it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 11.43it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:03, 12.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:08,  4.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:10<00:05,  5.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:02,  6.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  6.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00,  8.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.19it/s]
Evaluation performance at step 50: 0.45
{'loss': 2.0792, 'grad_norm': 0.3569815456867218, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.45}
{'eval_loss': 1.2247768640518188, 'eval_runtime': 3.5551, 'eval_samples_per_second': 278.476, 'eval_steps_per_second': 17.44, 'epoch': 0.08}
{'loss': 1.0724, 'grad_norm': 0.11727400124073029, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 0.936616837978363, 'eval_runtime': 3.549, 'eval_samples_per_second': 278.948, 'eval_steps_per_second': 17.469, 'epoch': 0.12}
{'loss': 0.8981, 'grad_norm': 0.0931142047047615, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 0.7798522710800171, 'eval_runtime': 3.5553, 'eval_samples_per_second': 278.458, 'eval_steps_per_second': 17.439, 'epoch': 0.16}
{'loss': 0.7826, 'grad_norm': 0.07050222903490067, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 0.7486822605133057, 'eval_runtime': 3.5617, 'eval_samples_per_second': 277.961, 'eval_steps_per_second': 17.408, 'epoch': 0.2}
{'loss': 0.7355, 'grad_norm': 0.06508903950452805, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 0.7201172113418579, 'eval_runtime': 3.5646, 'eval_samples_per_second': 277.729, 'eval_steps_per_second': 17.393, 'epoch': 0.24}
{'loss': 0.7281, 'grad_norm': 0.07507853209972382, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 0.6950536966323853, 'eval_runtime': 3.5813, 'eval_samples_per_second': 276.433, 'eval_steps_per_second': 17.312, 'epoch': 0.28}
{'loss': 0.7084, 'grad_norm': 0.06570219248533249, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 0.6679412722587585, 'eval_runtime': 3.5676, 'eval_samples_per_second': 277.499, 'eval_steps_per_second': 17.379, 'epoch': 0.32}
{'loss': 0.658, 'grad_norm': 0.08185402303934097, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 0.6366669535636902, 'eval_runtime': 3.5573, 'eval_samples_per_second': 278.299, 'eval_steps_per_second': 17.429, 'epoch': 0.36}
{'loss': 0.6477, 'grad_norm': 0.09334708750247955, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.6014420986175537, 'eval_runtime': 3.5514, 'eval_samples_per_second': 278.765, 'eval_steps_per_second': 17.458, 'epoch': 0.4}
{'loss': 0.599, 'grad_norm': 0.11598506569862366, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.5767931938171387, 'eval_runtime': 3.562, 'eval_samples_per_second': 277.933, 'eval_steps_per_second': 17.406, 'epoch': 0.44}
{'loss': 0.572, 'grad_norm': 0.1437786966562271, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.5283395648002625, 'eval_runtime': 3.5577, 'eval_samples_per_second': 278.271, 'eval_steps_per_second': 17.427, 'epoch': 0.48}
{'loss': 0.5329, 'grad_norm': 0.1091853603720665, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.482828289270401, 'eval_runtime': 3.5573, 'eval_samples_per_second': 278.299, 'eval_steps_per_second': 17.429, 'epoch': 0.53}
{'loss': 0.479, 'grad_norm': 0.14939063787460327, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.44083622097969055, 'eval_runtime': 3.564, 'eval_samples_per_second': 277.781, 'eval_steps_per_second': 17.396, 'epoch': 0.57}
{'loss': 0.441, 'grad_norm': 0.15365059673786163, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.39938274025917053, 'eval_runtime': 3.5576, 'eval_samples_per_second': 278.28, 'eval_steps_per_second': 17.428, 'epoch': 0.61}
{'loss': 0.4182, 'grad_norm': 0.13631324470043182, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.3666473627090454, 'eval_runtime': 3.5589, 'eval_samples_per_second': 278.178, 'eval_steps_per_second': 17.421, 'epoch': 0.65}
{'loss': 0.3912, 'grad_norm': 0.1874333769083023, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.33433693647384644, 'eval_runtime': 3.5643, 'eval_samples_per_second': 277.751, 'eval_steps_per_second': 17.395, 'epoch': 0.69}
{'loss': 0.3574, 'grad_norm': 0.14226186275482178, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.3147539496421814, 'eval_runtime': 3.5654, 'eval_samples_per_second': 277.665, 'eval_steps_per_second': 17.389, 'epoch': 0.73}
{'loss': 0.3371, 'grad_norm': 0.19044901430606842, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.2895161211490631, 'eval_runtime': 3.5641, 'eval_samples_per_second': 277.768, 'eval_steps_per_second': 17.396, 'epoch': 0.77}
{'loss': 0.3068, 'grad_norm': 0.12365082651376724, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.27218756079673767, 'eval_runtime': 3.5784, 'eval_samples_per_second': 276.663, 'eval_steps_per_second': 17.326, 'epoch': 0.81}
{'loss': 0.3055, 'grad_norm': 0.1406785398721695, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.26150816679000854, 'eval_runtime': 3.5758, 'eval_samples_per_second': 276.862, 'eval_steps_per_second': 17.339, 'epoch': 0.85}
{'loss': 0.3021, 'grad_norm': 0.17845837771892548, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.2528076171875, 'eval_runtime': 3.5768, 'eval_samples_per_second': 276.786, 'eval_steps_per_second': 17.334, 'epoch': 0.89}
{'loss': 0.2602, 'grad_norm': 0.13025416433811188, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.24647021293640137, 'eval_runtime': 3.5726, 'eval_samples_per_second': 277.11, 'eval_steps_per_second': 17.354, 'epoch': 0.93}
{'loss': 0.2824, 'grad_norm': 0.16172660887241364, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.2421182096004486, 'eval_runtime': 3.5905, 'eval_samples_per_second': 275.725, 'eval_steps_per_second': 17.268, 'epoch': 0.97}
{'train_runtime': 274.273, 'train_samples_per_second': 36.099, 'train_steps_per_second': 2.257, 'train_loss': 0.7684080881910679, 'epoch': 1.0}
train_results:  {'eval_loss': [3.395381450653076, 1.2247768640518188, 0.936616837978363, 0.7798522710800171, 0.7486822605133057, 0.7201172113418579, 0.6950536966323853, 0.6679412722587585, 0.6366669535636902, 0.6014420986175537, 0.5767931938171387, 0.5283395648002625, 0.482828289270401, 0.44083622097969055, 0.39938274025917053, 0.3666473627090454, 0.33433693647384644, 0.3147539496421814, 0.2895161211490631, 0.27218756079673767, 0.26150816679000854, 0.2528076171875, 0.24647021293640137, 0.2421182096004486], 'performance': [0.55, 0.45]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:04,  1.53it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 16.03it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 19.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 22.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 24.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 26.50it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 31.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 25.74it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.45]
current iteration observed (possibly low-fid or predicted) performance:  1.492368459701538
current iteration best possible performance (full train run):  0.672
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9481 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 1.0789338521067375
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.9163809913183623
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 1.101596346786122
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.974452847315402
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.6712594025108571
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0133, dtype=torch.float64), tensor(0.0424, dtype=torch.float64), 0, tensor(0.2799, dtype=torch.float64), tensor(0.1296, dtype=torch.float64), tensor(0.3101, dtype=torch.float64), 0, 0, tensor(0.2247, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 0.0381270858119486, 1.4800000190734872, 0]
normalized proposed parameters for next round by BO: [tensor(0.0133, dtype=torch.float64), tensor(0.0424, dtype=torch.float64), tensor(2.5038e-17, dtype=torch.float64), tensor(0.2799, dtype=torch.float64), tensor(0.1296, dtype=torch.float64), tensor(0.3101, dtype=torch.float64), tensor(3.7305e-17, dtype=torch.float64), tensor(1.9594e-17, dtype=torch.float64), tensor(0.2247, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3813, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.013
  gsm8k: 0.042
  rowan_hellaswag: 0
  sciq: 0.28
  triviaqa: 0.13
  truthfulqa_gen: 0.31
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.225

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0381270858119486,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734872,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0381270858119486
lora alpha:  1.4800000190734872
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9996
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:41,  1.63s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.52it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.86it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.26it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 11.24it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 12.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.35it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.14it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.44it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.88it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.3238, 'grad_norm': 0.44732657074928284, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.3750674724578857, 'eval_runtime': 6.5361, 'eval_samples_per_second': 152.844, 'eval_steps_per_second': 9.639, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:34,  1.04it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.45it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.65it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:23,  3.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:14,  4.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:09,  6.25it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:08<00:07,  7.10it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:09<00:05,  8.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:09<00:03, 10.75it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:09<00:02, 11.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:10<00:01, 10.81it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:11<00:01,  9.85it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:12<00:00, 10.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.13it/s]
Evaluation performance at step 50: 0.56
{'loss': 2.2335, 'grad_norm': 0.2692442536354065, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.4765583276748657, 'eval_runtime': 6.524, 'eval_samples_per_second': 153.126, 'eval_steps_per_second': 9.657, 'epoch': 0.08}
{'loss': 1.2335, 'grad_norm': 0.1020127460360527, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0829212665557861, 'eval_runtime': 6.5692, 'eval_samples_per_second': 152.074, 'eval_steps_per_second': 9.59, 'epoch': 0.12}
{'loss': 0.9961, 'grad_norm': 0.086113341152668, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9506709575653076, 'eval_runtime': 6.5607, 'eval_samples_per_second': 152.27, 'eval_steps_per_second': 9.603, 'epoch': 0.16}
{'loss': 0.9255, 'grad_norm': 0.05908776819705963, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9238709211349487, 'eval_runtime': 6.5552, 'eval_samples_per_second': 152.398, 'eval_steps_per_second': 9.611, 'epoch': 0.2}
{'loss': 0.9263, 'grad_norm': 0.07872848212718964, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9072415828704834, 'eval_runtime': 6.5693, 'eval_samples_per_second': 152.07, 'eval_steps_per_second': 9.59, 'epoch': 0.24}
{'loss': 0.9154, 'grad_norm': 0.05971774831414223, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8923106789588928, 'eval_runtime': 6.5803, 'eval_samples_per_second': 151.817, 'eval_steps_per_second': 9.574, 'epoch': 0.28}
{'loss': 0.891, 'grad_norm': 0.09610701352357864, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8795773386955261, 'eval_runtime': 6.6171, 'eval_samples_per_second': 150.973, 'eval_steps_per_second': 9.521, 'epoch': 0.32}
{'loss': 0.8659, 'grad_norm': 0.08433526009321213, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8727127909660339, 'eval_runtime': 6.6454, 'eval_samples_per_second': 150.329, 'eval_steps_per_second': 9.48, 'epoch': 0.36}
{'loss': 0.8918, 'grad_norm': 0.08413053303956985, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8658506274223328, 'eval_runtime': 6.6572, 'eval_samples_per_second': 150.063, 'eval_steps_per_second': 9.463, 'epoch': 0.4}
{'loss': 0.8646, 'grad_norm': 0.07873357832431793, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8631802797317505, 'eval_runtime': 6.6406, 'eval_samples_per_second': 150.437, 'eval_steps_per_second': 9.487, 'epoch': 0.44}
{'loss': 0.8389, 'grad_norm': 0.07448442280292511, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.858834445476532, 'eval_runtime': 6.6311, 'eval_samples_per_second': 150.654, 'eval_steps_per_second': 9.501, 'epoch': 0.48}
{'loss': 0.8636, 'grad_norm': 0.08010585606098175, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8525288701057434, 'eval_runtime': 6.6151, 'eval_samples_per_second': 151.017, 'eval_steps_per_second': 9.524, 'epoch': 0.52}
{'loss': 0.8438, 'grad_norm': 0.10419175773859024, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8501049876213074, 'eval_runtime': 6.6028, 'eval_samples_per_second': 151.299, 'eval_steps_per_second': 9.541, 'epoch': 0.56}
{'loss': 0.8565, 'grad_norm': 0.07859193533658981, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8491108417510986, 'eval_runtime': 6.5856, 'eval_samples_per_second': 151.695, 'eval_steps_per_second': 9.566, 'epoch': 0.6}
{'loss': 0.8604, 'grad_norm': 0.07462771981954575, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8438934683799744, 'eval_runtime': 6.5905, 'eval_samples_per_second': 151.582, 'eval_steps_per_second': 9.559, 'epoch': 0.64}
{'loss': 0.8504, 'grad_norm': 0.08525743335485458, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8427069783210754, 'eval_runtime': 6.5953, 'eval_samples_per_second': 151.472, 'eval_steps_per_second': 9.552, 'epoch': 0.68}
{'loss': 0.8299, 'grad_norm': 0.0859307050704956, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8400737047195435, 'eval_runtime': 6.6057, 'eval_samples_per_second': 151.233, 'eval_steps_per_second': 9.537, 'epoch': 0.72}
{'loss': 0.8608, 'grad_norm': 0.07322873175144196, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8366730809211731, 'eval_runtime': 6.5964, 'eval_samples_per_second': 151.447, 'eval_steps_per_second': 9.551, 'epoch': 0.76}
{'loss': 0.8602, 'grad_norm': 0.06343431770801544, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8354028463363647, 'eval_runtime': 6.5979, 'eval_samples_per_second': 151.412, 'eval_steps_per_second': 9.548, 'epoch': 0.8}
{'loss': 0.8347, 'grad_norm': 0.07587733119726181, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8337147831916809, 'eval_runtime': 6.5941, 'eval_samples_per_second': 151.5, 'eval_steps_per_second': 9.554, 'epoch': 0.84}
{'loss': 0.8568, 'grad_norm': 0.10604412853717804, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8318368792533875, 'eval_runtime': 6.6035, 'eval_samples_per_second': 151.283, 'eval_steps_per_second': 9.54, 'epoch': 0.88}
{'loss': 0.8279, 'grad_norm': 0.08516889065504074, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8308789730072021, 'eval_runtime': 6.6016, 'eval_samples_per_second': 151.327, 'eval_steps_per_second': 9.543, 'epoch': 0.92}
{'loss': 0.8092, 'grad_norm': 0.06860335916280746, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8302421569824219, 'eval_runtime': 6.6049, 'eval_samples_per_second': 151.25, 'eval_steps_per_second': 9.538, 'epoch': 0.96}
{'loss': 0.8453, 'grad_norm': 0.09062723070383072, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8297831416130066, 'eval_runtime': 6.6104, 'eval_samples_per_second': 151.126, 'eval_steps_per_second': 9.53, 'epoch': 1.0}
{'train_runtime': 436.1466, 'train_samples_per_second': 22.919, 'train_steps_per_second': 1.433, 'train_loss': 1.0762302032470703, 'epoch': 1.0}
train_results:  {'eval_loss': [3.3750674724578857, 1.4765583276748657, 1.0829212665557861, 0.9506709575653076, 0.9238709211349487, 0.9072415828704834, 0.8923106789588928, 0.8795773386955261, 0.8727127909660339, 0.8658506274223328, 0.8631802797317505, 0.858834445476532, 0.8525288701057434, 0.8501049876213074, 0.8491108417510986, 0.8438934683799744, 0.8427069783210754, 0.8400737047195435, 0.8366730809211731, 0.8354028463363647, 0.8337147831916809, 0.8318368792533875, 0.8308789730072021, 0.8302421569824219, 0.8297831416130066], 'performance': [0.54, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:55,  1.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 19.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:06,  7.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 11.17it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 14.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.98it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.4884382486343384
current iteration best possible performance (full train run):  0.63
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9877 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 1.0386552157322408
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.889658169090958
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 1.0595175426955954
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 1.0455528228715365
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.6983152897232439
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0324, dtype=torch.float64), 0, 0, tensor(0.0219, dtype=torch.float64), tensor(0.1121, dtype=torch.float64), tensor(0.8335, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.07126544464741473, 1.480000019073487, 0]
normalized proposed parameters for next round by BO: [tensor(0.0324, dtype=torch.float64), tensor(8.8392e-17, dtype=torch.float64), tensor(1.6557e-17, dtype=torch.float64), tensor(0.0219, dtype=torch.float64), tensor(0.1121, dtype=torch.float64), tensor(0.8335, dtype=torch.float64), tensor(1.1963e-17, dtype=torch.float64), tensor(6.8600e-19, dtype=torch.float64), tensor(1.2312e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7127, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.032
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.022
  triviaqa: 0.112
  truthfulqa_gen: 0.834
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07126544464741473,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.07126544464741473
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:48,  1.09s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:16,  5.55it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:09,  9.08it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  9.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:03<00:06, 10.59it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 11.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:03, 11.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 11.85it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:02, 13.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.99it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00,  9.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.03it/s]
Evaluation performance at step 25: 0.56
{'loss': 5.015, 'grad_norm': 0.5407060384750366, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.56}
{'eval_loss': 3.6374425888061523, 'eval_runtime': 4.9986, 'eval_samples_per_second': 199.855, 'eval_steps_per_second': 12.603, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:46,  1.08s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:13,  6.67it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.10it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:05, 13.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 14.05it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:03, 15.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 15.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 16.55it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 19.46it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 17.84it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:01, 13.50it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 11.63it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 13.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.68it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.3444, 'grad_norm': 0.4431542456150055, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.4642174243927002, 'eval_runtime': 3.8753, 'eval_samples_per_second': 257.785, 'eval_steps_per_second': 16.257, 'epoch': 0.08}
{'loss': 1.1989, 'grad_norm': 0.13872988522052765, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0420780181884766, 'eval_runtime': 3.8783, 'eval_samples_per_second': 257.589, 'eval_steps_per_second': 16.244, 'epoch': 0.12}
{'loss': 0.969, 'grad_norm': 0.1497158408164978, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8811430931091309, 'eval_runtime': 3.889, 'eval_samples_per_second': 256.878, 'eval_steps_per_second': 16.199, 'epoch': 0.16}
{'loss': 0.8475, 'grad_norm': 0.059490688145160675, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8381999135017395, 'eval_runtime': 3.8987, 'eval_samples_per_second': 256.242, 'eval_steps_per_second': 16.159, 'epoch': 0.2}
{'loss': 0.8133, 'grad_norm': 0.0653003603219986, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8154738545417786, 'eval_runtime': 3.8993, 'eval_samples_per_second': 256.201, 'eval_steps_per_second': 16.157, 'epoch': 0.24}
{'loss': 0.8128, 'grad_norm': 0.08367390930652618, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7927373051643372, 'eval_runtime': 3.9082, 'eval_samples_per_second': 255.616, 'eval_steps_per_second': 16.12, 'epoch': 0.28}
{'loss': 0.8044, 'grad_norm': 0.09522909671068192, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7775791883468628, 'eval_runtime': 3.8971, 'eval_samples_per_second': 256.346, 'eval_steps_per_second': 16.166, 'epoch': 0.32}
{'loss': 0.7809, 'grad_norm': 0.07697315514087677, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.762658417224884, 'eval_runtime': 3.8944, 'eval_samples_per_second': 256.52, 'eval_steps_per_second': 16.177, 'epoch': 0.36}
{'loss': 0.7718, 'grad_norm': 0.07954952120780945, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7505955100059509, 'eval_runtime': 3.8937, 'eval_samples_per_second': 256.568, 'eval_steps_per_second': 16.18, 'epoch': 0.4}
{'loss': 0.744, 'grad_norm': 0.09894777834415436, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7372795343399048, 'eval_runtime': 3.9014, 'eval_samples_per_second': 256.059, 'eval_steps_per_second': 16.148, 'epoch': 0.44}
{'loss': 0.7215, 'grad_norm': 0.09835799783468246, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7242149114608765, 'eval_runtime': 3.8987, 'eval_samples_per_second': 256.238, 'eval_steps_per_second': 16.159, 'epoch': 0.48}
{'loss': 0.7537, 'grad_norm': 0.10486853867769241, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7164778709411621, 'eval_runtime': 3.8992, 'eval_samples_per_second': 256.209, 'eval_steps_per_second': 16.157, 'epoch': 0.52}
{'loss': 0.7299, 'grad_norm': 0.12130691111087799, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7067283391952515, 'eval_runtime': 3.8884, 'eval_samples_per_second': 256.917, 'eval_steps_per_second': 16.202, 'epoch': 0.56}
{'loss': 0.712, 'grad_norm': 0.12361233681440353, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6944747567176819, 'eval_runtime': 3.8971, 'eval_samples_per_second': 256.347, 'eval_steps_per_second': 16.166, 'epoch': 0.6}
{'loss': 0.6973, 'grad_norm': 0.1104632243514061, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6868517398834229, 'eval_runtime': 3.8878, 'eval_samples_per_second': 256.958, 'eval_steps_per_second': 16.205, 'epoch': 0.64}
{'loss': 0.685, 'grad_norm': 0.11906778067350388, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6792637705802917, 'eval_runtime': 3.8847, 'eval_samples_per_second': 257.16, 'eval_steps_per_second': 16.217, 'epoch': 0.68}
{'loss': 0.6902, 'grad_norm': 0.12987546622753143, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6708332896232605, 'eval_runtime': 3.8824, 'eval_samples_per_second': 257.314, 'eval_steps_per_second': 16.227, 'epoch': 0.72}
{'loss': 0.7263, 'grad_norm': 0.14160646498203278, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6603296399116516, 'eval_runtime': 3.8886, 'eval_samples_per_second': 256.903, 'eval_steps_per_second': 16.201, 'epoch': 0.76}
{'loss': 0.6785, 'grad_norm': 0.2387309968471527, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6526244878768921, 'eval_runtime': 3.8913, 'eval_samples_per_second': 256.724, 'eval_steps_per_second': 16.19, 'epoch': 0.8}
{'loss': 0.6598, 'grad_norm': 0.1738871932029724, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6470847725868225, 'eval_runtime': 3.8959, 'eval_samples_per_second': 256.422, 'eval_steps_per_second': 16.171, 'epoch': 0.84}
{'loss': 0.656, 'grad_norm': 0.14717435836791992, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6442722082138062, 'eval_runtime': 3.8873, 'eval_samples_per_second': 256.99, 'eval_steps_per_second': 16.207, 'epoch': 0.88}
{'loss': 0.6626, 'grad_norm': 0.15084625780582428, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.638662576675415, 'eval_runtime': 3.8962, 'eval_samples_per_second': 256.403, 'eval_steps_per_second': 16.17, 'epoch': 0.92}
{'loss': 0.6602, 'grad_norm': 0.1506391167640686, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6360105276107788, 'eval_runtime': 3.8864, 'eval_samples_per_second': 257.047, 'eval_steps_per_second': 16.21, 'epoch': 0.96}
{'loss': 0.632, 'grad_norm': 0.15080764889717102, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6345489621162415, 'eval_runtime': 3.899, 'eval_samples_per_second': 256.217, 'eval_steps_per_second': 16.158, 'epoch': 1.0}
{'train_runtime': 290.6279, 'train_samples_per_second': 34.405, 'train_steps_per_second': 2.151, 'train_loss': 0.9906753326416016, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6374425888061523, 1.4642174243927002, 1.0420780181884766, 0.8811430931091309, 0.8381999135017395, 0.8154738545417786, 0.7927373051643372, 0.7775791883468628, 0.762658417224884, 0.7505955100059509, 0.7372795343399048, 0.7242149114608765, 0.7164778709411621, 0.7067283391952515, 0.6944747567176819, 0.6868517398834229, 0.6792637705802917, 0.6708332896232605, 0.6603296399116516, 0.6526244878768921, 0.6470847725868225, 0.6442722082138062, 0.638662576675415, 0.6360105276107788, 0.6345489621162415], 'performance': [0.56, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:29,  1.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.93it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 21.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 19.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 22.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 25.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 29.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 23.87it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.56, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4926913976669312
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4876 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.944609431548754
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.8078770859405265
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.7668779123084848
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.6584401093687825
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 1.1807415374454502
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0230, dtype=torch.float64), 0, 0, tensor(0.2820, dtype=torch.float64), tensor(0.0501, dtype=torch.float64), tensor(0.6448, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 0, 1, 0, 128, 0.030569215246383275, 1.4800000190734866, 0]
normalized proposed parameters for next round by BO: [tensor(0.0230, dtype=torch.float64), tensor(3.1312e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2820, dtype=torch.float64), tensor(0.0501, dtype=torch.float64), tensor(0.6448, dtype=torch.float64), tensor(1.3240e-17, dtype=torch.float64), tensor(3.5914e-18, dtype=torch.float64), tensor(5.0260e-18, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.3057, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.023
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.282
  triviaqa: 0.05
  truthfulqa_gen: 0.645
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.030569215246383275,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734866,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.030569215246383275
lora alpha:  1.4800000190734866
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:00,  1.21s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:18,  4.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  8.16it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06,  9.77it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.40it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:05,  9.81it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:04, 10.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:03, 10.85it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 11.92it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:02,  8.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01,  8.02it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00,  9.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.16it/s]
Evaluation performance at step 25: 0.54
{'loss': 5.08, 'grad_norm': 0.5131560564041138, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.6346635818481445, 'eval_runtime': 4.2524, 'eval_samples_per_second': 234.926, 'eval_steps_per_second': 14.815, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:09,  1.42it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:11,  7.90it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:02<00:06, 12.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:05, 12.91it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:03<00:04, 13.51it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 14.01it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:02, 14.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 17.36it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 15.87it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 12.82it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 14.09it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.95it/s]
Evaluation performance at step 50: 0.44
{'loss': 2.3112, 'grad_norm': 0.4153175354003906, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.44}
{'eval_loss': 1.4311821460723877, 'eval_runtime': 3.8755, 'eval_samples_per_second': 257.775, 'eval_steps_per_second': 16.256, 'epoch': 0.08}
{'loss': 1.1785, 'grad_norm': 0.1306840032339096, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0360716581344604, 'eval_runtime': 3.8675, 'eval_samples_per_second': 258.306, 'eval_steps_per_second': 16.29, 'epoch': 0.12}
{'loss': 0.9439, 'grad_norm': 0.11134642362594604, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8770725131034851, 'eval_runtime': 3.8728, 'eval_samples_per_second': 257.952, 'eval_steps_per_second': 16.267, 'epoch': 0.16}
{'loss': 0.875, 'grad_norm': 0.07975000888109207, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8435625433921814, 'eval_runtime': 3.8701, 'eval_samples_per_second': 258.131, 'eval_steps_per_second': 16.279, 'epoch': 0.2}
{'loss': 0.8466, 'grad_norm': 0.06188155338168144, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8233955502510071, 'eval_runtime': 3.8738, 'eval_samples_per_second': 257.886, 'eval_steps_per_second': 16.263, 'epoch': 0.24}
{'loss': 0.8364, 'grad_norm': 0.06638406962156296, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8043444156646729, 'eval_runtime': 3.8728, 'eval_samples_per_second': 257.95, 'eval_steps_per_second': 16.267, 'epoch': 0.28}
{'loss': 0.7981, 'grad_norm': 0.0717945545911789, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7892962694168091, 'eval_runtime': 3.8673, 'eval_samples_per_second': 258.323, 'eval_steps_per_second': 16.291, 'epoch': 0.32}
{'loss': 0.8045, 'grad_norm': 0.07701791822910309, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7755801677703857, 'eval_runtime': 3.8673, 'eval_samples_per_second': 258.317, 'eval_steps_per_second': 16.29, 'epoch': 0.36}
{'loss': 0.77, 'grad_norm': 0.09393154829740524, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7641700506210327, 'eval_runtime': 3.8688, 'eval_samples_per_second': 258.222, 'eval_steps_per_second': 16.284, 'epoch': 0.4}
{'loss': 0.7913, 'grad_norm': 0.08854011446237564, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7566023468971252, 'eval_runtime': 3.8727, 'eval_samples_per_second': 257.959, 'eval_steps_per_second': 16.268, 'epoch': 0.44}
{'loss': 0.7583, 'grad_norm': 0.08462275564670563, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7455653548240662, 'eval_runtime': 3.8713, 'eval_samples_per_second': 258.052, 'eval_steps_per_second': 16.274, 'epoch': 0.48}
{'loss': 0.7667, 'grad_norm': 0.10077515989542007, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.740773618221283, 'eval_runtime': 3.8721, 'eval_samples_per_second': 258.002, 'eval_steps_per_second': 16.27, 'epoch': 0.52}
{'loss': 0.7387, 'grad_norm': 0.09015324711799622, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.731873095035553, 'eval_runtime': 3.8753, 'eval_samples_per_second': 257.785, 'eval_steps_per_second': 16.257, 'epoch': 0.56}
{'loss': 0.7272, 'grad_norm': 0.0821494311094284, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7254762053489685, 'eval_runtime': 3.8796, 'eval_samples_per_second': 257.5, 'eval_steps_per_second': 16.239, 'epoch': 0.6}
{'loss': 0.7329, 'grad_norm': 0.10469081252813339, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7199494242668152, 'eval_runtime': 3.8729, 'eval_samples_per_second': 257.945, 'eval_steps_per_second': 16.267, 'epoch': 0.64}
{'loss': 0.7329, 'grad_norm': 0.08712245523929596, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7123775482177734, 'eval_runtime': 3.8756, 'eval_samples_per_second': 257.768, 'eval_steps_per_second': 16.256, 'epoch': 0.68}
{'loss': 0.7424, 'grad_norm': 0.10223261266946793, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7055273056030273, 'eval_runtime': 3.8719, 'eval_samples_per_second': 258.01, 'eval_steps_per_second': 16.271, 'epoch': 0.72}
{'loss': 0.7271, 'grad_norm': 0.10820112377405167, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7002320885658264, 'eval_runtime': 3.8755, 'eval_samples_per_second': 257.771, 'eval_steps_per_second': 16.256, 'epoch': 0.76}
{'loss': 0.7379, 'grad_norm': 0.14772595465183258, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6964667439460754, 'eval_runtime': 3.8775, 'eval_samples_per_second': 257.642, 'eval_steps_per_second': 16.248, 'epoch': 0.8}
{'loss': 0.7004, 'grad_norm': 0.1315324604511261, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6904828548431396, 'eval_runtime': 3.8754, 'eval_samples_per_second': 257.777, 'eval_steps_per_second': 16.256, 'epoch': 0.84}
{'loss': 0.7109, 'grad_norm': 0.10879926383495331, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6872687935829163, 'eval_runtime': 3.8789, 'eval_samples_per_second': 257.548, 'eval_steps_per_second': 16.242, 'epoch': 0.88}
{'loss': 0.7121, 'grad_norm': 0.1102379709482193, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.684338390827179, 'eval_runtime': 3.8733, 'eval_samples_per_second': 257.921, 'eval_steps_per_second': 16.265, 'epoch': 0.92}
{'loss': 0.7098, 'grad_norm': 0.10106455534696579, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6832061409950256, 'eval_runtime': 3.8788, 'eval_samples_per_second': 257.556, 'eval_steps_per_second': 16.242, 'epoch': 0.96}
{'loss': 0.6897, 'grad_norm': 0.13478490710258484, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6825541853904724, 'eval_runtime': 3.8776, 'eval_samples_per_second': 257.633, 'eval_steps_per_second': 16.247, 'epoch': 1.0}
{'train_runtime': 288.9924, 'train_samples_per_second': 34.6, 'train_steps_per_second': 2.163, 'train_loss': 1.0168948120117187, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6346635818481445, 1.4311821460723877, 1.0360716581344604, 0.8770725131034851, 0.8435625433921814, 0.8233955502510071, 0.8043444156646729, 0.7892962694168091, 0.7755801677703857, 0.7641700506210327, 0.7566023468971252, 0.7455653548240662, 0.740773618221283, 0.731873095035553, 0.7254762053489685, 0.7199494242668152, 0.7123775482177734, 0.7055273056030273, 0.7002320885658264, 0.6964667439460754, 0.6904828548431396, 0.6872687935829163, 0.684338390827179, 0.6832061409950256, 0.6825541853904724], 'performance': [0.54, 0.44]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:54,  1.16s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 12.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:03, 18.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 21.35it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 23.75it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 25.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 28.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.92it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.44]
current iteration observed (possibly low-fid or predicted) performance:  1.4880249500274658
current iteration best possible performance (full train run):  0.6930000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312, 1.4880249500274658]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9306 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.9898633992782371
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.9462646722123567
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 1.1151757586296736
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 1.151621070506323
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.6887562736709107
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0303, dtype=torch.float64), 0, 0, 0, 0, tensor(0.9697, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.03645911465255537, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0303, dtype=torch.float64), tensor(1.5591e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9697, dtype=torch.float64), tensor(4.4469e-16, dtype=torch.float64), tensor(4.3298e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3646, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.03
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.97
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03645911465255537,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.03645911465255537
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:45,  1.67s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:21,  4.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.68it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.07it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.48it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 10.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:02, 11.73it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 12.94it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  8.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.69it/s]
Evaluation performance at step 25: 0.55
{'loss': 5.0255, 'grad_norm': 0.4934402108192444, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.55}
{'eval_loss': 3.6207892894744873, 'eval_runtime': 4.2785, 'eval_samples_per_second': 233.491, 'eval_steps_per_second': 14.725, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:43,  1.05s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:14,  6.47it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:07, 10.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:14,  5.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:09,  6.86it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  8.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 11.05it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 12.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 15.55it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:01, 15.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:01, 17.19it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:08<00:00, 13.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 14.74it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.25it/s]
Evaluation performance at step 50: 0.49
{'loss': 2.2909, 'grad_norm': 0.4019779562950134, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.49}
{'eval_loss': 1.419852614402771, 'eval_runtime': 3.8114, 'eval_samples_per_second': 262.105, 'eval_steps_per_second': 16.529, 'epoch': 0.08}
{'loss': 1.1697, 'grad_norm': 0.08136140555143356, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9893453121185303, 'eval_runtime': 3.8001, 'eval_samples_per_second': 262.885, 'eval_steps_per_second': 16.578, 'epoch': 0.12}
{'loss': 0.9133, 'grad_norm': 0.13053469359874725, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8286174535751343, 'eval_runtime': 3.8095, 'eval_samples_per_second': 262.242, 'eval_steps_per_second': 16.538, 'epoch': 0.16}
{'loss': 0.8349, 'grad_norm': 0.07222577184438705, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7835679650306702, 'eval_runtime': 3.8235, 'eval_samples_per_second': 261.277, 'eval_steps_per_second': 16.477, 'epoch': 0.2}
{'loss': 0.7628, 'grad_norm': 0.07016820460557938, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7553900480270386, 'eval_runtime': 3.8262, 'eval_samples_per_second': 261.092, 'eval_steps_per_second': 16.465, 'epoch': 0.24}
{'loss': 0.7391, 'grad_norm': 0.0747809186577797, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7367170453071594, 'eval_runtime': 3.8133, 'eval_samples_per_second': 261.975, 'eval_steps_per_second': 16.521, 'epoch': 0.28}
{'loss': 0.7173, 'grad_norm': 0.07432820647954941, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7177441120147705, 'eval_runtime': 3.8085, 'eval_samples_per_second': 262.306, 'eval_steps_per_second': 16.542, 'epoch': 0.32}
{'loss': 0.6991, 'grad_norm': 0.07664056867361069, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7014757394790649, 'eval_runtime': 3.8062, 'eval_samples_per_second': 262.468, 'eval_steps_per_second': 16.552, 'epoch': 0.36}
{'loss': 0.7068, 'grad_norm': 0.07925061136484146, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6826776266098022, 'eval_runtime': 3.8105, 'eval_samples_per_second': 262.171, 'eval_steps_per_second': 16.533, 'epoch': 0.4}
{'loss': 0.6975, 'grad_norm': 0.08908259123563766, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6691040992736816, 'eval_runtime': 3.8114, 'eval_samples_per_second': 262.108, 'eval_steps_per_second': 16.529, 'epoch': 0.44}
{'loss': 0.6754, 'grad_norm': 0.10780737549066544, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6521921157836914, 'eval_runtime': 3.8131, 'eval_samples_per_second': 261.99, 'eval_steps_per_second': 16.522, 'epoch': 0.48}
{'loss': 0.6616, 'grad_norm': 0.10581151396036148, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6401227712631226, 'eval_runtime': 3.8165, 'eval_samples_per_second': 261.761, 'eval_steps_per_second': 16.507, 'epoch': 0.52}
{'loss': 0.6362, 'grad_norm': 0.126179039478302, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6283231377601624, 'eval_runtime': 3.8105, 'eval_samples_per_second': 262.169, 'eval_steps_per_second': 16.533, 'epoch': 0.56}
{'loss': 0.6259, 'grad_norm': 0.1302136331796646, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6120176315307617, 'eval_runtime': 3.8222, 'eval_samples_per_second': 261.369, 'eval_steps_per_second': 16.483, 'epoch': 0.6}
{'loss': 0.6282, 'grad_norm': 0.14038193225860596, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5993039608001709, 'eval_runtime': 3.814, 'eval_samples_per_second': 261.928, 'eval_steps_per_second': 16.518, 'epoch': 0.64}
{'loss': 0.6336, 'grad_norm': 0.1487286388874054, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.587813138961792, 'eval_runtime': 3.8148, 'eval_samples_per_second': 261.873, 'eval_steps_per_second': 16.514, 'epoch': 0.68}
{'loss': 0.5935, 'grad_norm': 0.17740552127361298, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5741220116615295, 'eval_runtime': 3.8154, 'eval_samples_per_second': 261.831, 'eval_steps_per_second': 16.512, 'epoch': 0.72}
{'loss': 0.5811, 'grad_norm': 0.1456460952758789, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5625787973403931, 'eval_runtime': 3.809, 'eval_samples_per_second': 262.271, 'eval_steps_per_second': 16.54, 'epoch': 0.76}
{'loss': 0.556, 'grad_norm': 0.18412980437278748, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5535383224487305, 'eval_runtime': 3.8119, 'eval_samples_per_second': 262.076, 'eval_steps_per_second': 16.527, 'epoch': 0.8}
{'loss': 0.5551, 'grad_norm': 0.22360438108444214, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5457942485809326, 'eval_runtime': 3.8139, 'eval_samples_per_second': 261.938, 'eval_steps_per_second': 16.519, 'epoch': 0.84}
{'loss': 0.5765, 'grad_norm': 0.17751897871494293, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5365750193595886, 'eval_runtime': 3.8093, 'eval_samples_per_second': 262.256, 'eval_steps_per_second': 16.539, 'epoch': 0.88}
{'loss': 0.5549, 'grad_norm': 0.21667009592056274, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5314517021179199, 'eval_runtime': 3.8095, 'eval_samples_per_second': 262.242, 'eval_steps_per_second': 16.538, 'epoch': 0.92}
{'loss': 0.5311, 'grad_norm': 0.19499002397060394, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5263192653656006, 'eval_runtime': 3.8072, 'eval_samples_per_second': 262.398, 'eval_steps_per_second': 16.548, 'epoch': 0.96}
{'loss': 0.5457, 'grad_norm': 0.1697765439748764, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5249159932136536, 'eval_runtime': 3.8096, 'eval_samples_per_second': 262.234, 'eval_steps_per_second': 16.537, 'epoch': 1.0}
{'train_runtime': 288.8776, 'train_samples_per_second': 34.613, 'train_steps_per_second': 2.164, 'train_loss': 0.9164680419921875, 'epoch': 1.0}
train_results:  {'eval_loss': [3.6207892894744873, 1.419852614402771, 0.9893453121185303, 0.8286174535751343, 0.7835679650306702, 0.7553900480270386, 0.7367170453071594, 0.7177441120147705, 0.7014757394790649, 0.6826776266098022, 0.6691040992736816, 0.6521921157836914, 0.6401227712631226, 0.6283231377601624, 0.6120176315307617, 0.5993039608001709, 0.587813138961792, 0.5741220116615295, 0.5625787973403931, 0.5535383224487305, 0.5457942485809326, 0.5365750193595886, 0.5314517021179199, 0.5263192653656006, 0.5249159932136536], 'performance': [0.55, 0.49]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:53,  1.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 19.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 23.27it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 25.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 27.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 31.10it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 25.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.55, 0.49]
current iteration observed (possibly low-fid or predicted) performance:  1.4948577880859375
current iteration best possible performance (full train run):  0.7035000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312, 1.4880249500274658, 1.4948577880859375]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1465 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 1.0609348593031944
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.6412693807268692
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 1.0856164863093523
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.8637244207124726
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.608368507166719
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0268, dtype=torch.float64), tensor(0.3357, dtype=torch.float64), 0, 0, tensor(0.0243, dtype=torch.float64), tensor(0.6132, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.03707336801597287, 1.4800000190734885, 0]
normalized proposed parameters for next round by BO: [tensor(0.0268, dtype=torch.float64), tensor(0.3357, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0243, dtype=torch.float64), tensor(0.6132, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8729e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3707, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.027
  gsm8k: 0.336
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.024
  truthfulqa_gen: 0.613
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03707336801597287,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734885,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.03707336801597287
lora alpha:  1.4800000190734885
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:44,  1.66s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.40it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.62it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.41it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.01it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.44it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 10.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:02, 11.74it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 10.43it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  9.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.89it/s]
Evaluation performance at step 25: 0.57
{'loss': 3.4862, 'grad_norm': 0.3111942708492279, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 2.7484872341156006, 'eval_runtime': 8.9092, 'eval_samples_per_second': 112.131, 'eval_steps_per_second': 7.071, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:45,  1.07s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:12,  7.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:07, 11.79it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:21,  3.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  4.94it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:07<00:08,  6.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.46it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:08<00:04, 10.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:02, 12.52it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 12.98it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 11.03it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:10<00:01, 10.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:11<00:00, 12.04it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.90it/s]
Evaluation performance at step 50: 0.54
{'loss': 1.9318, 'grad_norm': 0.2617745101451874, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.54}
{'eval_loss': 1.3757596015930176, 'eval_runtime': 8.8978, 'eval_samples_per_second': 112.274, 'eval_steps_per_second': 7.08, 'epoch': 0.08}
{'loss': 1.1557, 'grad_norm': 0.0636405274271965, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0490680932998657, 'eval_runtime': 8.9877, 'eval_samples_per_second': 111.152, 'eval_steps_per_second': 7.01, 'epoch': 0.12}
{'loss': 0.9721, 'grad_norm': 0.08946876972913742, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9271142482757568, 'eval_runtime': 8.9852, 'eval_samples_per_second': 111.183, 'eval_steps_per_second': 7.012, 'epoch': 0.16}
{'loss': 0.8786, 'grad_norm': 0.053337179124355316, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8885897397994995, 'eval_runtime': 9.007, 'eval_samples_per_second': 110.913, 'eval_steps_per_second': 6.995, 'epoch': 0.2}
{'loss': 0.8701, 'grad_norm': 0.05570390447974205, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8714435696601868, 'eval_runtime': 9.0036, 'eval_samples_per_second': 110.956, 'eval_steps_per_second': 6.997, 'epoch': 0.24}
{'loss': 0.8402, 'grad_norm': 0.057123709470033646, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8576992750167847, 'eval_runtime': 9.0026, 'eval_samples_per_second': 110.968, 'eval_steps_per_second': 6.998, 'epoch': 0.28}
{'loss': 0.8725, 'grad_norm': 0.05216486006975174, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.848181426525116, 'eval_runtime': 9.0088, 'eval_samples_per_second': 110.892, 'eval_steps_per_second': 6.993, 'epoch': 0.32}
{'loss': 0.834, 'grad_norm': 0.06042471528053284, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.840469241142273, 'eval_runtime': 9.0217, 'eval_samples_per_second': 110.733, 'eval_steps_per_second': 6.983, 'epoch': 0.36}
{'loss': 0.8497, 'grad_norm': 0.05548778548836708, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8339441418647766, 'eval_runtime': 9.0276, 'eval_samples_per_second': 110.661, 'eval_steps_per_second': 6.979, 'epoch': 0.4}
{'loss': 0.8534, 'grad_norm': 0.05944471061229706, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8293482661247253, 'eval_runtime': 9.0078, 'eval_samples_per_second': 110.903, 'eval_steps_per_second': 6.994, 'epoch': 0.44}
{'loss': 0.8224, 'grad_norm': 0.0647399052977562, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8237102031707764, 'eval_runtime': 9.0034, 'eval_samples_per_second': 110.958, 'eval_steps_per_second': 6.997, 'epoch': 0.48}
{'loss': 0.8438, 'grad_norm': 0.07187126576900482, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8181425929069519, 'eval_runtime': 8.9918, 'eval_samples_per_second': 111.101, 'eval_steps_per_second': 7.006, 'epoch': 0.52}
{'loss': 0.8199, 'grad_norm': 0.06856144219636917, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8136374354362488, 'eval_runtime': 9.0457, 'eval_samples_per_second': 110.439, 'eval_steps_per_second': 6.965, 'epoch': 0.56}
{'loss': 0.8202, 'grad_norm': 0.07606684416532516, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8100149035453796, 'eval_runtime': 9.0918, 'eval_samples_per_second': 109.88, 'eval_steps_per_second': 6.929, 'epoch': 0.6}
{'loss': 0.8115, 'grad_norm': 0.07703392952680588, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8056778907775879, 'eval_runtime': 9.0769, 'eval_samples_per_second': 110.059, 'eval_steps_per_second': 6.941, 'epoch': 0.64}
{'loss': 0.788, 'grad_norm': 0.07142026722431183, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8031072616577148, 'eval_runtime': 9.0622, 'eval_samples_per_second': 110.238, 'eval_steps_per_second': 6.952, 'epoch': 0.68}
{'loss': 0.7965, 'grad_norm': 0.08765768259763718, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7993633151054382, 'eval_runtime': 9.0518, 'eval_samples_per_second': 110.365, 'eval_steps_per_second': 6.96, 'epoch': 0.72}
{'loss': 0.7991, 'grad_norm': 0.06681177020072937, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7971374988555908, 'eval_runtime': 9.0769, 'eval_samples_per_second': 110.059, 'eval_steps_per_second': 6.941, 'epoch': 0.76}
{'loss': 0.8038, 'grad_norm': 0.07955095171928406, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.793728232383728, 'eval_runtime': 9.0626, 'eval_samples_per_second': 110.234, 'eval_steps_per_second': 6.952, 'epoch': 0.8}
{'loss': 0.808, 'grad_norm': 0.07110365480184555, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.792288064956665, 'eval_runtime': 9.062, 'eval_samples_per_second': 110.241, 'eval_steps_per_second': 6.952, 'epoch': 0.84}
{'loss': 0.7909, 'grad_norm': 0.10054275393486023, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7892699837684631, 'eval_runtime': 9.0661, 'eval_samples_per_second': 110.191, 'eval_steps_per_second': 6.949, 'epoch': 0.88}
{'loss': 0.7743, 'grad_norm': 0.07967891544103622, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7879968881607056, 'eval_runtime': 9.1018, 'eval_samples_per_second': 109.758, 'eval_steps_per_second': 6.922, 'epoch': 0.92}
{'loss': 0.8081, 'grad_norm': 0.0843767300248146, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7868569493293762, 'eval_runtime': 9.1113, 'eval_samples_per_second': 109.644, 'eval_steps_per_second': 6.915, 'epoch': 0.96}
{'loss': 0.8081, 'grad_norm': 0.07827602326869965, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7863773107528687, 'eval_runtime': 9.1258, 'eval_samples_per_second': 109.47, 'eval_steps_per_second': 6.903, 'epoch': 1.0}
{'train_runtime': 537.4115, 'train_samples_per_second': 18.604, 'train_steps_per_second': 1.163, 'train_loss': 0.9935579193115235, 'epoch': 1.0}
train_results:  {'eval_loss': [2.7484872341156006, 1.3757596015930176, 1.0490680932998657, 0.9271142482757568, 0.8885897397994995, 0.8714435696601868, 0.8576992750167847, 0.848181426525116, 0.840469241142273, 0.8339441418647766, 0.8293482661247253, 0.8237102031707764, 0.8181425929069519, 0.8136374354362488, 0.8100149035453796, 0.8056778907775879, 0.8031072616577148, 0.7993633151054382, 0.7971374988555908, 0.793728232383728, 0.792288064956665, 0.7892699837684631, 0.7879968881607056, 0.7868569493293762, 0.7863773107528687], 'performance': [0.57, 0.54]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:05,  1.52it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:29,  2.79it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:11,  5.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:02, 11.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 14.57it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:09<00:00, 16.89it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.60it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.54]
current iteration observed (possibly low-fid or predicted) performance:  1.4894438982009888
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312, 1.4880249500274658, 1.4948577880859375, 1.4894438982009888]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.4018 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.7580061075289788
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 1.0233844819909277
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.5116540795207642
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.6791185038755703
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 1.1842976204630669
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0272, dtype=torch.float64), 0, 0, 0, tensor(0.0385, dtype=torch.float64), tensor(0.5604, dtype=torch.float64), 0, tensor(0.2051, dtype=torch.float64), tensor(0.1687, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 3.903127820947817e-19, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0272, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8396e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0385, dtype=torch.float64), tensor(0.5604, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2051, dtype=torch.float64), tensor(0.1687, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(3.9031e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.027
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.039
  truthfulqa_gen: 0.56
  wikitext: 0
  mmlu: 0.205
  arc_challenge: 0.169

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.903127820947817e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  3.903127820947817e-19
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:43,  1.65s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:20,  4.43it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:10,  7.69it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:08,  8.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:06, 10.12it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:05, 10.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:04, 10.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:03, 10.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:02, 11.87it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:02, 13.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:02,  9.05it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:01,  8.82it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 10.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.59it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.2857, 'grad_norm': 0.5007951855659485, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.277498245239258, 'eval_runtime': 7.4537, 'eval_samples_per_second': 134.027, 'eval_steps_per_second': 8.452, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:45,  2.17it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:21,  3.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:13,  5.12it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  6.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  8.44it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:04, 10.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:12<00:08,  4.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:12<00:05,  5.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:13<00:03,  6.19it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:14<00:01,  6.72it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:14<00:00,  8.04it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.71it/s]
Evaluation performance at step 50: 0.51
{'loss': 2.2376, 'grad_norm': 0.291182279586792, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.51}
{'eval_loss': 1.5812501907348633, 'eval_runtime': 7.4374, 'eval_samples_per_second': 134.322, 'eval_steps_per_second': 8.471, 'epoch': 0.08}
{'loss': 1.3014, 'grad_norm': 0.09516659379005432, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1912586688995361, 'eval_runtime': 7.4856, 'eval_samples_per_second': 133.456, 'eval_steps_per_second': 8.416, 'epoch': 0.12}
{'loss': 1.0837, 'grad_norm': 0.11164098978042603, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0669106245040894, 'eval_runtime': 7.4979, 'eval_samples_per_second': 133.238, 'eval_steps_per_second': 8.402, 'epoch': 0.16}
{'loss': 1.0086, 'grad_norm': 0.08187463134527206, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0333971977233887, 'eval_runtime': 7.5109, 'eval_samples_per_second': 133.007, 'eval_steps_per_second': 8.388, 'epoch': 0.2}
{'loss': 1.0125, 'grad_norm': 0.05945970490574837, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0178078413009644, 'eval_runtime': 7.516, 'eval_samples_per_second': 132.917, 'eval_steps_per_second': 8.382, 'epoch': 0.24}
{'loss': 1.0128, 'grad_norm': 0.07818106561899185, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0050482749938965, 'eval_runtime': 7.4847, 'eval_samples_per_second': 133.472, 'eval_steps_per_second': 8.417, 'epoch': 0.28}
{'loss': 0.993, 'grad_norm': 0.0629756897687912, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9960616230964661, 'eval_runtime': 7.4845, 'eval_samples_per_second': 133.475, 'eval_steps_per_second': 8.417, 'epoch': 0.32}
{'loss': 1.0502, 'grad_norm': 0.07466541975736618, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9862471222877502, 'eval_runtime': 7.5021, 'eval_samples_per_second': 133.163, 'eval_steps_per_second': 8.398, 'epoch': 0.36}
{'loss': 0.9564, 'grad_norm': 0.0910278707742691, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9786834716796875, 'eval_runtime': 7.4975, 'eval_samples_per_second': 133.244, 'eval_steps_per_second': 8.403, 'epoch': 0.4}
{'loss': 0.9984, 'grad_norm': 0.0687544122338295, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9724103808403015, 'eval_runtime': 7.4903, 'eval_samples_per_second': 133.373, 'eval_steps_per_second': 8.411, 'epoch': 0.44}
{'loss': 0.9271, 'grad_norm': 0.09273452311754227, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.967008650302887, 'eval_runtime': 7.5023, 'eval_samples_per_second': 133.16, 'eval_steps_per_second': 8.397, 'epoch': 0.48}
{'loss': 0.9236, 'grad_norm': 0.08272643387317657, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9581243991851807, 'eval_runtime': 7.5038, 'eval_samples_per_second': 133.133, 'eval_steps_per_second': 8.396, 'epoch': 0.52}
{'loss': 0.9784, 'grad_norm': 0.11190958321094513, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.951284646987915, 'eval_runtime': 7.5028, 'eval_samples_per_second': 133.15, 'eval_steps_per_second': 8.397, 'epoch': 0.56}
{'loss': 0.988, 'grad_norm': 0.07972366362810135, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9454981088638306, 'eval_runtime': 7.4942, 'eval_samples_per_second': 133.303, 'eval_steps_per_second': 8.406, 'epoch': 0.6}
{'loss': 0.9377, 'grad_norm': 0.07089952379465103, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9399716854095459, 'eval_runtime': 7.4992, 'eval_samples_per_second': 133.214, 'eval_steps_per_second': 8.401, 'epoch': 0.64}
{'loss': 0.9366, 'grad_norm': 0.0877639576792717, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9356153607368469, 'eval_runtime': 7.4895, 'eval_samples_per_second': 133.386, 'eval_steps_per_second': 8.412, 'epoch': 0.68}
{'loss': 0.9298, 'grad_norm': 0.09590063989162445, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9304836988449097, 'eval_runtime': 7.4925, 'eval_samples_per_second': 133.333, 'eval_steps_per_second': 8.408, 'epoch': 0.72}
{'loss': 0.8612, 'grad_norm': 0.08718113601207733, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9270535111427307, 'eval_runtime': 7.4944, 'eval_samples_per_second': 133.299, 'eval_steps_per_second': 8.406, 'epoch': 0.76}
{'loss': 0.9477, 'grad_norm': 0.07812633365392685, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9240480661392212, 'eval_runtime': 7.4949, 'eval_samples_per_second': 133.29, 'eval_steps_per_second': 8.406, 'epoch': 0.8}
{'loss': 0.9386, 'grad_norm': 0.08606966584920883, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9211507439613342, 'eval_runtime': 7.5314, 'eval_samples_per_second': 132.644, 'eval_steps_per_second': 8.365, 'epoch': 0.84}
{'loss': 0.9386, 'grad_norm': 0.07397997379302979, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9190574884414673, 'eval_runtime': 7.5912, 'eval_samples_per_second': 131.599, 'eval_steps_per_second': 8.299, 'epoch': 0.88}
{'loss': 0.9368, 'grad_norm': 0.08073728531599045, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9169983267784119, 'eval_runtime': 7.6083, 'eval_samples_per_second': 131.303, 'eval_steps_per_second': 8.28, 'epoch': 0.92}
{'loss': 0.8808, 'grad_norm': 0.09062971919775009, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9158803820610046, 'eval_runtime': 7.5218, 'eval_samples_per_second': 132.813, 'eval_steps_per_second': 8.376, 'epoch': 0.96}
{'loss': 0.9009, 'grad_norm': 0.12354873865842819, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9154176712036133, 'eval_runtime': 7.5225, 'eval_samples_per_second': 132.801, 'eval_steps_per_second': 8.375, 'epoch': 1.0}
{'train_runtime': 467.6584, 'train_samples_per_second': 21.379, 'train_steps_per_second': 1.336, 'train_loss': 1.1586357391357422, 'epoch': 1.0}
train_results:  {'eval_loss': [3.277498245239258, 1.5812501907348633, 1.1912586688995361, 1.0669106245040894, 1.0333971977233887, 1.0178078413009644, 1.0050482749938965, 0.9960616230964661, 0.9862471222877502, 0.9786834716796875, 0.9724103808403015, 0.967008650302887, 0.9581243991851807, 0.951284646987915, 0.9454981088638306, 0.9399716854095459, 0.9356153607368469, 0.9304836988449097, 0.9270535111427307, 0.9240480661392212, 0.9211507439613342, 0.9190574884414673, 0.9169983267784119, 0.9158803820610046, 0.9154176712036133], 'performance': [0.54, 0.51]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:07,  1.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:06, 13.71it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 20.17it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 18.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 18.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:01, 17.51it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 21.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 19.48it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.51]
current iteration observed (possibly low-fid or predicted) performance:  1.4889435768127441
current iteration best possible performance (full train run):  0.651
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312, 1.4880249500274658, 1.4948577880859375, 1.4894438982009888, 1.4889435768127441]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1572 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 1.1824164575732
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 1.0620476118294402
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.8489684357244835
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.9118129570528669
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 1.145691870176815
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1880, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), tensor(0.7788, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.029238948702691166, 1.4800000190734894, 0]
normalized proposed parameters for next round by BO: [tensor(0.0095, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1953e-17, dtype=torch.float64), tensor(0.1880, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), tensor(0.7788, dtype=torch.float64), tensor(2.6406e-17, dtype=torch.float64), tensor(4.4748e-17, dtype=torch.float64), tensor(1.6172e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2924, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.188
  triviaqa: 0.024
  truthfulqa_gen: 0.779
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.029238948702691166,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (1.4800000190734894,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.029238948702691166
lora alpha:  1.4800000190734894
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9904
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  990
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:03<06:22,  3.87s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:49,  1.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:24,  3.39it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:08<00:19,  3.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:09<00:14,  4.61it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:10<00:11,  5.15it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:12<00:10,  4.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:14<00:08,  4.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:15<00:06,  5.27it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:16<00:04,  5.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:20<00:05,  3.76it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:22<00:02,  3.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:23<00:00,  4.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:23<00:00,  4.24it/s]
Evaluation performance at step 25: 0.54
{'loss': 4.9727, 'grad_norm': 0.44985029101371765, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 3.2974064350128174, 'eval_runtime': 3.8576, 'eval_samples_per_second': 256.634, 'eval_steps_per_second': 16.072, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:45,  1.07s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:02<00:19,  4.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:11,  7.37it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:03<00:09,  7.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:08,  7.94it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:06,  8.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:07,  6.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:05,  7.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:14<00:12,  2.74it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:15<00:07,  3.53it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:16<00:04,  4.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:17<00:02,  5.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:17<00:00,  6.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:17<00:00,  5.65it/s]
Evaluation performance at step 50: 0.44
{'loss': 1.9608, 'grad_norm': 0.29271864891052246, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.44}
{'eval_loss': 1.2257264852523804, 'eval_runtime': 10.7739, 'eval_samples_per_second': 91.889, 'eval_steps_per_second': 5.755, 'epoch': 0.08}
{'loss': 1.0649, 'grad_norm': 0.12240668386220932, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 0.9475675821304321, 'eval_runtime': 9.7524, 'eval_samples_per_second': 101.513, 'eval_steps_per_second': 6.357, 'epoch': 0.12}
{'loss': 0.9056, 'grad_norm': 0.06188375502824783, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 0.7992717623710632, 'eval_runtime': 8.0596, 'eval_samples_per_second': 122.835, 'eval_steps_per_second': 7.693, 'epoch': 0.16}
{'loss': 0.8005, 'grad_norm': 0.06492826342582703, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 0.7717542052268982, 'eval_runtime': 8.4411, 'eval_samples_per_second': 117.284, 'eval_steps_per_second': 7.345, 'epoch': 0.2}
{'loss': 0.767, 'grad_norm': 0.06181985139846802, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 0.7499637603759766, 'eval_runtime': 9.2487, 'eval_samples_per_second': 107.043, 'eval_steps_per_second': 6.704, 'epoch': 0.24}
{'loss': 0.7634, 'grad_norm': 0.06878521293401718, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 0.7283134460449219, 'eval_runtime': 8.1755, 'eval_samples_per_second': 121.093, 'eval_steps_per_second': 7.584, 'epoch': 0.28}
{'loss': 0.6992, 'grad_norm': 0.06438998132944107, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 0.7050548195838928, 'eval_runtime': 7.2863, 'eval_samples_per_second': 135.872, 'eval_steps_per_second': 8.509, 'epoch': 0.32}
{'loss': 0.7003, 'grad_norm': 0.08739350736141205, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 0.684908926486969, 'eval_runtime': 8.1758, 'eval_samples_per_second': 121.089, 'eval_steps_per_second': 7.583, 'epoch': 0.36}
{'loss': 0.672, 'grad_norm': 0.07395272701978683, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.6609525680541992, 'eval_runtime': 7.2566, 'eval_samples_per_second': 136.428, 'eval_steps_per_second': 8.544, 'epoch': 0.4}
{'loss': 0.6796, 'grad_norm': 0.08502896130084991, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.638391375541687, 'eval_runtime': 7.255, 'eval_samples_per_second': 136.457, 'eval_steps_per_second': 8.546, 'epoch': 0.44}
{'loss': 0.6373, 'grad_norm': 0.08709067106246948, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.6095916628837585, 'eval_runtime': 8.6701, 'eval_samples_per_second': 114.186, 'eval_steps_per_second': 7.151, 'epoch': 0.48}
{'loss': 0.6449, 'grad_norm': 0.09392207860946655, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.579436719417572, 'eval_runtime': 7.3584, 'eval_samples_per_second': 134.541, 'eval_steps_per_second': 8.426, 'epoch': 0.53}
{'loss': 0.6086, 'grad_norm': 0.10434021800756454, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.5505762100219727, 'eval_runtime': 8.1666, 'eval_samples_per_second': 121.226, 'eval_steps_per_second': 7.592, 'epoch': 0.57}
{'loss': 0.5751, 'grad_norm': 0.15105040371418, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.5229904055595398, 'eval_runtime': 7.4374, 'eval_samples_per_second': 133.111, 'eval_steps_per_second': 8.336, 'epoch': 0.61}
{'loss': 0.537, 'grad_norm': 0.14846540987491608, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.49400344491004944, 'eval_runtime': 7.8676, 'eval_samples_per_second': 125.832, 'eval_steps_per_second': 7.88, 'epoch': 0.65}
{'loss': 0.5123, 'grad_norm': 0.12194695323705673, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.48094162344932556, 'eval_runtime': 8.1094, 'eval_samples_per_second': 122.08, 'eval_steps_per_second': 7.645, 'epoch': 0.69}
{'loss': 0.5082, 'grad_norm': 0.15154516696929932, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.4450438916683197, 'eval_runtime': 7.8691, 'eval_samples_per_second': 125.808, 'eval_steps_per_second': 7.879, 'epoch': 0.73}
{'loss': 0.4936, 'grad_norm': 0.12791864573955536, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.42608702182769775, 'eval_runtime': 7.0833, 'eval_samples_per_second': 139.765, 'eval_steps_per_second': 8.753, 'epoch': 0.77}
{'loss': 0.4675, 'grad_norm': 0.15822270512580872, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.40590402483940125, 'eval_runtime': 7.5205, 'eval_samples_per_second': 131.639, 'eval_steps_per_second': 8.244, 'epoch': 0.81}
{'loss': 0.4185, 'grad_norm': 0.14844699203968048, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.3911871314048767, 'eval_runtime': 8.9065, 'eval_samples_per_second': 111.154, 'eval_steps_per_second': 6.961, 'epoch': 0.85}
{'loss': 0.4313, 'grad_norm': 0.16385865211486816, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.3772779107093811, 'eval_runtime': 7.9391, 'eval_samples_per_second': 124.699, 'eval_steps_per_second': 7.809, 'epoch': 0.89}
{'loss': 0.4164, 'grad_norm': 0.14805474877357483, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.36966562271118164, 'eval_runtime': 8.1846, 'eval_samples_per_second': 120.958, 'eval_steps_per_second': 7.575, 'epoch': 0.93}
{'loss': 0.4149, 'grad_norm': 0.1871432512998581, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.3625045716762543, 'eval_runtime': 7.6888, 'eval_samples_per_second': 128.758, 'eval_steps_per_second': 8.064, 'epoch': 0.97}
{'train_runtime': 676.6166, 'train_samples_per_second': 14.638, 'train_steps_per_second': 0.915, 'train_loss': 0.8464242214147417, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2974064350128174, 1.2257264852523804, 0.9475675821304321, 0.7992717623710632, 0.7717542052268982, 0.7499637603759766, 0.7283134460449219, 0.7050548195838928, 0.684908926486969, 0.6609525680541992, 0.638391375541687, 0.6095916628837585, 0.579436719417572, 0.5505762100219727, 0.5229904055595398, 0.49400344491004944, 0.48094162344932556, 0.4450438916683197, 0.42608702182769775, 0.40590402483940125, 0.3911871314048767, 0.3772779107093811, 0.36966562271118164, 0.3625045716762543], 'performance': [0.54, 0.44]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:28<47:17, 28.66s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:57<04:02,  2.92s/it]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [01:01<01:31,  1.37s/it]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [01:30<01:20,  1.59s/it]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [01:34<00:36,  1.05s/it]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [02:03<00:25,  1.33s/it]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [02:05<00:02,  1.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:06<00:00,  1.26s/it]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.44]
current iteration observed (possibly low-fid or predicted) performance:  1.4910489320755005
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312, 1.4880249500274658, 1.4948577880859375, 1.4894438982009888, 1.4889435768127441, 1.4910489320755005]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.5556 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.5495294313078707
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 1.0062161630127744
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 1.1032804896773605
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.9470072311234301
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 1.170358691364329
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.1489, dtype=torch.float64), 0, tensor(0.7637, dtype=torch.float64), tensor(0.0842, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 0, 0, 128, 0.016139350256655246, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0032, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.7993e-18, dtype=torch.float64), tensor(0.1489, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7637, dtype=torch.float64), tensor(0.0842, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1614, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.149
  triviaqa: 0
  truthfulqa_gen: 0.764
  wikitext: 0.084
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.016139350256655246,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.016139350256655246
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9966
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  996
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:02<04:02,  2.45s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:38,  2.35it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:25,  3.26it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:08<00:21,  3.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:09<00:17,  3.89it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:11<00:13,  4.27it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:14<00:13,  3.77it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:16<00:11,  3.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:17<00:08,  4.29it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:18<00:05,  4.59it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:22<00:05,  3.41it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:25<00:03,  3.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:26<00:00,  3.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:26<00:00,  3.71it/s]
Evaluation performance at step 25: 0.57
{'loss': 5.1965, 'grad_norm': 0.5504783987998962, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.57}
{'eval_loss': 4.626915454864502, 'eval_runtime': 5.9897, 'eval_samples_per_second': 166.285, 'eval_steps_per_second': 10.518, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:13,  1.34it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:15,  5.70it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:04<00:25,  3.29it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:15<00:58,  1.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:16<00:35,  1.91it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:24,  2.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:19<00:16,  3.12it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:21<00:12,  3.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:22<00:08,  4.30it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:23<00:05,  4.75it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:24<00:03,  5.14it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:26<00:01,  5.59it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:27<00:00,  6.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:27<00:00,  3.70it/s]
Evaluation performance at step 50: 0.59
{'loss': 3.5022, 'grad_norm': 0.21113333106040955, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 2.4191927909851074, 'eval_runtime': 4.9849, 'eval_samples_per_second': 199.803, 'eval_steps_per_second': 12.638, 'epoch': 0.08}
{'loss': 1.899, 'grad_norm': 0.07102585583925247, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.6131360530853271, 'eval_runtime': 5.0838, 'eval_samples_per_second': 195.918, 'eval_steps_per_second': 12.392, 'epoch': 0.12}
{'loss': 1.5281, 'grad_norm': 0.0724467858672142, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.449856162071228, 'eval_runtime': 5.3641, 'eval_samples_per_second': 185.678, 'eval_steps_per_second': 11.745, 'epoch': 0.16}
{'loss': 1.4046, 'grad_norm': 0.05873366817831993, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 1.3389124870300293, 'eval_runtime': 4.9139, 'eval_samples_per_second': 202.692, 'eval_steps_per_second': 12.821, 'epoch': 0.2}
{'loss': 1.3458, 'grad_norm': 0.07722461968660355, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 1.2949671745300293, 'eval_runtime': 4.4151, 'eval_samples_per_second': 225.587, 'eval_steps_per_second': 14.269, 'epoch': 0.24}
{'loss': 1.3059, 'grad_norm': 0.07487206161022186, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 1.26913583278656, 'eval_runtime': 4.4992, 'eval_samples_per_second': 221.371, 'eval_steps_per_second': 14.002, 'epoch': 0.28}
{'loss': 1.3123, 'grad_norm': 0.09303044527769089, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 1.2476550340652466, 'eval_runtime': 4.4061, 'eval_samples_per_second': 226.048, 'eval_steps_per_second': 14.298, 'epoch': 0.32}
{'loss': 1.3025, 'grad_norm': 0.07003714889287949, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 1.2301653623580933, 'eval_runtime': 4.3817, 'eval_samples_per_second': 227.307, 'eval_steps_per_second': 14.378, 'epoch': 0.36}
{'loss': 1.1865, 'grad_norm': 0.06671489775180817, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 1.2129677534103394, 'eval_runtime': 4.3714, 'eval_samples_per_second': 227.844, 'eval_steps_per_second': 14.412, 'epoch': 0.4}
{'loss': 1.1474, 'grad_norm': 0.08102774620056152, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 1.1968387365341187, 'eval_runtime': 4.6951, 'eval_samples_per_second': 212.137, 'eval_steps_per_second': 13.418, 'epoch': 0.44}
{'loss': 1.157, 'grad_norm': 0.07235527038574219, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 1.1790982484817505, 'eval_runtime': 4.398, 'eval_samples_per_second': 226.468, 'eval_steps_per_second': 14.325, 'epoch': 0.48}
{'loss': 1.1649, 'grad_norm': 0.07387126237154007, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 1.1620343923568726, 'eval_runtime': 4.369, 'eval_samples_per_second': 227.97, 'eval_steps_per_second': 14.42, 'epoch': 0.52}
{'loss': 1.1904, 'grad_norm': 0.07513875514268875, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 1.1442978382110596, 'eval_runtime': 4.3781, 'eval_samples_per_second': 227.494, 'eval_steps_per_second': 14.39, 'epoch': 0.56}
{'loss': 1.0952, 'grad_norm': 0.1236669197678566, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 1.1255860328674316, 'eval_runtime': 4.3659, 'eval_samples_per_second': 228.131, 'eval_steps_per_second': 14.43, 'epoch': 0.6}
{'loss': 1.1019, 'grad_norm': 0.08840050548315048, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 1.0993077754974365, 'eval_runtime': 4.3711, 'eval_samples_per_second': 227.86, 'eval_steps_per_second': 14.413, 'epoch': 0.64}
{'loss': 1.0621, 'grad_norm': 0.10209694504737854, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 1.0624425411224365, 'eval_runtime': 4.3299, 'eval_samples_per_second': 230.027, 'eval_steps_per_second': 14.55, 'epoch': 0.68}
{'loss': 1.0122, 'grad_norm': 0.0991474837064743, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 1.0172231197357178, 'eval_runtime': 4.3476, 'eval_samples_per_second': 229.093, 'eval_steps_per_second': 14.491, 'epoch': 0.72}
{'loss': 1.019, 'grad_norm': 0.08593465387821198, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 1.0003485679626465, 'eval_runtime': 4.3268, 'eval_samples_per_second': 230.191, 'eval_steps_per_second': 14.56, 'epoch': 0.76}
{'loss': 0.965, 'grad_norm': 0.10181573778390884, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.9881421327590942, 'eval_runtime': 4.3125, 'eval_samples_per_second': 230.956, 'eval_steps_per_second': 14.609, 'epoch': 0.8}
{'loss': 0.9742, 'grad_norm': 0.12464667856693268, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.9785507917404175, 'eval_runtime': 4.314, 'eval_samples_per_second': 230.877, 'eval_steps_per_second': 14.604, 'epoch': 0.84}
{'loss': 1.0221, 'grad_norm': 0.08816251903772354, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.9729891419410706, 'eval_runtime': 4.3188, 'eval_samples_per_second': 230.62, 'eval_steps_per_second': 14.587, 'epoch': 0.88}
{'loss': 0.9646, 'grad_norm': 0.10428491234779358, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.9677714705467224, 'eval_runtime': 4.3191, 'eval_samples_per_second': 230.603, 'eval_steps_per_second': 14.586, 'epoch': 0.92}
{'loss': 0.9656, 'grad_norm': 0.11326945573091507, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.9644446969032288, 'eval_runtime': 4.3095, 'eval_samples_per_second': 231.118, 'eval_steps_per_second': 14.619, 'epoch': 0.96}
{'train_runtime': 411.8907, 'train_samples_per_second': 24.196, 'train_steps_per_second': 1.513, 'train_loss': 1.431335966621509, 'epoch': 1.0}
train_results:  {'eval_loss': [4.626915454864502, 2.4191927909851074, 1.6131360530853271, 1.449856162071228, 1.3389124870300293, 1.2949671745300293, 1.26913583278656, 1.2476550340652466, 1.2301653623580933, 1.2129677534103394, 1.1968387365341187, 1.1790982484817505, 1.1620343923568726, 1.1442978382110596, 1.1255860328674316, 1.0993077754974365, 1.0624425411224365, 1.0172231197357178, 1.0003485679626465, 0.9881421327590942, 0.9785507917404175, 0.9729891419410706, 0.9677714705467224, 0.9644446969032288], 'performance': [0.57, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:57,  1.71it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 19.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 26.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 31.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 32.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 34.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 37.09it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 32.10it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.57, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.4901353120803833
current iteration best possible performance (full train run):  0.672
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312, 1.4880249500274658, 1.4948577880859375, 1.4894438982009888, 1.4889435768127441, 1.4910489320755005, 1.4901353120803833]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0767 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 1.1191138387217785
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 1.0043310557781981
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.7811051893542923
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 1.114816146413127
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.5003354972469338
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.8131, dtype=torch.float64), 0, 0, 0, 0, tensor(0.1869, dtype=torch.float64), 0, 0, 0, 32, 0, 1, 0, 1, 0, 2, 0.0, 28.63700252055991, 0]
normalized proposed parameters for next round by BO: [tensor(0.8131, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.2517e-18, dtype=torch.float64), tensor(5.0348e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1869, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5966, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.813
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.187
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (28.63700252055991,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  2
lora dropout:  0.0
lora alpha:  28.63700252055991
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 1,507,328 || all params: 8,031,768,576 || trainable%: 0.0188
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'truthfulqa_gen': (1.0, 'bleu_acc,none')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:44,  2.21it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:07, 11.43it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.17it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:20,  3.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:12,  5.40it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  7.33it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:06,  7.97it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:05,  8.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03, 10.78it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:08<00:02, 12.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:09<00:01, 13.36it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:09<00:00, 14.15it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:10<00:00, 14.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.78it/s]
Evaluation performance at step 25: 0.54
{'loss': 3.2935, 'grad_norm': 10.897435188293457, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.54}
{'eval_loss': 1.3578678369522095, 'eval_runtime': 4.8969, 'eval_samples_per_second': 204.005, 'eval_steps_per_second': 12.865, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:46,  2.13it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.37it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 14.32it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:04, 17.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 16.30it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:03, 16.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:02, 17.54it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:03<00:02, 15.77it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 17.60it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:04<00:01, 18.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:01, 17.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 19.38it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 17.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.18it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.0511, 'grad_norm': 3.1447134017944336, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 0.9072306752204895, 'eval_runtime': 4.7566, 'eval_samples_per_second': 210.025, 'eval_steps_per_second': 13.245, 'epoch': 0.08}
{'loss': 0.8919, 'grad_norm': 1.8764922618865967, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8790217638015747, 'eval_runtime': 4.7713, 'eval_samples_per_second': 209.376, 'eval_steps_per_second': 13.204, 'epoch': 0.12}
{'loss': 0.8781, 'grad_norm': 1.1856608390808105, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8602113127708435, 'eval_runtime': 4.7752, 'eval_samples_per_second': 209.208, 'eval_steps_per_second': 13.193, 'epoch': 0.16}
{'loss': 0.8653, 'grad_norm': 1.011017084121704, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8430584073066711, 'eval_runtime': 4.805, 'eval_samples_per_second': 207.91, 'eval_steps_per_second': 13.111, 'epoch': 0.2}
{'loss': 0.8283, 'grad_norm': 1.3759891986846924, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8318688273429871, 'eval_runtime': 4.7786, 'eval_samples_per_second': 209.058, 'eval_steps_per_second': 13.184, 'epoch': 0.24}
{'loss': 0.8508, 'grad_norm': 1.1476792097091675, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8161797523498535, 'eval_runtime': 4.7807, 'eval_samples_per_second': 208.965, 'eval_steps_per_second': 13.178, 'epoch': 0.28}
{'loss': 0.8132, 'grad_norm': 1.2948691844940186, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8094550967216492, 'eval_runtime': 4.7863, 'eval_samples_per_second': 208.72, 'eval_steps_per_second': 13.163, 'epoch': 0.32}
{'loss': 0.8076, 'grad_norm': 1.0165330171585083, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.796326220035553, 'eval_runtime': 4.7774, 'eval_samples_per_second': 209.11, 'eval_steps_per_second': 13.187, 'epoch': 0.36}
{'loss': 0.8143, 'grad_norm': 1.0489424467086792, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.786999523639679, 'eval_runtime': 4.7912, 'eval_samples_per_second': 208.507, 'eval_steps_per_second': 13.149, 'epoch': 0.4}
{'loss': 0.8207, 'grad_norm': 1.1007225513458252, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7767677903175354, 'eval_runtime': 4.7891, 'eval_samples_per_second': 208.598, 'eval_steps_per_second': 13.155, 'epoch': 0.44}
{'loss': 0.789, 'grad_norm': 1.299071192741394, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7680746912956238, 'eval_runtime': 4.7906, 'eval_samples_per_second': 208.535, 'eval_steps_per_second': 13.151, 'epoch': 0.48}
{'loss': 0.807, 'grad_norm': 1.2383729219436646, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7627831697463989, 'eval_runtime': 4.8005, 'eval_samples_per_second': 208.101, 'eval_steps_per_second': 13.124, 'epoch': 0.52}
{'loss': 0.7846, 'grad_norm': 1.0543067455291748, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7565298676490784, 'eval_runtime': 4.7905, 'eval_samples_per_second': 208.536, 'eval_steps_per_second': 13.151, 'epoch': 0.56}
{'loss': 0.7884, 'grad_norm': 1.3896458148956299, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.748229444026947, 'eval_runtime': 4.7894, 'eval_samples_per_second': 208.586, 'eval_steps_per_second': 13.154, 'epoch': 0.6}
{'loss': 0.7709, 'grad_norm': 1.3120447397232056, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7425878643989563, 'eval_runtime': 4.8116, 'eval_samples_per_second': 207.625, 'eval_steps_per_second': 13.093, 'epoch': 0.64}
{'loss': 0.7867, 'grad_norm': 1.2737255096435547, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7356377840042114, 'eval_runtime': 4.8067, 'eval_samples_per_second': 207.833, 'eval_steps_per_second': 13.107, 'epoch': 0.68}
{'loss': 0.7835, 'grad_norm': 1.2244147062301636, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7303559184074402, 'eval_runtime': 4.8044, 'eval_samples_per_second': 207.932, 'eval_steps_per_second': 13.113, 'epoch': 0.72}
{'loss': 0.7707, 'grad_norm': 1.3450236320495605, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7257670164108276, 'eval_runtime': 4.8295, 'eval_samples_per_second': 206.856, 'eval_steps_per_second': 13.045, 'epoch': 0.76}
{'loss': 0.7718, 'grad_norm': 1.1207789182662964, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7193575501441956, 'eval_runtime': 4.8026, 'eval_samples_per_second': 208.01, 'eval_steps_per_second': 13.118, 'epoch': 0.8}
{'loss': 0.7541, 'grad_norm': 1.209965467453003, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7153541445732117, 'eval_runtime': 4.8158, 'eval_samples_per_second': 207.441, 'eval_steps_per_second': 13.082, 'epoch': 0.84}
{'loss': 0.7613, 'grad_norm': 1.1321581602096558, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7127028703689575, 'eval_runtime': 4.8582, 'eval_samples_per_second': 205.631, 'eval_steps_per_second': 12.968, 'epoch': 0.88}
{'loss': 0.756, 'grad_norm': 1.2795404195785522, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7088574171066284, 'eval_runtime': 4.8266, 'eval_samples_per_second': 206.98, 'eval_steps_per_second': 13.053, 'epoch': 0.92}
{'loss': 0.7511, 'grad_norm': 1.0352779626846313, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7074834108352661, 'eval_runtime': 4.8374, 'eval_samples_per_second': 206.517, 'eval_steps_per_second': 13.024, 'epoch': 0.96}
{'loss': 0.7305, 'grad_norm': 1.3363583087921143, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.706706702709198, 'eval_runtime': 4.8016, 'eval_samples_per_second': 208.057, 'eval_steps_per_second': 13.121, 'epoch': 1.0}
{'train_runtime': 323.7805, 'train_samples_per_second': 30.882, 'train_steps_per_second': 1.93, 'train_loss': 0.908815151977539, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3578678369522095, 0.9072306752204895, 0.8790217638015747, 0.8602113127708435, 0.8430584073066711, 0.8318688273429871, 0.8161797523498535, 0.8094550967216492, 0.796326220035553, 0.786999523639679, 0.7767677903175354, 0.7680746912956238, 0.7627831697463989, 0.7565298676490784, 0.748229444026947, 0.7425878643989563, 0.7356377840042114, 0.7303559184074402, 0.7257670164108276, 0.7193575501441956, 0.7153541445732117, 0.7127028703689575, 0.7088574171066284, 0.7074834108352661, 0.706706702709198], 'performance': [0.54, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['truthfulqa_gen']
WARNING:lm_eval.api.task:truthfulqa_gen: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:49,  1.10s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:02<00:08,  9.63it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:02<00:04, 16.01it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:03<00:03, 16.83it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 16.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 20.18it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 25.11it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.94it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.54, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.244809865951538
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.8925
BO observations:  [1.4319740533828735, 1.4756853580474854, 1.4684956073760986, 1.4780199527740479, 1.488027811050415, 1.4894435405731201, 1.481907844543457, 1.489302635192871, 1.461254358291626, 1.4595184326171875, 1.4905487298965454, 1.4822300672531128, 1.486533284187317, 1.489725112915039, 1.4922642707824707, 1.490051507949829, 1.4731359481811523, 1.4908437728881836, 1.4874308109283447, 1.492409348487854, 1.492368459701538, 1.4884382486343384, 1.4926913976669312, 1.4880249500274658, 1.4948577880859375, 1.4894438982009888, 1.4889435768127441, 1.4910489320755005, 1.4901353120803833, 1.244809865951538]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0335 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.8590389734016022
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.7543279197445083
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.6304626789351087
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 1.115528549948649
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 1.2392387357931485
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0146, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7101, dtype=torch.float64), tensor(0.2753, dtype=torch.float64), 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0451339191558521, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0.0146, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.4561e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.1536e-18, dtype=torch.float64), tensor(0.7101, dtype=torch.float64), tensor(0.2753, dtype=torch.float64), tensor(3.8180e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.4513, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.6930000000000001, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'truthfulqa_gen', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_truthfulqa_gen_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['truthfulqa_gen'], 'weight': [1.0], 'random': [[0.7140000000000001, 0.7875000000000001, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925], [0.7665, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001, 0.9345000000000001], [0.6930000000000001, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925, 0.8925]], 'random_full_inputs': [[[0, 0.11220027908159681, 0.05121365494297493, 0.07433341049205285, 0, 0.431299761621717, 0.08964221600140299, 0.0282674462366433, 0.21304323162361216, 32, 0, 0, 0, 1, 0, 108, 1.7347234759768072e-19, 44.7393382712138, 0], [0, 0, 0.25716748221830965, 0, 0, 0.5387466911283546, 0.20408582665333586, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0], [0, 0, 0, 0, 0, 0.9636099579674448, 0, 0, 0.036390042032555206, 32, 1, 0, 0, 1, 0, 128, 0.0, 47.99999999999999, 0], [0, 0, 0, 0, 0, 0.4754370251591431, 0, 0.5245629748408566, 0, 32, 0, 0, 0, 1, 0, 128, 4.163336342344338e-18, 47.999999999999986, 0], [0, 0, 0, 0.7132741726680799, 0, 0.28672582733192015, 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0, 1.0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 2.2204460492503132e-17, 1.4800000190734863, 0], [0, 0.872209700674553, 0, 0, 0, 0.12779029932544705, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 1.544370808988263e-17, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0.3431863309178289, 0, 0, 0.6568136690821704, 32, 0, 1, 0, 1, 0, 128, 4.823128858793852e-16, 1.4800000190734974, 1], [0, 0, 0.5519821640305885, 0, 0, 0.4480178359694113, 0, 0, 0, 32, 1, 1, 1, 0, 1, 128, 6.938893903907229e-19, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0.7736170064469142, 0.22638299355308616, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0.9999999999999999, 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.0, 1.4800000190734992, 1], [0, 0, 0, 0, 0, 0.22652706169843584, 0, 0.7734729383015642, 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0.31122933590921753, 0, 0, 0.6887706640907832, 32, 1, 0, 1, 0, 1, 128, 0.09999999999999999, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0.9508424164702584, 0.04894168197108643, 0, 0, 32, 0, 0, 1, 0, 0, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0, 0, 0.07194810198452722, 0.5133368869067019, 0.41471501110877124, 0, 0, 32, 0, 1, 0, 1, 1, 128, 9.8879238130678e-18, 1.4800000190734885, 1], [0, 0, 0, 0, 0, 0.4509349970720305, 0, 0.5490650029279697, 0, 32, 1, 1, 0, 1, 0, 128, 1.2143064331837614e-18, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0.3657979282009307, 0.63420207179907, 0, 0, 32, 0, 1, 0, 1, 1, 128, 4.510281053460344e-18, 1.4800000190734872, 1], [0, 0, 0, 0, 0, 0.39259773250316776, 0.6074022674968319, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1], [0.05813542788953156, 0, 0, 0, 0, 0.759491744467943, 0.17438081559139854, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.021277591853632226, 1.480000019073489, 1], [0.03771479200896724, 0, 0, 0, 0, 0.5719183802520509, 0.39036682773898534, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.010995560471403178, 1.4800000190734863, 0], [0.025070241595817624, 0, 0, 0.9749297584041823, 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.04385404076228295, 1.4800000190734872, 1], [0.02035898382547232, 0, 0, 0, 0.05542625837807313, 0.9242147577964546, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.07395916901245965, 1.4800000190734863, 0], [0.025982777772300458, 0.382060274159867, 0, 0, 0.03230723809989182, 0.5596203598251315, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.05606954315896882, 1.4800000190734863, 1], [0.021925444481265575, 0, 0, 0, 0, 0.9780745555187341, 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.05091150911852535, 1.4800000190734863, 0], [0.02004914230447171, 0, 0, 0, 0, 0.8282759138183952, 0, 0, 0.15167494387713323, 32, 0, 1, 0, 0, 0, 128, 0.04400387535664682, 1.4800000190734863, 1], [0.03260213102001828, 0, 0, 0, 0.03825215782610881, 0.19397499069001806, 0.5309467929509468, 0.19870134702935396, 0, 32, 0, 1, 0, 1, 1, 128, 0.051684007394717194, 1.4800000190734863, 1], [0.01093913561003974, 0, 0, 0.31277824941016974, 0.02601109008785059, 0.65027152489194, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.04150508454220725, 1.4800000190734868, 1], [0.02579027075983451, 0, 0, 0, 0, 0.9742097292401654, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.032591677257890556, 1.48000001907349, 1], [0.014716036734253996, 0, 0, 0, 0.01888342451346265, 0.9664005387522833, 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.02958109019200819, 1.4800000190734863, 0], [0.7344756693732347, 0, 0, 0, 0, 0.26552433062676495, 0, 0, 0, 32, 0, 1, 0, 1, 0, 2, 3.469446951953612e-19, 30.628957133504635, 1]], [[0, 0.1052264391666097, 0.0539864321765052, 0.08020647470449777, 0, 0.43505877967846773, 0.09045520998604689, 0.026743155262056423, 0.20832350902581642, 32, 0, 0, 0, 1, 0, 114, 8.868754143115545e-20, 44.250211883418324, 0], [0, 0, 0, 0, 0, 1.0, 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0], [0, 0, 0.44919861192017224, 0, 0, 0.5508013880798274, 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 1.5959455978986636e-17, 47.99999999999952, 0], [0, 0, 0, 0, 0, 0.4490124658036694, 0, 0.5509875341963306, 0, 32, 0, 1, 0, 1, 0, 128, 3.1225022567582517e-18, 47.99999999999997, 0], [0, 0, 0, 0, 0, 0.11635259076535183, 0.8836474092346482, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734919, 0], [0, 0, 0, 0.9999999999999999, 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0], [0, 0.8654467612781556, 0, 0, 0, 0.1345532387218449, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734912, 0], [0, 0, 0, 0, 0, 0.7080668201648898, 0, 0.2919331798351101, 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0, 0, 1.0, 0, 32, 0, 0, 0, 0, 1, 128, 2.9582283945787947e-32, 1.4800000190734863, 1], [0, 0, 0, 0.5175584869152313, 0, 0, 0.4824415130847685, 0, 0, 32, 0, 0, 0, 0, 1, 128, 0.1, 48.0, 1], [0, 0, 0, 0, 0, 0.5357796526065699, 0, 0, 0.46422034739343054, 32, 0, 1, 0, 0, 1, 128, 3.145712932229657e-17, 1.480000019073495, 1], [0, 0, 0, 0.20939919176814414, 0, 0.790600808231856, 0, 0, 0, 32, 1, 1, 1, 0, 1, 128, 0.0, 1.4800000190734863, 1], [0, 0, 0, 0.2914571936096614, 0.07989007646969101, 0.5856457784204158, 0.04300695150023201, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.480000019073487, 0], [0.05598873074464776, 0, 0, 0.390258551152304, 0, 0.5537527181030482, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 1.5612511283791264e-18, 1.4800000190734877, 0], [0, 0, 0, 0.542027448080645, 0, 0.45797255191935515, 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 1.2490009027032991e-17, 1.480000019073493, 0], [0.0459842358570318, 0, 0, 0, 0, 0.4536692859979932, 0.500346478144975, 0, 0, 32, 0, 1, 0, 1, 1, 128, 1.1796119636642291e-17, 1.480000019073488, 1], [0.03559989808331077, 0.524361589510259, 0, 0, 0, 0.4400385124064306, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 3.903127820947816e-19, 1.4800000190734868, 0], [0.015045234948708195, 0, 0, 0, 0.14887442361134867, 0.48563271489737897, 0.3504476265425637, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.014785424765510613, 1.480000019073487, 0], [0, 0, 0, 0, 0, 0.9952506650117107, 0, 0, 0, 32, 0, 1, 0, 0, 1, 128, 0.011498953512302743, 1.4800000190734863, 0], [0.03477086072063697, 0, 0, 0, 0.09482989427502114, 0.7228847041721465, 0, 0.14751454083219534, 0, 32, 0, 1, 0, 1, 0, 128, 0.0715183488212877, 1.4800000190734863, 0], [0.025402194861655986, 0, 0, 0, 0.0480851842027157, 0.5192497449379777, 0, 0, 0.4072628759976504, 32, 0, 1, 0, 1, 0, 128, 0.045865767956081084, 1.4800000190734863, 0], [0.030600482641916962, 0, 0, 0, 0.05494228865310801, 0.9144572287049749, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.05766034835689902, 1.4800000190734932, 0], [0.02596100037743932, 0, 0.2770530105711168, 0.08444472071354091, 0.05934497784643004, 0.4703342267885659, 0, 0.08286206370290718, 0, 32, 0, 1, 0, 1, 1, 128, 0.04439906689195769, 1.4800000190734888, 1], [0.02946331172755158, 0, 0, 0, 0.07972783424551642, 0.5652881263971495, 0, 0.32552072762978246, 0, 32, 0, 1, 0, 0, 1, 128, 0.0, 1.4800000190734863, 0], [0.015728692925940983, 0, 0, 0, 0, 0.9842713070740582, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.017511757971282706, 1.4800000190734866, 1], [0.013754125247618891, 0, 0, 0, 0, 0.5969483990409614, 0.3892974757114199, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.052621615115908994, 1.4800000190734863, 0], [0.011155976717809137, 0, 0, 0.08747705296323274, 0.04851512417809137, 0.3925853750368913, 0.2975805857672857, 0, 0.16268588533668973, 32, 0, 1, 0, 1, 0, 128, 0.02484031902837845, 1.4800000190734866, 1], [0.014679413183553787, 0, 0, 0.27903700178339264, 0.01501327482000023, 0.6912703102130533, 0, 0, 0, 32, 1, 1, 0, 1, 0, 128, 0.03919765927266111, 1.4800000190734863, 0], [0.023309949159448087, 0, 0.13804392225814682, 0, 0, 0.5923139359435179, 0.139562689497524, 0, 0.10676950314136326, 32, 0, 1, 0, 1, 0, 128, 0.035758179398772835, 1.4800000190734899, 0], [0.7941443241595957, 0, 0, 0, 0, 0.20585567584040437, 0, 0, 0, 32, 0, 1, 0, 1, 0, 2, 5.344144507667857e-19, 32.6885954376126, 0]], [[0, 0.1052264391666097, 0.0539864321765052, 0.08020647470449777, 0, 0.43505877967846773, 0.09045520998604689, 0.026743155262056423, 0.20832350902581642, 32, 0, 0, 0, 1, 0, 114, 8.868754143115545e-20, 44.250211883418324, 0], [0, 0, 0, 0, 0, 1.0, 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0], [0, 0, 0.45398230532819295, 0, 0, 0.5460176946718072, 0, 0, 0, 32, 0, 0, 0, 1, 0, 128, 3.469446951953639e-19, 47.99999999999996, 0], [0, 0, 0, 0, 0, 0.44260099470330794, 0, 0.5573990052966917, 0, 32, 0, 1, 0, 1, 0, 128, 5.898059818321145e-18, 48.0, 0], [0, 0, 0, 0, 0, 0.11471254676781793, 0.8852874532321824, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0, 1.0, 0, 0, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 5.551115123125783e-17, 1.4800000190734908, 0], [0, 0.8788003488384917, 0, 0, 0, 0.1211996511615079, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 2.7402852908514988e-17, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0.802791094300615, 0, 0.19720890569938476, 0, 32, 0, 1, 0, 1, 1, 128, 0.1, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0, 0, 1.0, 0, 32, 0, 0, 0, 0, 1, 128, 3.6107514858430517e-17, 1.4800000190734863, 1], [0, 0, 0, 0.6029314802620127, 0, 0, 0.3970685197379874, 0, 0, 32, 0, 0, 0, 0, 1, 128, 0.1, 48.0, 1], [0, 0, 0, 0, 0, 0.5431465880656284, 0, 0, 0.4568534119343716, 32, 0, 1, 0, 1, 0, 128, 5.689893001203928e-17, 1.4800000190734863, 1], [0, 0, 0, 0.17976727102109658, 0, 0.8202327289789032, 0, 0, 0, 32, 1, 1, 1, 0, 1, 128, 0.0, 1.4800000190734863, 1], [0, 0, 0, 0, 0, 0.3298146273816388, 0, 0, 0.6699405527326052, 32, 0, 1, 0, 1, 1, 128, 4.938540895671435e-18, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0.48714159464316475, 0.5128584053567219, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.0, 1.4800000190750122, 0], [0.06449752799812253, 0, 0, 0.3645870373405164, 0, 0.5709154346613609, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 1.0408340855860846e-18, 1.4800000190734863, 0], [0.05448511205664372, 0, 0, 0, 0, 0.6074957759623312, 0, 0, 0.3380191119810251, 32, 0, 1, 0, 1, 0, 128, 0.07831912402982244, 1.4800000190734863, 0], [0.04123717179837406, 0, 0.4428052654189962, 0.03998275059174201, 0, 0.4759748121908869, 0, 0, 0, 32, 0, 0, 0, 0, 1, 128, 0.012362123173718615, 1.4800000190734897, 0], [0.031099706606369196, 0, 0, 0.6094429823161613, 0, 0.35945731107747114, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 4.302551936337919e-17, 1.4800000190734908, 1], [0.04212223152501646, 0, 0, 0, 0, 0.4134409049206684, 0.5444368635543149, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.0, 1.4800000190734912, 1], [0.014936489322970443, 0, 0, 0.3989551015382102, 0, 0.5861084091388198, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.06739935957276309, 1.4800000190734874, 0], [0, 0, 0, 0, 0.07120826476731655, 0.9189801620259404, 0, 0, 0, 32, 0, 1, 0, 1, 1, 128, 0.0, 1.4800000190734885, 0], [0.013338526813569755, 0.04237381086725226, 0, 0.27989873860854003, 0.12959252367946286, 0.3101135036171406, 0, 0, 0.22468289641403438, 32, 0, 1, 0, 1, 0, 128, 0.0381270858119486, 1.4800000190734872, 0], [0.03241610008841527, 0, 0, 0.021929416323925872, 0.11212004981820264, 0.8335344337694564, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.07126544464741473, 1.480000019073487, 0], [0.023021974406170875, 0, 0, 0.2820382139620585, 0.05012066803682939, 0.6448191435949412, 0, 0, 0, 32, 1, 1, 0, 1, 0, 128, 0.030569215246383275, 1.4800000190734866, 0], [0.03034474204322835, 0, 0, 0, 0, 0.9696552579567714, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.03645911465255537, 1.4800000190734863, 0], [0.026836019244218096, 0.3356986165108174, 0, 0, 0.02425936945551849, 0.613205994789446, 0, 0, 0, 32, 0, 1, 0, 1, 0, 128, 0.03707336801597287, 1.4800000190734885, 0], [0.027171940267040125, 0, 0, 0, 0.0385198135496373, 0.5604254930617933, 0, 0.20514149390148517, 0.16874125922004407, 32, 0, 1, 0, 1, 0, 128, 3.903127820947817e-19, 1.4800000190734863, 0], [0, 0, 0, 0.1879618993642727, 0.023751989530971122, 0.7788297570344446, 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.029238948702691166, 1.4800000190734894, 0], [0, 0, 0, 0.14890008765894805, 0, 0.7636966575289958, 0.0841685187760118, 0, 0, 32, 0, 1, 0, 0, 0, 128, 0.016139350256655246, 1.4800000190734863, 0], [0.8131281713371714, 0, 0, 0, 0, 0.18687182866282853, 0, 0, 0, 32, 0, 1, 0, 1, 0, 2, 0.0, 28.63700252055991, 0]]], 'random_full_train_performance': [0.6930000000000001, 0.8925, 0.6405, 0.7875000000000001, 0.6615000000000001, 0.462, 0.5145, 0.672, 0.5984999999999999, 0.441, 0.6194999999999999, 0.7140000000000001, 0.6405, 0.6825000000000001, 0.7035000000000001, 0.651, 0.5565000000000001, 0.6405, 0.6825000000000001, 0.651, 0.672, 0.63, 0.7035000000000001, 0.6930000000000001, 0.7035000000000001, 0.6194999999999999, 0.651, 0.6615000000000001, 0.672, 0.7140000000000001]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: | 0.055 MB of 0.055 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: - 0.055 MB of 0.055 MB uploadedwandb: \ 0.055 MB of 0.081 MB uploadedwandb: | 0.072 MB of 2.461 MB uploadedwandb: / 2.253 MB of 2.461 MB uploadedwandb: - 2.461 MB of 2.461 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb:            eval/runtime ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÇ
wandb: eval/samples_per_second ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñá‚ñÜ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÜ
wandb:   eval/steps_per_second ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñá‚ñÜ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÜ
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÖ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ
wandb:         train/grad_norm ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÇ
wandb:              train/loss ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.70671
wandb:             eval/runtime 4.8016
wandb:  eval/samples_per_second 208.057
wandb:    eval/steps_per_second 13.121
wandb:               total_flos 5.368960475136e+16
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 1.33636
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.7305
wandb:               train_loss 0.90882
wandb:            train_runtime 323.7805
wandb: train_samples_per_second 30.882
wandb:   train_steps_per_second 1.93
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/s084mrnh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_224259-s084mrnh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

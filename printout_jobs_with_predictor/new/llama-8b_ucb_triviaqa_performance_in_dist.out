2026-01-01 07:29:15.494790: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-01 07:29:15.523406: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-01 07:29:15.523449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-01 07:29:15.524380: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-01 07:29:15.528881: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-01 07:29:16.429627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'triviaqa', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_triviaqa_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['triviaqa']
evaluation tasks and weights:  {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/triviaqa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 2, 0, 0, 0, 1, 0, 29, 0.05396168787624587, 37, 1]
Checking history sample input_X_between_0_1:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2265625, 0.5396168787624587, 0.7708333333333334, 1.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 7, 0, 0, 1, 0, 0, 101, 0.003930648435578799, 29, 1]
Checking history sample input_X_between_0_1:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7890625, 0.039306484355787985, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 22, 1, 0, 0, 1, 1, 3, 0.04742737743265794, 11, 1]
Checking history sample input_X_between_0_1:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 0.6875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0234375, 0.47427377432657936, 0.22916666666666666, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 13, 1, 0, 1, 1, 1, 123, 0.09371289547650785, 48, 0]
Checking history sample input_X_between_0_1:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 0.40625, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9609375, 0.9371289547650785, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 13, 0, 0, 1, 1, 0, 127, 0.09560839072699708, 9, 1]
Checking history sample input_X_between_0_1:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 0.40625, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9921875, 0.9560839072699707, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 9, 0, 0, 1, 1, 1, 16, 0.08052765798611784, 41, 1]
Checking history sample input_X_between_0_1:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 0.28125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.8052765798611784, 0.8541666666666666, 1.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 5, 0, 1, 0, 1, 0, 59, 0.05350476033435025, 24, 1]
Checking history sample input_X_between_0_1:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 0.15625, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4609375, 0.5350476033435024, 0.5, 1.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 2, 1, 0, 0, 1, 0, 52, 0.0964290393747826, 9, 0]
Checking history sample input_X_between_0_1:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 0.0625, 1.0, 0.0, 0.0, 1.0, 0.0, 0.40625, 0.964290393747826, 0.1875, 0.0]
Checking history sample performance at 625 steps:  0.44
Checking history sample input_X:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 7, 1, 1, 0, 0, 1, 121, 0.029877603091235272, 27, 0]
Checking history sample input_X_between_0_1:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 0.21875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9453125, 0.2987760309123527, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 5, 1, 1, 0, 0, 0, 99, 0.012004498902397865, 15, 0]
Checking history sample input_X_between_0_1:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 0.15625, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7734375, 0.12004498902397864, 0.3125, 0.0]
Checking history sample performance at 625 steps:  0.57
Checking history sample input_X:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 24, 1, 0, 1, 1, 1, 39, 0.002627655727844236, 41, 0]
Checking history sample input_X_between_0_1:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3046875, 0.02627655727844236, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 24, 0, 0, 0, 0, 1, 93, 0.03163409179138251, 48, 0]
Checking history sample input_X_between_0_1:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 0.75, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7265625, 0.31634091791382507, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 28, 1, 0, 0, 0, 0, 89, 0.0740409362882843, 48, 0]
Checking history sample input_X_between_0_1:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 0.875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.740409362882843, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.63
Checking history sample input_X:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 28, 1, 0, 1, 1, 1, 2, 0.052382422456560135, 28, 1]
Checking history sample input_X_between_0_1:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 0.875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.015625, 0.5238242245656013, 0.5833333333333334, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 11, 0, 0, 0, 1, 1, 54, 0.06588137610749685, 20, 1]
Checking history sample input_X_between_0_1:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 0.34375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.421875, 0.6588137610749685, 0.4166666666666667, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 27, 0, 0, 1, 1, 1, 76, 0.04383658958637696, 17, 0]
Checking history sample input_X_between_0_1:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 0.84375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.59375, 0.4383658958637696, 0.3541666666666667, 0.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 29, 0, 1, 0, 0, 1, 82, 0.07326489367366754, 40, 1]
Checking history sample input_X_between_0_1:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 0.90625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.640625, 0.7326489367366753, 0.8333333333333334, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 1, 0, 0, 0, 1, 0, 44, 0.09959850306478138, 19, 0]
Checking history sample input_X_between_0_1:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.34375, 0.9959850306478137, 0.3958333333333333, 0.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 26, 1, 1, 1, 0, 1, 67, 0.08108306564701856, 9, 1]
Checking history sample input_X_between_0_1:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 0.8125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5234375, 0.8108306564701856, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 29, 0, 0, 0, 1, 1, 29, 0.08410329802499458, 20, 0]
Checking history sample input_X_between_0_1:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 0.90625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2265625, 0.8410329802499458, 0.4166666666666667, 0.0]
Checking history sample performance at 625 steps:  0.61
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7838 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8663053512573242, 0.6104556322097778, 0.1576862335205078, 0.6849040985107422, 0.2709343433380127, 0.20644766092300415, 0.8059491515159607, 0.3803449869155884, 0.1260056495666504, 0.2942119836807251, 0.45175331830978394, 0.020893871784210205, 0.5323565006256104, 0.0868343710899353, 0.47639185190200806, 0.23811274766921997, 0.059806764125823975, 0.29222404956817627, 0.9543546438217163]  ‚Üí  acq = 0.7000464480432271
X = [0.19962340593338013, 0.10408633947372437, 0.3163992166519165, 0.8352195620536804, 0.22287851572036743, 0.08342081308364868, 0.21177387237548828, 0.801550567150116, 0.15315043926239014, 0.8429861664772034, 0.16091710329055786, 0.35346490144729614, 0.15701919794082642, 0.30561745166778564, 0.4626479744911194, 0.3895573318004608, 0.22439324855804443, 0.04646556079387665, 0.5180809497833252]  ‚Üí  acq = 0.7000464480432271
X = [0.5013364553451538, 0.993431031703949, 0.027590811252593994, 0.643821120262146, 0.8155908584594727, 0.567894697189331, 0.5996852517127991, 0.9961401224136353, 0.9868699312210083, 0.33322975039482117, 0.8962882161140442, 0.5438151359558105, 0.013676285743713379, 0.2425292730331421, 0.4254099726676941, 0.8090561032295227, 0.2127872109413147, 0.9148797988891602, 0.8995180726051331]  ‚Üí  acq = 0.7000464480432271
X = [0.06021469831466675, 0.14234894514083862, 0.016043901443481445, 0.5767596364021301, 0.43451398611068726, 0.40810757875442505, 0.8462990522384644, 0.3286171555519104, 0.1588396430015564, 0.26326224207878113, 0.7472301721572876, 0.2237873673439026, 0.8571920990943909, 0.2763305902481079, 0.8875132203102112, 0.6516942977905273, 0.5127571821212769, 0.142607182264328, 0.26917004585266113]  ‚Üí  acq = 0.7000464480432271
X = [0.40717577934265137, 0.8198034763336182, 0.8351718187332153, 0.45121294260025024, 0.5452027916908264, 0.14588141441345215, 0.10879987478256226, 0.6115124821662903, 0.5307215452194214, 0.20146062970161438, 0.3066774606704712, 0.8121109008789062, 0.9566571712493896, 0.550686776638031, 0.6057085990905762, 0.6023871302604675, 0.5312032103538513, 0.47986504435539246, 0.20117956399917603]  ‚Üí  acq = 0.7000464480432271
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.7158, dtype=torch.float64), 0, tensor(0.0463, dtype=torch.float64), 0, tensor(0.1622, dtype=torch.float64), 0, 0, 0, tensor(0.0756, dtype=torch.float64), 4, 1, 1, 0, 0, 1, 110, 0.0, 16.38004380516881, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.7158, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0463, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1622, dtype=torch.float64), tensor(9.5992e-19, dtype=torch.float64), tensor(1.7040e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0756, dtype=torch.float64), tensor(0.1221, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8618, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3413, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.716
  gsm8k: 0
  rowan_hellaswag: 0.046
  sciq: 0
  triviaqa: 0.162
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.076

LoRA Parameters:
  lora_r: (110,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (4,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (16.38004380516881,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  4
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  110
lora dropout:  0.0
lora alpha:  16.38004380516881
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 13,967,360 || all params: 8,044,228,608 || trainable%: 0.1736
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260101_073129-2pqalfpx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/2pqalfpx
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.69it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.62it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.74it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 52.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 58.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 67.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 71.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 76.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 76.62it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 16.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 31.47it/s]
Evaluation performance at step 25: 0.6
{'loss': 4.5458, 'grad_norm': 0.5061988830566406, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.6}
{'eval_loss': 3.544365167617798, 'eval_runtime': 6.7614, 'eval_samples_per_second': 147.751, 'eval_steps_per_second': 9.318, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.48it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 40.65it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 52.79it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 61.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:00, 67.12it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:06,  7.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 12.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 19.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 26.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.83it/s]
Evaluation performance at step 50: 0.61
{'loss': 2.6505, 'grad_norm': 0.311587393283844, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.86643385887146, 'eval_runtime': 6.7524, 'eval_samples_per_second': 147.948, 'eval_steps_per_second': 9.33, 'epoch': 0.08}
{'loss': 1.5655, 'grad_norm': 0.13763515651226044, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.4978998899459839, 'eval_runtime': 6.7816, 'eval_samples_per_second': 147.311, 'eval_steps_per_second': 9.29, 'epoch': 0.12}
{'loss': 1.4188, 'grad_norm': 0.13296225666999817, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3784253597259521, 'eval_runtime': 6.8138, 'eval_samples_per_second': 146.615, 'eval_steps_per_second': 9.246, 'epoch': 0.16}
{'loss': 1.3224, 'grad_norm': 0.13206103444099426, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.304240107536316, 'eval_runtime': 6.805, 'eval_samples_per_second': 146.804, 'eval_steps_per_second': 9.258, 'epoch': 0.2}
{'loss': 1.2656, 'grad_norm': 0.13743750751018524, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.2115260362625122, 'eval_runtime': 6.8175, 'eval_samples_per_second': 146.534, 'eval_steps_per_second': 9.241, 'epoch': 0.24}
{'loss': 1.1758, 'grad_norm': 0.14152216911315918, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.148335576057434, 'eval_runtime': 6.832, 'eval_samples_per_second': 146.224, 'eval_steps_per_second': 9.221, 'epoch': 0.28}
{'loss': 1.1247, 'grad_norm': 0.12162179499864578, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1257888078689575, 'eval_runtime': 6.8379, 'eval_samples_per_second': 146.097, 'eval_steps_per_second': 9.213, 'epoch': 0.32}
{'loss': 1.126, 'grad_norm': 0.12122239172458649, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1173113584518433, 'eval_runtime': 6.8475, 'eval_samples_per_second': 145.892, 'eval_steps_per_second': 9.2, 'epoch': 0.36}
{'loss': 1.135, 'grad_norm': 0.10675973445177078, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1103203296661377, 'eval_runtime': 6.8462, 'eval_samples_per_second': 145.921, 'eval_steps_per_second': 9.202, 'epoch': 0.4}
{'loss': 1.1239, 'grad_norm': 0.12921975553035736, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.106430172920227, 'eval_runtime': 6.8523, 'eval_samples_per_second': 145.79, 'eval_steps_per_second': 9.194, 'epoch': 0.44}
{'loss': 1.086, 'grad_norm': 0.13149096071720123, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0992759466171265, 'eval_runtime': 6.8656, 'eval_samples_per_second': 145.509, 'eval_steps_per_second': 9.176, 'epoch': 0.48}
{'loss': 1.1194, 'grad_norm': 0.14258722960948944, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0936837196350098, 'eval_runtime': 6.8538, 'eval_samples_per_second': 145.757, 'eval_steps_per_second': 9.192, 'epoch': 0.52}
{'loss': 1.1075, 'grad_norm': 0.12364210188388824, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0905303955078125, 'eval_runtime': 6.8619, 'eval_samples_per_second': 145.586, 'eval_steps_per_second': 9.181, 'epoch': 0.56}
{'loss': 1.0701, 'grad_norm': 0.1384643018245697, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.087075114250183, 'eval_runtime': 6.8496, 'eval_samples_per_second': 145.848, 'eval_steps_per_second': 9.198, 'epoch': 0.6}
{'loss': 1.0834, 'grad_norm': 0.11772365868091583, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0833382606506348, 'eval_runtime': 6.8573, 'eval_samples_per_second': 145.684, 'eval_steps_per_second': 9.187, 'epoch': 0.64}
{'loss': 1.1016, 'grad_norm': 0.12930741906166077, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0798996686935425, 'eval_runtime': 6.8563, 'eval_samples_per_second': 145.706, 'eval_steps_per_second': 9.189, 'epoch': 0.68}
{'loss': 1.081, 'grad_norm': 0.12598709762096405, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0773059129714966, 'eval_runtime': 6.8533, 'eval_samples_per_second': 145.77, 'eval_steps_per_second': 9.193, 'epoch': 0.72}
{'loss': 1.0965, 'grad_norm': 0.14767231047153473, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0750669240951538, 'eval_runtime': 6.8527, 'eval_samples_per_second': 145.781, 'eval_steps_per_second': 9.193, 'epoch': 0.76}
{'loss': 1.0944, 'grad_norm': 0.13910336792469025, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0731937885284424, 'eval_runtime': 6.8625, 'eval_samples_per_second': 145.575, 'eval_steps_per_second': 9.18, 'epoch': 0.8}
{'loss': 1.0659, 'grad_norm': 0.12085980176925659, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0714821815490723, 'eval_runtime': 6.8617, 'eval_samples_per_second': 145.59, 'eval_steps_per_second': 9.181, 'epoch': 0.84}
{'loss': 1.0902, 'grad_norm': 0.13775886595249176, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0695490837097168, 'eval_runtime': 6.8499, 'eval_samples_per_second': 145.841, 'eval_steps_per_second': 9.197, 'epoch': 0.88}
{'loss': 1.0751, 'grad_norm': 0.13101695477962494, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0687354803085327, 'eval_runtime': 6.8807, 'eval_samples_per_second': 145.188, 'eval_steps_per_second': 9.156, 'epoch': 0.92}
{'loss': 1.0985, 'grad_norm': 0.12963038682937622, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0679525136947632, 'eval_runtime': 6.8877, 'eval_samples_per_second': 145.04, 'eval_steps_per_second': 9.147, 'epoch': 0.96}
{'loss': 1.0342, 'grad_norm': 0.1393144428730011, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.067762851715088, 'eval_runtime': 6.8577, 'eval_samples_per_second': 145.675, 'eval_steps_per_second': 9.187, 'epoch': 1.0}
{'train_runtime': 338.8686, 'train_samples_per_second': 29.504, 'train_steps_per_second': 1.844, 'train_loss': 1.346309310913086, 'epoch': 1.0}
train_results:  {'eval_loss': [3.544365167617798, 1.86643385887146, 1.4978998899459839, 1.3784253597259521, 1.304240107536316, 1.2115260362625122, 1.148335576057434, 1.1257888078689575, 1.1173113584518433, 1.1103203296661377, 1.106430172920227, 1.0992759466171265, 1.0936837196350098, 1.0905303955078125, 1.087075114250183, 1.0833382606506348, 1.0798996686935425, 1.0773059129714966, 1.0750669240951538, 1.0731937885284424, 1.0714821815490723, 1.0695490837097168, 1.0687354803085327, 1.0679525136947632, 1.067762851715088], 'performance': [0.6, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:25,  3.90it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 21.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.63it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 32.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 43.61it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 53.94it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 45.67it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.6, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  0.9851824641227722
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.6405
BO observations:  [0.9851824641227722]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2325 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.0142814894867334
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 1.0235651428310195
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.0397747518344813
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.0204991220709725
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.024688924321104
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.8403, dtype=torch.float64), 0, 0, 0, tensor(0.1593, dtype=torch.float64), 0, 0, 0, 0, 2, 1, 0, 1, 1, 1, 128, 0.06318159091447405, 36.63087834233318, 0]
normalized proposed parameters for next round by BO: [tensor(0.8403, dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1593, dtype=torch.float64), tensor(2.3984e-18, dtype=torch.float64), tensor(6.1163e-06, dtype=torch.float64), tensor(1.7145e-18, dtype=torch.float64), tensor(7.1080e-18, dtype=torch.float64), tensor(0.0587, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6318, dtype=torch.float64), tensor(0.7631, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.84
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.159
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06318159091447405,)
  num_layers_to_apply: (2,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (36.63087834233318,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  2
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.06318159091447405
lora alpha:  36.63087834233318
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,252,928 || all params: 8,046,514,176 || trainable%: 0.2020
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9994
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:13,  7.36it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 50.73it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 58.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 64.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 69.06it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.23it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 61.00it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.82it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 62.52it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 35.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.51it/s]
Evaluation performance at step 25: 0.59
{'loss': 4.1884, 'grad_norm': 1.7117260694503784, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.59}
{'eval_loss': 2.4882352352142334, 'eval_runtime': 4.7902, 'eval_samples_per_second': 208.552, 'eval_steps_per_second': 13.152, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.00it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 28.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.21it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 52.74it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:12,  5.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:04, 10.61it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:04<00:03, 13.66it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:01, 17.68it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:04<00:00, 27.71it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 32.61it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.22it/s]
Evaluation performance at step 50: 0.56
{'loss': 1.6285, 'grad_norm': 0.4320082366466522, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.1871806383132935, 'eval_runtime': 4.4244, 'eval_samples_per_second': 225.792, 'eval_steps_per_second': 14.239, 'epoch': 0.08}
{'loss': 1.0828, 'grad_norm': 0.24413122236728668, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.061057686805725, 'eval_runtime': 4.4314, 'eval_samples_per_second': 225.437, 'eval_steps_per_second': 14.217, 'epoch': 0.12}
{'loss': 1.0339, 'grad_norm': 0.20722386240959167, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0313968658447266, 'eval_runtime': 4.4371, 'eval_samples_per_second': 225.147, 'eval_steps_per_second': 14.198, 'epoch': 0.16}
{'loss': 1.0298, 'grad_norm': 0.2192305028438568, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0166079998016357, 'eval_runtime': 4.4487, 'eval_samples_per_second': 224.558, 'eval_steps_per_second': 14.161, 'epoch': 0.2}
{'loss': 1.0052, 'grad_norm': 0.20442691445350647, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0029159784317017, 'eval_runtime': 4.4633, 'eval_samples_per_second': 223.825, 'eval_steps_per_second': 14.115, 'epoch': 0.24}
{'loss': 0.9782, 'grad_norm': 0.22205793857574463, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9941621422767639, 'eval_runtime': 4.4334, 'eval_samples_per_second': 225.336, 'eval_steps_per_second': 14.21, 'epoch': 0.28}
{'loss': 1.0154, 'grad_norm': 0.27883148193359375, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9871448278427124, 'eval_runtime': 4.4373, 'eval_samples_per_second': 225.137, 'eval_steps_per_second': 14.198, 'epoch': 0.32}
{'loss': 0.9876, 'grad_norm': 0.22629110515117645, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9791455268859863, 'eval_runtime': 4.4407, 'eval_samples_per_second': 224.964, 'eval_steps_per_second': 14.187, 'epoch': 0.36}
{'loss': 0.9738, 'grad_norm': 0.19839321076869965, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9753336310386658, 'eval_runtime': 4.4476, 'eval_samples_per_second': 224.614, 'eval_steps_per_second': 14.165, 'epoch': 0.4}
{'loss': 0.983, 'grad_norm': 0.2320307195186615, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9673869013786316, 'eval_runtime': 4.4509, 'eval_samples_per_second': 224.45, 'eval_steps_per_second': 14.154, 'epoch': 0.44}
{'loss': 0.9698, 'grad_norm': 0.2331816703081131, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9630094766616821, 'eval_runtime': 4.4537, 'eval_samples_per_second': 224.307, 'eval_steps_per_second': 14.145, 'epoch': 0.48}
{'loss': 0.9606, 'grad_norm': 0.216647207736969, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9579010605812073, 'eval_runtime': 4.4537, 'eval_samples_per_second': 224.309, 'eval_steps_per_second': 14.146, 'epoch': 0.52}
{'loss': 0.9393, 'grad_norm': 0.20350052416324615, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9522895812988281, 'eval_runtime': 4.4514, 'eval_samples_per_second': 224.423, 'eval_steps_per_second': 14.153, 'epoch': 0.56}
{'loss': 0.9605, 'grad_norm': 0.2198939174413681, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9490904211997986, 'eval_runtime': 4.4578, 'eval_samples_per_second': 224.104, 'eval_steps_per_second': 14.133, 'epoch': 0.6}
{'loss': 0.952, 'grad_norm': 0.22262369096279144, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.944718599319458, 'eval_runtime': 4.4616, 'eval_samples_per_second': 223.913, 'eval_steps_per_second': 14.121, 'epoch': 0.64}
{'loss': 0.9599, 'grad_norm': 0.2228679656982422, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9417651295661926, 'eval_runtime': 4.4814, 'eval_samples_per_second': 222.922, 'eval_steps_per_second': 14.058, 'epoch': 0.68}
{'loss': 0.9549, 'grad_norm': 0.21617715060710907, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9389680027961731, 'eval_runtime': 4.4733, 'eval_samples_per_second': 223.327, 'eval_steps_per_second': 14.084, 'epoch': 0.72}
{'loss': 0.9567, 'grad_norm': 0.22881054878234863, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9367220997810364, 'eval_runtime': 4.4893, 'eval_samples_per_second': 222.528, 'eval_steps_per_second': 14.033, 'epoch': 0.76}
{'loss': 0.9417, 'grad_norm': 0.2347431629896164, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9334025382995605, 'eval_runtime': 4.491, 'eval_samples_per_second': 222.446, 'eval_steps_per_second': 14.028, 'epoch': 0.8}
{'loss': 0.9485, 'grad_norm': 0.2575102746486664, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.931390106678009, 'eval_runtime': 4.4886, 'eval_samples_per_second': 222.566, 'eval_steps_per_second': 14.036, 'epoch': 0.84}
{'loss': 0.9282, 'grad_norm': 0.21763600409030914, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.929795503616333, 'eval_runtime': 4.4697, 'eval_samples_per_second': 223.504, 'eval_steps_per_second': 14.095, 'epoch': 0.88}
{'loss': 0.9143, 'grad_norm': 0.23862922191619873, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.927841305732727, 'eval_runtime': 4.462, 'eval_samples_per_second': 223.889, 'eval_steps_per_second': 14.119, 'epoch': 0.92}
{'loss': 0.9328, 'grad_norm': 0.22257114946842194, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.926578938961029, 'eval_runtime': 4.4673, 'eval_samples_per_second': 223.624, 'eval_steps_per_second': 14.102, 'epoch': 0.96}
{'loss': 0.9531, 'grad_norm': 0.2795736789703369, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9263092279434204, 'eval_runtime': 4.4673, 'eval_samples_per_second': 223.626, 'eval_steps_per_second': 14.103, 'epoch': 1.0}
{'train_runtime': 222.8208, 'train_samples_per_second': 44.852, 'train_steps_per_second': 2.805, 'train_loss': 1.1271590972900392, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4882352352142334, 1.1871806383132935, 1.061057686805725, 1.0313968658447266, 1.0166079998016357, 1.0029159784317017, 0.9941621422767639, 0.9871448278427124, 0.9791455268859863, 0.9753336310386658, 0.9673869013786316, 0.9630094766616821, 0.9579010605812073, 0.9522895812988281, 0.9490904211997986, 0.944718599319458, 0.9417651295661926, 0.9389680027961731, 0.9367220997810364, 0.9334025382995605, 0.931390106678009, 0.929795503616333, 0.927841305732727, 0.926578938961029, 0.9263092279434204], 'performance': [0.59, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<08:00,  4.85s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:09<00:39,  2.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:24,  2.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:14<00:10,  4.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:14<00:04,  7.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:14<00:01, 11.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.80it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.59, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.1012190580368042
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.6405
BO observations:  [0.9851824641227722, 1.1012190580368042]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0174 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.1196149696278697
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 1.099389656331835
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.1580532116255247
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 1.1887416296505173
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 0.9706109093618192
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.9553, dtype=torch.float64), 0, 0, 0, tensor(0.0446, dtype=torch.float64), 0, 0, 0, 0, 26, 1, 0, 1, 1, 1, 128, 5.4210108624275225e-21, 13.494683229879406, 0]
normalized proposed parameters for next round by BO: [tensor(0.9553, dtype=torch.float64), tensor(4.0695e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.7297e-19, dtype=torch.float64), tensor(0.0446, dtype=torch.float64), tensor(4.0658e-19, dtype=torch.float64), tensor(5.4248e-19, dtype=torch.float64), tensor(8.4063e-19, dtype=torch.float64), tensor(0.0002, dtype=torch.float64), tensor(0.8223, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(5.4210e-20, dtype=torch.float64), tensor(0.2811, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.955
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.045
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.4210108624275225e-21,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (13.494683229879406,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  5.4210108624275225e-21
lora alpha:  13.494683229879406
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 211,288,064 || all params: 8,241,549,312 || trainable%: 2.5637
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9997
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.07it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.21it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.22it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.98it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.48it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.62it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.63it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.80it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 56.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 56.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.39it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.5501, 'grad_norm': 0.5508278608322144, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.5734269618988037, 'eval_runtime': 5.3079, 'eval_samples_per_second': 188.21, 'eval_steps_per_second': 11.869, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.72it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.65it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.02it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.12it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.10it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 47.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.12it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.17it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 49.87it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 49.14it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 50.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.63it/s]
Evaluation performance at step 50: 0.66
{'loss': 1.1698, 'grad_norm': 0.1338701993227005, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.66}
{'eval_loss': 0.9592413306236267, 'eval_runtime': 5.3225, 'eval_samples_per_second': 187.695, 'eval_steps_per_second': 11.837, 'epoch': 0.08}
{'loss': 0.92, 'grad_norm': 0.10860731452703476, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9011234045028687, 'eval_runtime': 5.325, 'eval_samples_per_second': 187.606, 'eval_steps_per_second': 11.831, 'epoch': 0.12}
{'loss': 0.8885, 'grad_norm': 0.0972815528512001, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8802544474601746, 'eval_runtime': 5.3477, 'eval_samples_per_second': 186.811, 'eval_steps_per_second': 11.781, 'epoch': 0.16}
{'loss': 0.8875, 'grad_norm': 0.0989716425538063, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8641600608825684, 'eval_runtime': 5.3407, 'eval_samples_per_second': 187.052, 'eval_steps_per_second': 11.796, 'epoch': 0.2}
{'loss': 0.8512, 'grad_norm': 0.11276285350322723, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8530135154724121, 'eval_runtime': 5.3394, 'eval_samples_per_second': 187.099, 'eval_steps_per_second': 11.799, 'epoch': 0.24}
{'loss': 0.8511, 'grad_norm': 0.09564706683158875, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8371031880378723, 'eval_runtime': 5.3373, 'eval_samples_per_second': 187.172, 'eval_steps_per_second': 11.804, 'epoch': 0.28}
{'loss': 0.855, 'grad_norm': 0.09641987085342407, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8282909989356995, 'eval_runtime': 5.3712, 'eval_samples_per_second': 185.991, 'eval_steps_per_second': 11.729, 'epoch': 0.32}
{'loss': 0.8368, 'grad_norm': 0.11006104946136475, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8219102621078491, 'eval_runtime': 5.3325, 'eval_samples_per_second': 187.341, 'eval_steps_per_second': 11.814, 'epoch': 0.36}
{'loss': 0.8226, 'grad_norm': 0.1157161071896553, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.809359073638916, 'eval_runtime': 5.2996, 'eval_samples_per_second': 188.504, 'eval_steps_per_second': 11.888, 'epoch': 0.4}
{'loss': 0.8211, 'grad_norm': 0.10814490169286728, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8002035021781921, 'eval_runtime': 5.2949, 'eval_samples_per_second': 188.673, 'eval_steps_per_second': 11.898, 'epoch': 0.44}
{'loss': 0.8191, 'grad_norm': 0.11198901385068893, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7892027497291565, 'eval_runtime': 5.302, 'eval_samples_per_second': 188.42, 'eval_steps_per_second': 11.882, 'epoch': 0.48}
{'loss': 0.802, 'grad_norm': 0.11345680058002472, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7842152714729309, 'eval_runtime': 5.2742, 'eval_samples_per_second': 189.413, 'eval_steps_per_second': 11.945, 'epoch': 0.52}
{'loss': 0.7978, 'grad_norm': 0.12643708288669586, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7731454372406006, 'eval_runtime': 5.2604, 'eval_samples_per_second': 189.909, 'eval_steps_per_second': 11.976, 'epoch': 0.56}
{'loss': 0.7971, 'grad_norm': 0.11618522554636002, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7621189951896667, 'eval_runtime': 5.2743, 'eval_samples_per_second': 189.409, 'eval_steps_per_second': 11.945, 'epoch': 0.6}
{'loss': 0.8129, 'grad_norm': 0.13479800522327423, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7552382946014404, 'eval_runtime': 5.2637, 'eval_samples_per_second': 189.792, 'eval_steps_per_second': 11.969, 'epoch': 0.64}
{'loss': 0.7939, 'grad_norm': 0.12942133843898773, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7464220523834229, 'eval_runtime': 5.2656, 'eval_samples_per_second': 189.721, 'eval_steps_per_second': 11.964, 'epoch': 0.68}
{'loss': 0.7981, 'grad_norm': 0.1291058510541916, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7366947531700134, 'eval_runtime': 5.2708, 'eval_samples_per_second': 189.536, 'eval_steps_per_second': 11.953, 'epoch': 0.72}
{'loss': 0.7732, 'grad_norm': 0.13801993429660797, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7296593189239502, 'eval_runtime': 5.2665, 'eval_samples_per_second': 189.69, 'eval_steps_per_second': 11.962, 'epoch': 0.76}
{'loss': 0.7582, 'grad_norm': 0.1417645514011383, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.725272536277771, 'eval_runtime': 5.2656, 'eval_samples_per_second': 189.722, 'eval_steps_per_second': 11.964, 'epoch': 0.8}
{'loss': 0.7813, 'grad_norm': 0.1449664682149887, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7164695858955383, 'eval_runtime': 5.2674, 'eval_samples_per_second': 189.656, 'eval_steps_per_second': 11.96, 'epoch': 0.84}
{'loss': 0.7726, 'grad_norm': 0.1428661197423935, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7128673195838928, 'eval_runtime': 5.2645, 'eval_samples_per_second': 189.763, 'eval_steps_per_second': 11.967, 'epoch': 0.88}
{'loss': 0.7552, 'grad_norm': 0.1500999927520752, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.708948016166687, 'eval_runtime': 5.2605, 'eval_samples_per_second': 189.907, 'eval_steps_per_second': 11.976, 'epoch': 0.92}
{'loss': 0.7858, 'grad_norm': 0.15818659961223602, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.705898642539978, 'eval_runtime': 5.2618, 'eval_samples_per_second': 189.857, 'eval_steps_per_second': 11.973, 'epoch': 0.96}
{'loss': 0.7754, 'grad_norm': 0.15726910531520844, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.704653263092041, 'eval_runtime': 5.2658, 'eval_samples_per_second': 189.717, 'eval_steps_per_second': 11.964, 'epoch': 1.0}
{'train_runtime': 323.3548, 'train_samples_per_second': 30.917, 'train_steps_per_second': 1.933, 'train_loss': 0.939044302368164, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5734269618988037, 0.9592413306236267, 0.9011234045028687, 0.8802544474601746, 0.8641600608825684, 0.8530135154724121, 0.8371031880378723, 0.8282909989356995, 0.8219102621078491, 0.809359073638916, 0.8002035021781921, 0.7892027497291565, 0.7842152714729309, 0.7731454372406006, 0.7621189951896667, 0.7552382946014404, 0.7464220523834229, 0.7366947531700134, 0.7296593189239502, 0.725272536277771, 0.7164695858955383, 0.7128673195838928, 0.708948016166687, 0.705898642539978, 0.704653263092041], 'performance': [0.61, 0.66]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.74it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.15it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 47.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.61it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.66]
current iteration observed (possibly low-fid or predicted) performance:  1.11042058467865
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9871 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 1.1298370384559546
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.129150423954928
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 1.0948325421372018
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.133443387851275
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.0680976327663954
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, 0, 10, 1, 0, 1, 1, 1, 69, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.1536e-18, dtype=torch.float64), tensor(1.3903e-17, dtype=torch.float64), tensor(2.0535e-17, dtype=torch.float64), tensor(1.2762e-16, dtype=torch.float64), tensor(0.3014, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5364, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 1.0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (69,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  69
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 43,806,720 || all params: 8,074,067,968 || trainable%: 0.5426
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.93it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.17it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 51.30it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 57.68it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:07,  7.23it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:04<00:02, 11.83it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:01, 14.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:01, 17.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 21.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 18.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 16.83it/s]
Evaluation performance at step 25: 0.58
{'loss': 3.4311, 'grad_norm': 1.3516682386398315, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.58}
{'eval_loss': 1.5586196184158325, 'eval_runtime': 5.7002, 'eval_samples_per_second': 175.434, 'eval_steps_per_second': 11.052, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.35it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.58it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.82it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:14,  4.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:05,  9.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:02, 14.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 21.47it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 25.02it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:05<00:00, 28.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.83it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.1365, 'grad_norm': 0.3512623906135559, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 0.9898214340209961, 'eval_runtime': 4.9566, 'eval_samples_per_second': 201.752, 'eval_steps_per_second': 12.71, 'epoch': 0.08}
{'loss': 0.9565, 'grad_norm': 0.2279237061738968, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.944313108921051, 'eval_runtime': 4.9597, 'eval_samples_per_second': 201.627, 'eval_steps_per_second': 12.702, 'epoch': 0.12}
{'loss': 0.9243, 'grad_norm': 0.22172750532627106, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9195212125778198, 'eval_runtime': 4.97, 'eval_samples_per_second': 201.206, 'eval_steps_per_second': 12.676, 'epoch': 0.16}
{'loss': 0.9153, 'grad_norm': 0.25250008702278137, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9018502235412598, 'eval_runtime': 4.9764, 'eval_samples_per_second': 200.949, 'eval_steps_per_second': 12.66, 'epoch': 0.2}
{'loss': 0.9121, 'grad_norm': 0.23246222734451294, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8880867958068848, 'eval_runtime': 4.9888, 'eval_samples_per_second': 200.448, 'eval_steps_per_second': 12.628, 'epoch': 0.24}
{'loss': 0.8924, 'grad_norm': 0.23353436589241028, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8746106624603271, 'eval_runtime': 4.9784, 'eval_samples_per_second': 200.869, 'eval_steps_per_second': 12.655, 'epoch': 0.28}
{'loss': 0.8834, 'grad_norm': 0.22707109153270721, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8646382093429565, 'eval_runtime': 4.9735, 'eval_samples_per_second': 201.066, 'eval_steps_per_second': 12.667, 'epoch': 0.32}
{'loss': 0.8768, 'grad_norm': 0.22832423448562622, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.853053092956543, 'eval_runtime': 4.9805, 'eval_samples_per_second': 200.785, 'eval_steps_per_second': 12.649, 'epoch': 0.36}
{'loss': 0.8703, 'grad_norm': 0.2608618438243866, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8418262600898743, 'eval_runtime': 4.9853, 'eval_samples_per_second': 200.59, 'eval_steps_per_second': 12.637, 'epoch': 0.4}
{'loss': 0.856, 'grad_norm': 0.31803762912750244, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.827357292175293, 'eval_runtime': 5.0039, 'eval_samples_per_second': 199.843, 'eval_steps_per_second': 12.59, 'epoch': 0.44}
{'loss': 0.8502, 'grad_norm': 0.24824649095535278, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8203311562538147, 'eval_runtime': 4.9898, 'eval_samples_per_second': 200.411, 'eval_steps_per_second': 12.626, 'epoch': 0.48}
{'loss': 0.8577, 'grad_norm': 0.26186126470565796, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8091124296188354, 'eval_runtime': 4.9862, 'eval_samples_per_second': 200.554, 'eval_steps_per_second': 12.635, 'epoch': 0.52}
{'loss': 0.8179, 'grad_norm': 0.27716633677482605, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8003607392311096, 'eval_runtime': 4.9804, 'eval_samples_per_second': 200.785, 'eval_steps_per_second': 12.649, 'epoch': 0.56}
{'loss': 0.8382, 'grad_norm': 0.28030702471733093, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7894353866577148, 'eval_runtime': 4.9996, 'eval_samples_per_second': 200.014, 'eval_steps_per_second': 12.601, 'epoch': 0.6}
{'loss': 0.829, 'grad_norm': 0.2988353967666626, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7780987620353699, 'eval_runtime': 5.003, 'eval_samples_per_second': 199.881, 'eval_steps_per_second': 12.592, 'epoch': 0.64}
{'loss': 0.8256, 'grad_norm': 0.2771397829055786, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7710251212120056, 'eval_runtime': 4.9847, 'eval_samples_per_second': 200.612, 'eval_steps_per_second': 12.639, 'epoch': 0.68}
{'loss': 0.8309, 'grad_norm': 0.28847381472587585, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7603740096092224, 'eval_runtime': 4.9863, 'eval_samples_per_second': 200.549, 'eval_steps_per_second': 12.635, 'epoch': 0.72}
{'loss': 0.8095, 'grad_norm': 0.292213499546051, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7528464198112488, 'eval_runtime': 4.9933, 'eval_samples_per_second': 200.269, 'eval_steps_per_second': 12.617, 'epoch': 0.76}
{'loss': 0.8214, 'grad_norm': 0.33591926097869873, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7427740693092346, 'eval_runtime': 4.9965, 'eval_samples_per_second': 200.142, 'eval_steps_per_second': 12.609, 'epoch': 0.8}
{'loss': 0.8001, 'grad_norm': 0.30856597423553467, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7375227808952332, 'eval_runtime': 4.9912, 'eval_samples_per_second': 200.352, 'eval_steps_per_second': 12.622, 'epoch': 0.84}
{'loss': 0.8035, 'grad_norm': 0.3191513121128082, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7306429147720337, 'eval_runtime': 5.0071, 'eval_samples_per_second': 199.717, 'eval_steps_per_second': 12.582, 'epoch': 0.88}
{'loss': 0.8089, 'grad_norm': 0.3589524030685425, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7257985472679138, 'eval_runtime': 5.0143, 'eval_samples_per_second': 199.43, 'eval_steps_per_second': 12.564, 'epoch': 0.92}
{'loss': 0.7911, 'grad_norm': 0.3350473642349243, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7214365005493164, 'eval_runtime': 5.0122, 'eval_samples_per_second': 199.514, 'eval_steps_per_second': 12.569, 'epoch': 0.96}
{'loss': 0.8104, 'grad_norm': 0.34898558259010315, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7202028632164001, 'eval_runtime': 4.9741, 'eval_samples_per_second': 201.043, 'eval_steps_per_second': 12.666, 'epoch': 1.0}
{'train_runtime': 268.3084, 'train_samples_per_second': 37.271, 'train_steps_per_second': 2.329, 'train_loss': 0.9659676330566406, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5586196184158325, 0.9898214340209961, 0.944313108921051, 0.9195212125778198, 0.9018502235412598, 0.8880867958068848, 0.8746106624603271, 0.8646382093429565, 0.853053092956543, 0.8418262600898743, 0.827357292175293, 0.8203311562538147, 0.8091124296188354, 0.8003607392311096, 0.7894353866577148, 0.7780987620353699, 0.7710251212120056, 0.7603740096092224, 0.7528464198112488, 0.7427740693092346, 0.7375227808952332, 0.7306429147720337, 0.7257985472679138, 0.7214365005493164, 0.7202028632164001], 'performance': [0.58, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:25,  3.87it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.90it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 64.53it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 71.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 67.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 70.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.58, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.0049550533294678
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7750 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.149631403580234
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 1.1902077693784794
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.143900467930086
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.8739874796711223
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 1.1062836652439438
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 2, 2.776940493660833e-18, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(3.1374e-16, dtype=torch.float64), tensor(8.0491e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.0735e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.5707e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(2.7769e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 1.0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (2.776940493660833e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  2
lora dropout:  2.776940493660833e-18
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.53it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.82it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.48it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.24it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 45.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.36it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.85it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 58.57it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.29it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.12it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 44.94it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.06it/s]
Evaluation performance at step 25: 0.61
{'loss': 5.0276, 'grad_norm': 3.474794387817383, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.9476137161254883, 'eval_runtime': 4.8006, 'eval_samples_per_second': 208.307, 'eval_steps_per_second': 13.123, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  6.16it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.53it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.96it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.06it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.09it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 56.83it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 61.52it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 62.20it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.61it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 64.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.28it/s]
Evaluation performance at step 50: 0.62
{'loss': 2.0369, 'grad_norm': 3.0925917625427246, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.3929904699325562, 'eval_runtime': 3.7351, 'eval_samples_per_second': 267.727, 'eval_steps_per_second': 16.867, 'epoch': 0.08}
{'loss': 1.2029, 'grad_norm': 1.8145110607147217, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0213210582733154, 'eval_runtime': 3.7294, 'eval_samples_per_second': 268.138, 'eval_steps_per_second': 16.893, 'epoch': 0.12}
{'loss': 1.0173, 'grad_norm': 0.38470277190208435, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9723876714706421, 'eval_runtime': 3.7303, 'eval_samples_per_second': 268.078, 'eval_steps_per_second': 16.889, 'epoch': 0.16}
{'loss': 0.9552, 'grad_norm': 0.3877006471157074, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9612126350402832, 'eval_runtime': 3.7348, 'eval_samples_per_second': 267.752, 'eval_steps_per_second': 16.868, 'epoch': 0.2}
{'loss': 0.9707, 'grad_norm': 0.38408514857292175, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9496591687202454, 'eval_runtime': 3.7336, 'eval_samples_per_second': 267.835, 'eval_steps_per_second': 16.874, 'epoch': 0.24}
{'loss': 0.9661, 'grad_norm': 0.3332771062850952, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9406101107597351, 'eval_runtime': 3.7214, 'eval_samples_per_second': 268.719, 'eval_steps_per_second': 16.929, 'epoch': 0.28}
{'loss': 0.9487, 'grad_norm': 0.3536449074745178, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9340263605117798, 'eval_runtime': 3.7242, 'eval_samples_per_second': 268.515, 'eval_steps_per_second': 16.916, 'epoch': 0.32}
{'loss': 0.9975, 'grad_norm': 0.42301201820373535, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9284652471542358, 'eval_runtime': 3.7233, 'eval_samples_per_second': 268.577, 'eval_steps_per_second': 16.92, 'epoch': 0.36}
{'loss': 0.9707, 'grad_norm': 0.42340824007987976, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9242005348205566, 'eval_runtime': 3.7253, 'eval_samples_per_second': 268.437, 'eval_steps_per_second': 16.912, 'epoch': 0.4}
{'loss': 0.9321, 'grad_norm': 0.38231968879699707, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9189831018447876, 'eval_runtime': 3.7416, 'eval_samples_per_second': 267.269, 'eval_steps_per_second': 16.838, 'epoch': 0.44}
{'loss': 0.9619, 'grad_norm': 0.41373395919799805, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9128932952880859, 'eval_runtime': 3.7537, 'eval_samples_per_second': 266.402, 'eval_steps_per_second': 16.783, 'epoch': 0.48}
{'loss': 0.9581, 'grad_norm': 0.4119218587875366, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9082107543945312, 'eval_runtime': 3.7693, 'eval_samples_per_second': 265.299, 'eval_steps_per_second': 16.714, 'epoch': 0.52}
{'loss': 0.9513, 'grad_norm': 0.3588605523109436, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9042728543281555, 'eval_runtime': 3.7842, 'eval_samples_per_second': 264.254, 'eval_steps_per_second': 16.648, 'epoch': 0.56}
{'loss': 0.9234, 'grad_norm': 0.5024827718734741, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.900629997253418, 'eval_runtime': 3.7677, 'eval_samples_per_second': 265.413, 'eval_steps_per_second': 16.721, 'epoch': 0.6}
{'loss': 0.9707, 'grad_norm': 0.4236033260822296, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8977295160293579, 'eval_runtime': 3.7646, 'eval_samples_per_second': 265.63, 'eval_steps_per_second': 16.735, 'epoch': 0.64}
{'loss': 0.9576, 'grad_norm': 0.42253729701042175, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8949394822120667, 'eval_runtime': 3.7635, 'eval_samples_per_second': 265.71, 'eval_steps_per_second': 16.74, 'epoch': 0.68}
{'loss': 0.9487, 'grad_norm': 0.45537686347961426, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.891594648361206, 'eval_runtime': 3.7579, 'eval_samples_per_second': 266.103, 'eval_steps_per_second': 16.765, 'epoch': 0.72}
{'loss': 0.9248, 'grad_norm': 0.37097108364105225, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8900485634803772, 'eval_runtime': 3.7654, 'eval_samples_per_second': 265.576, 'eval_steps_per_second': 16.731, 'epoch': 0.76}
{'loss': 0.949, 'grad_norm': 0.510105550289154, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.88712078332901, 'eval_runtime': 3.7806, 'eval_samples_per_second': 264.51, 'eval_steps_per_second': 16.664, 'epoch': 0.8}
{'loss': 0.9403, 'grad_norm': 0.4136471748352051, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8851004838943481, 'eval_runtime': 3.7601, 'eval_samples_per_second': 265.949, 'eval_steps_per_second': 16.755, 'epoch': 0.84}
{'loss': 0.9251, 'grad_norm': 0.42909902334213257, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8830369710922241, 'eval_runtime': 3.7602, 'eval_samples_per_second': 265.943, 'eval_steps_per_second': 16.754, 'epoch': 0.88}
{'loss': 0.9285, 'grad_norm': 0.4419809579849243, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8816139101982117, 'eval_runtime': 3.7646, 'eval_samples_per_second': 265.63, 'eval_steps_per_second': 16.735, 'epoch': 0.92}
{'loss': 0.9506, 'grad_norm': 0.39057251811027527, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8813256621360779, 'eval_runtime': 3.7355, 'eval_samples_per_second': 267.702, 'eval_steps_per_second': 16.865, 'epoch': 0.96}
{'loss': 0.9398, 'grad_norm': 0.41298842430114746, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8809834718704224, 'eval_runtime': 3.7261, 'eval_samples_per_second': 268.378, 'eval_steps_per_second': 16.908, 'epoch': 1.0}
{'train_runtime': 243.6643, 'train_samples_per_second': 41.04, 'train_steps_per_second': 2.565, 'train_loss': 1.1702181549072266, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9476137161254883, 1.3929904699325562, 1.0213210582733154, 0.9723876714706421, 0.9612126350402832, 0.9496591687202454, 0.9406101107597351, 0.9340263605117798, 0.9284652471542358, 0.9242005348205566, 0.9189831018447876, 0.9128932952880859, 0.9082107543945312, 0.9042728543281555, 0.900629997253418, 0.8977295160293579, 0.8949394822120667, 0.891594648361206, 0.8900485634803772, 0.88712078332901, 0.8851004838943481, 0.8830369710922241, 0.8816139101982117, 0.8813256621360779, 0.8809834718704224], 'performance': [0.61, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.18it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.58it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 60.65it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 64.35it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 63.64it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  0.7605749368667603
current iteration best possible performance (full train run):  0.651
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0150 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.1240335061451414
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 0.8062225384781335
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 1.1554069208460054
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.8122945439802278
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.157075407930114
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.8432, dtype=torch.float64), 0, 0, tensor(0.1568, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 29.813058415553638, 0]
normalized proposed parameters for next round by BO: [tensor(0.8432, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.8593e-18, dtype=torch.float64), tensor(0.1568, dtype=torch.float64), tensor(2.6110e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6211, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.843
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.157
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (29.813058415553638,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  29.813058415553638
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:22,  4.38it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 28.67it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.71it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 37.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.65it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 42.51it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 48.56it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 43.73it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 46.63it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 45.52it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 44.12it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 43.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 42.84it/s]
Evaluation performance at step 25: 0.63
{'loss': 3.029, 'grad_norm': 0.46565937995910645, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.1819790601730347, 'eval_runtime': 5.4845, 'eval_samples_per_second': 182.149, 'eval_steps_per_second': 11.487, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.67it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 28.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.93it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 37.68it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.74it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 41.70it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 43.24it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 46.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 43.81it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 46.66it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 45.74it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 43.75it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 43.79it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 42.68it/s]
Evaluation performance at step 50: 0.66
{'loss': 1.0441, 'grad_norm': 0.5084910988807678, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.66}
{'eval_loss': 0.8900929689407349, 'eval_runtime': 5.4745, 'eval_samples_per_second': 182.481, 'eval_steps_per_second': 11.508, 'epoch': 0.08}
{'loss': 0.8874, 'grad_norm': 0.1556054949760437, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8532575368881226, 'eval_runtime': 5.4803, 'eval_samples_per_second': 182.29, 'eval_steps_per_second': 11.496, 'epoch': 0.12}
{'loss': 0.8592, 'grad_norm': 0.15779776871204376, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8359538912773132, 'eval_runtime': 5.4786, 'eval_samples_per_second': 182.346, 'eval_steps_per_second': 11.499, 'epoch': 0.16}
{'loss': 0.8515, 'grad_norm': 0.16105595231056213, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8182487487792969, 'eval_runtime': 5.495, 'eval_samples_per_second': 181.801, 'eval_steps_per_second': 11.465, 'epoch': 0.2}
{'loss': 0.8335, 'grad_norm': 0.17425450682640076, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8068919777870178, 'eval_runtime': 5.4959, 'eval_samples_per_second': 181.771, 'eval_steps_per_second': 11.463, 'epoch': 0.24}
{'loss': 0.8189, 'grad_norm': 0.15851396322250366, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7937853336334229, 'eval_runtime': 5.49, 'eval_samples_per_second': 181.968, 'eval_steps_per_second': 11.475, 'epoch': 0.28}
{'loss': 0.8359, 'grad_norm': 0.16103915870189667, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7833568453788757, 'eval_runtime': 5.5006, 'eval_samples_per_second': 181.617, 'eval_steps_per_second': 11.453, 'epoch': 0.32}
{'loss': 0.7957, 'grad_norm': 0.15525777637958527, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7682639360427856, 'eval_runtime': 5.4961, 'eval_samples_per_second': 181.765, 'eval_steps_per_second': 11.463, 'epoch': 0.36}
{'loss': 0.8165, 'grad_norm': 0.16366015374660492, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7532945275306702, 'eval_runtime': 5.5251, 'eval_samples_per_second': 180.811, 'eval_steps_per_second': 11.403, 'epoch': 0.4}
{'loss': 0.7868, 'grad_norm': 0.16424168646335602, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7415022253990173, 'eval_runtime': 5.4987, 'eval_samples_per_second': 181.678, 'eval_steps_per_second': 11.457, 'epoch': 0.44}
{'loss': 0.8, 'grad_norm': 0.1535736471414566, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7338255643844604, 'eval_runtime': 5.4905, 'eval_samples_per_second': 181.949, 'eval_steps_per_second': 11.474, 'epoch': 0.48}
{'loss': 0.7909, 'grad_norm': 0.1528252363204956, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.720920741558075, 'eval_runtime': 5.4546, 'eval_samples_per_second': 183.149, 'eval_steps_per_second': 11.55, 'epoch': 0.52}
{'loss': 0.7683, 'grad_norm': 0.16352611780166626, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7103567123413086, 'eval_runtime': 5.4464, 'eval_samples_per_second': 183.423, 'eval_steps_per_second': 11.567, 'epoch': 0.56}
{'loss': 0.7585, 'grad_norm': 0.181852787733078, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.696682333946228, 'eval_runtime': 5.444, 'eval_samples_per_second': 183.505, 'eval_steps_per_second': 11.572, 'epoch': 0.6}
{'loss': 0.7623, 'grad_norm': 0.19088077545166016, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6887889504432678, 'eval_runtime': 5.4366, 'eval_samples_per_second': 183.753, 'eval_steps_per_second': 11.588, 'epoch': 0.64}
{'loss': 0.7683, 'grad_norm': 0.19068405032157898, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6779164671897888, 'eval_runtime': 5.4192, 'eval_samples_per_second': 184.345, 'eval_steps_per_second': 11.625, 'epoch': 0.68}
{'loss': 0.7701, 'grad_norm': 0.1676773577928543, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6700904965400696, 'eval_runtime': 5.4096, 'eval_samples_per_second': 184.673, 'eval_steps_per_second': 11.646, 'epoch': 0.72}
{'loss': 0.7562, 'grad_norm': 0.20162826776504517, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6604321599006653, 'eval_runtime': 5.4083, 'eval_samples_per_second': 184.716, 'eval_steps_per_second': 11.649, 'epoch': 0.76}
{'loss': 0.725, 'grad_norm': 0.1717175990343094, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.648166835308075, 'eval_runtime': 5.4106, 'eval_samples_per_second': 184.638, 'eval_steps_per_second': 11.644, 'epoch': 0.8}
{'loss': 0.7464, 'grad_norm': 0.21654126048088074, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.640815258026123, 'eval_runtime': 5.4048, 'eval_samples_per_second': 184.835, 'eval_steps_per_second': 11.656, 'epoch': 0.84}
{'loss': 0.7322, 'grad_norm': 0.1844589114189148, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.635266125202179, 'eval_runtime': 5.4107, 'eval_samples_per_second': 184.632, 'eval_steps_per_second': 11.643, 'epoch': 0.88}
{'loss': 0.7286, 'grad_norm': 0.19237715005874634, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6282932162284851, 'eval_runtime': 5.4058, 'eval_samples_per_second': 184.803, 'eval_steps_per_second': 11.654, 'epoch': 0.92}
{'loss': 0.7146, 'grad_norm': 0.20471720397472382, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6216350197792053, 'eval_runtime': 5.406, 'eval_samples_per_second': 184.795, 'eval_steps_per_second': 11.654, 'epoch': 0.96}
{'loss': 0.7122, 'grad_norm': 0.22251100838184357, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6203033328056335, 'eval_runtime': 5.4285, 'eval_samples_per_second': 184.029, 'eval_steps_per_second': 11.605, 'epoch': 1.0}
{'train_runtime': 331.0797, 'train_samples_per_second': 30.201, 'train_steps_per_second': 1.888, 'train_loss': 0.8836752838134766, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1819790601730347, 0.8900929689407349, 0.8532575368881226, 0.8359538912773132, 0.8182487487792969, 0.8068919777870178, 0.7937853336334229, 0.7833568453788757, 0.7682639360427856, 0.7532945275306702, 0.7415022253990173, 0.7338255643844604, 0.720920741558075, 0.7103567123413086, 0.696682333946228, 0.6887889504432678, 0.6779164671897888, 0.6700904965400696, 0.6604321599006653, 0.648166835308075, 0.640815258026123, 0.635266125202179, 0.6282932162284851, 0.6216350197792053, 0.6203033328056335], 'performance': [0.63, 0.66]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:34,  2.89it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.90it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.05it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.24it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 57.83it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.72it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.66]
current iteration observed (possibly low-fid or predicted) performance:  1.1872422695159912
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0520 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.117262292208976
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 1.006689442651907
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 0.9633600054270389
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 1.0839198828774927
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 1.1320374923540744
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.8485, dtype=torch.float64), 0, 0, tensor(0.1515, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 86, 0.1, 27.060918620034414, 0]
normalized proposed parameters for next round by BO: [tensor(0.8485, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1286e-17, dtype=torch.float64), tensor(0.1515, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2554e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6697, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5638, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.849
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.151
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (86,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (27.060918620034414,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  86
lora dropout:  0.1
lora alpha:  27.060918620034414
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 174,718,976 || all params: 8,204,980,224 || trainable%: 2.1294
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.24it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 34.34it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.29it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 46.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 48.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.50it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.63it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 57.24it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.85it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.86it/s]
Evaluation performance at step 25: 0.63
{'loss': 3.0864, 'grad_norm': 0.59930419921875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.1900098323822021, 'eval_runtime': 5.9587, 'eval_samples_per_second': 167.654, 'eval_steps_per_second': 10.573, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.77it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.22it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.27it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.84it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.17it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.04it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.89it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 54.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.81it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 52.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 56.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.38it/s]
Evaluation performance at step 50: 0.64
{'loss': 1.0289, 'grad_norm': 0.36672648787498474, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.64}
{'eval_loss': 0.8970196843147278, 'eval_runtime': 5.6978, 'eval_samples_per_second': 175.331, 'eval_steps_per_second': 11.057, 'epoch': 0.08}
{'loss': 0.8841, 'grad_norm': 0.1810581237077713, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8576610088348389, 'eval_runtime': 5.7167, 'eval_samples_per_second': 174.75, 'eval_steps_per_second': 11.02, 'epoch': 0.12}
{'loss': 0.8571, 'grad_norm': 0.16542725265026093, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8381391763687134, 'eval_runtime': 5.7086, 'eval_samples_per_second': 174.998, 'eval_steps_per_second': 11.036, 'epoch': 0.16}
{'loss': 0.8367, 'grad_norm': 0.16908854246139526, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8243165016174316, 'eval_runtime': 5.712, 'eval_samples_per_second': 174.896, 'eval_steps_per_second': 11.029, 'epoch': 0.2}
{'loss': 0.8281, 'grad_norm': 0.16418185830116272, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8133280873298645, 'eval_runtime': 5.7057, 'eval_samples_per_second': 175.089, 'eval_steps_per_second': 11.042, 'epoch': 0.24}
{'loss': 0.8221, 'grad_norm': 0.19210097193717957, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7995110154151917, 'eval_runtime': 5.6873, 'eval_samples_per_second': 175.655, 'eval_steps_per_second': 11.077, 'epoch': 0.28}
{'loss': 0.8217, 'grad_norm': 0.18257369101047516, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7870873212814331, 'eval_runtime': 5.6798, 'eval_samples_per_second': 175.886, 'eval_steps_per_second': 11.092, 'epoch': 0.32}
{'loss': 0.8135, 'grad_norm': 0.1803472340106964, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7760439515113831, 'eval_runtime': 5.6866, 'eval_samples_per_second': 175.676, 'eval_steps_per_second': 11.079, 'epoch': 0.36}
{'loss': 0.8022, 'grad_norm': 0.18447616696357727, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7583113312721252, 'eval_runtime': 5.6854, 'eval_samples_per_second': 175.713, 'eval_steps_per_second': 11.081, 'epoch': 0.4}
{'loss': 0.7968, 'grad_norm': 0.17693620920181274, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7452727556228638, 'eval_runtime': 5.6864, 'eval_samples_per_second': 175.682, 'eval_steps_per_second': 11.079, 'epoch': 0.44}
{'loss': 0.7887, 'grad_norm': 0.18822863698005676, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7336803674697876, 'eval_runtime': 5.6855, 'eval_samples_per_second': 175.711, 'eval_steps_per_second': 11.081, 'epoch': 0.48}
{'loss': 0.7928, 'grad_norm': 0.18337126076221466, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7265629172325134, 'eval_runtime': 5.7056, 'eval_samples_per_second': 175.09, 'eval_steps_per_second': 11.042, 'epoch': 0.52}
{'loss': 0.7717, 'grad_norm': 0.1716875433921814, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.71488356590271, 'eval_runtime': 5.7215, 'eval_samples_per_second': 174.605, 'eval_steps_per_second': 11.011, 'epoch': 0.56}
{'loss': 0.7673, 'grad_norm': 0.19630150496959686, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7027536034584045, 'eval_runtime': 5.7183, 'eval_samples_per_second': 174.704, 'eval_steps_per_second': 11.017, 'epoch': 0.6}
{'loss': 0.7797, 'grad_norm': 0.20928005874156952, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6939340233802795, 'eval_runtime': 5.6942, 'eval_samples_per_second': 175.441, 'eval_steps_per_second': 11.064, 'epoch': 0.64}
{'loss': 0.7579, 'grad_norm': 0.22748973965644836, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.68265700340271, 'eval_runtime': 5.6845, 'eval_samples_per_second': 175.74, 'eval_steps_per_second': 11.083, 'epoch': 0.68}
{'loss': 0.7523, 'grad_norm': 0.2091606706380844, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6741768717765808, 'eval_runtime': 5.6968, 'eval_samples_per_second': 175.361, 'eval_steps_per_second': 11.059, 'epoch': 0.72}
{'loss': 0.7258, 'grad_norm': 0.21692374348640442, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6648101806640625, 'eval_runtime': 5.6972, 'eval_samples_per_second': 175.35, 'eval_steps_per_second': 11.058, 'epoch': 0.76}
{'loss': 0.7442, 'grad_norm': 0.2251739501953125, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6534276008605957, 'eval_runtime': 5.695, 'eval_samples_per_second': 175.417, 'eval_steps_per_second': 11.062, 'epoch': 0.8}
{'loss': 0.7455, 'grad_norm': 0.25047144293785095, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.650076687335968, 'eval_runtime': 5.689, 'eval_samples_per_second': 175.601, 'eval_steps_per_second': 11.074, 'epoch': 0.84}
{'loss': 0.7413, 'grad_norm': 0.24162523448467255, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6393269300460815, 'eval_runtime': 5.6934, 'eval_samples_per_second': 175.468, 'eval_steps_per_second': 11.066, 'epoch': 0.88}
{'loss': 0.7274, 'grad_norm': 0.22263535857200623, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6330183744430542, 'eval_runtime': 5.6935, 'eval_samples_per_second': 175.464, 'eval_steps_per_second': 11.065, 'epoch': 0.92}
{'loss': 0.729, 'grad_norm': 0.23423920571804047, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6292516589164734, 'eval_runtime': 5.688, 'eval_samples_per_second': 175.634, 'eval_steps_per_second': 11.076, 'epoch': 0.96}
{'loss': 0.738, 'grad_norm': 0.24275615811347961, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.628028929233551, 'eval_runtime': 5.6845, 'eval_samples_per_second': 175.741, 'eval_steps_per_second': 11.083, 'epoch': 1.0}
{'train_runtime': 342.4019, 'train_samples_per_second': 29.203, 'train_steps_per_second': 1.825, 'train_loss': 0.8855626403808594, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1900098323822021, 0.8970196843147278, 0.8576610088348389, 0.8381391763687134, 0.8243165016174316, 0.8133280873298645, 0.7995110154151917, 0.7870873212814331, 0.7760439515113831, 0.7583113312721252, 0.7452727556228638, 0.7336803674697876, 0.7265629172325134, 0.71488356590271, 0.7027536034584045, 0.6939340233802795, 0.68265700340271, 0.6741768717765808, 0.6648101806640625, 0.6534276008605957, 0.650076687335968, 0.6393269300460815, 0.6330183744430542, 0.6292516589164734, 0.628028929233551], 'performance': [0.63, 0.64]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.94it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.37it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.93it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 75.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 58.49it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.64]
current iteration observed (possibly low-fid or predicted) performance:  1.0541033744812012
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4223 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 0.886733645353319
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.8445163997132014
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.0668838956500537
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 1.0889255077380369
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 0.7195261822789094
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7558, dtype=torch.float64), 0, 0, tensor(0.2442, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7558, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9327e-17, dtype=torch.float64), tensor(0.2442, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0260e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.4209e-18, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.756
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.244
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.53it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 37.58it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 43.31it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 45.92it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 49.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 46.38it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 50.65it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 49.97it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 48.37it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 47.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 44.96it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.816, 'grad_norm': 0.6135005950927734, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.1274374723434448, 'eval_runtime': 5.5305, 'eval_samples_per_second': 180.636, 'eval_steps_per_second': 11.391, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:22,  4.36it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 25.36it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.28it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 37.66it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.45it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 40.90it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 42.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 45.18it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 43.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 47.06it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 46.73it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 45.43it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 47.82it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 43.07it/s]
Evaluation performance at step 50: 0.66
{'loss': 0.9698, 'grad_norm': 0.5023154616355896, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.66}
{'eval_loss': 0.8849824666976929, 'eval_runtime': 5.5428, 'eval_samples_per_second': 180.235, 'eval_steps_per_second': 11.366, 'epoch': 0.08}
{'loss': 0.8675, 'grad_norm': 0.21697114408016205, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8529588580131531, 'eval_runtime': 5.6274, 'eval_samples_per_second': 177.524, 'eval_steps_per_second': 11.195, 'epoch': 0.12}
{'loss': 0.8441, 'grad_norm': 0.1960398554801941, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8331361413002014, 'eval_runtime': 5.552, 'eval_samples_per_second': 179.934, 'eval_steps_per_second': 11.347, 'epoch': 0.16}
{'loss': 0.8504, 'grad_norm': 0.22000141441822052, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8175985217094421, 'eval_runtime': 5.5525, 'eval_samples_per_second': 179.918, 'eval_steps_per_second': 11.346, 'epoch': 0.2}
{'loss': 0.8227, 'grad_norm': 0.19784551858901978, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8058102130889893, 'eval_runtime': 5.5716, 'eval_samples_per_second': 179.304, 'eval_steps_per_second': 11.307, 'epoch': 0.24}
{'loss': 0.8279, 'grad_norm': 0.25137680768966675, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7848293781280518, 'eval_runtime': 5.5591, 'eval_samples_per_second': 179.705, 'eval_steps_per_second': 11.333, 'epoch': 0.28}
{'loss': 0.7956, 'grad_norm': 0.20891940593719482, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7729357481002808, 'eval_runtime': 5.5513, 'eval_samples_per_second': 179.956, 'eval_steps_per_second': 11.349, 'epoch': 0.32}
{'loss': 0.7956, 'grad_norm': 0.2019442468881607, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7614251375198364, 'eval_runtime': 5.572, 'eval_samples_per_second': 179.29, 'eval_steps_per_second': 11.307, 'epoch': 0.36}
{'loss': 0.8024, 'grad_norm': 0.22655171155929565, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7431526184082031, 'eval_runtime': 5.5616, 'eval_samples_per_second': 179.624, 'eval_steps_per_second': 11.328, 'epoch': 0.4}
{'loss': 0.7817, 'grad_norm': 0.22623510658740997, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7241319417953491, 'eval_runtime': 5.5989, 'eval_samples_per_second': 178.427, 'eval_steps_per_second': 11.252, 'epoch': 0.44}
{'loss': 0.7833, 'grad_norm': 0.21569454669952393, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7105115652084351, 'eval_runtime': 5.5648, 'eval_samples_per_second': 179.521, 'eval_steps_per_second': 11.321, 'epoch': 0.48}
{'loss': 0.7648, 'grad_norm': 0.2178686112165451, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7019860148429871, 'eval_runtime': 5.5616, 'eval_samples_per_second': 179.626, 'eval_steps_per_second': 11.328, 'epoch': 0.52}
{'loss': 0.7595, 'grad_norm': 0.2135302871465683, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.687751829624176, 'eval_runtime': 5.58, 'eval_samples_per_second': 179.033, 'eval_steps_per_second': 11.29, 'epoch': 0.56}
{'loss': 0.7721, 'grad_norm': 0.2707652449607849, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6764470934867859, 'eval_runtime': 5.5579, 'eval_samples_per_second': 179.745, 'eval_steps_per_second': 11.335, 'epoch': 0.6}
{'loss': 0.7502, 'grad_norm': 0.2397889643907547, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6604470610618591, 'eval_runtime': 5.5956, 'eval_samples_per_second': 178.534, 'eval_steps_per_second': 11.259, 'epoch': 0.64}
{'loss': 0.7584, 'grad_norm': 0.21770384907722473, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6498197317123413, 'eval_runtime': 5.5943, 'eval_samples_per_second': 178.575, 'eval_steps_per_second': 11.262, 'epoch': 0.68}
{'loss': 0.73, 'grad_norm': 0.2228708416223526, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6326791644096375, 'eval_runtime': 5.6115, 'eval_samples_per_second': 178.026, 'eval_steps_per_second': 11.227, 'epoch': 0.72}
{'loss': 0.7214, 'grad_norm': 0.22258985042572021, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.621525228023529, 'eval_runtime': 5.5727, 'eval_samples_per_second': 179.267, 'eval_steps_per_second': 11.305, 'epoch': 0.76}
{'loss': 0.7276, 'grad_norm': 0.24760150909423828, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6094199419021606, 'eval_runtime': 5.5971, 'eval_samples_per_second': 178.485, 'eval_steps_per_second': 11.256, 'epoch': 0.8}
{'loss': 0.7132, 'grad_norm': 0.22087347507476807, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6057482361793518, 'eval_runtime': 5.5749, 'eval_samples_per_second': 179.198, 'eval_steps_per_second': 11.301, 'epoch': 0.84}
{'loss': 0.7141, 'grad_norm': 0.2515973150730133, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5949900150299072, 'eval_runtime': 5.5812, 'eval_samples_per_second': 178.995, 'eval_steps_per_second': 11.288, 'epoch': 0.88}
{'loss': 0.6982, 'grad_norm': 0.26197513937950134, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5843759179115295, 'eval_runtime': 5.5829, 'eval_samples_per_second': 178.939, 'eval_steps_per_second': 11.284, 'epoch': 0.92}
{'loss': 0.7031, 'grad_norm': 0.2210460901260376, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.582719624042511, 'eval_runtime': 5.5795, 'eval_samples_per_second': 179.047, 'eval_steps_per_second': 11.291, 'epoch': 0.96}
{'loss': 0.7157, 'grad_norm': 0.2814381420612335, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5796641111373901, 'eval_runtime': 5.6053, 'eval_samples_per_second': 178.223, 'eval_steps_per_second': 11.239, 'epoch': 1.0}
{'train_runtime': 334.5792, 'train_samples_per_second': 29.885, 'train_steps_per_second': 1.868, 'train_loss': 0.8593961273193359, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1274374723434448, 0.8849824666976929, 0.8529588580131531, 0.8331361413002014, 0.8175985217094421, 0.8058102130889893, 0.7848293781280518, 0.7729357481002808, 0.7614251375198364, 0.7431526184082031, 0.7241319417953491, 0.7105115652084351, 0.7019860148429871, 0.687751829624176, 0.6764470934867859, 0.6604470610618591, 0.6498197317123413, 0.6326791644096375, 0.621525228023529, 0.6094199419021606, 0.6057482361793518, 0.5949900150299072, 0.5843759179115295, 0.582719624042511, 0.5796641111373901], 'performance': [0.65, 0.66]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:39,  2.52it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 26.92it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 35.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 41.92it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 44.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 48.29it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 60.60it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.14it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.66]
current iteration observed (possibly low-fid or predicted) performance:  1.2495856285095215
current iteration best possible performance (full train run):  0.609
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3010 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 1.0230727752547086
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 1.1615084299975096
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.1251634864869746
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.1342801804118947
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.096090147777687
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7608, dtype=torch.float64), 0, tensor(0.2392, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7608, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2392, dtype=torch.float64), tensor(7.6488e-17, dtype=torch.float64), tensor(7.4587e-18, dtype=torch.float64), tensor(4.1682e-17, dtype=torch.float64), tensor(2.3661e-17, dtype=torch.float64), tensor(1.4301e-17, dtype=torch.float64), tensor(4.2673e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.761
  gsm8k: 0
  rowan_hellaswag: 0.239
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.17it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.25it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.41it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 45.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 55.41it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.28it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.03it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 56.97it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.27it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.971, 'grad_norm': 0.5404698848724365, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.5380758047103882, 'eval_runtime': 10.6338, 'eval_samples_per_second': 93.945, 'eval_steps_per_second': 5.924, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.19it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.85it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.94it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 52.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.48it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.62it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.72it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.16it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 57.91it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 57.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 57.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.98it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.3793, 'grad_norm': 0.5125910043716431, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.2710493803024292, 'eval_runtime': 10.5992, 'eval_samples_per_second': 94.252, 'eval_steps_per_second': 5.944, 'epoch': 0.08}
{'loss': 1.2818, 'grad_norm': 0.22208145260810852, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2180063724517822, 'eval_runtime': 10.6297, 'eval_samples_per_second': 93.982, 'eval_steps_per_second': 5.927, 'epoch': 0.12}
{'loss': 1.2191, 'grad_norm': 0.18356384336948395, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1952235698699951, 'eval_runtime': 10.6583, 'eval_samples_per_second': 93.73, 'eval_steps_per_second': 5.911, 'epoch': 0.16}
{'loss': 1.2318, 'grad_norm': 0.220805823802948, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1825348138809204, 'eval_runtime': 10.6758, 'eval_samples_per_second': 93.576, 'eval_steps_per_second': 5.901, 'epoch': 0.2}
{'loss': 1.2236, 'grad_norm': 0.1944715678691864, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1696438789367676, 'eval_runtime': 10.6707, 'eval_samples_per_second': 93.621, 'eval_steps_per_second': 5.904, 'epoch': 0.24}
{'loss': 1.2496, 'grad_norm': 0.18690401315689087, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1552098989486694, 'eval_runtime': 10.6864, 'eval_samples_per_second': 93.483, 'eval_steps_per_second': 5.895, 'epoch': 0.28}
{'loss': 1.1379, 'grad_norm': 0.1840522438287735, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1435147523880005, 'eval_runtime': 10.6876, 'eval_samples_per_second': 93.473, 'eval_steps_per_second': 5.895, 'epoch': 0.32}
{'loss': 1.1748, 'grad_norm': 0.18247376382350922, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1317936182022095, 'eval_runtime': 10.7002, 'eval_samples_per_second': 93.362, 'eval_steps_per_second': 5.888, 'epoch': 0.36}
{'loss': 1.1728, 'grad_norm': 0.17095603048801422, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1159656047821045, 'eval_runtime': 10.6999, 'eval_samples_per_second': 93.365, 'eval_steps_per_second': 5.888, 'epoch': 0.4}
{'loss': 1.1654, 'grad_norm': 0.17178122699260712, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1019285917282104, 'eval_runtime': 10.744, 'eval_samples_per_second': 92.982, 'eval_steps_per_second': 5.864, 'epoch': 0.44}
{'loss': 1.2008, 'grad_norm': 0.1706322878599167, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.090381145477295, 'eval_runtime': 10.6847, 'eval_samples_per_second': 93.498, 'eval_steps_per_second': 5.896, 'epoch': 0.48}
{'loss': 1.1622, 'grad_norm': 0.1982184648513794, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0831297636032104, 'eval_runtime': 10.6766, 'eval_samples_per_second': 93.57, 'eval_steps_per_second': 5.901, 'epoch': 0.52}
{'loss': 1.1246, 'grad_norm': 0.21209080517292023, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0691399574279785, 'eval_runtime': 10.6905, 'eval_samples_per_second': 93.448, 'eval_steps_per_second': 5.893, 'epoch': 0.56}
{'loss': 1.1546, 'grad_norm': 0.17308226227760315, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0590505599975586, 'eval_runtime': 10.6804, 'eval_samples_per_second': 93.536, 'eval_steps_per_second': 5.899, 'epoch': 0.6}
{'loss': 1.1237, 'grad_norm': 0.16685959696769714, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.053397536277771, 'eval_runtime': 10.6786, 'eval_samples_per_second': 93.551, 'eval_steps_per_second': 5.9, 'epoch': 0.64}
{'loss': 1.103, 'grad_norm': 0.1998424530029297, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0400546789169312, 'eval_runtime': 10.7934, 'eval_samples_per_second': 92.556, 'eval_steps_per_second': 5.837, 'epoch': 0.68}
{'loss': 1.0997, 'grad_norm': 0.2270987331867218, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.033740758895874, 'eval_runtime': 10.8144, 'eval_samples_per_second': 92.376, 'eval_steps_per_second': 5.826, 'epoch': 0.72}
{'loss': 1.0991, 'grad_norm': 0.21800798177719116, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0217533111572266, 'eval_runtime': 10.8086, 'eval_samples_per_second': 92.427, 'eval_steps_per_second': 5.829, 'epoch': 0.76}
{'loss': 1.0878, 'grad_norm': 0.21323265135288239, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0133764743804932, 'eval_runtime': 10.7931, 'eval_samples_per_second': 92.559, 'eval_steps_per_second': 5.837, 'epoch': 0.8}
{'loss': 1.1117, 'grad_norm': 0.19991134107112885, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0039421319961548, 'eval_runtime': 10.7452, 'eval_samples_per_second': 92.971, 'eval_steps_per_second': 5.863, 'epoch': 0.84}
{'loss': 1.1245, 'grad_norm': 0.19366982579231262, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9979068040847778, 'eval_runtime': 10.7261, 'eval_samples_per_second': 93.138, 'eval_steps_per_second': 5.874, 'epoch': 0.88}
{'loss': 1.1038, 'grad_norm': 0.22576594352722168, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9923476576805115, 'eval_runtime': 10.7076, 'eval_samples_per_second': 93.298, 'eval_steps_per_second': 5.884, 'epoch': 0.92}
{'loss': 1.0615, 'grad_norm': 0.21894621849060059, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9892554879188538, 'eval_runtime': 10.7299, 'eval_samples_per_second': 93.105, 'eval_steps_per_second': 5.871, 'epoch': 0.96}
{'loss': 1.1146, 'grad_norm': 0.22212758660316467, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9868311882019043, 'eval_runtime': 10.7205, 'eval_samples_per_second': 93.186, 'eval_steps_per_second': 5.877, 'epoch': 1.0}
{'train_runtime': 565.1129, 'train_samples_per_second': 17.694, 'train_steps_per_second': 1.106, 'train_loss': 1.2351466705322265, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5380758047103882, 1.2710493803024292, 1.2180063724517822, 1.1952235698699951, 1.1825348138809204, 1.1696438789367676, 1.1552098989486694, 1.1435147523880005, 1.1317936182022095, 1.1159656047821045, 1.1019285917282104, 1.090381145477295, 1.0831297636032104, 1.0691399574279785, 1.0590505599975586, 1.053397536277771, 1.0400546789169312, 1.033740758895874, 1.0217533111572266, 1.0133764743804932, 1.0039421319961548, 0.9979068040847778, 0.9923476576805115, 0.9892554879188538, 0.9868311882019043], 'performance': [0.65, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.65it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.59it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 41.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 70.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.2492868900299072
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0522 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 1.1332122319377276
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9132037046772874
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.0865439282497271
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.9460432433423315
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 1.1014263169379714
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7703, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.2297, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 1.0691902393211288e-16, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7703, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.3895e-18, dtype=torch.float64), tensor(8.3207e-17, dtype=torch.float64), tensor(1.4578e-17, dtype=torch.float64), tensor(4.9095e-17, dtype=torch.float64), tensor(4.0326e-17, dtype=torch.float64), tensor(0.2297, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0692e-15, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.77
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.23

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.0691902393211288e-16,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  1.0691902393211288e-16
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.87it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.71it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.90it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.19it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 47.47it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.90it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.29it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 51.68it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 50.91it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 51.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.67it/s]
Evaluation performance at step 25: 0.67
{'loss': 2.5637, 'grad_norm': 0.38840582966804504, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.67}
{'eval_loss': 1.0981959104537964, 'eval_runtime': 7.2703, 'eval_samples_per_second': 137.408, 'eval_steps_per_second': 8.665, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.94it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.73it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.35it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.29it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.16it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.31it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 54.53it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 52.68it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 53.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.59it/s]
Evaluation performance at step 50: 0.61
{'loss': 0.9822, 'grad_norm': 0.5534400343894958, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 0.8962929248809814, 'eval_runtime': 6.6989, 'eval_samples_per_second': 149.129, 'eval_steps_per_second': 9.405, 'epoch': 0.08}
{'loss': 0.8772, 'grad_norm': 0.266309529542923, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8542724847793579, 'eval_runtime': 6.7069, 'eval_samples_per_second': 148.95, 'eval_steps_per_second': 9.393, 'epoch': 0.12}
{'loss': 0.8439, 'grad_norm': 0.21838924288749695, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8328197598457336, 'eval_runtime': 6.7238, 'eval_samples_per_second': 148.577, 'eval_steps_per_second': 9.37, 'epoch': 0.16}
{'loss': 0.8351, 'grad_norm': 0.23916618525981903, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8141236901283264, 'eval_runtime': 6.7295, 'eval_samples_per_second': 148.451, 'eval_steps_per_second': 9.362, 'epoch': 0.2}
{'loss': 0.8186, 'grad_norm': 0.1974436640739441, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7934175729751587, 'eval_runtime': 6.7245, 'eval_samples_per_second': 148.562, 'eval_steps_per_second': 9.369, 'epoch': 0.24}
{'loss': 0.823, 'grad_norm': 0.21196959912776947, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7724164724349976, 'eval_runtime': 6.731, 'eval_samples_per_second': 148.417, 'eval_steps_per_second': 9.36, 'epoch': 0.28}
{'loss': 0.8005, 'grad_norm': 0.22094440460205078, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7558923959732056, 'eval_runtime': 6.7347, 'eval_samples_per_second': 148.336, 'eval_steps_per_second': 9.354, 'epoch': 0.32}
{'loss': 0.7831, 'grad_norm': 0.21605859696865082, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7389400005340576, 'eval_runtime': 6.7838, 'eval_samples_per_second': 147.262, 'eval_steps_per_second': 9.287, 'epoch': 0.36}
{'loss': 0.7574, 'grad_norm': 0.22276650369167328, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7133882641792297, 'eval_runtime': 6.7326, 'eval_samples_per_second': 148.382, 'eval_steps_per_second': 9.357, 'epoch': 0.4}
{'loss': 0.7878, 'grad_norm': 0.22499001026153564, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6925146579742432, 'eval_runtime': 6.7767, 'eval_samples_per_second': 147.418, 'eval_steps_per_second': 9.297, 'epoch': 0.44}
{'loss': 0.726, 'grad_norm': 0.21695293486118317, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6716532707214355, 'eval_runtime': 6.794, 'eval_samples_per_second': 147.04, 'eval_steps_per_second': 9.273, 'epoch': 0.48}
{'loss': 0.7082, 'grad_norm': 0.2515813708305359, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6574884653091431, 'eval_runtime': 6.7785, 'eval_samples_per_second': 147.378, 'eval_steps_per_second': 9.294, 'epoch': 0.52}
{'loss': 0.7335, 'grad_norm': 0.24690409004688263, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6438159346580505, 'eval_runtime': 6.748, 'eval_samples_per_second': 148.044, 'eval_steps_per_second': 9.336, 'epoch': 0.56}
{'loss': 0.7147, 'grad_norm': 0.26862001419067383, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6278385519981384, 'eval_runtime': 6.7444, 'eval_samples_per_second': 148.124, 'eval_steps_per_second': 9.341, 'epoch': 0.6}
{'loss': 0.7172, 'grad_norm': 0.28523382544517517, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.608086884021759, 'eval_runtime': 6.7428, 'eval_samples_per_second': 148.159, 'eval_steps_per_second': 9.343, 'epoch': 0.64}
{'loss': 0.6952, 'grad_norm': 0.2584877014160156, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.59128737449646, 'eval_runtime': 6.7469, 'eval_samples_per_second': 148.069, 'eval_steps_per_second': 9.338, 'epoch': 0.68}
{'loss': 0.6972, 'grad_norm': 0.2393423318862915, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5754539966583252, 'eval_runtime': 6.8309, 'eval_samples_per_second': 146.248, 'eval_steps_per_second': 9.223, 'epoch': 0.72}
{'loss': 0.6675, 'grad_norm': 0.2926763892173767, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5587671399116516, 'eval_runtime': 6.779, 'eval_samples_per_second': 147.366, 'eval_steps_per_second': 9.293, 'epoch': 0.76}
{'loss': 0.6883, 'grad_norm': 0.2811438739299774, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.543777346611023, 'eval_runtime': 6.7886, 'eval_samples_per_second': 147.159, 'eval_steps_per_second': 9.28, 'epoch': 0.8}
{'loss': 0.6404, 'grad_norm': 0.24659495055675507, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5279414653778076, 'eval_runtime': 6.7686, 'eval_samples_per_second': 147.593, 'eval_steps_per_second': 9.308, 'epoch': 0.84}
{'loss': 0.6428, 'grad_norm': 0.32969874143600464, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5181334614753723, 'eval_runtime': 6.8358, 'eval_samples_per_second': 146.143, 'eval_steps_per_second': 9.216, 'epoch': 0.88}
{'loss': 0.6342, 'grad_norm': 0.2556775212287903, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5122039318084717, 'eval_runtime': 6.8327, 'eval_samples_per_second': 146.208, 'eval_steps_per_second': 9.22, 'epoch': 0.92}
{'loss': 0.6355, 'grad_norm': 0.30886781215667725, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5024926066398621, 'eval_runtime': 6.8454, 'eval_samples_per_second': 145.938, 'eval_steps_per_second': 9.203, 'epoch': 0.96}
{'loss': 0.6378, 'grad_norm': 0.26456978917121887, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5020061135292053, 'eval_runtime': 6.8235, 'eval_samples_per_second': 146.406, 'eval_steps_per_second': 9.233, 'epoch': 1.0}
{'train_runtime': 396.9993, 'train_samples_per_second': 25.186, 'train_steps_per_second': 1.574, 'train_loss': 0.8164466094970703, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0981959104537964, 0.8962929248809814, 0.8542724847793579, 0.8328197598457336, 0.8141236901283264, 0.7934175729751587, 0.7724164724349976, 0.7558923959732056, 0.7389400005340576, 0.7133882641792297, 0.6925146579742432, 0.6716532707214355, 0.6574884653091431, 0.6438159346580505, 0.6278385519981384, 0.608086884021759, 0.59128737449646, 0.5754539966583252, 0.5587671399116516, 0.543777346611023, 0.5279414653778076, 0.5181334614753723, 0.5122039318084717, 0.5024926066398621, 0.5020061135292053], 'performance': [0.67, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:39,  2.50it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 28.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 36.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 41.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 44.60it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 47.05it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 60.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.23it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.67, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2484190464019775
current iteration best possible performance (full train run):  0.651
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9490 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.9475021237849647
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.8870279773354142
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 1.076587598368559
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.8102309369468911
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 1.2278616387838377
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 2, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.3613e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.9188e-17, dtype=torch.float64), tensor(1.0809e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.2706e-16, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 1.0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.10it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 26.33it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.29it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.96it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 48.48it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.05it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.37it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.62it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.82it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 56.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:03<00:00, 13.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 28.38it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.22, 'grad_norm': 1.0728089809417725, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.9783380031585693, 'eval_runtime': 9.9709, 'eval_samples_per_second': 100.292, 'eval_steps_per_second': 6.318, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.82it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.57it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.15it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 39.16it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.51it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 48.40it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.08it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 50.41it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.55it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.84it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.55it/s]
Evaluation performance at step 50: 0.6
{'loss': 2.6383, 'grad_norm': 0.7800630927085876, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 2.340101480484009, 'eval_runtime': 9.9398, 'eval_samples_per_second': 100.606, 'eval_steps_per_second': 6.338, 'epoch': 0.08}
{'loss': 2.2535, 'grad_norm': 1.2325323820114136, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 2.216172933578491, 'eval_runtime': 9.937, 'eval_samples_per_second': 100.634, 'eval_steps_per_second': 6.34, 'epoch': 0.12}
{'loss': 2.1143, 'grad_norm': 1.2745121717453003, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 2.1846861839294434, 'eval_runtime': 9.901, 'eval_samples_per_second': 101.0, 'eval_steps_per_second': 6.363, 'epoch': 0.16}
{'loss': 2.1748, 'grad_norm': 0.6556995511054993, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 2.156785488128662, 'eval_runtime': 9.9104, 'eval_samples_per_second': 100.904, 'eval_steps_per_second': 6.357, 'epoch': 0.2}
{'loss': 2.1465, 'grad_norm': 2.25408935546875, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 2.140563488006592, 'eval_runtime': 9.9042, 'eval_samples_per_second': 100.968, 'eval_steps_per_second': 6.361, 'epoch': 0.24}
{'loss': 2.1588, 'grad_norm': 0.8984510898590088, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 2.1267271041870117, 'eval_runtime': 9.9239, 'eval_samples_per_second': 100.767, 'eval_steps_per_second': 6.348, 'epoch': 0.28}
{'loss': 2.1865, 'grad_norm': 0.7920570969581604, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 2.1166024208068848, 'eval_runtime': 9.9332, 'eval_samples_per_second': 100.672, 'eval_steps_per_second': 6.342, 'epoch': 0.32}
{'loss': 2.0983, 'grad_norm': 0.6962898969650269, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 2.1049301624298096, 'eval_runtime': 9.9204, 'eval_samples_per_second': 100.802, 'eval_steps_per_second': 6.351, 'epoch': 0.36}
{'loss': 2.2084, 'grad_norm': 0.7727980017662048, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 2.0936059951782227, 'eval_runtime': 9.9108, 'eval_samples_per_second': 100.9, 'eval_steps_per_second': 6.357, 'epoch': 0.4}
{'loss': 2.1503, 'grad_norm': 0.6191772818565369, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 2.0854721069335938, 'eval_runtime': 9.9137, 'eval_samples_per_second': 100.871, 'eval_steps_per_second': 6.355, 'epoch': 0.44}
{'loss': 2.0948, 'grad_norm': 0.7486169934272766, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 2.0777056217193604, 'eval_runtime': 9.9278, 'eval_samples_per_second': 100.728, 'eval_steps_per_second': 6.346, 'epoch': 0.48}
{'loss': 2.0661, 'grad_norm': 0.7234595417976379, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 2.0687081813812256, 'eval_runtime': 9.9281, 'eval_samples_per_second': 100.724, 'eval_steps_per_second': 6.346, 'epoch': 0.52}
{'loss': 2.1259, 'grad_norm': 0.6718156933784485, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 2.061466693878174, 'eval_runtime': 9.9259, 'eval_samples_per_second': 100.747, 'eval_steps_per_second': 6.347, 'epoch': 0.56}
{'loss': 2.1484, 'grad_norm': 0.6122044324874878, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 2.0532853603363037, 'eval_runtime': 9.9191, 'eval_samples_per_second': 100.816, 'eval_steps_per_second': 6.351, 'epoch': 0.6}
{'loss': 2.1293, 'grad_norm': 0.8842862844467163, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 2.0459420680999756, 'eval_runtime': 9.924, 'eval_samples_per_second': 100.766, 'eval_steps_per_second': 6.348, 'epoch': 0.64}
{'loss': 2.0975, 'grad_norm': 0.7115453481674194, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 2.039982318878174, 'eval_runtime': 9.9124, 'eval_samples_per_second': 100.884, 'eval_steps_per_second': 6.356, 'epoch': 0.68}
{'loss': 2.0574, 'grad_norm': 1.3815720081329346, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 2.032390832901001, 'eval_runtime': 9.9142, 'eval_samples_per_second': 100.865, 'eval_steps_per_second': 6.354, 'epoch': 0.72}
{'loss': 2.0646, 'grad_norm': 1.3312467336654663, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 2.026841163635254, 'eval_runtime': 9.9424, 'eval_samples_per_second': 100.579, 'eval_steps_per_second': 6.337, 'epoch': 0.76}
{'loss': 2.0819, 'grad_norm': 1.843719244003296, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 2.020158052444458, 'eval_runtime': 9.928, 'eval_samples_per_second': 100.725, 'eval_steps_per_second': 6.346, 'epoch': 0.8}
{'loss': 2.1114, 'grad_norm': 0.709199845790863, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 2.0176432132720947, 'eval_runtime': 9.9059, 'eval_samples_per_second': 100.95, 'eval_steps_per_second': 6.36, 'epoch': 0.84}
{'loss': 2.0733, 'grad_norm': 0.6889787316322327, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 2.0133001804351807, 'eval_runtime': 9.9165, 'eval_samples_per_second': 100.842, 'eval_steps_per_second': 6.353, 'epoch': 0.88}
{'loss': 2.1509, 'grad_norm': 1.2465996742248535, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 2.010368824005127, 'eval_runtime': 9.9506, 'eval_samples_per_second': 100.496, 'eval_steps_per_second': 6.331, 'epoch': 0.92}
{'loss': 2.0433, 'grad_norm': 0.6475875973701477, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 2.0087954998016357, 'eval_runtime': 9.9979, 'eval_samples_per_second': 100.021, 'eval_steps_per_second': 6.301, 'epoch': 0.96}
{'loss': 2.0562, 'grad_norm': 0.8700113296508789, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 2.008063793182373, 'eval_runtime': 9.9804, 'eval_samples_per_second': 100.196, 'eval_steps_per_second': 6.312, 'epoch': 1.0}
{'train_runtime': 538.4172, 'train_samples_per_second': 18.573, 'train_steps_per_second': 1.161, 'train_loss': 2.1860285095214844, 'epoch': 1.0}
train_results:  {'eval_loss': [2.9783380031585693, 2.340101480484009, 2.216172933578491, 2.1846861839294434, 2.156785488128662, 2.140563488006592, 2.1267271041870117, 2.1166024208068848, 2.1049301624298096, 2.0936059951782227, 2.0854721069335938, 2.0777056217193604, 2.0687081813812256, 2.061466693878174, 2.0532853603363037, 2.0459420680999756, 2.039982318878174, 2.032390832901001, 2.026841163635254, 2.020158052444458, 2.0176432132720947, 2.0133001804351807, 2.010368824005127, 2.0087954998016357, 2.008063793182373], 'performance': [0.61, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.82it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 18.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 27.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 28.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:00, 36.92it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 43.49it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 55.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 39.22it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  0.7614403963088989
current iteration best possible performance (full train run):  0.5670000000000001
max performance so far:  0.6615000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0818 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 1.076813263076383
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.9810706108985665
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 0.7393002674823894
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.9138201262894371
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.7761878408675157
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7160, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2840, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 47.999999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(0.7160, dtype=torch.float64), tensor(6.0646e-18, dtype=torch.float64), tensor(2.0725e-17, dtype=torch.float64), tensor(2.1480e-17, dtype=torch.float64), tensor(2.3678e-17, dtype=torch.float64), tensor(5.1049e-17, dtype=torch.float64), tensor(8.5437e-17, dtype=torch.float64), tensor(0.2840, dtype=torch.float64), tensor(7.0205e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.716
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.284
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.999999999999986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  47.999999999999986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.08it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.05it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.95it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.37it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.85it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.36it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.84it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.30it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.6978, 'grad_norm': 0.8926153779029846, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.2751061916351318, 'eval_runtime': 8.816, 'eval_samples_per_second': 113.317, 'eval_steps_per_second': 7.146, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  4.96it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.59it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.39it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.22it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.02it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.21it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.49it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.55it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.49it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.20it/s]
Evaluation performance at step 50: 0.64
{'loss': 1.1709, 'grad_norm': 0.566874623298645, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.64}
{'eval_loss': 1.0513885021209717, 'eval_runtime': 8.8516, 'eval_samples_per_second': 112.861, 'eval_steps_per_second': 7.117, 'epoch': 0.08}
{'loss': 1.0467, 'grad_norm': 0.2330201268196106, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9986898303031921, 'eval_runtime': 8.7999, 'eval_samples_per_second': 113.524, 'eval_steps_per_second': 7.159, 'epoch': 0.12}
{'loss': 1.016, 'grad_norm': 0.20473556220531464, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9776460528373718, 'eval_runtime': 8.8117, 'eval_samples_per_second': 113.372, 'eval_steps_per_second': 7.15, 'epoch': 0.16}
{'loss': 0.9934, 'grad_norm': 0.2007676064968109, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9619477391242981, 'eval_runtime': 8.8195, 'eval_samples_per_second': 113.272, 'eval_steps_per_second': 7.143, 'epoch': 0.2}
{'loss': 0.9898, 'grad_norm': 0.20714697241783142, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9495521783828735, 'eval_runtime': 8.8233, 'eval_samples_per_second': 113.223, 'eval_steps_per_second': 7.14, 'epoch': 0.24}
{'loss': 0.9577, 'grad_norm': 0.26693257689476013, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9295312166213989, 'eval_runtime': 8.8263, 'eval_samples_per_second': 113.184, 'eval_steps_per_second': 7.138, 'epoch': 0.28}
{'loss': 0.9569, 'grad_norm': 0.21478740870952606, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9151220321655273, 'eval_runtime': 8.8467, 'eval_samples_per_second': 112.924, 'eval_steps_per_second': 7.121, 'epoch': 0.32}
{'loss': 0.9668, 'grad_norm': 0.18958498537540436, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9033553004264832, 'eval_runtime': 8.84, 'eval_samples_per_second': 113.009, 'eval_steps_per_second': 7.127, 'epoch': 0.36}
{'loss': 0.9221, 'grad_norm': 0.19968625903129578, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8839848637580872, 'eval_runtime': 8.8501, 'eval_samples_per_second': 112.88, 'eval_steps_per_second': 7.119, 'epoch': 0.4}
{'loss': 0.963, 'grad_norm': 0.20027093589305878, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8712210655212402, 'eval_runtime': 8.8846, 'eval_samples_per_second': 112.441, 'eval_steps_per_second': 7.091, 'epoch': 0.44}
{'loss': 0.9205, 'grad_norm': 0.24549901485443115, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8555130958557129, 'eval_runtime': 8.8477, 'eval_samples_per_second': 112.91, 'eval_steps_per_second': 7.12, 'epoch': 0.48}
{'loss': 0.9114, 'grad_norm': 0.21451091766357422, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8465558886528015, 'eval_runtime': 8.834, 'eval_samples_per_second': 113.086, 'eval_steps_per_second': 7.132, 'epoch': 0.52}
{'loss': 0.9208, 'grad_norm': 0.24535278975963593, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8290509581565857, 'eval_runtime': 8.8392, 'eval_samples_per_second': 113.02, 'eval_steps_per_second': 7.127, 'epoch': 0.56}
{'loss': 0.8901, 'grad_norm': 0.2230120152235031, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8172907829284668, 'eval_runtime': 8.8333, 'eval_samples_per_second': 113.094, 'eval_steps_per_second': 7.132, 'epoch': 0.6}
{'loss': 0.9124, 'grad_norm': 0.3907138407230377, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8056943416595459, 'eval_runtime': 8.8632, 'eval_samples_per_second': 112.713, 'eval_steps_per_second': 7.108, 'epoch': 0.64}
{'loss': 0.9089, 'grad_norm': 0.22039422392845154, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7938728332519531, 'eval_runtime': 8.9005, 'eval_samples_per_second': 112.241, 'eval_steps_per_second': 7.078, 'epoch': 0.68}
{'loss': 0.9064, 'grad_norm': 0.2184707075357437, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7876139283180237, 'eval_runtime': 8.9269, 'eval_samples_per_second': 111.909, 'eval_steps_per_second': 7.057, 'epoch': 0.72}
{'loss': 0.895, 'grad_norm': 0.21775946021080017, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7765788435935974, 'eval_runtime': 8.9265, 'eval_samples_per_second': 111.914, 'eval_steps_per_second': 7.058, 'epoch': 0.76}
{'loss': 0.9433, 'grad_norm': 0.22216404974460602, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7645416259765625, 'eval_runtime': 8.9324, 'eval_samples_per_second': 111.84, 'eval_steps_per_second': 7.053, 'epoch': 0.8}
{'loss': 0.8452, 'grad_norm': 0.2573537230491638, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7526744604110718, 'eval_runtime': 8.9776, 'eval_samples_per_second': 111.277, 'eval_steps_per_second': 7.017, 'epoch': 0.84}
{'loss': 0.8614, 'grad_norm': 0.21435266733169556, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7471261620521545, 'eval_runtime': 8.9401, 'eval_samples_per_second': 111.744, 'eval_steps_per_second': 7.047, 'epoch': 0.88}
{'loss': 0.8928, 'grad_norm': 0.21073119342327118, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7406097650527954, 'eval_runtime': 8.9621, 'eval_samples_per_second': 111.47, 'eval_steps_per_second': 7.03, 'epoch': 0.92}
{'loss': 0.8741, 'grad_norm': 0.20973297953605652, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7351274490356445, 'eval_runtime': 8.9835, 'eval_samples_per_second': 111.203, 'eval_steps_per_second': 7.013, 'epoch': 0.96}
{'loss': 0.8701, 'grad_norm': 0.23990844190120697, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7349909543991089, 'eval_runtime': 8.9596, 'eval_samples_per_second': 111.5, 'eval_steps_per_second': 7.032, 'epoch': 1.0}
{'train_runtime': 488.2323, 'train_samples_per_second': 20.48, 'train_steps_per_second': 1.28, 'train_loss': 1.0093379608154296, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2751061916351318, 1.0513885021209717, 0.9986898303031921, 0.9776460528373718, 0.9619477391242981, 0.9495521783828735, 0.9295312166213989, 0.9151220321655273, 0.9033553004264832, 0.8839848637580872, 0.8712210655212402, 0.8555130958557129, 0.8465558886528015, 0.8290509581565857, 0.8172907829284668, 0.8056943416595459, 0.7938728332519531, 0.7876139283180237, 0.7765788435935974, 0.7645416259765625, 0.7526744604110718, 0.7471261620521545, 0.7406097650527954, 0.7351274490356445, 0.7349909543991089], 'performance': [0.65, 0.64]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:40,  2.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 29.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 38.48it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.53it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.68it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.59it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 67.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.65it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.64]
current iteration observed (possibly low-fid or predicted) performance:  1.2480429410934448
current iteration best possible performance (full train run):  0.672
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0960 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 1.1340487923559022
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 1.0509591264253308
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 1.1227478248389509
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.7838031705784344
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.8390493112999374
proposed candidate layer mask is:  tensor([0., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.8292, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.1708, dtype=torch.float64), 0, 32, 0, 1, 0, 0, 0, 128, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.8292, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5940e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.6164e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1708, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.829
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.171
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:14,  6.90it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 53.90it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 59.91it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 64.33it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 73.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:00<00:00, 76.27it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 80.13it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 79.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 78.47it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 73.83it/s]
Evaluation performance at step 25: 0.62
{'loss': 3.581, 'grad_norm': 1.232952356338501, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 2.0914669036865234, 'eval_runtime': 6.8451, 'eval_samples_per_second': 145.943, 'eval_steps_per_second': 9.204, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:29,  1.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 20.13it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:02, 28.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 36.77it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 66.28it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 68.50it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 70.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 46.09it/s]
Evaluation performance at step 50: 0.63
{'loss': 1.4082, 'grad_norm': 0.3363468647003174, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 1.241804599761963, 'eval_runtime': 6.8044, 'eval_samples_per_second': 146.816, 'eval_steps_per_second': 9.259, 'epoch': 0.08}
{'loss': 1.1919, 'grad_norm': 0.458176851272583, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1446040868759155, 'eval_runtime': 6.9297, 'eval_samples_per_second': 144.161, 'eval_steps_per_second': 9.091, 'epoch': 0.12}
{'loss': 1.0857, 'grad_norm': 0.33118245005607605, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0785126686096191, 'eval_runtime': 6.8622, 'eval_samples_per_second': 145.58, 'eval_steps_per_second': 9.181, 'epoch': 0.16}
{'loss': 1.0578, 'grad_norm': 0.29551661014556885, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.038317322731018, 'eval_runtime': 6.8863, 'eval_samples_per_second': 145.071, 'eval_steps_per_second': 9.149, 'epoch': 0.2}
{'loss': 1.0131, 'grad_norm': 0.27225178480148315, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0012916326522827, 'eval_runtime': 6.8864, 'eval_samples_per_second': 145.068, 'eval_steps_per_second': 9.148, 'epoch': 0.24}
{'loss': 1.0009, 'grad_norm': 0.31951940059661865, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9885306358337402, 'eval_runtime': 6.8862, 'eval_samples_per_second': 145.073, 'eval_steps_per_second': 9.149, 'epoch': 0.28}
{'loss': 0.9506, 'grad_norm': 0.2681945562362671, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9844474792480469, 'eval_runtime': 6.8547, 'eval_samples_per_second': 145.74, 'eval_steps_per_second': 9.191, 'epoch': 0.32}
{'loss': 0.96, 'grad_norm': 0.25283029675483704, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9745250344276428, 'eval_runtime': 6.8564, 'eval_samples_per_second': 145.702, 'eval_steps_per_second': 9.188, 'epoch': 0.36}
{'loss': 0.9301, 'grad_norm': 0.25642484426498413, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9686688780784607, 'eval_runtime': 6.8543, 'eval_samples_per_second': 145.749, 'eval_steps_per_second': 9.191, 'epoch': 0.4}
{'loss': 0.9667, 'grad_norm': 0.26338139176368713, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9626372456550598, 'eval_runtime': 6.8594, 'eval_samples_per_second': 145.64, 'eval_steps_per_second': 9.185, 'epoch': 0.44}
{'loss': 0.9634, 'grad_norm': 0.2480902224779129, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9581421613693237, 'eval_runtime': 6.8467, 'eval_samples_per_second': 145.91, 'eval_steps_per_second': 9.202, 'epoch': 0.48}
{'loss': 0.9098, 'grad_norm': 0.26728251576423645, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9575510025024414, 'eval_runtime': 6.8703, 'eval_samples_per_second': 145.409, 'eval_steps_per_second': 9.17, 'epoch': 0.52}
{'loss': 0.9419, 'grad_norm': 0.2713346779346466, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9523401856422424, 'eval_runtime': 6.8612, 'eval_samples_per_second': 145.602, 'eval_steps_per_second': 9.182, 'epoch': 0.56}
{'loss': 0.9477, 'grad_norm': 0.2682666480541229, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9489486813545227, 'eval_runtime': 6.8749, 'eval_samples_per_second': 145.311, 'eval_steps_per_second': 9.164, 'epoch': 0.6}
{'loss': 0.9404, 'grad_norm': 0.24267515540122986, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9463683366775513, 'eval_runtime': 6.865, 'eval_samples_per_second': 145.521, 'eval_steps_per_second': 9.177, 'epoch': 0.64}
{'loss': 0.9256, 'grad_norm': 0.2559538781642914, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9431277513504028, 'eval_runtime': 6.8754, 'eval_samples_per_second': 145.3, 'eval_steps_per_second': 9.163, 'epoch': 0.68}
{'loss': 0.9533, 'grad_norm': 0.27939507365226746, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9421519637107849, 'eval_runtime': 6.865, 'eval_samples_per_second': 145.522, 'eval_steps_per_second': 9.177, 'epoch': 0.72}
{'loss': 0.961, 'grad_norm': 0.2713620364665985, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9395589232444763, 'eval_runtime': 6.8765, 'eval_samples_per_second': 145.277, 'eval_steps_per_second': 9.162, 'epoch': 0.76}
{'loss': 0.8861, 'grad_norm': 0.25197118520736694, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9381779432296753, 'eval_runtime': 6.883, 'eval_samples_per_second': 145.14, 'eval_steps_per_second': 9.153, 'epoch': 0.8}
{'loss': 0.973, 'grad_norm': 0.24917642772197723, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9368562698364258, 'eval_runtime': 6.8722, 'eval_samples_per_second': 145.369, 'eval_steps_per_second': 9.167, 'epoch': 0.84}
{'loss': 0.9384, 'grad_norm': 0.2931042015552521, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9344727396965027, 'eval_runtime': 6.8634, 'eval_samples_per_second': 145.554, 'eval_steps_per_second': 9.179, 'epoch': 0.88}
{'loss': 0.9467, 'grad_norm': 0.28416767716407776, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9332056045532227, 'eval_runtime': 6.871, 'eval_samples_per_second': 145.393, 'eval_steps_per_second': 9.169, 'epoch': 0.92}
{'loss': 0.9291, 'grad_norm': 0.26525044441223145, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9324172139167786, 'eval_runtime': 6.8867, 'eval_samples_per_second': 145.063, 'eval_steps_per_second': 9.148, 'epoch': 0.96}
{'loss': 0.9111, 'grad_norm': 0.30098211765289307, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9320234656333923, 'eval_runtime': 6.9378, 'eval_samples_per_second': 143.994, 'eval_steps_per_second': 9.081, 'epoch': 1.0}
{'train_runtime': 374.7315, 'train_samples_per_second': 26.683, 'train_steps_per_second': 1.668, 'train_loss': 1.0909450317382812, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0914669036865234, 1.241804599761963, 1.1446040868759155, 1.0785126686096191, 1.038317322731018, 1.0012916326522827, 0.9885306358337402, 0.9844474792480469, 0.9745250344276428, 0.9686688780784607, 0.9626372456550598, 0.9581421613693237, 0.9575510025024414, 0.9523401856422424, 0.9489486813545227, 0.9463683366775513, 0.9431277513504028, 0.9421519637107849, 0.9395589232444763, 0.9381779432296753, 0.9368562698364258, 0.9344727396965027, 0.9332056045532227, 0.9324172139167786, 0.9320234656333923], 'performance': [0.62, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:29,  3.40it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 41.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 57.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 54.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 65.17it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 73.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 70.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.2356748580932617
current iteration best possible performance (full train run):  0.5880000000000001
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7354 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.9798740454423344
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 1.124291753987385
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.7565793723408478
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.9458218545895051
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 1.126238962957515
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(1.0000, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, 20, 1, 0, 1, 1, 1, 2, 0.06393295880120368, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5605e-14, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.5007e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6156, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.6393, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 1.0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.06393295880120368,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.06393295880120368
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 2,539,520 || all params: 8,032,800,768 || trainable%: 0.0316
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.94it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 30.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 41.89it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.30it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.79it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.33it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 64.47it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 64.46it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 26.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 40.28it/s]
Evaluation performance at step 25: 0.61
{'loss': 2.5698, 'grad_norm': 0.6787462830543518, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.0100412368774414, 'eval_runtime': 10.2585, 'eval_samples_per_second': 97.382, 'eval_steps_per_second': 6.141, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.48it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.12it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 46.37it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.58it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 52.01it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 55.21it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.84it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.18it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 57.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.90it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.461, 'grad_norm': 0.8651196956634521, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.1462304592132568, 'eval_runtime': 10.3655, 'eval_samples_per_second': 96.377, 'eval_steps_per_second': 6.078, 'epoch': 0.08}
{'loss': 0.9776, 'grad_norm': 0.2775762379169464, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.940026581287384, 'eval_runtime': 10.4387, 'eval_samples_per_second': 95.702, 'eval_steps_per_second': 6.035, 'epoch': 0.12}
{'loss': 0.8998, 'grad_norm': 0.23468941450119019, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8877163529396057, 'eval_runtime': 10.4415, 'eval_samples_per_second': 95.676, 'eval_steps_per_second': 6.034, 'epoch': 0.16}
{'loss': 0.8804, 'grad_norm': 0.2117956280708313, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8744660019874573, 'eval_runtime': 10.4869, 'eval_samples_per_second': 95.262, 'eval_steps_per_second': 6.008, 'epoch': 0.2}
{'loss': 0.8601, 'grad_norm': 0.25874063372612, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8649829030036926, 'eval_runtime': 10.4955, 'eval_samples_per_second': 95.184, 'eval_steps_per_second': 6.003, 'epoch': 0.24}
{'loss': 0.861, 'grad_norm': 0.22845841944217682, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8575302362442017, 'eval_runtime': 10.4819, 'eval_samples_per_second': 95.307, 'eval_steps_per_second': 6.01, 'epoch': 0.28}
{'loss': 0.8355, 'grad_norm': 0.2520473003387451, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8508417010307312, 'eval_runtime': 10.505, 'eval_samples_per_second': 95.098, 'eval_steps_per_second': 5.997, 'epoch': 0.32}
{'loss': 0.8551, 'grad_norm': 0.24022699892520905, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8458715677261353, 'eval_runtime': 10.5022, 'eval_samples_per_second': 95.123, 'eval_steps_per_second': 5.999, 'epoch': 0.36}
{'loss': 0.8469, 'grad_norm': 0.2528109848499298, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8393380045890808, 'eval_runtime': 10.4864, 'eval_samples_per_second': 95.266, 'eval_steps_per_second': 6.008, 'epoch': 0.4}
{'loss': 0.8644, 'grad_norm': 0.22214266657829285, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8340481519699097, 'eval_runtime': 10.4189, 'eval_samples_per_second': 95.884, 'eval_steps_per_second': 6.047, 'epoch': 0.44}
{'loss': 0.8095, 'grad_norm': 0.254820853471756, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8292010426521301, 'eval_runtime': 10.3941, 'eval_samples_per_second': 96.112, 'eval_steps_per_second': 6.061, 'epoch': 0.48}
{'loss': 0.8368, 'grad_norm': 0.24199306964874268, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8248559236526489, 'eval_runtime': 10.355, 'eval_samples_per_second': 96.476, 'eval_steps_per_second': 6.084, 'epoch': 0.52}
{'loss': 0.8354, 'grad_norm': 0.29461395740509033, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8215094804763794, 'eval_runtime': 10.3471, 'eval_samples_per_second': 96.549, 'eval_steps_per_second': 6.089, 'epoch': 0.56}
{'loss': 0.8213, 'grad_norm': 0.29334431886672974, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8175675868988037, 'eval_runtime': 10.3525, 'eval_samples_per_second': 96.499, 'eval_steps_per_second': 6.086, 'epoch': 0.6}
{'loss': 0.8355, 'grad_norm': 0.3485560119152069, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8144748210906982, 'eval_runtime': 10.3557, 'eval_samples_per_second': 96.469, 'eval_steps_per_second': 6.084, 'epoch': 0.64}
{'loss': 0.8185, 'grad_norm': 0.28508949279785156, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8104689717292786, 'eval_runtime': 10.3474, 'eval_samples_per_second': 96.546, 'eval_steps_per_second': 6.088, 'epoch': 0.68}
{'loss': 0.8271, 'grad_norm': 0.2627047598361969, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8073627948760986, 'eval_runtime': 10.3404, 'eval_samples_per_second': 96.611, 'eval_steps_per_second': 6.093, 'epoch': 0.72}
{'loss': 0.8064, 'grad_norm': 0.3115055561065674, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8047696352005005, 'eval_runtime': 10.3409, 'eval_samples_per_second': 96.607, 'eval_steps_per_second': 6.092, 'epoch': 0.76}
{'loss': 0.8025, 'grad_norm': 0.2806586027145386, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8022121787071228, 'eval_runtime': 10.3315, 'eval_samples_per_second': 96.695, 'eval_steps_per_second': 6.098, 'epoch': 0.8}
{'loss': 0.8108, 'grad_norm': 0.32152917981147766, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8005639910697937, 'eval_runtime': 10.3423, 'eval_samples_per_second': 96.594, 'eval_steps_per_second': 6.091, 'epoch': 0.84}
{'loss': 0.8123, 'grad_norm': 0.2975890636444092, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.798926830291748, 'eval_runtime': 10.3433, 'eval_samples_per_second': 96.584, 'eval_steps_per_second': 6.091, 'epoch': 0.88}
{'loss': 0.8244, 'grad_norm': 0.3201512098312378, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7975553870201111, 'eval_runtime': 10.345, 'eval_samples_per_second': 96.568, 'eval_steps_per_second': 6.09, 'epoch': 0.92}
{'loss': 0.8056, 'grad_norm': 0.31526070833206177, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7966363430023193, 'eval_runtime': 10.3734, 'eval_samples_per_second': 96.304, 'eval_steps_per_second': 6.073, 'epoch': 0.96}
{'loss': 0.836, 'grad_norm': 0.3283572494983673, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7962697744369507, 'eval_runtime': 10.3954, 'eval_samples_per_second': 96.1, 'eval_steps_per_second': 6.06, 'epoch': 1.0}
{'train_runtime': 516.3468, 'train_samples_per_second': 19.365, 'train_steps_per_second': 1.21, 'train_loss': 0.9357436401367187, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0100412368774414, 1.1462304592132568, 0.940026581287384, 0.8877163529396057, 0.8744660019874573, 0.8649829030036926, 0.8575302362442017, 0.8508417010307312, 0.8458715677261353, 0.8393380045890808, 0.8340481519699097, 0.8292010426521301, 0.8248559236526489, 0.8215094804763794, 0.8175675868988037, 0.8144748210906982, 0.8104689717292786, 0.8073627948760986, 0.8047696352005005, 0.8022121787071228, 0.8005639910697937, 0.798926830291748, 0.7975553870201111, 0.7966363430023193, 0.7962697744369507], 'performance': [0.61, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:28,  3.53it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 52.15it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 60.88it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 66.74it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 71.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 69.51it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  0.6988314986228943
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8517 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.6243948845299054
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 0.732957271582888
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.7157315819208273
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 1.0587332343539788
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 1.0746114729879246
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, 0, tensor(1., dtype=torch.float64), 32, 0, 0, 1, 1, 1, 2, 4.163336342344338e-18, 1.480000019073487, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(8.6183e-17, dtype=torch.float64), tensor(1.0910e-16, dtype=torch.float64), tensor(1.6653e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.0247e-17, dtype=torch.float64), tensor(6.0943e-17, dtype=torch.float64), tensor(4.4168e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(4.1633e-17, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 1.0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (4.163336342344338e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (1.480000019073487,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  2
lora dropout:  4.163336342344338e-18
lora alpha:  1.480000019073487
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,538,944 || all params: 8,033,800,192 || trainable%: 0.0441
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.71it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.45it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 46.64it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.57it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 55.36it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 62.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.73it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 63.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 10.92it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 24.75it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.6794, 'grad_norm': 1.6587523221969604, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.414043426513672, 'eval_runtime': 7.3155, 'eval_samples_per_second': 136.696, 'eval_steps_per_second': 8.612, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.83it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.33it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.65it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 49.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 52.33it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 56.43it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 56.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 62.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.60it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.11it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 62.28it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 51.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.37it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.5507, 'grad_norm': 1.8650673627853394, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.085687518119812, 'eval_runtime': 7.3083, 'eval_samples_per_second': 136.831, 'eval_steps_per_second': 8.62, 'epoch': 0.08}
{'loss': 0.985, 'grad_norm': 0.4702582359313965, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8847055435180664, 'eval_runtime': 7.3296, 'eval_samples_per_second': 136.433, 'eval_steps_per_second': 8.595, 'epoch': 0.12}
{'loss': 0.856, 'grad_norm': 0.2574552595615387, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8091579079627991, 'eval_runtime': 7.3453, 'eval_samples_per_second': 136.142, 'eval_steps_per_second': 8.577, 'epoch': 0.16}
{'loss': 0.7976, 'grad_norm': 0.30811989307403564, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7862553000450134, 'eval_runtime': 7.4227, 'eval_samples_per_second': 134.722, 'eval_steps_per_second': 8.487, 'epoch': 0.2}
{'loss': 0.7848, 'grad_norm': 0.317046582698822, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7610086798667908, 'eval_runtime': 7.4484, 'eval_samples_per_second': 134.258, 'eval_steps_per_second': 8.458, 'epoch': 0.24}
{'loss': 0.7628, 'grad_norm': 0.3619537651538849, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7327660322189331, 'eval_runtime': 7.4398, 'eval_samples_per_second': 134.412, 'eval_steps_per_second': 8.468, 'epoch': 0.28}
{'loss': 0.7352, 'grad_norm': 0.4512309730052948, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6931576728820801, 'eval_runtime': 7.4664, 'eval_samples_per_second': 133.933, 'eval_steps_per_second': 8.438, 'epoch': 0.32}
{'loss': 0.6967, 'grad_norm': 0.5939388871192932, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.648548424243927, 'eval_runtime': 7.4808, 'eval_samples_per_second': 133.676, 'eval_steps_per_second': 8.422, 'epoch': 0.36}
{'loss': 0.6357, 'grad_norm': 0.7686590552330017, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6025415062904358, 'eval_runtime': 7.4633, 'eval_samples_per_second': 133.988, 'eval_steps_per_second': 8.441, 'epoch': 0.4}
{'loss': 0.6121, 'grad_norm': 0.8592535257339478, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5491313338279724, 'eval_runtime': 7.4525, 'eval_samples_per_second': 134.183, 'eval_steps_per_second': 8.454, 'epoch': 0.44}
{'loss': 0.549, 'grad_norm': 0.7983654737472534, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4933672547340393, 'eval_runtime': 7.4458, 'eval_samples_per_second': 134.303, 'eval_steps_per_second': 8.461, 'epoch': 0.48}
{'loss': 0.5466, 'grad_norm': 0.9054643511772156, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4589412808418274, 'eval_runtime': 7.3994, 'eval_samples_per_second': 135.146, 'eval_steps_per_second': 8.514, 'epoch': 0.52}
{'loss': 0.4802, 'grad_norm': 1.070246696472168, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.41490882635116577, 'eval_runtime': 7.401, 'eval_samples_per_second': 135.118, 'eval_steps_per_second': 8.512, 'epoch': 0.56}
{'loss': 0.425, 'grad_norm': 1.1243243217468262, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.36491575837135315, 'eval_runtime': 7.3701, 'eval_samples_per_second': 135.683, 'eval_steps_per_second': 8.548, 'epoch': 0.6}
{'loss': 0.3951, 'grad_norm': 1.1166507005691528, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.32384058833122253, 'eval_runtime': 7.3524, 'eval_samples_per_second': 136.01, 'eval_steps_per_second': 8.569, 'epoch': 0.64}
{'loss': 0.3616, 'grad_norm': 1.1085433959960938, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.287072092294693, 'eval_runtime': 7.3503, 'eval_samples_per_second': 136.048, 'eval_steps_per_second': 8.571, 'epoch': 0.68}
{'loss': 0.3332, 'grad_norm': 1.1505320072174072, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.25691527128219604, 'eval_runtime': 7.3562, 'eval_samples_per_second': 135.939, 'eval_steps_per_second': 8.564, 'epoch': 0.72}
{'loss': 0.2996, 'grad_norm': 1.2288548946380615, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2379187047481537, 'eval_runtime': 7.3534, 'eval_samples_per_second': 135.992, 'eval_steps_per_second': 8.567, 'epoch': 0.76}
{'loss': 0.2687, 'grad_norm': 1.288453459739685, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.2141718566417694, 'eval_runtime': 7.3627, 'eval_samples_per_second': 135.82, 'eval_steps_per_second': 8.557, 'epoch': 0.8}
{'loss': 0.2433, 'grad_norm': 0.9613176584243774, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.1990566998720169, 'eval_runtime': 7.3587, 'eval_samples_per_second': 135.894, 'eval_steps_per_second': 8.561, 'epoch': 0.84}
{'loss': 0.226, 'grad_norm': 1.022153615951538, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.18630895018577576, 'eval_runtime': 7.3625, 'eval_samples_per_second': 135.824, 'eval_steps_per_second': 8.557, 'epoch': 0.88}
{'loss': 0.2218, 'grad_norm': 1.0400654077529907, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.1758306920528412, 'eval_runtime': 7.3672, 'eval_samples_per_second': 135.736, 'eval_steps_per_second': 8.551, 'epoch': 0.92}
{'loss': 0.2243, 'grad_norm': 0.98360276222229, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.16765888035297394, 'eval_runtime': 7.3581, 'eval_samples_per_second': 135.904, 'eval_steps_per_second': 8.562, 'epoch': 0.96}
{'loss': 0.1967, 'grad_norm': 1.4125914573669434, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.16469787061214447, 'eval_runtime': 7.3655, 'eval_samples_per_second': 135.767, 'eval_steps_per_second': 8.553, 'epoch': 1.0}
{'train_runtime': 424.8143, 'train_samples_per_second': 23.54, 'train_steps_per_second': 1.471, 'train_loss': 0.6746877151489258, 'epoch': 1.0}
train_results:  {'eval_loss': [2.414043426513672, 1.085687518119812, 0.8847055435180664, 0.8091579079627991, 0.7862553000450134, 0.7610086798667908, 0.7327660322189331, 0.6931576728820801, 0.648548424243927, 0.6025415062904358, 0.5491313338279724, 0.4933672547340393, 0.4589412808418274, 0.41490882635116577, 0.36491575837135315, 0.32384058833122253, 0.287072092294693, 0.25691527128219604, 0.2379187047481537, 0.2141718566417694, 0.1990566998720169, 0.18630895018577576, 0.1758306920528412, 0.16765888035297394, 0.16469787061214447], 'performance': [0.61, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.99it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.68it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.06it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.92it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.04it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  0.7528432011604309
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8126 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.9102223690218814
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.7551942184132837
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.6299199652433332
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.6562511393210589
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.793941021992367
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7880, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2120, dtype=torch.float64), 0, 32, 0, 1, 1, 0, 0, 128, 9.280208701726489e-19, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.7880, dtype=torch.float64), tensor(3.3920e-17, dtype=torch.float64), tensor(1.6387e-19, dtype=torch.float64), tensor(2.2333e-17, dtype=torch.float64), tensor(2.8642e-17, dtype=torch.float64), tensor(1.7881e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2120, dtype=torch.float64), tensor(9.1213e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(9.2802e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.788
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.212
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (9.280208701726489e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  128
lora dropout:  9.280208701726489e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  6.14it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.03it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 48.49it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 53.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 56.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 61.12it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 64.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 68.57it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 67.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 67.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.72it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.9028, 'grad_norm': 0.583198070526123, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.388246774673462, 'eval_runtime': 7.0228, 'eval_samples_per_second': 142.251, 'eval_steps_per_second': 8.971, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.85it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 45.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 50.41it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.13it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 60.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 63.27it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.18it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 63.53it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.09it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 63.17it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 64.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 59.17it/s]
Evaluation performance at step 50: 0.66
{'loss': 1.2714, 'grad_norm': 0.26075059175491333, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.66}
{'eval_loss': 1.1842973232269287, 'eval_runtime': 7.1027, 'eval_samples_per_second': 140.651, 'eval_steps_per_second': 8.87, 'epoch': 0.08}
{'loss': 1.125, 'grad_norm': 0.26925885677337646, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.070132851600647, 'eval_runtime': 7.0832, 'eval_samples_per_second': 141.038, 'eval_steps_per_second': 8.894, 'epoch': 0.12}
{'loss': 1.0508, 'grad_norm': 0.45907849073410034, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0106412172317505, 'eval_runtime': 7.063, 'eval_samples_per_second': 141.442, 'eval_steps_per_second': 8.92, 'epoch': 0.16}
{'loss': 0.9571, 'grad_norm': 0.2177118957042694, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9676780700683594, 'eval_runtime': 7.066, 'eval_samples_per_second': 141.381, 'eval_steps_per_second': 8.916, 'epoch': 0.2}
{'loss': 0.9511, 'grad_norm': 0.18854381144046783, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.952225923538208, 'eval_runtime': 7.074, 'eval_samples_per_second': 141.222, 'eval_steps_per_second': 8.906, 'epoch': 0.24}
{'loss': 0.9592, 'grad_norm': 0.21496716141700745, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9429494142532349, 'eval_runtime': 7.081, 'eval_samples_per_second': 141.081, 'eval_steps_per_second': 8.897, 'epoch': 0.28}
{'loss': 0.9656, 'grad_norm': 0.19510318338871002, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9319052696228027, 'eval_runtime': 7.0923, 'eval_samples_per_second': 140.856, 'eval_steps_per_second': 8.883, 'epoch': 0.32}
{'loss': 0.9693, 'grad_norm': 0.17606157064437866, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9212537407875061, 'eval_runtime': 7.1051, 'eval_samples_per_second': 140.604, 'eval_steps_per_second': 8.867, 'epoch': 0.36}
{'loss': 0.9202, 'grad_norm': 0.18244726955890656, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9115932583808899, 'eval_runtime': 7.0903, 'eval_samples_per_second': 140.898, 'eval_steps_per_second': 8.885, 'epoch': 0.4}
{'loss': 0.9193, 'grad_norm': 0.20889808237552643, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8990185260772705, 'eval_runtime': 7.101, 'eval_samples_per_second': 140.684, 'eval_steps_per_second': 8.872, 'epoch': 0.44}
{'loss': 0.9641, 'grad_norm': 0.22067159414291382, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8918001055717468, 'eval_runtime': 7.1039, 'eval_samples_per_second': 140.627, 'eval_steps_per_second': 8.868, 'epoch': 0.48}
{'loss': 0.9532, 'grad_norm': 0.22531239688396454, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8871411085128784, 'eval_runtime': 7.121, 'eval_samples_per_second': 140.29, 'eval_steps_per_second': 8.847, 'epoch': 0.52}
{'loss': 0.9089, 'grad_norm': 0.18861232697963715, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8788540959358215, 'eval_runtime': 7.0973, 'eval_samples_per_second': 140.758, 'eval_steps_per_second': 8.877, 'epoch': 0.56}
{'loss': 0.8922, 'grad_norm': 0.20754730701446533, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8661736845970154, 'eval_runtime': 7.1155, 'eval_samples_per_second': 140.398, 'eval_steps_per_second': 8.854, 'epoch': 0.6}
{'loss': 0.8837, 'grad_norm': 0.2204434722661972, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8604968786239624, 'eval_runtime': 7.1224, 'eval_samples_per_second': 140.263, 'eval_steps_per_second': 8.845, 'epoch': 0.64}
{'loss': 0.912, 'grad_norm': 0.20478665828704834, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8530770540237427, 'eval_runtime': 7.234, 'eval_samples_per_second': 138.098, 'eval_steps_per_second': 8.709, 'epoch': 0.68}
{'loss': 0.8736, 'grad_norm': 0.2209034264087677, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8446894288063049, 'eval_runtime': 7.2178, 'eval_samples_per_second': 138.408, 'eval_steps_per_second': 8.728, 'epoch': 0.72}
{'loss': 0.8838, 'grad_norm': 0.23070086538791656, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.838379979133606, 'eval_runtime': 7.2168, 'eval_samples_per_second': 138.427, 'eval_steps_per_second': 8.73, 'epoch': 0.76}
{'loss': 0.8702, 'grad_norm': 0.24369411170482635, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8314537405967712, 'eval_runtime': 7.2162, 'eval_samples_per_second': 138.438, 'eval_steps_per_second': 8.73, 'epoch': 0.8}
{'loss': 0.8854, 'grad_norm': 0.22397249937057495, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8247873187065125, 'eval_runtime': 7.2083, 'eval_samples_per_second': 138.591, 'eval_steps_per_second': 8.74, 'epoch': 0.84}
{'loss': 0.9002, 'grad_norm': 0.23286667466163635, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8188657164573669, 'eval_runtime': 7.2116, 'eval_samples_per_second': 138.526, 'eval_steps_per_second': 8.736, 'epoch': 0.88}
{'loss': 0.8557, 'grad_norm': 0.22887815535068512, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.814447283744812, 'eval_runtime': 7.2174, 'eval_samples_per_second': 138.416, 'eval_steps_per_second': 8.729, 'epoch': 0.92}
{'loss': 0.8352, 'grad_norm': 0.23755188286304474, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8124780654907227, 'eval_runtime': 7.2145, 'eval_samples_per_second': 138.471, 'eval_steps_per_second': 8.732, 'epoch': 0.96}
{'loss': 0.8601, 'grad_norm': 0.23986689746379852, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8114556074142456, 'eval_runtime': 7.2038, 'eval_samples_per_second': 138.676, 'eval_steps_per_second': 8.745, 'epoch': 1.0}
{'train_runtime': 402.3117, 'train_samples_per_second': 24.854, 'train_steps_per_second': 1.554, 'train_loss': 1.0188051666259765, 'epoch': 1.0}
train_results:  {'eval_loss': [1.388246774673462, 1.1842973232269287, 1.070132851600647, 1.0106412172317505, 0.9676780700683594, 0.952225923538208, 0.9429494142532349, 0.9319052696228027, 0.9212537407875061, 0.9115932583808899, 0.8990185260772705, 0.8918001055717468, 0.8871411085128784, 0.8788540959358215, 0.8661736845970154, 0.8604968786239624, 0.8530770540237427, 0.8446894288063049, 0.838379979133606, 0.8314537405967712, 0.8247873187065125, 0.8188657164573669, 0.814447283744812, 0.8124780654907227, 0.8114556074142456], 'performance': [0.64, 0.66]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.06it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.09it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.87it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 58.72it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.66]
current iteration observed (possibly low-fid or predicted) performance:  1.2379395961761475
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4972 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 1.08949196699982
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.7845829078254383
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 1.0788671672345704
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 1.0828524511918358
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 1.1066364981291672
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7827, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2173, dtype=torch.float64), 0, 32, 0, 0, 1, 1, 1, 128, 2.081668171172169e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7827, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1384e-17, dtype=torch.float64), tensor(5.6885e-18, dtype=torch.float64), tensor(0.2173, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.0817e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.783
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.217
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.081668171172169e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.081668171172169e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.41it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 41.79it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.91it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.99it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.83it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 59.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.38it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.6991, 'grad_norm': 0.4418606460094452, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.222192406654358, 'eval_runtime': 7.6621, 'eval_samples_per_second': 130.382, 'eval_steps_per_second': 8.222, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.41it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.33it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.60it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.27it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.58it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.89it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.95it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.12it/s]
Evaluation performance at step 50: 0.65
{'loss': 1.1028, 'grad_norm': 0.5318027138710022, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.65}
{'eval_loss': 1.0031094551086426, 'eval_runtime': 7.6732, 'eval_samples_per_second': 130.194, 'eval_steps_per_second': 8.21, 'epoch': 0.08}
{'loss': 0.9975, 'grad_norm': 0.18767417967319489, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9557186961174011, 'eval_runtime': 7.7053, 'eval_samples_per_second': 129.651, 'eval_steps_per_second': 8.176, 'epoch': 0.12}
{'loss': 0.9671, 'grad_norm': 0.2102063000202179, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9379745721817017, 'eval_runtime': 7.684, 'eval_samples_per_second': 130.011, 'eval_steps_per_second': 8.199, 'epoch': 0.16}
{'loss': 0.9695, 'grad_norm': 0.2237144410610199, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9192233085632324, 'eval_runtime': 7.6728, 'eval_samples_per_second': 130.201, 'eval_steps_per_second': 8.211, 'epoch': 0.2}
{'loss': 0.9271, 'grad_norm': 0.19733338057994843, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9046721458435059, 'eval_runtime': 7.6823, 'eval_samples_per_second': 130.04, 'eval_steps_per_second': 8.201, 'epoch': 0.24}
{'loss': 0.9622, 'grad_norm': 0.1991642564535141, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8876592516899109, 'eval_runtime': 7.6806, 'eval_samples_per_second': 130.068, 'eval_steps_per_second': 8.203, 'epoch': 0.28}
{'loss': 0.9398, 'grad_norm': 0.23284812271595, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8741470575332642, 'eval_runtime': 7.6934, 'eval_samples_per_second': 129.852, 'eval_steps_per_second': 8.189, 'epoch': 0.32}
{'loss': 0.9448, 'grad_norm': 0.18713630735874176, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8561643958091736, 'eval_runtime': 7.6851, 'eval_samples_per_second': 129.991, 'eval_steps_per_second': 8.198, 'epoch': 0.36}
{'loss': 0.9277, 'grad_norm': 0.19848711788654327, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.840781569480896, 'eval_runtime': 7.6943, 'eval_samples_per_second': 129.837, 'eval_steps_per_second': 8.188, 'epoch': 0.4}
{'loss': 0.8896, 'grad_norm': 0.21586845815181732, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8281708359718323, 'eval_runtime': 7.6842, 'eval_samples_per_second': 130.007, 'eval_steps_per_second': 8.199, 'epoch': 0.44}
{'loss': 0.8953, 'grad_norm': 0.20350109040737152, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.813823938369751, 'eval_runtime': 7.6799, 'eval_samples_per_second': 130.079, 'eval_steps_per_second': 8.203, 'epoch': 0.48}
{'loss': 0.8883, 'grad_norm': 0.22289226949214935, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8039520382881165, 'eval_runtime': 7.707, 'eval_samples_per_second': 129.623, 'eval_steps_per_second': 8.174, 'epoch': 0.52}
{'loss': 0.9022, 'grad_norm': 0.20262300968170166, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7901105284690857, 'eval_runtime': 7.7001, 'eval_samples_per_second': 129.738, 'eval_steps_per_second': 8.182, 'epoch': 0.56}
{'loss': 0.8756, 'grad_norm': 0.1954900622367859, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7789373993873596, 'eval_runtime': 7.7317, 'eval_samples_per_second': 129.208, 'eval_steps_per_second': 8.148, 'epoch': 0.6}
{'loss': 0.8713, 'grad_norm': 0.2208305448293686, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7680302858352661, 'eval_runtime': 7.8114, 'eval_samples_per_second': 127.889, 'eval_steps_per_second': 8.065, 'epoch': 0.64}
{'loss': 0.8702, 'grad_norm': 0.20302416384220123, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7571243047714233, 'eval_runtime': 7.7614, 'eval_samples_per_second': 128.714, 'eval_steps_per_second': 8.117, 'epoch': 0.68}
{'loss': 0.8535, 'grad_norm': 0.21820704638957977, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.745704710483551, 'eval_runtime': 7.7373, 'eval_samples_per_second': 129.115, 'eval_steps_per_second': 8.142, 'epoch': 0.72}
{'loss': 0.8686, 'grad_norm': 0.2156587690114975, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7339895963668823, 'eval_runtime': 7.7311, 'eval_samples_per_second': 129.218, 'eval_steps_per_second': 8.149, 'epoch': 0.76}
{'loss': 0.8848, 'grad_norm': 0.24016213417053223, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7210781574249268, 'eval_runtime': 7.7336, 'eval_samples_per_second': 129.177, 'eval_steps_per_second': 8.146, 'epoch': 0.8}
{'loss': 0.8844, 'grad_norm': 0.24643918871879578, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7119181752204895, 'eval_runtime': 7.7296, 'eval_samples_per_second': 129.243, 'eval_steps_per_second': 8.15, 'epoch': 0.84}
{'loss': 0.8432, 'grad_norm': 0.2185066044330597, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7081464529037476, 'eval_runtime': 7.7302, 'eval_samples_per_second': 129.233, 'eval_steps_per_second': 8.15, 'epoch': 0.88}
{'loss': 0.7986, 'grad_norm': 0.2174319475889206, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6984702944755554, 'eval_runtime': 7.7246, 'eval_samples_per_second': 129.327, 'eval_steps_per_second': 8.156, 'epoch': 0.92}
{'loss': 0.7991, 'grad_norm': 0.25142595171928406, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6946422457695007, 'eval_runtime': 7.731, 'eval_samples_per_second': 129.219, 'eval_steps_per_second': 8.149, 'epoch': 0.96}
{'loss': 0.8556, 'grad_norm': 0.26075825095176697, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6934521198272705, 'eval_runtime': 7.7292, 'eval_samples_per_second': 129.25, 'eval_steps_per_second': 8.151, 'epoch': 1.0}
{'train_runtime': 438.0972, 'train_samples_per_second': 22.824, 'train_steps_per_second': 1.427, 'train_loss': 0.9767084167480469, 'epoch': 1.0}
train_results:  {'eval_loss': [1.222192406654358, 1.0031094551086426, 0.9557186961174011, 0.9379745721817017, 0.9192233085632324, 0.9046721458435059, 0.8876592516899109, 0.8741470575332642, 0.8561643958091736, 0.840781569480896, 0.8281708359718323, 0.813823938369751, 0.8039520382881165, 0.7901105284690857, 0.7789373993873596, 0.7680302858352661, 0.7571243047714233, 0.745704710483551, 0.7339895963668823, 0.7210781574249268, 0.7119181752204895, 0.7081464529037476, 0.6984702944755554, 0.6946422457695007, 0.6934521198272705], 'performance': [0.63, 0.65]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.54it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.85it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.65]
current iteration observed (possibly low-fid or predicted) performance:  1.2422839403152466
current iteration best possible performance (full train run):  0.672
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3839 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 1.1512778845094784
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.8335509154325025
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.7291269790635968
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 1.016082072587838
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.633141811003675
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(1.0000, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 2, 1.2767564783189302e-16, 1.4800000190734883, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(6.0950e-16, dtype=torch.float64), tensor(1.7472e-15, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.8274e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.2768e-15, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 1.0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (1.2767564783189302e-16,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734883,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  2
lora dropout:  1.2767564783189302e-16
lora alpha:  1.4800000190734883
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.65it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.43it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.06it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 35.06it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.03it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 39.00it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 40.57it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 46.18it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 44.21it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 48.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 48.14it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 48.25it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 48.06it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 42.84it/s]
Evaluation performance at step 25: 0.61
{'loss': 5.1326, 'grad_norm': 3.589401960372925, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 3.2547266483306885, 'eval_runtime': 4.0358, 'eval_samples_per_second': 247.535, 'eval_steps_per_second': 15.61, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.58it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.40it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.09it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 45.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 47.94it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 50.11it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 49.90it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 50.79it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 54.60it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.45it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.8994, 'grad_norm': 3.2016496658325195, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.187986969947815, 'eval_runtime': 3.6573, 'eval_samples_per_second': 273.151, 'eval_steps_per_second': 17.226, 'epoch': 0.08}
{'loss': 0.9782, 'grad_norm': 1.1884639263153076, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8157874941825867, 'eval_runtime': 3.6861, 'eval_samples_per_second': 271.015, 'eval_steps_per_second': 17.091, 'epoch': 0.12}
{'loss': 0.7611, 'grad_norm': 0.3547326624393463, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7432844042778015, 'eval_runtime': 3.6657, 'eval_samples_per_second': 272.523, 'eval_steps_per_second': 17.186, 'epoch': 0.16}
{'loss': 0.7146, 'grad_norm': 0.3205818235874176, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7305029630661011, 'eval_runtime': 3.6811, 'eval_samples_per_second': 271.386, 'eval_steps_per_second': 17.114, 'epoch': 0.2}
{'loss': 0.7288, 'grad_norm': 0.31482917070388794, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7256587743759155, 'eval_runtime': 3.6563, 'eval_samples_per_second': 273.226, 'eval_steps_per_second': 17.23, 'epoch': 0.24}
{'loss': 0.6918, 'grad_norm': 0.31549403071403503, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7130573987960815, 'eval_runtime': 3.6461, 'eval_samples_per_second': 273.988, 'eval_steps_per_second': 17.279, 'epoch': 0.28}
{'loss': 0.7193, 'grad_norm': 0.38513830304145813, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7036706209182739, 'eval_runtime': 3.6495, 'eval_samples_per_second': 273.734, 'eval_steps_per_second': 17.263, 'epoch': 0.32}
{'loss': 0.7032, 'grad_norm': 0.3631090521812439, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6993573307991028, 'eval_runtime': 3.6303, 'eval_samples_per_second': 275.184, 'eval_steps_per_second': 17.354, 'epoch': 0.36}
{'loss': 0.7018, 'grad_norm': 0.3436080515384674, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.689947783946991, 'eval_runtime': 3.6212, 'eval_samples_per_second': 275.877, 'eval_steps_per_second': 17.398, 'epoch': 0.4}
{'loss': 0.7033, 'grad_norm': 0.36791667342185974, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6831722855567932, 'eval_runtime': 3.6301, 'eval_samples_per_second': 275.2, 'eval_steps_per_second': 17.355, 'epoch': 0.44}
{'loss': 0.678, 'grad_norm': 0.4259888231754303, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.673188328742981, 'eval_runtime': 3.6281, 'eval_samples_per_second': 275.347, 'eval_steps_per_second': 17.364, 'epoch': 0.48}
{'loss': 0.7096, 'grad_norm': 0.38745561242103577, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6695482134819031, 'eval_runtime': 3.6253, 'eval_samples_per_second': 275.567, 'eval_steps_per_second': 17.378, 'epoch': 0.52}
{'loss': 0.6942, 'grad_norm': 0.4875141680240631, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6611088514328003, 'eval_runtime': 3.6126, 'eval_samples_per_second': 276.531, 'eval_steps_per_second': 17.439, 'epoch': 0.56}
{'loss': 0.6847, 'grad_norm': 0.4138927757740021, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6537662148475647, 'eval_runtime': 3.6134, 'eval_samples_per_second': 276.47, 'eval_steps_per_second': 17.435, 'epoch': 0.6}
{'loss': 0.6822, 'grad_norm': 0.5297021865844727, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6473743915557861, 'eval_runtime': 3.6114, 'eval_samples_per_second': 276.626, 'eval_steps_per_second': 17.445, 'epoch': 0.64}
{'loss': 0.6882, 'grad_norm': 0.47647106647491455, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6410261392593384, 'eval_runtime': 3.6131, 'eval_samples_per_second': 276.491, 'eval_steps_per_second': 17.436, 'epoch': 0.68}
{'loss': 0.6673, 'grad_norm': 0.5980938076972961, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6340914964675903, 'eval_runtime': 3.6149, 'eval_samples_per_second': 276.356, 'eval_steps_per_second': 17.428, 'epoch': 0.72}
{'loss': 0.6528, 'grad_norm': 0.5230445265769958, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6289618015289307, 'eval_runtime': 3.6142, 'eval_samples_per_second': 276.409, 'eval_steps_per_second': 17.431, 'epoch': 0.76}
{'loss': 0.6724, 'grad_norm': 0.5684268474578857, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6233190298080444, 'eval_runtime': 3.6136, 'eval_samples_per_second': 276.458, 'eval_steps_per_second': 17.434, 'epoch': 0.8}
{'loss': 0.6723, 'grad_norm': 0.5566569566726685, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6212366223335266, 'eval_runtime': 3.6138, 'eval_samples_per_second': 276.438, 'eval_steps_per_second': 17.433, 'epoch': 0.84}
{'loss': 0.6572, 'grad_norm': 0.5877745747566223, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6152170300483704, 'eval_runtime': 3.6134, 'eval_samples_per_second': 276.468, 'eval_steps_per_second': 17.435, 'epoch': 0.88}
{'loss': 0.6484, 'grad_norm': 0.47185927629470825, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6120640635490417, 'eval_runtime': 3.6148, 'eval_samples_per_second': 276.361, 'eval_steps_per_second': 17.428, 'epoch': 0.92}
{'loss': 0.6807, 'grad_norm': 0.5918586850166321, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6105796098709106, 'eval_runtime': 3.615, 'eval_samples_per_second': 276.349, 'eval_steps_per_second': 17.427, 'epoch': 0.96}
{'loss': 0.661, 'grad_norm': 0.5237861275672913, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6107243895530701, 'eval_runtime': 3.6349, 'eval_samples_per_second': 274.839, 'eval_steps_per_second': 17.332, 'epoch': 1.0}
{'train_runtime': 247.8708, 'train_samples_per_second': 40.34, 'train_steps_per_second': 2.521, 'train_loss': 0.9273084381103516, 'epoch': 1.0}
train_results:  {'eval_loss': [3.2547266483306885, 1.187986969947815, 0.8157874941825867, 0.7432844042778015, 0.7305029630661011, 0.7256587743759155, 0.7130573987960815, 0.7036706209182739, 0.6993573307991028, 0.689947783946991, 0.6831722855567932, 0.673188328742981, 0.6695482134819031, 0.6611088514328003, 0.6537662148475647, 0.6473743915557861, 0.6410261392593384, 0.6340914964675903, 0.6289618015289307, 0.6233190298080444, 0.6212366223335266, 0.6152170300483704, 0.6120640635490417, 0.6105796098709106, 0.6107243895530701], 'performance': [0.61, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.18it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.18it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 55.07it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.85it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.64it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  0.7603490352630615
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0342 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.8656112645307704
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.8892185670306535
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 1.100028618015988
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.624071165486539
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 1.060074550902145
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7578, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2422, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.160699181775697e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7578, dtype=torch.float64), tensor(1.7743e-17, dtype=torch.float64), tensor(3.6194e-17, dtype=torch.float64), tensor(2.7341e-17, dtype=torch.float64), tensor(8.5030e-18, dtype=torch.float64), tensor(0.2422, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.8776e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.1607e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.758
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.242
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.160699181775697e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.160699181775697e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.06it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.29it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.47it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.04it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.67it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.03it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.59it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.87it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.72it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.7897, 'grad_norm': 0.6059232354164124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.1287202835083008, 'eval_runtime': 5.3672, 'eval_samples_per_second': 186.129, 'eval_steps_per_second': 11.738, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.04it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.09it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.89it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.44it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.15it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 48.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.17it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 52.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.63it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 52.59it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.56it/s]
Evaluation performance at step 50: 0.64
{'loss': 0.9661, 'grad_norm': 0.4819391965866089, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.64}
{'eval_loss': 0.8850432634353638, 'eval_runtime': 5.367, 'eval_samples_per_second': 186.138, 'eval_steps_per_second': 11.738, 'epoch': 0.08}
{'loss': 0.858, 'grad_norm': 0.22443994879722595, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8474772572517395, 'eval_runtime': 5.3818, 'eval_samples_per_second': 185.624, 'eval_steps_per_second': 11.706, 'epoch': 0.12}
{'loss': 0.848, 'grad_norm': 0.23880049586296082, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8258076906204224, 'eval_runtime': 5.4173, 'eval_samples_per_second': 184.41, 'eval_steps_per_second': 11.629, 'epoch': 0.16}
{'loss': 0.8264, 'grad_norm': 0.2051416039466858, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8059683442115784, 'eval_runtime': 5.3914, 'eval_samples_per_second': 185.294, 'eval_steps_per_second': 11.685, 'epoch': 0.2}
{'loss': 0.8187, 'grad_norm': 0.20829766988754272, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.781482994556427, 'eval_runtime': 5.3799, 'eval_samples_per_second': 185.692, 'eval_steps_per_second': 11.71, 'epoch': 0.24}
{'loss': 0.8096, 'grad_norm': 0.2599988877773285, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7644504904747009, 'eval_runtime': 5.3802, 'eval_samples_per_second': 185.683, 'eval_steps_per_second': 11.71, 'epoch': 0.28}
{'loss': 0.792, 'grad_norm': 0.2063128650188446, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7460758090019226, 'eval_runtime': 5.3873, 'eval_samples_per_second': 185.437, 'eval_steps_per_second': 11.694, 'epoch': 0.32}
{'loss': 0.768, 'grad_norm': 0.22192320227622986, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.725059986114502, 'eval_runtime': 5.3864, 'eval_samples_per_second': 185.467, 'eval_steps_per_second': 11.696, 'epoch': 0.36}
{'loss': 0.7661, 'grad_norm': 0.20777958631515503, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7065613865852356, 'eval_runtime': 5.3824, 'eval_samples_per_second': 185.607, 'eval_steps_per_second': 11.705, 'epoch': 0.4}
{'loss': 0.765, 'grad_norm': 0.214854896068573, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6849241852760315, 'eval_runtime': 5.3922, 'eval_samples_per_second': 185.269, 'eval_steps_per_second': 11.684, 'epoch': 0.44}
{'loss': 0.7083, 'grad_norm': 0.24127532541751862, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6699293255805969, 'eval_runtime': 5.3883, 'eval_samples_per_second': 185.4, 'eval_steps_per_second': 11.692, 'epoch': 0.48}
{'loss': 0.7291, 'grad_norm': 0.24657022953033447, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6617430448532104, 'eval_runtime': 5.384, 'eval_samples_per_second': 185.548, 'eval_steps_per_second': 11.701, 'epoch': 0.52}
{'loss': 0.7195, 'grad_norm': 0.22739258408546448, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6424944996833801, 'eval_runtime': 5.3816, 'eval_samples_per_second': 185.631, 'eval_steps_per_second': 11.706, 'epoch': 0.56}
{'loss': 0.7082, 'grad_norm': 0.2730354368686676, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6274537444114685, 'eval_runtime': 5.3845, 'eval_samples_per_second': 185.532, 'eval_steps_per_second': 11.7, 'epoch': 0.6}
{'loss': 0.6924, 'grad_norm': 0.21301694214344025, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6174104809761047, 'eval_runtime': 5.3839, 'eval_samples_per_second': 185.553, 'eval_steps_per_second': 11.702, 'epoch': 0.64}
{'loss': 0.6983, 'grad_norm': 0.24263493716716766, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6003811955451965, 'eval_runtime': 5.3849, 'eval_samples_per_second': 185.519, 'eval_steps_per_second': 11.699, 'epoch': 0.68}
{'loss': 0.6668, 'grad_norm': 0.2116176337003708, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5868818163871765, 'eval_runtime': 5.3874, 'eval_samples_per_second': 185.434, 'eval_steps_per_second': 11.694, 'epoch': 0.72}
{'loss': 0.6598, 'grad_norm': 0.27527299523353577, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5721796154975891, 'eval_runtime': 5.3885, 'eval_samples_per_second': 185.394, 'eval_steps_per_second': 11.692, 'epoch': 0.76}
{'loss': 0.6652, 'grad_norm': 0.28525909781455994, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5611870288848877, 'eval_runtime': 5.3842, 'eval_samples_per_second': 185.544, 'eval_steps_per_second': 11.701, 'epoch': 0.8}
{'loss': 0.633, 'grad_norm': 0.25244805216789246, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.551001787185669, 'eval_runtime': 5.3855, 'eval_samples_per_second': 185.499, 'eval_steps_per_second': 11.698, 'epoch': 0.84}
{'loss': 0.6462, 'grad_norm': 0.2378658503293991, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5437527298927307, 'eval_runtime': 5.3829, 'eval_samples_per_second': 185.587, 'eval_steps_per_second': 11.704, 'epoch': 0.88}
{'loss': 0.6444, 'grad_norm': 0.24407431483268738, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5361747741699219, 'eval_runtime': 5.3842, 'eval_samples_per_second': 185.543, 'eval_steps_per_second': 11.701, 'epoch': 0.92}
{'loss': 0.6298, 'grad_norm': 0.2897752523422241, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5279460549354553, 'eval_runtime': 5.383, 'eval_samples_per_second': 185.586, 'eval_steps_per_second': 11.704, 'epoch': 0.96}
{'loss': 0.6442, 'grad_norm': 0.2643650770187378, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5271053314208984, 'eval_runtime': 5.3849, 'eval_samples_per_second': 185.519, 'eval_steps_per_second': 11.699, 'epoch': 1.0}
{'train_runtime': 329.5157, 'train_samples_per_second': 30.345, 'train_steps_per_second': 1.897, 'train_loss': 0.818106120300293, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1287202835083008, 0.8850432634353638, 0.8474772572517395, 0.8258076906204224, 0.8059683442115784, 0.781482994556427, 0.7644504904747009, 0.7460758090019226, 0.725059986114502, 0.7065613865852356, 0.6849241852760315, 0.6699293255805969, 0.6617430448532104, 0.6424944996833801, 0.6274537444114685, 0.6174104809761047, 0.6003811955451965, 0.5868818163871765, 0.5721796154975891, 0.5611870288848877, 0.551001787185669, 0.5437527298927307, 0.5361747741699219, 0.5279460549354553, 0.5271053314208984], 'performance': [0.64, 0.64]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:30,  3.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.85it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.09it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 57.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.34it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.64]
current iteration observed (possibly low-fid or predicted) performance:  1.2489604949951172
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.0883 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.8937119960971137
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 1.1038634161229333
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 1.0325214689482949
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.749838716799229
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.6155327055453467
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7361, dtype=torch.float64), tensor(0.2639, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 2.553512956637859e-16, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.7361, dtype=torch.float64), tensor(0.2639, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.5535e-15, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.736
  gsm8k: 0.264
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.553512956637859e-16,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.553512956637859e-16
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 226,492,416 || all params: 8,256,753,664 || trainable%: 2.7431
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:22,  4.33it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.99it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.09it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 37.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.17it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 42.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 46.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 43.70it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 46.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 47.27it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 43.76it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 43.00it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 42.56it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.4388, 'grad_norm': 0.466907799243927, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.1297608613967896, 'eval_runtime': 9.7949, 'eval_samples_per_second': 101.992, 'eval_steps_per_second': 6.432, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:22,  4.35it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.84it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 33.83it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 36.96it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 40.36it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 42.86it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 47.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 43.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 46.67it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 46.82it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 43.51it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 44.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 41.86it/s]
Evaluation performance at step 50: 0.64
{'loss': 0.9996, 'grad_norm': 0.4520778954029083, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.64}
{'eval_loss': 0.9053245186805725, 'eval_runtime': 9.7927, 'eval_samples_per_second': 102.015, 'eval_steps_per_second': 6.433, 'epoch': 0.08}
{'loss': 0.9024, 'grad_norm': 0.19027502834796906, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8650673627853394, 'eval_runtime': 9.801, 'eval_samples_per_second': 101.928, 'eval_steps_per_second': 6.428, 'epoch': 0.12}
{'loss': 0.8787, 'grad_norm': 0.18332819640636444, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8479029536247253, 'eval_runtime': 9.8519, 'eval_samples_per_second': 101.402, 'eval_steps_per_second': 6.395, 'epoch': 0.16}
{'loss': 0.8616, 'grad_norm': 0.1849738508462906, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8365654945373535, 'eval_runtime': 9.8509, 'eval_samples_per_second': 101.412, 'eval_steps_per_second': 6.395, 'epoch': 0.2}
{'loss': 0.8344, 'grad_norm': 0.18762782216072083, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8220747113227844, 'eval_runtime': 9.8655, 'eval_samples_per_second': 101.262, 'eval_steps_per_second': 6.386, 'epoch': 0.24}
{'loss': 0.8391, 'grad_norm': 0.17998768389225006, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8089730143547058, 'eval_runtime': 9.851, 'eval_samples_per_second': 101.411, 'eval_steps_per_second': 6.395, 'epoch': 0.28}
{'loss': 0.8425, 'grad_norm': 0.1755838841199875, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8002842664718628, 'eval_runtime': 9.8049, 'eval_samples_per_second': 101.888, 'eval_steps_per_second': 6.425, 'epoch': 0.32}
{'loss': 0.8167, 'grad_norm': 0.18046934902668, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7865492701530457, 'eval_runtime': 9.7878, 'eval_samples_per_second': 102.065, 'eval_steps_per_second': 6.437, 'epoch': 0.36}
{'loss': 0.7956, 'grad_norm': 0.1815134882926941, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7714347839355469, 'eval_runtime': 9.7789, 'eval_samples_per_second': 102.159, 'eval_steps_per_second': 6.442, 'epoch': 0.4}
{'loss': 0.8236, 'grad_norm': 0.16142651438713074, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7588968873023987, 'eval_runtime': 9.7755, 'eval_samples_per_second': 102.194, 'eval_steps_per_second': 6.445, 'epoch': 0.44}
{'loss': 0.8008, 'grad_norm': 0.19894425570964813, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7472517490386963, 'eval_runtime': 9.7718, 'eval_samples_per_second': 102.233, 'eval_steps_per_second': 6.447, 'epoch': 0.48}
{'loss': 0.7937, 'grad_norm': 0.19870944321155548, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.738118588924408, 'eval_runtime': 9.7711, 'eval_samples_per_second': 102.241, 'eval_steps_per_second': 6.448, 'epoch': 0.52}
{'loss': 0.7827, 'grad_norm': 0.18922095000743866, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.728011429309845, 'eval_runtime': 9.7735, 'eval_samples_per_second': 102.215, 'eval_steps_per_second': 6.446, 'epoch': 0.56}
{'loss': 0.8006, 'grad_norm': 0.1782132089138031, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7194963693618774, 'eval_runtime': 9.7568, 'eval_samples_per_second': 102.39, 'eval_steps_per_second': 6.457, 'epoch': 0.6}
{'loss': 0.7976, 'grad_norm': 0.21897345781326294, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7098619341850281, 'eval_runtime': 9.7223, 'eval_samples_per_second': 102.753, 'eval_steps_per_second': 6.48, 'epoch': 0.64}
{'loss': 0.7947, 'grad_norm': 0.19860227406024933, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.702333927154541, 'eval_runtime': 9.7199, 'eval_samples_per_second': 102.778, 'eval_steps_per_second': 6.482, 'epoch': 0.68}
{'loss': 0.769, 'grad_norm': 0.20922601222991943, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6894131898880005, 'eval_runtime': 9.7267, 'eval_samples_per_second': 102.707, 'eval_steps_per_second': 6.477, 'epoch': 0.72}
{'loss': 0.782, 'grad_norm': 0.1882214993238449, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6806890964508057, 'eval_runtime': 9.7528, 'eval_samples_per_second': 102.432, 'eval_steps_per_second': 6.46, 'epoch': 0.76}
{'loss': 0.7646, 'grad_norm': 0.20183007419109344, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6712844371795654, 'eval_runtime': 9.7729, 'eval_samples_per_second': 102.222, 'eval_steps_per_second': 6.446, 'epoch': 0.8}
{'loss': 0.7715, 'grad_norm': 0.2104017436504364, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6628900766372681, 'eval_runtime': 9.7714, 'eval_samples_per_second': 102.237, 'eval_steps_per_second': 6.447, 'epoch': 0.84}
{'loss': 0.7626, 'grad_norm': 0.20563295483589172, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.656727135181427, 'eval_runtime': 9.7746, 'eval_samples_per_second': 102.203, 'eval_steps_per_second': 6.445, 'epoch': 0.88}
{'loss': 0.7457, 'grad_norm': 0.217927947640419, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6500827074050903, 'eval_runtime': 9.7752, 'eval_samples_per_second': 102.198, 'eval_steps_per_second': 6.445, 'epoch': 0.92}
{'loss': 0.7573, 'grad_norm': 0.2625521123409271, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6470808982849121, 'eval_runtime': 9.7705, 'eval_samples_per_second': 102.247, 'eval_steps_per_second': 6.448, 'epoch': 0.96}
{'loss': 0.7471, 'grad_norm': 0.22632211446762085, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6456816792488098, 'eval_runtime': 9.777, 'eval_samples_per_second': 102.179, 'eval_steps_per_second': 6.444, 'epoch': 1.0}
{'train_runtime': 539.2461, 'train_samples_per_second': 18.543, 'train_steps_per_second': 1.159, 'train_loss': 0.8761176483154297, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1297608613967896, 0.9053245186805725, 0.8650673627853394, 0.8479029536247253, 0.8365654945373535, 0.8220747113227844, 0.8089730143547058, 0.8002842664718628, 0.7865492701530457, 0.7714347839355469, 0.7588968873023987, 0.7472517490386963, 0.738118588924408, 0.728011429309845, 0.7194963693618774, 0.7098619341850281, 0.702333927154541, 0.6894131898880005, 0.6806890964508057, 0.6712844371795654, 0.6628900766372681, 0.656727135181427, 0.6500827074050903, 0.6470808982849121, 0.6456816792488098], 'performance': [0.63, 0.64]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.71it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.85it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.85it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.06it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.64]
current iteration observed (possibly low-fid or predicted) performance:  1.239900827407837
current iteration best possible performance (full train run):  0.672
max performance so far:  0.672
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2045 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.5745986083300749
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 1.1362417092145118
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.7009778223565386
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 1.0539656707781981
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 1.0690614288335685
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 2, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(8.6874e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.2204e-16, dtype=torch.float64), tensor(2.1277e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 1.0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  2
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 4,063,232 || all params: 8,034,324,480 || trainable%: 0.0506
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  10000
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.35it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.52it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.08it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.10it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.54it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.76it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.99it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.37it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 57.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.49it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.9701, 'grad_norm': 1.1077619791030884, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 3.1588408946990967, 'eval_runtime': 11.2245, 'eval_samples_per_second': 89.091, 'eval_steps_per_second': 5.613, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.39it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.93it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 41.81it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 46.12it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 48.76it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 61.22it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.56it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.79it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 57.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.86it/s]
Evaluation performance at step 50: 0.61
{'loss': 2.5233, 'grad_norm': 1.3339508771896362, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 2.0892832279205322, 'eval_runtime': 11.1731, 'eval_samples_per_second': 89.501, 'eval_steps_per_second': 5.639, 'epoch': 0.08}
{'loss': 1.9428, 'grad_norm': 0.30695247650146484, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.857629656791687, 'eval_runtime': 11.2586, 'eval_samples_per_second': 88.821, 'eval_steps_per_second': 5.596, 'epoch': 0.12}
{'loss': 1.8123, 'grad_norm': 0.3209264576435089, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.7885024547576904, 'eval_runtime': 11.3696, 'eval_samples_per_second': 87.954, 'eval_steps_per_second': 5.541, 'epoch': 0.16}
{'loss': 1.8026, 'grad_norm': 0.27621060609817505, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.7687387466430664, 'eval_runtime': 11.3914, 'eval_samples_per_second': 87.785, 'eval_steps_per_second': 5.53, 'epoch': 0.2}
{'loss': 1.7752, 'grad_norm': 0.23520433902740479, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.7537789344787598, 'eval_runtime': 11.3615, 'eval_samples_per_second': 88.017, 'eval_steps_per_second': 5.545, 'epoch': 0.24}
{'loss': 1.7708, 'grad_norm': 0.22962646186351776, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.74360990524292, 'eval_runtime': 11.3941, 'eval_samples_per_second': 87.764, 'eval_steps_per_second': 5.529, 'epoch': 0.28}
{'loss': 1.7503, 'grad_norm': 0.27627861499786377, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.735073447227478, 'eval_runtime': 11.3095, 'eval_samples_per_second': 88.421, 'eval_steps_per_second': 5.571, 'epoch': 0.32}
{'loss': 1.7358, 'grad_norm': 0.3164270520210266, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.7302045822143555, 'eval_runtime': 11.3144, 'eval_samples_per_second': 88.383, 'eval_steps_per_second': 5.568, 'epoch': 0.36}
{'loss': 1.7406, 'grad_norm': 0.28324994444847107, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.7245484590530396, 'eval_runtime': 11.2956, 'eval_samples_per_second': 88.53, 'eval_steps_per_second': 5.577, 'epoch': 0.4}
{'loss': 1.7367, 'grad_norm': 0.2481992095708847, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.7184337377548218, 'eval_runtime': 11.2714, 'eval_samples_per_second': 88.72, 'eval_steps_per_second': 5.589, 'epoch': 0.44}
{'loss': 1.7306, 'grad_norm': 0.29831498861312866, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.7149254083633423, 'eval_runtime': 11.2553, 'eval_samples_per_second': 88.847, 'eval_steps_per_second': 5.597, 'epoch': 0.48}
{'loss': 1.7116, 'grad_norm': 0.31235772371292114, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.7112878561019897, 'eval_runtime': 11.2514, 'eval_samples_per_second': 88.877, 'eval_steps_per_second': 5.599, 'epoch': 0.52}
{'loss': 1.7243, 'grad_norm': 0.2674482762813568, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.706791877746582, 'eval_runtime': 11.2525, 'eval_samples_per_second': 88.869, 'eval_steps_per_second': 5.599, 'epoch': 0.56}
{'loss': 1.7494, 'grad_norm': 0.25599977374076843, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.702426791191101, 'eval_runtime': 11.2628, 'eval_samples_per_second': 88.788, 'eval_steps_per_second': 5.594, 'epoch': 0.6}
{'loss': 1.7196, 'grad_norm': 0.2885074317455292, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.6996455192565918, 'eval_runtime': 11.2584, 'eval_samples_per_second': 88.823, 'eval_steps_per_second': 5.596, 'epoch': 0.64}
{'loss': 1.718, 'grad_norm': 0.3155882954597473, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.6967785358428955, 'eval_runtime': 11.2639, 'eval_samples_per_second': 88.779, 'eval_steps_per_second': 5.593, 'epoch': 0.68}
{'loss': 1.7211, 'grad_norm': 0.2726430892944336, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.6934524774551392, 'eval_runtime': 11.2559, 'eval_samples_per_second': 88.843, 'eval_steps_per_second': 5.597, 'epoch': 0.72}
{'loss': 1.697, 'grad_norm': 0.26614809036254883, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.6918784379959106, 'eval_runtime': 11.2608, 'eval_samples_per_second': 88.803, 'eval_steps_per_second': 5.595, 'epoch': 0.76}
{'loss': 1.7012, 'grad_norm': 0.2577090561389923, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.6900475025177002, 'eval_runtime': 11.2496, 'eval_samples_per_second': 88.892, 'eval_steps_per_second': 5.6, 'epoch': 0.8}
{'loss': 1.7097, 'grad_norm': 0.31431201100349426, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.688334345817566, 'eval_runtime': 11.3068, 'eval_samples_per_second': 88.443, 'eval_steps_per_second': 5.572, 'epoch': 0.84}
{'loss': 1.7357, 'grad_norm': 0.2953701615333557, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.6864062547683716, 'eval_runtime': 11.3326, 'eval_samples_per_second': 88.241, 'eval_steps_per_second': 5.559, 'epoch': 0.88}
{'loss': 1.7127, 'grad_norm': 0.287559449672699, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.6854314804077148, 'eval_runtime': 11.3209, 'eval_samples_per_second': 88.332, 'eval_steps_per_second': 5.565, 'epoch': 0.92}
{'loss': 1.7166, 'grad_norm': 0.26272618770599365, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.6844884157180786, 'eval_runtime': 11.3142, 'eval_samples_per_second': 88.385, 'eval_steps_per_second': 5.568, 'epoch': 0.96}
{'loss': 1.7073, 'grad_norm': 0.2956598997116089, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.684482216835022, 'eval_runtime': 11.2846, 'eval_samples_per_second': 88.616, 'eval_steps_per_second': 5.583, 'epoch': 1.0}
{'train_runtime': 587.4473, 'train_samples_per_second': 17.023, 'train_steps_per_second': 1.064, 'train_loss': 1.8646079711914063, 'epoch': 1.0}
train_results:  {'eval_loss': [3.1588408946990967, 2.0892832279205322, 1.857629656791687, 1.7885024547576904, 1.7687387466430664, 1.7537789344787598, 1.74360990524292, 1.735073447227478, 1.7302045822143555, 1.7245484590530396, 1.7184337377548218, 1.7149254083633423, 1.7112878561019897, 1.706791877746582, 1.702426791191101, 1.6996455192565918, 1.6967785358428955, 1.6934524774551392, 1.6918784379959106, 1.6900475025177002, 1.688334345817566, 1.6864062547683716, 1.6854314804077148, 1.6844884157180786, 1.684482216835022], 'performance': [0.61, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 41.44it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 39.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.66it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  0.7607021331787109
current iteration best possible performance (full train run):  0.7140000000000001
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1441 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 1.135786580282958
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 1.0153079847863253
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 1.091555342271892
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 0.961142634498587
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.7203931447668148
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7764, dtype=torch.float64), 0, 0, 0, tensor(0.2236, dtype=torch.float64), 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.0785207688568522e-33, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7764, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.9183e-18, dtype=torch.float64), tensor(0.2236, dtype=torch.float64), tensor(5.8035e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.0841e-17, dtype=torch.float64), tensor(2.2818e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0785e-32, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.776
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.224
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.0785207688568522e-33,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.0785207688568522e-33
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  4.99it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.93it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.73it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.02it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.60it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.94it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.18it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.33it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.92it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.8055, 'grad_norm': 0.4469112455844879, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.139281153678894, 'eval_runtime': 5.5775, 'eval_samples_per_second': 179.111, 'eval_steps_per_second': 11.295, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.95it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.43it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.99it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 47.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.76it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 44.01it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 44.70it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 50.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.45it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 52.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 53.43it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.75it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.0022, 'grad_norm': 0.5003465414047241, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 0.9041785597801208, 'eval_runtime': 5.3866, 'eval_samples_per_second': 185.46, 'eval_steps_per_second': 11.696, 'epoch': 0.08}
{'loss': 0.9213, 'grad_norm': 0.24507486820220947, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8709264397621155, 'eval_runtime': 5.4252, 'eval_samples_per_second': 184.142, 'eval_steps_per_second': 11.613, 'epoch': 0.12}
{'loss': 0.8785, 'grad_norm': 0.22072778642177582, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8479583263397217, 'eval_runtime': 5.4277, 'eval_samples_per_second': 184.056, 'eval_steps_per_second': 11.607, 'epoch': 0.16}
{'loss': 0.8627, 'grad_norm': 0.18249669671058655, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8306594491004944, 'eval_runtime': 5.428, 'eval_samples_per_second': 184.044, 'eval_steps_per_second': 11.606, 'epoch': 0.2}
{'loss': 0.8797, 'grad_norm': 0.22577720880508423, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8236663341522217, 'eval_runtime': 5.4456, 'eval_samples_per_second': 183.451, 'eval_steps_per_second': 11.569, 'epoch': 0.24}
{'loss': 0.8572, 'grad_norm': 0.213239848613739, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8056747317314148, 'eval_runtime': 5.4219, 'eval_samples_per_second': 184.254, 'eval_steps_per_second': 11.62, 'epoch': 0.28}
{'loss': 0.8392, 'grad_norm': 0.22132566571235657, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7865023016929626, 'eval_runtime': 5.4149, 'eval_samples_per_second': 184.492, 'eval_steps_per_second': 11.635, 'epoch': 0.32}
{'loss': 0.8408, 'grad_norm': 0.17498622834682465, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7764868140220642, 'eval_runtime': 5.4206, 'eval_samples_per_second': 184.298, 'eval_steps_per_second': 11.622, 'epoch': 0.36}
{'loss': 0.8325, 'grad_norm': 0.23765835165977478, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7636900544166565, 'eval_runtime': 5.4298, 'eval_samples_per_second': 183.985, 'eval_steps_per_second': 11.603, 'epoch': 0.4}
{'loss': 0.8147, 'grad_norm': 0.25325626134872437, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7426449656486511, 'eval_runtime': 5.3894, 'eval_samples_per_second': 185.365, 'eval_steps_per_second': 11.69, 'epoch': 0.44}
{'loss': 0.8059, 'grad_norm': 0.24998490512371063, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7291356325149536, 'eval_runtime': 5.3803, 'eval_samples_per_second': 185.679, 'eval_steps_per_second': 11.709, 'epoch': 0.48}
{'loss': 0.7969, 'grad_norm': 0.24578501284122467, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7191593647003174, 'eval_runtime': 5.3811, 'eval_samples_per_second': 185.65, 'eval_steps_per_second': 11.708, 'epoch': 0.52}
{'loss': 0.8, 'grad_norm': 0.20719259977340698, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7111258506774902, 'eval_runtime': 5.393, 'eval_samples_per_second': 185.239, 'eval_steps_per_second': 11.682, 'epoch': 0.56}
{'loss': 0.7935, 'grad_norm': 0.22594304382801056, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6953395009040833, 'eval_runtime': 5.3895, 'eval_samples_per_second': 185.362, 'eval_steps_per_second': 11.689, 'epoch': 0.6}
{'loss': 0.7853, 'grad_norm': 0.26877859234809875, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6794085502624512, 'eval_runtime': 5.41, 'eval_samples_per_second': 184.658, 'eval_steps_per_second': 11.645, 'epoch': 0.64}
{'loss': 0.7696, 'grad_norm': 0.21495629847049713, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.672537088394165, 'eval_runtime': 5.438, 'eval_samples_per_second': 183.706, 'eval_steps_per_second': 11.585, 'epoch': 0.68}
{'loss': 0.7538, 'grad_norm': 0.22587093710899353, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.654851496219635, 'eval_runtime': 5.3976, 'eval_samples_per_second': 185.081, 'eval_steps_per_second': 11.672, 'epoch': 0.72}
{'loss': 0.7838, 'grad_norm': 0.21671131253242493, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6454681158065796, 'eval_runtime': 5.4062, 'eval_samples_per_second': 184.787, 'eval_steps_per_second': 11.653, 'epoch': 0.76}
{'loss': 0.7522, 'grad_norm': 0.2352631688117981, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6318318843841553, 'eval_runtime': 5.4044, 'eval_samples_per_second': 184.85, 'eval_steps_per_second': 11.657, 'epoch': 0.8}
{'loss': 0.7571, 'grad_norm': 0.25077754259109497, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6195493340492249, 'eval_runtime': 5.4058, 'eval_samples_per_second': 184.8, 'eval_steps_per_second': 11.654, 'epoch': 0.84}
{'loss': 0.753, 'grad_norm': 0.22799745202064514, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6141387820243835, 'eval_runtime': 5.4061, 'eval_samples_per_second': 184.793, 'eval_steps_per_second': 11.654, 'epoch': 0.88}
{'loss': 0.7303, 'grad_norm': 0.23947486281394958, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.60285884141922, 'eval_runtime': 5.4064, 'eval_samples_per_second': 184.779, 'eval_steps_per_second': 11.653, 'epoch': 0.92}
{'loss': 0.7401, 'grad_norm': 0.25309428572654724, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5971298217773438, 'eval_runtime': 5.4005, 'eval_samples_per_second': 184.982, 'eval_steps_per_second': 11.666, 'epoch': 0.96}
{'loss': 0.7494, 'grad_norm': 0.25932979583740234, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5954354405403137, 'eval_runtime': 5.4068, 'eval_samples_per_second': 184.768, 'eval_steps_per_second': 11.652, 'epoch': 1.0}
{'train_runtime': 320.8173, 'train_samples_per_second': 31.167, 'train_steps_per_second': 1.948, 'train_loss': 0.8922107482910157, 'epoch': 1.0}
train_results:  {'eval_loss': [1.139281153678894, 0.9041785597801208, 0.8709264397621155, 0.8479583263397217, 0.8306594491004944, 0.8236663341522217, 0.8056747317314148, 0.7865023016929626, 0.7764868140220642, 0.7636900544166565, 0.7426449656486511, 0.7291356325149536, 0.7191593647003174, 0.7111258506774902, 0.6953395009040833, 0.6794085502624512, 0.672537088394165, 0.654851496219635, 0.6454681158065796, 0.6318318843841553, 0.6195493340492249, 0.6141387820243835, 0.60285884141922, 0.5971298217773438, 0.5954354405403137], 'performance': [0.65, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:30,  3.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.21it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.28it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.92it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.64it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 68.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.60it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2495477199554443
current iteration best possible performance (full train run):  0.6825000000000001
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3401 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 1.1171236496125947
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.9745651842568348
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 1.1002415914394008
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 1.1315104328819325
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.6139496294725724
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7752, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2248, dtype=torch.float64), 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.7752, dtype=torch.float64), tensor(3.8253e-17, dtype=torch.float64), tensor(2.4003e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.3194e-17, dtype=torch.float64), tensor(2.3536e-18, dtype=torch.float64), tensor(3.5108e-17, dtype=torch.float64), tensor(0.2248, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.775
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.225
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.13it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.47it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.28it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.27it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.50it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.20it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.41it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.71it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.95it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.48it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.7062, 'grad_norm': 0.5202251076698303, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.23093581199646, 'eval_runtime': 8.3896, 'eval_samples_per_second': 119.076, 'eval_steps_per_second': 7.509, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.05it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.19it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.70it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.16it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.24it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.68it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.57it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.97it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.67it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.63it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.40it/s]
Evaluation performance at step 50: 0.65
{'loss': 1.0923, 'grad_norm': 0.6424024105072021, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.65}
{'eval_loss': 1.0108433961868286, 'eval_runtime': 8.3773, 'eval_samples_per_second': 119.251, 'eval_steps_per_second': 7.52, 'epoch': 0.08}
{'loss': 0.9726, 'grad_norm': 0.21666520833969116, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9585336446762085, 'eval_runtime': 8.3961, 'eval_samples_per_second': 118.984, 'eval_steps_per_second': 7.503, 'epoch': 0.12}
{'loss': 0.9742, 'grad_norm': 0.19795167446136475, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9360904097557068, 'eval_runtime': 8.4011, 'eval_samples_per_second': 118.912, 'eval_steps_per_second': 7.499, 'epoch': 0.16}
{'loss': 0.9566, 'grad_norm': 0.19459901750087738, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9196877479553223, 'eval_runtime': 8.4203, 'eval_samples_per_second': 118.642, 'eval_steps_per_second': 7.482, 'epoch': 0.2}
{'loss': 0.9621, 'grad_norm': 0.2155035436153412, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9048398733139038, 'eval_runtime': 8.4148, 'eval_samples_per_second': 118.72, 'eval_steps_per_second': 7.487, 'epoch': 0.24}
{'loss': 0.9976, 'grad_norm': 0.2521134614944458, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8925496339797974, 'eval_runtime': 8.4104, 'eval_samples_per_second': 118.781, 'eval_steps_per_second': 7.491, 'epoch': 0.28}
{'loss': 0.9351, 'grad_norm': 0.2038090080022812, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8768512010574341, 'eval_runtime': 8.4048, 'eval_samples_per_second': 118.861, 'eval_steps_per_second': 7.496, 'epoch': 0.32}
{'loss': 0.917, 'grad_norm': 0.19742447137832642, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8607006072998047, 'eval_runtime': 8.4102, 'eval_samples_per_second': 118.784, 'eval_steps_per_second': 7.491, 'epoch': 0.36}
{'loss': 0.9306, 'grad_norm': 0.24291320145130157, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8485487699508667, 'eval_runtime': 8.4091, 'eval_samples_per_second': 118.8, 'eval_steps_per_second': 7.492, 'epoch': 0.4}
{'loss': 0.9216, 'grad_norm': 0.24276314675807953, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8304179906845093, 'eval_runtime': 8.3904, 'eval_samples_per_second': 119.064, 'eval_steps_per_second': 7.509, 'epoch': 0.44}
{'loss': 0.9183, 'grad_norm': 0.30601686239242554, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8181749582290649, 'eval_runtime': 8.3829, 'eval_samples_per_second': 119.171, 'eval_steps_per_second': 7.515, 'epoch': 0.48}
{'loss': 0.8653, 'grad_norm': 0.23645079135894775, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.811866819858551, 'eval_runtime': 8.3916, 'eval_samples_per_second': 119.047, 'eval_steps_per_second': 7.507, 'epoch': 0.52}
{'loss': 0.8952, 'grad_norm': 0.2001732885837555, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8004972338676453, 'eval_runtime': 8.4177, 'eval_samples_per_second': 118.679, 'eval_steps_per_second': 7.484, 'epoch': 0.56}
{'loss': 0.88, 'grad_norm': 0.21339449286460876, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.78321772813797, 'eval_runtime': 8.4276, 'eval_samples_per_second': 118.539, 'eval_steps_per_second': 7.475, 'epoch': 0.6}
{'loss': 0.8958, 'grad_norm': 0.243777796626091, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7702597379684448, 'eval_runtime': 8.4377, 'eval_samples_per_second': 118.397, 'eval_steps_per_second': 7.467, 'epoch': 0.64}
{'loss': 0.8717, 'grad_norm': 0.22751253843307495, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7561344504356384, 'eval_runtime': 8.4419, 'eval_samples_per_second': 118.338, 'eval_steps_per_second': 7.463, 'epoch': 0.68}
{'loss': 0.8352, 'grad_norm': 0.213144451379776, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.744391918182373, 'eval_runtime': 8.4534, 'eval_samples_per_second': 118.178, 'eval_steps_per_second': 7.453, 'epoch': 0.72}
{'loss': 0.8474, 'grad_norm': 0.21913042664527893, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7349058985710144, 'eval_runtime': 8.4381, 'eval_samples_per_second': 118.392, 'eval_steps_per_second': 7.466, 'epoch': 0.76}
{'loss': 0.8791, 'grad_norm': 0.22169916331768036, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7279306650161743, 'eval_runtime': 8.4522, 'eval_samples_per_second': 118.195, 'eval_steps_per_second': 7.454, 'epoch': 0.8}
{'loss': 0.8376, 'grad_norm': 0.20866121351718903, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7162840962409973, 'eval_runtime': 8.4262, 'eval_samples_per_second': 118.559, 'eval_steps_per_second': 7.477, 'epoch': 0.84}
{'loss': 0.8484, 'grad_norm': 0.21425843238830566, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.708552896976471, 'eval_runtime': 8.4338, 'eval_samples_per_second': 118.452, 'eval_steps_per_second': 7.47, 'epoch': 0.88}
{'loss': 0.873, 'grad_norm': 0.2550346851348877, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7012991905212402, 'eval_runtime': 8.4227, 'eval_samples_per_second': 118.607, 'eval_steps_per_second': 7.48, 'epoch': 0.92}
{'loss': 0.8029, 'grad_norm': 0.23248136043548584, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6966433525085449, 'eval_runtime': 8.422, 'eval_samples_per_second': 118.618, 'eval_steps_per_second': 7.48, 'epoch': 0.96}
{'loss': 0.8635, 'grad_norm': 0.22619184851646423, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6951436996459961, 'eval_runtime': 8.4297, 'eval_samples_per_second': 118.51, 'eval_steps_per_second': 7.474, 'epoch': 1.0}
{'train_runtime': 455.9988, 'train_samples_per_second': 21.928, 'train_steps_per_second': 1.371, 'train_loss': 0.9791789276123047, 'epoch': 1.0}
train_results:  {'eval_loss': [1.23093581199646, 1.0108433961868286, 0.9585336446762085, 0.9360904097557068, 0.9196877479553223, 0.9048398733139038, 0.8925496339797974, 0.8768512010574341, 0.8607006072998047, 0.8485487699508667, 0.8304179906845093, 0.8181749582290649, 0.811866819858551, 0.8004972338676453, 0.78321772813797, 0.7702597379684448, 0.7561344504356384, 0.744391918182373, 0.7349058985710144, 0.7279306650161743, 0.7162840962409973, 0.708552896976471, 0.7012991905212402, 0.6966433525085449, 0.6951436996459961], 'performance': [0.64, 0.65]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:34,  2.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.12it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.15it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.79it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.02it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.65]
current iteration observed (possibly low-fid or predicted) performance:  1.2450722455978394
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.9494 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 1.0844323925698336
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.6622895153649947
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.6984066242567873
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.6358149509460225
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 1.0734409473761277
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7462, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2538, dtype=torch.float64), 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7462, dtype=torch.float64), tensor(3.7135e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0830e-17, dtype=torch.float64), tensor(0.2538, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3950e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.746
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.254
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.11it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.47it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.13it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.59it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.40it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.13it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.64it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.61it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.80it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.85it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.15it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.8037, 'grad_norm': 0.5418165922164917, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.1302341222763062, 'eval_runtime': 5.2487, 'eval_samples_per_second': 190.333, 'eval_steps_per_second': 12.003, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.00it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.94it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.28it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 39.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 38.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 44.91it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.74it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 45.83it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 45.88it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 51.61it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.29it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 52.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.62it/s]
Evaluation performance at step 50: 0.63
{'loss': 0.9681, 'grad_norm': 0.4267195165157318, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 0.8902006149291992, 'eval_runtime': 5.2385, 'eval_samples_per_second': 190.702, 'eval_steps_per_second': 12.026, 'epoch': 0.08}
{'loss': 0.8773, 'grad_norm': 0.2828907072544098, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8526515960693359, 'eval_runtime': 5.2409, 'eval_samples_per_second': 190.617, 'eval_steps_per_second': 12.021, 'epoch': 0.12}
{'loss': 0.8322, 'grad_norm': 0.24160784482955933, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8318256735801697, 'eval_runtime': 5.2514, 'eval_samples_per_second': 190.236, 'eval_steps_per_second': 11.997, 'epoch': 0.16}
{'loss': 0.8324, 'grad_norm': 0.20966334640979767, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8138021230697632, 'eval_runtime': 5.2275, 'eval_samples_per_second': 191.105, 'eval_steps_per_second': 12.052, 'epoch': 0.2}
{'loss': 0.8072, 'grad_norm': 0.22948430478572845, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7942332625389099, 'eval_runtime': 5.216, 'eval_samples_per_second': 191.527, 'eval_steps_per_second': 12.078, 'epoch': 0.24}
{'loss': 0.7851, 'grad_norm': 0.2548758089542389, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7733725905418396, 'eval_runtime': 5.2312, 'eval_samples_per_second': 190.97, 'eval_steps_per_second': 12.043, 'epoch': 0.28}
{'loss': 0.8014, 'grad_norm': 0.20018674433231354, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7608569860458374, 'eval_runtime': 5.2312, 'eval_samples_per_second': 190.969, 'eval_steps_per_second': 12.043, 'epoch': 0.32}
{'loss': 0.76, 'grad_norm': 0.21339307725429535, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7380936741828918, 'eval_runtime': 5.2291, 'eval_samples_per_second': 191.046, 'eval_steps_per_second': 12.048, 'epoch': 0.36}
{'loss': 0.7595, 'grad_norm': 0.2741585075855255, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7186885476112366, 'eval_runtime': 5.2268, 'eval_samples_per_second': 191.131, 'eval_steps_per_second': 12.053, 'epoch': 0.4}
{'loss': 0.7396, 'grad_norm': 0.25995567440986633, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.702372133731842, 'eval_runtime': 5.2248, 'eval_samples_per_second': 191.204, 'eval_steps_per_second': 12.058, 'epoch': 0.44}
{'loss': 0.7271, 'grad_norm': 0.23486557602882385, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6880738735198975, 'eval_runtime': 5.2273, 'eval_samples_per_second': 191.112, 'eval_steps_per_second': 12.052, 'epoch': 0.48}
{'loss': 0.7367, 'grad_norm': 0.25974035263061523, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6788572669029236, 'eval_runtime': 5.2278, 'eval_samples_per_second': 191.095, 'eval_steps_per_second': 12.051, 'epoch': 0.52}
{'loss': 0.7324, 'grad_norm': 0.23567818105220795, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6610445976257324, 'eval_runtime': 5.2337, 'eval_samples_per_second': 190.879, 'eval_steps_per_second': 12.037, 'epoch': 0.56}
{'loss': 0.7038, 'grad_norm': 0.24924974143505096, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6472542881965637, 'eval_runtime': 5.2479, 'eval_samples_per_second': 190.36, 'eval_steps_per_second': 12.005, 'epoch': 0.6}
{'loss': 0.7237, 'grad_norm': 0.24055762588977814, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.639373242855072, 'eval_runtime': 5.2473, 'eval_samples_per_second': 190.384, 'eval_steps_per_second': 12.006, 'epoch': 0.64}
{'loss': 0.7052, 'grad_norm': 0.24294710159301758, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6247240900993347, 'eval_runtime': 5.2522, 'eval_samples_per_second': 190.205, 'eval_steps_per_second': 11.995, 'epoch': 0.68}
{'loss': 0.6737, 'grad_norm': 0.2248115986585617, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6112727522850037, 'eval_runtime': 5.2478, 'eval_samples_per_second': 190.364, 'eval_steps_per_second': 12.005, 'epoch': 0.72}
{'loss': 0.6969, 'grad_norm': 0.2725737988948822, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6020824909210205, 'eval_runtime': 5.2509, 'eval_samples_per_second': 190.255, 'eval_steps_per_second': 11.998, 'epoch': 0.76}
{'loss': 0.6738, 'grad_norm': 0.2887214124202728, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5906844139099121, 'eval_runtime': 5.2612, 'eval_samples_per_second': 189.879, 'eval_steps_per_second': 11.974, 'epoch': 0.8}
{'loss': 0.6571, 'grad_norm': 0.2715696394443512, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5818961262702942, 'eval_runtime': 5.2301, 'eval_samples_per_second': 191.011, 'eval_steps_per_second': 12.046, 'epoch': 0.84}
{'loss': 0.6517, 'grad_norm': 0.2454320192337036, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5716645121574402, 'eval_runtime': 5.2583, 'eval_samples_per_second': 189.985, 'eval_steps_per_second': 11.981, 'epoch': 0.88}
{'loss': 0.6592, 'grad_norm': 0.2648679316043854, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5646382570266724, 'eval_runtime': 5.2625, 'eval_samples_per_second': 189.832, 'eval_steps_per_second': 11.971, 'epoch': 0.92}
{'loss': 0.6578, 'grad_norm': 0.25507956743240356, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5618804693222046, 'eval_runtime': 5.2374, 'eval_samples_per_second': 190.745, 'eval_steps_per_second': 12.029, 'epoch': 0.96}
{'loss': 0.6596, 'grad_norm': 0.23225338757038116, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5602970719337463, 'eval_runtime': 5.2251, 'eval_samples_per_second': 191.193, 'eval_steps_per_second': 12.057, 'epoch': 1.0}
{'train_runtime': 310.9455, 'train_samples_per_second': 32.157, 'train_steps_per_second': 2.01, 'train_loss': 0.8249932708740234, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1302341222763062, 0.8902006149291992, 0.8526515960693359, 0.8318256735801697, 0.8138021230697632, 0.7942332625389099, 0.7733725905418396, 0.7608569860458374, 0.7380936741828918, 0.7186885476112366, 0.702372133731842, 0.6880738735198975, 0.6788572669029236, 0.6610445976257324, 0.6472542881965637, 0.639373242855072, 0.6247240900993347, 0.6112727522850037, 0.6020824909210205, 0.5906844139099121, 0.5818961262702942, 0.5716645121574402, 0.5646382570266724, 0.5618804693222046, 0.5602970719337463], 'performance': [0.65, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.75it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.26it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.28it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.67it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 57.79it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.89it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.2470932006835938
current iteration best possible performance (full train run):  0.63
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394, 1.2470932006835938]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6045 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.8423851594706694
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.5868338816727052
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 1.1260680601144868
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 1.0719893057831111
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.6963477966182768
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7511, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2489, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.03427786955009829, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7511, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.9309e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2489, dtype=torch.float64), tensor(7.0713e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3428, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.751
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.249
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03427786955009829,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.03427786955009829
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.10it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.74it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.51it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.83it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.37it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.45it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.70it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.97it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.63it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.7757, 'grad_norm': 0.387287437915802, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.347403883934021, 'eval_runtime': 7.6289, 'eval_samples_per_second': 130.949, 'eval_steps_per_second': 8.258, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.04it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.93it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.33it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.87it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.15it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.72it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.79it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.72it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.1962, 'grad_norm': 0.49544113874435425, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.1245492696762085, 'eval_runtime': 7.6301, 'eval_samples_per_second': 130.93, 'eval_steps_per_second': 8.257, 'epoch': 0.08}
{'loss': 1.2114, 'grad_norm': 0.25909560918807983, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0723533630371094, 'eval_runtime': 7.6298, 'eval_samples_per_second': 130.934, 'eval_steps_per_second': 8.257, 'epoch': 0.12}
{'loss': 1.092, 'grad_norm': 0.23057962954044342, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0467379093170166, 'eval_runtime': 7.6435, 'eval_samples_per_second': 130.7, 'eval_steps_per_second': 8.242, 'epoch': 0.16}
{'loss': 1.097, 'grad_norm': 0.27024614810943604, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0228644609451294, 'eval_runtime': 7.6559, 'eval_samples_per_second': 130.487, 'eval_steps_per_second': 8.229, 'epoch': 0.2}
{'loss': 1.0392, 'grad_norm': 0.229357048869133, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0100575685501099, 'eval_runtime': 7.6695, 'eval_samples_per_second': 130.256, 'eval_steps_per_second': 8.214, 'epoch': 0.24}
{'loss': 1.032, 'grad_norm': 0.3022860884666443, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9950163960456848, 'eval_runtime': 7.6666, 'eval_samples_per_second': 130.305, 'eval_steps_per_second': 8.217, 'epoch': 0.28}
{'loss': 1.0407, 'grad_norm': 0.30299559235572815, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9797307848930359, 'eval_runtime': 7.681, 'eval_samples_per_second': 130.062, 'eval_steps_per_second': 8.202, 'epoch': 0.32}
{'loss': 1.0581, 'grad_norm': 0.23175066709518433, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9680638313293457, 'eval_runtime': 7.6873, 'eval_samples_per_second': 129.955, 'eval_steps_per_second': 8.195, 'epoch': 0.36}
{'loss': 1.0336, 'grad_norm': 0.253296434879303, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9497256875038147, 'eval_runtime': 7.7106, 'eval_samples_per_second': 129.562, 'eval_steps_per_second': 8.171, 'epoch': 0.4}
{'loss': 0.9968, 'grad_norm': 0.26546743512153625, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9310352206230164, 'eval_runtime': 7.7353, 'eval_samples_per_second': 129.148, 'eval_steps_per_second': 8.144, 'epoch': 0.44}
{'loss': 1.1071, 'grad_norm': 0.2618599236011505, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9145074486732483, 'eval_runtime': 7.7293, 'eval_samples_per_second': 129.249, 'eval_steps_per_second': 8.151, 'epoch': 0.48}
{'loss': 1.0065, 'grad_norm': 0.22681574523448944, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9065249562263489, 'eval_runtime': 7.772, 'eval_samples_per_second': 128.539, 'eval_steps_per_second': 8.106, 'epoch': 0.52}
{'loss': 0.9955, 'grad_norm': 0.2559186518192291, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8898661732673645, 'eval_runtime': 7.7544, 'eval_samples_per_second': 128.83, 'eval_steps_per_second': 8.124, 'epoch': 0.56}
{'loss': 1.0601, 'grad_norm': 0.2644878327846527, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8739286065101624, 'eval_runtime': 7.6917, 'eval_samples_per_second': 129.881, 'eval_steps_per_second': 8.191, 'epoch': 0.6}
{'loss': 0.9995, 'grad_norm': 0.24295489490032196, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8582768440246582, 'eval_runtime': 7.683, 'eval_samples_per_second': 130.028, 'eval_steps_per_second': 8.2, 'epoch': 0.64}
{'loss': 1.0045, 'grad_norm': 0.2402154803276062, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8427354097366333, 'eval_runtime': 7.694, 'eval_samples_per_second': 129.842, 'eval_steps_per_second': 8.188, 'epoch': 0.68}
{'loss': 1.0145, 'grad_norm': 0.29499107599258423, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8286890983581543, 'eval_runtime': 7.6995, 'eval_samples_per_second': 129.749, 'eval_steps_per_second': 8.182, 'epoch': 0.72}
{'loss': 0.9299, 'grad_norm': 0.29215148091316223, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8182113766670227, 'eval_runtime': 7.6833, 'eval_samples_per_second': 130.022, 'eval_steps_per_second': 8.2, 'epoch': 0.76}
{'loss': 0.9013, 'grad_norm': 0.311716765165329, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8035867810249329, 'eval_runtime': 7.685, 'eval_samples_per_second': 129.994, 'eval_steps_per_second': 8.198, 'epoch': 0.8}
{'loss': 0.9168, 'grad_norm': 0.27300307154655457, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7976423501968384, 'eval_runtime': 7.6799, 'eval_samples_per_second': 130.08, 'eval_steps_per_second': 8.203, 'epoch': 0.84}
{'loss': 0.9713, 'grad_norm': 0.2864932119846344, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7897593379020691, 'eval_runtime': 7.67, 'eval_samples_per_second': 130.247, 'eval_steps_per_second': 8.214, 'epoch': 0.88}
{'loss': 0.9543, 'grad_norm': 0.25970885157585144, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7793014049530029, 'eval_runtime': 7.6741, 'eval_samples_per_second': 130.178, 'eval_steps_per_second': 8.209, 'epoch': 0.92}
{'loss': 0.9213, 'grad_norm': 0.23490603268146515, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7760497331619263, 'eval_runtime': 7.6694, 'eval_samples_per_second': 130.258, 'eval_steps_per_second': 8.214, 'epoch': 0.96}
{'loss': 0.966, 'grad_norm': 0.267109751701355, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7743744850158691, 'eval_runtime': 7.6377, 'eval_samples_per_second': 130.799, 'eval_steps_per_second': 8.249, 'epoch': 1.0}
{'train_runtime': 429.5732, 'train_samples_per_second': 23.277, 'train_steps_per_second': 1.455, 'train_loss': 1.0928514099121094, 'epoch': 1.0}
train_results:  {'eval_loss': [1.347403883934021, 1.1245492696762085, 1.0723533630371094, 1.0467379093170166, 1.0228644609451294, 1.0100575685501099, 0.9950163960456848, 0.9797307848930359, 0.9680638313293457, 0.9497256875038147, 0.9310352206230164, 0.9145074486732483, 0.9065249562263489, 0.8898661732673645, 0.8739286065101624, 0.8582768440246582, 0.8427354097366333, 0.8286890983581543, 0.8182113766670227, 0.8035867810249329, 0.7976423501968384, 0.7897593379020691, 0.7793014049530029, 0.7760497331619263, 0.7743744850158691], 'performance': [0.65, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.71it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.24it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 47.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.12it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 71.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2496857643127441
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394, 1.2470932006835938, 1.2496857643127441]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6912 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 1.069491623798531
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.5871135413565792
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 1.0957103906185672
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.612523774878654
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.9713927476005604
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7424, dtype=torch.float64), 0, 0, tensor(0.2576, dtype=torch.float64), 0, 0, 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.04191629066424122, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.7424, dtype=torch.float64), tensor(1.0009e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2576, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1128e-17, dtype=torch.float64), tensor(1.3406e-17, dtype=torch.float64), tensor(1.2309e-19, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4192, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.742
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.258
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04191629066424122,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.04191629066424122
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.42it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.36it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.47it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.05it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.75it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 61.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.93it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.59it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.63it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.08it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 59.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.41it/s]
Evaluation performance at step 25: 0.66
{'loss': 2.8355, 'grad_norm': 0.7318129539489746, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.66}
{'eval_loss': 1.1136482954025269, 'eval_runtime': 5.171, 'eval_samples_per_second': 193.194, 'eval_steps_per_second': 12.183, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.02it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.45it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.02it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 52.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.43it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.75it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.72it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.87it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 51.85it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.83it/s]
Evaluation performance at step 50: 0.63
{'loss': 0.9858, 'grad_norm': 0.3862825930118561, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 0.8730660676956177, 'eval_runtime': 5.1245, 'eval_samples_per_second': 194.945, 'eval_steps_per_second': 12.294, 'epoch': 0.08}
{'loss': 0.8767, 'grad_norm': 0.24722731113433838, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8466731905937195, 'eval_runtime': 5.1149, 'eval_samples_per_second': 195.314, 'eval_steps_per_second': 12.317, 'epoch': 0.12}
{'loss': 0.8354, 'grad_norm': 0.21037527918815613, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8342010378837585, 'eval_runtime': 5.1006, 'eval_samples_per_second': 195.858, 'eval_steps_per_second': 12.351, 'epoch': 0.16}
{'loss': 0.8323, 'grad_norm': 0.21825513243675232, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8183093667030334, 'eval_runtime': 5.1069, 'eval_samples_per_second': 195.619, 'eval_steps_per_second': 12.336, 'epoch': 0.2}
{'loss': 0.8301, 'grad_norm': 0.2191162407398224, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8066059350967407, 'eval_runtime': 5.1122, 'eval_samples_per_second': 195.416, 'eval_steps_per_second': 12.324, 'epoch': 0.24}
{'loss': 0.8388, 'grad_norm': 0.23402436077594757, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7942426204681396, 'eval_runtime': 5.1142, 'eval_samples_per_second': 195.339, 'eval_steps_per_second': 12.319, 'epoch': 0.28}
{'loss': 0.8137, 'grad_norm': 0.23003198206424713, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7857418060302734, 'eval_runtime': 5.1191, 'eval_samples_per_second': 195.152, 'eval_steps_per_second': 12.307, 'epoch': 0.32}
{'loss': 0.8089, 'grad_norm': 0.21823076903820038, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7694971561431885, 'eval_runtime': 5.1193, 'eval_samples_per_second': 195.144, 'eval_steps_per_second': 12.306, 'epoch': 0.36}
{'loss': 0.8037, 'grad_norm': 0.20705963671207428, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.757359504699707, 'eval_runtime': 5.1077, 'eval_samples_per_second': 195.587, 'eval_steps_per_second': 12.334, 'epoch': 0.4}
{'loss': 0.791, 'grad_norm': 0.222114697098732, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7454302310943604, 'eval_runtime': 5.1003, 'eval_samples_per_second': 195.869, 'eval_steps_per_second': 12.352, 'epoch': 0.44}
{'loss': 0.8123, 'grad_norm': 0.20841513574123383, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7332661151885986, 'eval_runtime': 5.0929, 'eval_samples_per_second': 196.155, 'eval_steps_per_second': 12.37, 'epoch': 0.48}
{'loss': 0.7919, 'grad_norm': 0.2195228636264801, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7259055972099304, 'eval_runtime': 5.0987, 'eval_samples_per_second': 195.933, 'eval_steps_per_second': 12.356, 'epoch': 0.52}
{'loss': 0.791, 'grad_norm': 0.2337220013141632, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7137385606765747, 'eval_runtime': 5.1015, 'eval_samples_per_second': 195.823, 'eval_steps_per_second': 12.349, 'epoch': 0.56}
{'loss': 0.7694, 'grad_norm': 0.23056921362876892, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6998860239982605, 'eval_runtime': 5.1224, 'eval_samples_per_second': 195.027, 'eval_steps_per_second': 12.299, 'epoch': 0.6}
{'loss': 0.752, 'grad_norm': 0.23177118599414825, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6870774626731873, 'eval_runtime': 5.0964, 'eval_samples_per_second': 196.02, 'eval_steps_per_second': 12.362, 'epoch': 0.64}
{'loss': 0.7555, 'grad_norm': 0.27857908606529236, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6821669340133667, 'eval_runtime': 5.0928, 'eval_samples_per_second': 196.159, 'eval_steps_per_second': 12.37, 'epoch': 0.68}
{'loss': 0.7603, 'grad_norm': 0.23409458994865417, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6692001819610596, 'eval_runtime': 5.0943, 'eval_samples_per_second': 196.101, 'eval_steps_per_second': 12.367, 'epoch': 0.72}
{'loss': 0.7399, 'grad_norm': 0.27615082263946533, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.65986168384552, 'eval_runtime': 5.0969, 'eval_samples_per_second': 196.0, 'eval_steps_per_second': 12.36, 'epoch': 0.76}
{'loss': 0.7189, 'grad_norm': 0.22954022884368896, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6536664962768555, 'eval_runtime': 5.0972, 'eval_samples_per_second': 195.99, 'eval_steps_per_second': 12.36, 'epoch': 0.8}
{'loss': 0.7311, 'grad_norm': 0.24161241948604584, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.645016610622406, 'eval_runtime': 5.098, 'eval_samples_per_second': 195.96, 'eval_steps_per_second': 12.358, 'epoch': 0.84}
{'loss': 0.7369, 'grad_norm': 0.26315099000930786, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6355065107345581, 'eval_runtime': 5.1004, 'eval_samples_per_second': 195.868, 'eval_steps_per_second': 12.352, 'epoch': 0.88}
{'loss': 0.7232, 'grad_norm': 0.24075768887996674, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6299795508384705, 'eval_runtime': 5.0956, 'eval_samples_per_second': 196.051, 'eval_steps_per_second': 12.364, 'epoch': 0.92}
{'loss': 0.7202, 'grad_norm': 0.2397887259721756, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6248350143432617, 'eval_runtime': 5.119, 'eval_samples_per_second': 195.156, 'eval_steps_per_second': 12.307, 'epoch': 0.96}
{'loss': 0.7179, 'grad_norm': 0.24907898902893066, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6237549781799316, 'eval_runtime': 5.1068, 'eval_samples_per_second': 195.621, 'eval_steps_per_second': 12.336, 'epoch': 1.0}
{'train_runtime': 308.2969, 'train_samples_per_second': 32.433, 'train_steps_per_second': 2.027, 'train_loss': 0.8708932922363282, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1136482954025269, 0.8730660676956177, 0.8466731905937195, 0.8342010378837585, 0.8183093667030334, 0.8066059350967407, 0.7942426204681396, 0.7857418060302734, 0.7694971561431885, 0.757359504699707, 0.7454302310943604, 0.7332661151885986, 0.7259055972099304, 0.7137385606765747, 0.6998860239982605, 0.6870774626731873, 0.6821669340133667, 0.6692001819610596, 0.65986168384552, 0.6536664962768555, 0.645016610622406, 0.6355065107345581, 0.6299795508384705, 0.6248350143432617, 0.6237549781799316], 'performance': [0.66, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:30,  3.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 48.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 61.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 65.42it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 64.39it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.66, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.2384381294250488
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394, 1.2470932006835938, 1.2496857643127441, 1.2384381294250488]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1963 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.6983778002332243
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 1.2275296654616172
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.6139688526575353
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.6575143752975032
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 1.0124422431296083
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7609, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2391, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 0, 1, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.7609, dtype=torch.float64), tensor(1.2298e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7337e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.4801e-17, dtype=torch.float64), tensor(0.2391, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.761
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.239
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.52it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.61it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.80it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.27it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.01it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.74it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 61.64it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.33it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 62.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.93it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 60.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.93it/s]
Evaluation performance at step 25: 0.64
{'loss': 3.0033, 'grad_norm': 0.4092892110347748, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.528997540473938, 'eval_runtime': 7.3792, 'eval_samples_per_second': 135.381, 'eval_steps_per_second': 8.538, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.62it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.38it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.59it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.58it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.81it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.21it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.45it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.77it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.92it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 56.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.59it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.22it/s]
Evaluation performance at step 50: 0.63
{'loss': 1.3636, 'grad_norm': 0.21817751228809357, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 1.2928366661071777, 'eval_runtime': 7.3944, 'eval_samples_per_second': 135.102, 'eval_steps_per_second': 8.52, 'epoch': 0.08}
{'loss': 1.2528, 'grad_norm': 0.24719072878360748, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1654765605926514, 'eval_runtime': 7.4408, 'eval_samples_per_second': 134.26, 'eval_steps_per_second': 8.467, 'epoch': 0.12}
{'loss': 1.1678, 'grad_norm': 0.23588742315769196, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.101121425628662, 'eval_runtime': 7.428, 'eval_samples_per_second': 134.492, 'eval_steps_per_second': 8.481, 'epoch': 0.16}
{'loss': 1.1488, 'grad_norm': 0.19662852585315704, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0517736673355103, 'eval_runtime': 7.4529, 'eval_samples_per_second': 134.041, 'eval_steps_per_second': 8.453, 'epoch': 0.2}
{'loss': 1.0878, 'grad_norm': 0.3635745048522949, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0285004377365112, 'eval_runtime': 7.4429, 'eval_samples_per_second': 134.223, 'eval_steps_per_second': 8.464, 'epoch': 0.24}
{'loss': 1.0889, 'grad_norm': 0.2306874692440033, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0082502365112305, 'eval_runtime': 7.4534, 'eval_samples_per_second': 134.032, 'eval_steps_per_second': 8.452, 'epoch': 0.28}
{'loss': 1.019, 'grad_norm': 0.20505711436271667, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9928626418113708, 'eval_runtime': 7.4485, 'eval_samples_per_second': 134.121, 'eval_steps_per_second': 8.458, 'epoch': 0.32}
{'loss': 1.0783, 'grad_norm': 0.16967496275901794, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9767605066299438, 'eval_runtime': 7.4484, 'eval_samples_per_second': 134.123, 'eval_steps_per_second': 8.458, 'epoch': 0.36}
{'loss': 0.9885, 'grad_norm': 0.2178170084953308, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9599447846412659, 'eval_runtime': 7.4438, 'eval_samples_per_second': 134.205, 'eval_steps_per_second': 8.463, 'epoch': 0.4}
{'loss': 1.0261, 'grad_norm': 0.21098937094211578, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9488507509231567, 'eval_runtime': 7.4428, 'eval_samples_per_second': 134.223, 'eval_steps_per_second': 8.465, 'epoch': 0.44}
{'loss': 1.0967, 'grad_norm': 0.2291695773601532, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9332119226455688, 'eval_runtime': 7.4205, 'eval_samples_per_second': 134.626, 'eval_steps_per_second': 8.49, 'epoch': 0.48}
{'loss': 1.0121, 'grad_norm': 0.2372344583272934, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.924897313117981, 'eval_runtime': 7.4234, 'eval_samples_per_second': 134.574, 'eval_steps_per_second': 8.487, 'epoch': 0.52}
{'loss': 0.951, 'grad_norm': 0.20157521963119507, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9138527512550354, 'eval_runtime': 7.4183, 'eval_samples_per_second': 134.667, 'eval_steps_per_second': 8.493, 'epoch': 0.56}
{'loss': 1.0514, 'grad_norm': 0.23264364898204803, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8994392156600952, 'eval_runtime': 7.406, 'eval_samples_per_second': 134.89, 'eval_steps_per_second': 8.507, 'epoch': 0.6}
{'loss': 0.9647, 'grad_norm': 0.22376777231693268, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8902272582054138, 'eval_runtime': 7.4323, 'eval_samples_per_second': 134.413, 'eval_steps_per_second': 8.476, 'epoch': 0.64}
{'loss': 1.0097, 'grad_norm': 0.22524698078632355, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8778231739997864, 'eval_runtime': 7.4461, 'eval_samples_per_second': 134.164, 'eval_steps_per_second': 8.461, 'epoch': 0.68}
{'loss': 0.9572, 'grad_norm': 0.22619354724884033, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8677729964256287, 'eval_runtime': 7.4554, 'eval_samples_per_second': 133.997, 'eval_steps_per_second': 8.45, 'epoch': 0.72}
{'loss': 0.9629, 'grad_norm': 0.26088178157806396, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8599756956100464, 'eval_runtime': 7.4641, 'eval_samples_per_second': 133.841, 'eval_steps_per_second': 8.44, 'epoch': 0.76}
{'loss': 0.917, 'grad_norm': 0.3493427634239197, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8482254147529602, 'eval_runtime': 7.4536, 'eval_samples_per_second': 134.03, 'eval_steps_per_second': 8.452, 'epoch': 0.8}
{'loss': 0.9618, 'grad_norm': 0.2673338055610657, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8375269770622253, 'eval_runtime': 7.446, 'eval_samples_per_second': 134.167, 'eval_steps_per_second': 8.461, 'epoch': 0.84}
{'loss': 0.9771, 'grad_norm': 0.2435690462589264, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8340517282485962, 'eval_runtime': 7.4549, 'eval_samples_per_second': 134.006, 'eval_steps_per_second': 8.451, 'epoch': 0.88}
{'loss': 0.9786, 'grad_norm': 0.333404541015625, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8266813158988953, 'eval_runtime': 7.465, 'eval_samples_per_second': 133.824, 'eval_steps_per_second': 8.439, 'epoch': 0.92}
{'loss': 0.9355, 'grad_norm': 0.21796680986881256, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8228728175163269, 'eval_runtime': 7.4667, 'eval_samples_per_second': 133.794, 'eval_steps_per_second': 8.437, 'epoch': 0.96}
{'loss': 0.9686, 'grad_norm': 0.24149666726589203, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.820652186870575, 'eval_runtime': 7.519, 'eval_samples_per_second': 132.863, 'eval_steps_per_second': 8.379, 'epoch': 1.0}
{'train_runtime': 405.3137, 'train_samples_per_second': 24.67, 'train_steps_per_second': 1.542, 'train_loss': 1.1187788482666015, 'epoch': 1.0}
train_results:  {'eval_loss': [1.528997540473938, 1.2928366661071777, 1.1654765605926514, 1.101121425628662, 1.0517736673355103, 1.0285004377365112, 1.0082502365112305, 0.9928626418113708, 0.9767605066299438, 0.9599447846412659, 0.9488507509231567, 0.9332119226455688, 0.924897313117981, 0.9138527512550354, 0.8994392156600952, 0.8902272582054138, 0.8778231739997864, 0.8677729964256287, 0.8599756956100464, 0.8482254147529602, 0.8375269770622253, 0.8340517282485962, 0.8266813158988953, 0.8228728175163269, 0.820652186870575], 'performance': [0.64, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:34,  2.91it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.14it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 73.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.53it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.2506792545318604
current iteration best possible performance (full train run):  0.651
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394, 1.2470932006835938, 1.2496857643127441, 1.2384381294250488, 1.2506792545318604]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5913 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 1.0094968223466092
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.8112153960747495
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.642917859738558
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.5530439851752881
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.9344112086463351
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.7385, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2615, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.1102230246251783e-17, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0.7385, dtype=torch.float64), tensor(2.2679e-17, dtype=torch.float64), tensor(5.6449e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.0766e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2615, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8492e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.1102e-16, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.738
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.262
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.1102230246251783e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.1102230246251783e-17
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.15it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.40it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.64it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.88it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.78it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.05it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.43it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.24it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.42it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.81it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.19it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.7927, 'grad_norm': 0.49741366505622864, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.3836277723312378, 'eval_runtime': 7.8821, 'eval_samples_per_second': 126.743, 'eval_steps_per_second': 7.993, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.11it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.03it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.33it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.98it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.17it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.83it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.11it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.81it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.33it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.2516, 'grad_norm': 0.44147181510925293, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.1503658294677734, 'eval_runtime': 7.8777, 'eval_samples_per_second': 126.813, 'eval_steps_per_second': 7.997, 'epoch': 0.08}
{'loss': 1.1334, 'grad_norm': 0.2892902195453644, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0894144773483276, 'eval_runtime': 7.9003, 'eval_samples_per_second': 126.45, 'eval_steps_per_second': 7.974, 'epoch': 0.12}
{'loss': 1.1159, 'grad_norm': 0.2146235853433609, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0612421035766602, 'eval_runtime': 7.9195, 'eval_samples_per_second': 126.144, 'eval_steps_per_second': 7.955, 'epoch': 0.16}
{'loss': 1.0891, 'grad_norm': 0.24010725319385529, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0412607192993164, 'eval_runtime': 7.9124, 'eval_samples_per_second': 126.258, 'eval_steps_per_second': 7.962, 'epoch': 0.2}
{'loss': 1.1105, 'grad_norm': 0.22956693172454834, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0249816179275513, 'eval_runtime': 7.9205, 'eval_samples_per_second': 126.129, 'eval_steps_per_second': 7.954, 'epoch': 0.24}
{'loss': 1.1183, 'grad_norm': 0.3879221975803375, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0115844011306763, 'eval_runtime': 7.9418, 'eval_samples_per_second': 125.79, 'eval_steps_per_second': 7.933, 'epoch': 0.28}
{'loss': 1.1461, 'grad_norm': 0.5272737145423889, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9936773777008057, 'eval_runtime': 7.9344, 'eval_samples_per_second': 125.907, 'eval_steps_per_second': 7.94, 'epoch': 0.32}
{'loss': 1.0022, 'grad_norm': 0.19759008288383484, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9800109267234802, 'eval_runtime': 7.9336, 'eval_samples_per_second': 125.921, 'eval_steps_per_second': 7.941, 'epoch': 0.36}
{'loss': 1.035, 'grad_norm': 0.2824316918849945, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9638218879699707, 'eval_runtime': 7.9295, 'eval_samples_per_second': 125.985, 'eval_steps_per_second': 7.945, 'epoch': 0.4}
{'loss': 1.0164, 'grad_norm': 0.30482417345046997, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9420716166496277, 'eval_runtime': 7.9053, 'eval_samples_per_second': 126.372, 'eval_steps_per_second': 7.969, 'epoch': 0.44}
{'loss': 1.1046, 'grad_norm': 0.36173591017723083, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9233964085578918, 'eval_runtime': 7.9124, 'eval_samples_per_second': 126.257, 'eval_steps_per_second': 7.962, 'epoch': 0.48}
{'loss': 0.9921, 'grad_norm': 0.3236004114151001, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.920640230178833, 'eval_runtime': 7.9101, 'eval_samples_per_second': 126.294, 'eval_steps_per_second': 7.964, 'epoch': 0.52}
{'loss': 1.0401, 'grad_norm': 0.2499597668647766, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9031215310096741, 'eval_runtime': 7.9492, 'eval_samples_per_second': 125.673, 'eval_steps_per_second': 7.925, 'epoch': 0.56}
{'loss': 1.0154, 'grad_norm': 0.3116816282272339, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8897410035133362, 'eval_runtime': 7.9318, 'eval_samples_per_second': 125.949, 'eval_steps_per_second': 7.943, 'epoch': 0.6}
{'loss': 0.9921, 'grad_norm': 0.28870725631713867, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8751828670501709, 'eval_runtime': 7.9386, 'eval_samples_per_second': 125.841, 'eval_steps_per_second': 7.936, 'epoch': 0.64}
{'loss': 0.9431, 'grad_norm': 0.2405337244272232, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8609369993209839, 'eval_runtime': 7.9121, 'eval_samples_per_second': 126.262, 'eval_steps_per_second': 7.962, 'epoch': 0.68}
{'loss': 0.9628, 'grad_norm': 0.38222211599349976, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8433356285095215, 'eval_runtime': 7.8998, 'eval_samples_per_second': 126.458, 'eval_steps_per_second': 7.975, 'epoch': 0.72}
{'loss': 1.0321, 'grad_norm': 0.30707263946533203, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8340283632278442, 'eval_runtime': 7.9022, 'eval_samples_per_second': 126.42, 'eval_steps_per_second': 7.972, 'epoch': 0.76}
{'loss': 0.9836, 'grad_norm': 0.2747913897037506, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8183711171150208, 'eval_runtime': 7.9013, 'eval_samples_per_second': 126.434, 'eval_steps_per_second': 7.973, 'epoch': 0.8}
{'loss': 0.9673, 'grad_norm': 0.2691607177257538, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8134307861328125, 'eval_runtime': 7.9122, 'eval_samples_per_second': 126.261, 'eval_steps_per_second': 7.962, 'epoch': 0.84}
{'loss': 0.9925, 'grad_norm': 0.26751095056533813, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.803952157497406, 'eval_runtime': 7.9044, 'eval_samples_per_second': 126.386, 'eval_steps_per_second': 7.97, 'epoch': 0.88}
{'loss': 0.973, 'grad_norm': 0.2435722053050995, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7967678308486938, 'eval_runtime': 7.8998, 'eval_samples_per_second': 126.459, 'eval_steps_per_second': 7.975, 'epoch': 0.92}
{'loss': 0.9531, 'grad_norm': 0.2516547739505768, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7937528491020203, 'eval_runtime': 7.902, 'eval_samples_per_second': 126.424, 'eval_steps_per_second': 7.973, 'epoch': 0.96}
{'loss': 0.9372, 'grad_norm': 0.2968434691429138, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7914590835571289, 'eval_runtime': 7.8991, 'eval_samples_per_second': 126.471, 'eval_steps_per_second': 7.976, 'epoch': 1.0}
{'train_runtime': 449.8568, 'train_samples_per_second': 22.227, 'train_steps_per_second': 1.389, 'train_loss': 1.1080109497070312, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3836277723312378, 1.1503658294677734, 1.0894144773483276, 1.0612421035766602, 1.0412607192993164, 1.0249816179275513, 1.0115844011306763, 0.9936773777008057, 0.9800109267234802, 0.9638218879699707, 0.9420716166496277, 0.9233964085578918, 0.920640230178833, 0.9031215310096741, 0.8897410035133362, 0.8751828670501709, 0.8609369993209839, 0.8433356285095215, 0.8340283632278442, 0.8183711171150208, 0.8134307861328125, 0.803952157497406, 0.7967678308486938, 0.7937528491020203, 0.7914590835571289], 'performance': [0.64, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.29it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.63it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.60it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 70.77it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.16it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2496057748794556
current iteration best possible performance (full train run):  0.63
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394, 1.2470932006835938, 1.2496857643127441, 1.2384381294250488, 1.2506792545318604, 1.2496057748794556]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5511 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.8570303132562425
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 1.0999645607338362
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 1.0706229851486038
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 1.1561217137323578
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 1.0296035424559136
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.9512, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.0488, dtype=torch.float64), 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.4905731076440046e-17, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.9512, dtype=torch.float64), tensor(4.0032e-17, dtype=torch.float64), tensor(3.0794e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0428e-17, dtype=torch.float64), tensor(0.0488, dtype=torch.float64), tensor(1.1562e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.4906e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.951
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.049
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.4905731076440046e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  2.4905731076440046e-17
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.14it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.43it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.91it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.17it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.73it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.78it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.65it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 53.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.94it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.7576, 'grad_norm': 0.40183618664741516, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.1569900512695312, 'eval_runtime': 6.1562, 'eval_samples_per_second': 162.275, 'eval_steps_per_second': 10.234, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  4.96it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.61it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.42it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 47.28it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.38it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.99it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.21it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.55it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 53.46it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.45it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.0303, 'grad_norm': 0.6166712641716003, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 0.952328622341156, 'eval_runtime': 6.1372, 'eval_samples_per_second': 162.777, 'eval_steps_per_second': 10.265, 'epoch': 0.08}
{'loss': 0.9586, 'grad_norm': 0.23890328407287598, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9083839058876038, 'eval_runtime': 6.1541, 'eval_samples_per_second': 162.331, 'eval_steps_per_second': 10.237, 'epoch': 0.12}
{'loss': 0.8921, 'grad_norm': 0.19022640585899353, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.883698046207428, 'eval_runtime': 6.164, 'eval_samples_per_second': 162.069, 'eval_steps_per_second': 10.221, 'epoch': 0.16}
{'loss': 0.8904, 'grad_norm': 0.18314337730407715, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8636146187782288, 'eval_runtime': 6.1604, 'eval_samples_per_second': 162.165, 'eval_steps_per_second': 10.227, 'epoch': 0.2}
{'loss': 0.8873, 'grad_norm': 0.2080836296081543, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8457034826278687, 'eval_runtime': 6.1693, 'eval_samples_per_second': 161.932, 'eval_steps_per_second': 10.212, 'epoch': 0.24}
{'loss': 0.8483, 'grad_norm': 0.22615192830562592, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8311683535575867, 'eval_runtime': 6.1702, 'eval_samples_per_second': 161.907, 'eval_steps_per_second': 10.21, 'epoch': 0.28}
{'loss': 0.8777, 'grad_norm': 0.21750885248184204, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8083768486976624, 'eval_runtime': 6.1686, 'eval_samples_per_second': 161.95, 'eval_steps_per_second': 10.213, 'epoch': 0.32}
{'loss': 0.8266, 'grad_norm': 0.19373346865177155, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7930190563201904, 'eval_runtime': 6.1719, 'eval_samples_per_second': 161.862, 'eval_steps_per_second': 10.208, 'epoch': 0.36}
{'loss': 0.8275, 'grad_norm': 0.23565010726451874, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7753677368164062, 'eval_runtime': 6.1753, 'eval_samples_per_second': 161.773, 'eval_steps_per_second': 10.202, 'epoch': 0.4}
{'loss': 0.8087, 'grad_norm': 0.20300374925136566, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.75739985704422, 'eval_runtime': 6.1722, 'eval_samples_per_second': 161.854, 'eval_steps_per_second': 10.207, 'epoch': 0.44}
{'loss': 0.8049, 'grad_norm': 0.21630090475082397, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7420294880867004, 'eval_runtime': 6.1724, 'eval_samples_per_second': 161.849, 'eval_steps_per_second': 10.207, 'epoch': 0.48}
{'loss': 0.8266, 'grad_norm': 0.2273463010787964, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7293013334274292, 'eval_runtime': 6.1521, 'eval_samples_per_second': 162.384, 'eval_steps_per_second': 10.24, 'epoch': 0.52}
{'loss': 0.7639, 'grad_norm': 0.22556057572364807, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.715146541595459, 'eval_runtime': 6.1353, 'eval_samples_per_second': 162.828, 'eval_steps_per_second': 10.268, 'epoch': 0.56}
{'loss': 0.8034, 'grad_norm': 0.21454448997974396, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6988658308982849, 'eval_runtime': 6.1381, 'eval_samples_per_second': 162.755, 'eval_steps_per_second': 10.264, 'epoch': 0.6}
{'loss': 0.7712, 'grad_norm': 0.2257114201784134, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6881344318389893, 'eval_runtime': 6.1323, 'eval_samples_per_second': 162.907, 'eval_steps_per_second': 10.273, 'epoch': 0.64}
{'loss': 0.7394, 'grad_norm': 0.21821078658103943, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.666162371635437, 'eval_runtime': 6.1366, 'eval_samples_per_second': 162.793, 'eval_steps_per_second': 10.266, 'epoch': 0.68}
{'loss': 0.7951, 'grad_norm': 0.23898376524448395, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6496021151542664, 'eval_runtime': 6.1397, 'eval_samples_per_second': 162.711, 'eval_steps_per_second': 10.261, 'epoch': 0.72}
{'loss': 0.7444, 'grad_norm': 0.2969079613685608, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6383891105651855, 'eval_runtime': 6.1471, 'eval_samples_per_second': 162.516, 'eval_steps_per_second': 10.249, 'epoch': 0.76}
{'loss': 0.7438, 'grad_norm': 0.2401120960712433, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6234848499298096, 'eval_runtime': 6.1506, 'eval_samples_per_second': 162.422, 'eval_steps_per_second': 10.243, 'epoch': 0.8}
{'loss': 0.748, 'grad_norm': 0.2850622534751892, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6096495389938354, 'eval_runtime': 6.2079, 'eval_samples_per_second': 160.923, 'eval_steps_per_second': 10.148, 'epoch': 0.84}
{'loss': 0.7212, 'grad_norm': 0.25880783796310425, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6037449836730957, 'eval_runtime': 6.1821, 'eval_samples_per_second': 161.596, 'eval_steps_per_second': 10.191, 'epoch': 0.88}
{'loss': 0.6923, 'grad_norm': 0.3983107805252075, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5858221054077148, 'eval_runtime': 6.1833, 'eval_samples_per_second': 161.565, 'eval_steps_per_second': 10.189, 'epoch': 0.92}
{'loss': 0.7023, 'grad_norm': 0.2630298137664795, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5840845108032227, 'eval_runtime': 6.2124, 'eval_samples_per_second': 160.806, 'eval_steps_per_second': 10.141, 'epoch': 0.96}
{'loss': 0.7158, 'grad_norm': 0.292611688375473, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5800545811653137, 'eval_runtime': 6.1867, 'eval_samples_per_second': 161.477, 'eval_steps_per_second': 10.183, 'epoch': 1.0}
{'train_runtime': 362.9031, 'train_samples_per_second': 27.553, 'train_steps_per_second': 1.722, 'train_loss': 0.8870957946777344, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1569900512695312, 0.952328622341156, 0.9083839058876038, 0.883698046207428, 0.8636146187782288, 0.8457034826278687, 0.8311683535575867, 0.8083768486976624, 0.7930190563201904, 0.7753677368164062, 0.75739985704422, 0.7420294880867004, 0.7293013334274292, 0.715146541595459, 0.6988658308982849, 0.6881344318389893, 0.666162371635437, 0.6496021151542664, 0.6383891105651855, 0.6234848499298096, 0.6096495389938354, 0.6037449836730957, 0.5858221054077148, 0.5840845108032227, 0.5800545811653137], 'performance': [0.65, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:34,  2.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 29.85it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 47.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.23it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 65.91it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.68it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2497221231460571
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394, 1.2470932006835938, 1.2496857643127441, 1.2384381294250488, 1.2506792545318604, 1.2496057748794556, 1.2497221231460571]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4481 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.9812257342090995
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 1.1522863010667768
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.8082612666498163
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 1.0870207023422065
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.6757562612136082
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.9046, dtype=torch.float64), 0, 0, 0, 0, tensor(0.0954, dtype=torch.float64), 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.9046, dtype=torch.float64), tensor(4.0005e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0954, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7626e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.905
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.095
  wikitext: 0
  mmlu: 0
  arc_challenge: 0

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.11it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 11.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:04, 20.11it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:02, 27.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 33.44it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 39.34it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.16it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 49.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.61it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 53.66it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 53.64it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 54.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 41.13it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.7528, 'grad_norm': 0.8762494921684265, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.105663537979126, 'eval_runtime': 5.429, 'eval_samples_per_second': 184.01, 'eval_steps_per_second': 11.604, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.11it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.96it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.34it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.01it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.17it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.38it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.63it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.00it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.98it/s]
Evaluation performance at step 50: 0.63
{'loss': 0.9759, 'grad_norm': 0.39283478260040283, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 0.8746368885040283, 'eval_runtime': 5.4438, 'eval_samples_per_second': 183.511, 'eval_steps_per_second': 11.573, 'epoch': 0.08}
{'loss': 0.875, 'grad_norm': 0.2091812789440155, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8435249328613281, 'eval_runtime': 5.4622, 'eval_samples_per_second': 182.895, 'eval_steps_per_second': 11.534, 'epoch': 0.12}
{'loss': 0.8561, 'grad_norm': 0.21573114395141602, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8200538754463196, 'eval_runtime': 5.4392, 'eval_samples_per_second': 183.665, 'eval_steps_per_second': 11.582, 'epoch': 0.16}
{'loss': 0.8489, 'grad_norm': 0.18449416756629944, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8029018044471741, 'eval_runtime': 5.4378, 'eval_samples_per_second': 183.712, 'eval_steps_per_second': 11.585, 'epoch': 0.2}
{'loss': 0.8368, 'grad_norm': 0.2066335827112198, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7839969992637634, 'eval_runtime': 5.4437, 'eval_samples_per_second': 183.515, 'eval_steps_per_second': 11.573, 'epoch': 0.24}
{'loss': 0.8071, 'grad_norm': 0.2075604349374771, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7726563215255737, 'eval_runtime': 5.444, 'eval_samples_per_second': 183.504, 'eval_steps_per_second': 11.572, 'epoch': 0.28}
{'loss': 0.8148, 'grad_norm': 0.21488629281520844, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7522845268249512, 'eval_runtime': 5.4287, 'eval_samples_per_second': 184.021, 'eval_steps_per_second': 11.605, 'epoch': 0.32}
{'loss': 0.7772, 'grad_norm': 0.184194877743721, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7361891865730286, 'eval_runtime': 5.4319, 'eval_samples_per_second': 183.914, 'eval_steps_per_second': 11.598, 'epoch': 0.36}
{'loss': 0.7707, 'grad_norm': 0.21991389989852905, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7184734344482422, 'eval_runtime': 5.4325, 'eval_samples_per_second': 183.894, 'eval_steps_per_second': 11.597, 'epoch': 0.4}
{'loss': 0.7795, 'grad_norm': 0.21393801271915436, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.705252468585968, 'eval_runtime': 5.4379, 'eval_samples_per_second': 183.71, 'eval_steps_per_second': 11.585, 'epoch': 0.44}
{'loss': 0.7608, 'grad_norm': 0.19059912860393524, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6917451024055481, 'eval_runtime': 5.4551, 'eval_samples_per_second': 183.13, 'eval_steps_per_second': 11.549, 'epoch': 0.48}
{'loss': 0.7534, 'grad_norm': 0.2397162914276123, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6746147871017456, 'eval_runtime': 5.4486, 'eval_samples_per_second': 183.35, 'eval_steps_per_second': 11.563, 'epoch': 0.52}
{'loss': 0.7382, 'grad_norm': 0.221236452460289, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6612383723258972, 'eval_runtime': 5.456, 'eval_samples_per_second': 183.101, 'eval_steps_per_second': 11.547, 'epoch': 0.56}
{'loss': 0.7397, 'grad_norm': 0.23735742270946503, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6481795907020569, 'eval_runtime': 5.4555, 'eval_samples_per_second': 183.12, 'eval_steps_per_second': 11.548, 'epoch': 0.6}
{'loss': 0.7286, 'grad_norm': 0.25767332315444946, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6302039623260498, 'eval_runtime': 5.4682, 'eval_samples_per_second': 182.694, 'eval_steps_per_second': 11.521, 'epoch': 0.64}
{'loss': 0.7289, 'grad_norm': 0.2267439216375351, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.6155171990394592, 'eval_runtime': 5.4649, 'eval_samples_per_second': 182.803, 'eval_steps_per_second': 11.528, 'epoch': 0.68}
{'loss': 0.7268, 'grad_norm': 0.24231700599193573, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6042426824569702, 'eval_runtime': 5.4745, 'eval_samples_per_second': 182.482, 'eval_steps_per_second': 11.508, 'epoch': 0.72}
{'loss': 0.6972, 'grad_norm': 0.24059560894966125, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5858414769172668, 'eval_runtime': 5.5221, 'eval_samples_per_second': 180.909, 'eval_steps_per_second': 11.409, 'epoch': 0.76}
{'loss': 0.6791, 'grad_norm': 0.28960493206977844, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.570816695690155, 'eval_runtime': 5.5104, 'eval_samples_per_second': 181.293, 'eval_steps_per_second': 11.433, 'epoch': 0.8}
{'loss': 0.6836, 'grad_norm': 0.27139315009117126, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5583084225654602, 'eval_runtime': 5.4961, 'eval_samples_per_second': 181.765, 'eval_steps_per_second': 11.463, 'epoch': 0.84}
{'loss': 0.6683, 'grad_norm': 0.27039507031440735, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5502617359161377, 'eval_runtime': 5.4977, 'eval_samples_per_second': 181.714, 'eval_steps_per_second': 11.459, 'epoch': 0.88}
{'loss': 0.6949, 'grad_norm': 0.256405770778656, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.544196367263794, 'eval_runtime': 5.4661, 'eval_samples_per_second': 182.763, 'eval_steps_per_second': 11.526, 'epoch': 0.92}
{'loss': 0.6726, 'grad_norm': 0.2861897945404053, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5368391871452332, 'eval_runtime': 5.4779, 'eval_samples_per_second': 182.369, 'eval_steps_per_second': 11.501, 'epoch': 0.96}
{'loss': 0.67, 'grad_norm': 0.2716388702392578, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5349456071853638, 'eval_runtime': 5.4746, 'eval_samples_per_second': 182.478, 'eval_steps_per_second': 11.508, 'epoch': 1.0}
{'train_runtime': 331.9838, 'train_samples_per_second': 30.119, 'train_steps_per_second': 1.883, 'train_loss': 0.8414768402099609, 'epoch': 1.0}
train_results:  {'eval_loss': [1.105663537979126, 0.8746368885040283, 0.8435249328613281, 0.8200538754463196, 0.8029018044471741, 0.7839969992637634, 0.7726563215255737, 0.7522845268249512, 0.7361891865730286, 0.7184734344482422, 0.705252468585968, 0.6917451024055481, 0.6746147871017456, 0.6612383723258972, 0.6481795907020569, 0.6302039623260498, 0.6155171990394592, 0.6042426824569702, 0.5858414769172668, 0.570816695690155, 0.5583084225654602, 0.5502617359161377, 0.544196367263794, 0.5368391871452332, 0.5349456071853638], 'performance': [0.65, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:34,  2.88it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 47.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.42it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 72.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.17it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.2496123313903809
current iteration best possible performance (full train run):  0.63
max performance so far:  0.7140000000000001
BO observations:  [0.9851824641227722, 1.1012190580368042, 1.11042058467865, 1.0049550533294678, 0.7605749368667603, 1.1872422695159912, 1.0541033744812012, 1.2495856285095215, 1.2492868900299072, 1.2484190464019775, 0.7614403963088989, 1.2480429410934448, 1.2356748580932617, 0.6988314986228943, 0.7528432011604309, 1.2379395961761475, 1.2422839403152466, 0.7603490352630615, 1.2489604949951172, 1.239900827407837, 0.7607021331787109, 1.2495477199554443, 1.2450722455978394, 1.2470932006835938, 1.2496857643127441, 1.2384381294250488, 1.2506792545318604, 1.2496057748794556, 1.2497221231460571, 1.2496123313903809]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2387 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.9964541612352451
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.5534283153469047
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.6035758167171782
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.6029337780851711
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 1.007448683929918
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(1., dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1584e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.8754e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.6405, 0.6405, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/triviaqa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 2, 0, 0, 0, 1, 0, 29, 0.05396168787624587, 37, 1]
Checking history sample input_X_between_0_1:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2265625, 0.5396168787624587, 0.7708333333333334, 1.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 7, 0, 0, 1, 0, 0, 101, 0.003930648435578799, 29, 1]
Checking history sample input_X_between_0_1:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7890625, 0.039306484355787985, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 22, 1, 0, 0, 1, 1, 3, 0.04742737743265794, 11, 1]
Checking history sample input_X_between_0_1:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 0.6875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0234375, 0.47427377432657936, 0.22916666666666666, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 13, 1, 0, 1, 1, 1, 123, 0.09371289547650785, 48, 0]
Checking history sample input_X_between_0_1:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 0.40625, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9609375, 0.9371289547650785, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 13, 0, 0, 1, 1, 0, 127, 0.09560839072699708, 9, 1]
Checking history sample input_X_between_0_1:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 0.40625, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9921875, 0.9560839072699707, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 9, 0, 0, 1, 1, 1, 16, 0.08052765798611784, 41, 1]
Checking history sample input_X_between_0_1:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 0.28125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.8052765798611784, 0.8541666666666666, 1.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 5, 0, 1, 0, 1, 0, 59, 0.05350476033435025, 24, 1]
Checking history sample input_X_between_0_1:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 0.15625, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4609375, 0.5350476033435024, 0.5, 1.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 2, 1, 0, 0, 1, 0, 52, 0.0964290393747826, 9, 0]
Checking history sample input_X_between_0_1:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 0.0625, 1.0, 0.0, 0.0, 1.0, 0.0, 0.40625, 0.964290393747826, 0.1875, 0.0]
Checking history sample performance at 625 steps:  0.44
Checking history sample input_X:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 7, 1, 1, 0, 0, 1, 121, 0.029877603091235272, 27, 0]
Checking history sample input_X_between_0_1:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 0.21875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9453125, 0.2987760309123527, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 5, 1, 1, 0, 0, 0, 99, 0.012004498902397865, 15, 0]
Checking history sample input_X_between_0_1:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 0.15625, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7734375, 0.12004498902397864, 0.3125, 0.0]
Checking history sample performance at 625 steps:  0.57
Checking history sample input_X:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 24, 1, 0, 1, 1, 1, 39, 0.002627655727844236, 41, 0]
Checking history sample input_X_between_0_1:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3046875, 0.02627655727844236, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 24, 0, 0, 0, 0, 1, 93, 0.03163409179138251, 48, 0]
Checking history sample input_X_between_0_1:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 0.75, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7265625, 0.31634091791382507, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 28, 1, 0, 0, 0, 0, 89, 0.0740409362882843, 48, 0]
Checking history sample input_X_between_0_1:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 0.875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.740409362882843, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.63
Checking history sample input_X:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 28, 1, 0, 1, 1, 1, 2, 0.052382422456560135, 28, 1]
Checking history sample input_X_between_0_1:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 0.875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.015625, 0.5238242245656013, 0.5833333333333334, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 11, 0, 0, 0, 1, 1, 54, 0.06588137610749685, 20, 1]
Checking history sample input_X_between_0_1:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 0.34375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.421875, 0.6588137610749685, 0.4166666666666667, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 27, 0, 0, 1, 1, 1, 76, 0.04383658958637696, 17, 0]
Checking history sample input_X_between_0_1:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 0.84375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.59375, 0.4383658958637696, 0.3541666666666667, 0.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 29, 0, 1, 0, 0, 1, 82, 0.07326489367366754, 40, 1]
Checking history sample input_X_between_0_1:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 0.90625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.640625, 0.7326489367366753, 0.8333333333333334, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 1, 0, 0, 0, 1, 0, 44, 0.09959850306478138, 19, 0]
Checking history sample input_X_between_0_1:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.34375, 0.9959850306478137, 0.3958333333333333, 0.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 26, 1, 1, 1, 0, 1, 67, 0.08108306564701856, 9, 1]
Checking history sample input_X_between_0_1:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 0.8125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5234375, 0.8108306564701856, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 29, 0, 0, 0, 1, 1, 29, 0.08410329802499458, 20, 0]
Checking history sample input_X_between_0_1:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 0.90625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2265625, 0.8410329802499458, 0.4166666666666667, 0.0]
Checking history sample performance at 625 steps:  0.61
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2618 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.7000464480432271
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.7000464480432271
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.7000464480432271
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.7000464480432271
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.7000464480432271
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.0778, dtype=torch.float64), tensor(0.0172, dtype=torch.float64), tensor(0.1145, dtype=torch.float64), 0, 0, tensor(0.1100, dtype=torch.float64), 0, tensor(0.0722, dtype=torch.float64), tensor(0.6015, dtype=torch.float64), 32, 1, 0, 0, 0, 0, 119, 0.018879750832746307, 31.43773300116672, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.0778, dtype=torch.float64), tensor(0.0172, dtype=torch.float64), tensor(0.1145, dtype=torch.float64), tensor(2.8888e-19, dtype=torch.float64), tensor(0.0069, dtype=torch.float64), tensor(0.1100, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0722, dtype=torch.float64), tensor(0.6015, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9294, dtype=torch.float64), tensor(0.1888, dtype=torch.float64), tensor(0.6550, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.078
  gsm8k: 0.017
  rowan_hellaswag: 0.114
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.11
  wikitext: 0
  mmlu: 0.072
  arc_challenge: 0.601

LoRA Parameters:
  lora_r: (119,)
  lora_dropout: (0.018879750832746307,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (31.43773300116672,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  119
lora dropout:  0.018879750832746307
lora alpha:  31.43773300116672
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 31,195,136 || all params: 8,061,456,384 || trainable%: 0.3870
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9927
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  992
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.50it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.22it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 49.23it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 57.12it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 62.19it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 72.26it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 75.66it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 80.31it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 79.69it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 79.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 71.30it/s]
Evaluation performance at step 25: 0.63
{'loss': 3.7784, 'grad_norm': 0.34553104639053345, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 2.8424947261810303, 'eval_runtime': 8.3764, 'eval_samples_per_second': 118.429, 'eval_steps_per_second': 7.402, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:13,  7.11it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 51.05it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 58.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 63.92it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 73.54it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:00<00:00, 73.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 71.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 78.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 78.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 78.42it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 71.09it/s]
Evaluation performance at step 50: 0.57
{'loss': 2.1771, 'grad_norm': 0.24565519392490387, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 1.7468229532241821, 'eval_runtime': 8.3824, 'eval_samples_per_second': 118.343, 'eval_steps_per_second': 7.396, 'epoch': 0.08}
{'loss': 1.6129, 'grad_norm': 0.13439473509788513, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.53086256980896, 'eval_runtime': 8.4025, 'eval_samples_per_second': 118.06, 'eval_steps_per_second': 7.379, 'epoch': 0.12}
{'loss': 1.4868, 'grad_norm': 0.1578826904296875, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.483065128326416, 'eval_runtime': 8.4902, 'eval_samples_per_second': 116.841, 'eval_steps_per_second': 7.303, 'epoch': 0.16}
{'loss': 1.4888, 'grad_norm': 0.10517196357250214, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.4588103294372559, 'eval_runtime': 8.5684, 'eval_samples_per_second': 115.774, 'eval_steps_per_second': 7.236, 'epoch': 0.2}
{'loss': 1.4179, 'grad_norm': 0.12161318212747574, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.44232177734375, 'eval_runtime': 8.5569, 'eval_samples_per_second': 115.93, 'eval_steps_per_second': 7.246, 'epoch': 0.24}
{'loss': 1.4224, 'grad_norm': 0.14509832859039307, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.429701328277588, 'eval_runtime': 8.5274, 'eval_samples_per_second': 116.33, 'eval_steps_per_second': 7.271, 'epoch': 0.28}
{'loss': 1.438, 'grad_norm': 0.12687350809574127, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.420255184173584, 'eval_runtime': 8.5225, 'eval_samples_per_second': 116.397, 'eval_steps_per_second': 7.275, 'epoch': 0.32}
{'loss': 1.3865, 'grad_norm': 0.11417850106954575, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.4141954183578491, 'eval_runtime': 8.5154, 'eval_samples_per_second': 116.495, 'eval_steps_per_second': 7.281, 'epoch': 0.36}
{'loss': 1.3927, 'grad_norm': 0.1087796613574028, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.408434271812439, 'eval_runtime': 8.5055, 'eval_samples_per_second': 116.63, 'eval_steps_per_second': 7.289, 'epoch': 0.4}
{'loss': 1.3721, 'grad_norm': 0.12051573395729065, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.400076985359192, 'eval_runtime': 8.4956, 'eval_samples_per_second': 116.767, 'eval_steps_per_second': 7.298, 'epoch': 0.44}
{'loss': 1.4217, 'grad_norm': 0.16112051904201508, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.3927277326583862, 'eval_runtime': 8.4854, 'eval_samples_per_second': 116.907, 'eval_steps_per_second': 7.307, 'epoch': 0.48}
{'loss': 1.378, 'grad_norm': 0.15060943365097046, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.3862043619155884, 'eval_runtime': 8.4715, 'eval_samples_per_second': 117.099, 'eval_steps_per_second': 7.319, 'epoch': 0.52}
{'loss': 1.3401, 'grad_norm': 0.13302604854106903, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.380398154258728, 'eval_runtime': 8.4776, 'eval_samples_per_second': 117.014, 'eval_steps_per_second': 7.313, 'epoch': 0.56}
{'loss': 1.4145, 'grad_norm': 0.16167239844799042, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.3741068840026855, 'eval_runtime': 8.5566, 'eval_samples_per_second': 115.934, 'eval_steps_per_second': 7.246, 'epoch': 0.6}
{'loss': 1.3704, 'grad_norm': 0.1497384011745453, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.3705867528915405, 'eval_runtime': 8.5412, 'eval_samples_per_second': 116.143, 'eval_steps_per_second': 7.259, 'epoch': 0.64}
{'loss': 1.342, 'grad_norm': 0.14296980202198029, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.368670105934143, 'eval_runtime': 8.5354, 'eval_samples_per_second': 116.222, 'eval_steps_per_second': 7.264, 'epoch': 0.68}
{'loss': 1.3886, 'grad_norm': 0.12762387096881866, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.3625729084014893, 'eval_runtime': 8.5417, 'eval_samples_per_second': 116.137, 'eval_steps_per_second': 7.259, 'epoch': 0.72}
{'loss': 1.391, 'grad_norm': 0.13417449593544006, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.359317660331726, 'eval_runtime': 8.5447, 'eval_samples_per_second': 116.096, 'eval_steps_per_second': 7.256, 'epoch': 0.76}
{'loss': 1.3784, 'grad_norm': 0.20961466431617737, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.356388807296753, 'eval_runtime': 8.5162, 'eval_samples_per_second': 116.484, 'eval_steps_per_second': 7.28, 'epoch': 0.81}
{'loss': 1.386, 'grad_norm': 0.12933479249477386, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.3540127277374268, 'eval_runtime': 8.4812, 'eval_samples_per_second': 116.964, 'eval_steps_per_second': 7.31, 'epoch': 0.85}
{'loss': 1.3543, 'grad_norm': 0.14706295728683472, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.3521753549575806, 'eval_runtime': 8.4952, 'eval_samples_per_second': 116.772, 'eval_steps_per_second': 7.298, 'epoch': 0.89}
{'loss': 1.3811, 'grad_norm': 0.1391129344701767, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.3504585027694702, 'eval_runtime': 8.4904, 'eval_samples_per_second': 116.837, 'eval_steps_per_second': 7.302, 'epoch': 0.93}
{'loss': 1.3332, 'grad_norm': 0.12589232623577118, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.3491629362106323, 'eval_runtime': 8.484, 'eval_samples_per_second': 116.927, 'eval_steps_per_second': 7.308, 'epoch': 0.97}
{'train_runtime': 449.0855, 'train_samples_per_second': 22.105, 'train_steps_per_second': 1.383, 'train_loss': 1.5283053239954458, 'epoch': 1.0}
train_results:  {'eval_loss': [2.8424947261810303, 1.7468229532241821, 1.53086256980896, 1.483065128326416, 1.4588103294372559, 1.44232177734375, 1.429701328277588, 1.420255184173584, 1.4141954183578491, 1.408434271812439, 1.400076985359192, 1.3927277326583862, 1.3862043619155884, 1.380398154258728, 1.3741068840026855, 1.3705867528915405, 1.368670105934143, 1.3625729084014893, 1.359317660331726, 1.356388807296753, 1.3540127277374268, 1.3521753549575806, 1.3504585027694702, 1.3491629362106323], 'performance': [0.63, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:24,  4.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 47.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 61.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 62.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 71.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 78.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 77.14it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.1595380306243896
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.6405
BO observations:  [1.1595380306243896]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1382 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.1792968121205565
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 1.1792979626017723
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.2165746789300573
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.1694641106027088
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.1792979654874447
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1486, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.8514, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.05176818064288932, 8.388502412888624, 0]
normalized proposed parameters for next round by BO: [tensor(0.1486, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.1014e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1949e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8514, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5177, dtype=torch.float64), tensor(0.1748, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.149
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.851

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05176818064288932,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (8.388502412888624,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.05176818064288932
lora alpha:  8.388502412888624
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.06it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.54it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.76it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.93it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.24it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.85it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.09it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.48it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.78it/s]
Evaluation performance at step 25: 0.62
{'loss': 3.4383, 'grad_norm': 0.3038925230503082, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.9600279331207275, 'eval_runtime': 7.2657, 'eval_samples_per_second': 137.496, 'eval_steps_per_second': 8.671, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.23it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 47.93it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.60it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 52.23it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 57.68it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 59.25it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 63.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 67.37it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 66.90it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 67.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 61.00it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.4013, 'grad_norm': 0.22152531147003174, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.1587259769439697, 'eval_runtime': 6.5036, 'eval_samples_per_second': 153.608, 'eval_steps_per_second': 9.687, 'epoch': 0.08}
{'loss': 1.1059, 'grad_norm': 0.07838048785924911, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0452795028686523, 'eval_runtime': 6.5543, 'eval_samples_per_second': 152.418, 'eval_steps_per_second': 9.612, 'epoch': 0.12}
{'loss': 1.0369, 'grad_norm': 0.08026725053787231, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0060803890228271, 'eval_runtime': 6.5466, 'eval_samples_per_second': 152.599, 'eval_steps_per_second': 9.623, 'epoch': 0.16}
{'loss': 0.9753, 'grad_norm': 0.08131225407123566, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9705612659454346, 'eval_runtime': 6.5629, 'eval_samples_per_second': 152.22, 'eval_steps_per_second': 9.599, 'epoch': 0.2}
{'loss': 0.9618, 'grad_norm': 0.11111607402563095, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9276792407035828, 'eval_runtime': 6.571, 'eval_samples_per_second': 152.031, 'eval_steps_per_second': 9.588, 'epoch': 0.24}
{'loss': 0.8943, 'grad_norm': 0.11789248138666153, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8714998960494995, 'eval_runtime': 6.5821, 'eval_samples_per_second': 151.775, 'eval_steps_per_second': 9.571, 'epoch': 0.28}
{'loss': 0.8404, 'grad_norm': 0.11955579370260239, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.803246796131134, 'eval_runtime': 6.5829, 'eval_samples_per_second': 151.758, 'eval_steps_per_second': 9.57, 'epoch': 0.32}
{'loss': 0.8044, 'grad_norm': 0.14202363789081573, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.759121298789978, 'eval_runtime': 6.5967, 'eval_samples_per_second': 151.439, 'eval_steps_per_second': 9.55, 'epoch': 0.36}
{'loss': 0.7502, 'grad_norm': 0.1562698632478714, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7038955688476562, 'eval_runtime': 6.5971, 'eval_samples_per_second': 151.43, 'eval_steps_per_second': 9.55, 'epoch': 0.4}
{'loss': 0.6995, 'grad_norm': 0.20661693811416626, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6456509232521057, 'eval_runtime': 6.5969, 'eval_samples_per_second': 151.434, 'eval_steps_per_second': 9.55, 'epoch': 0.44}
{'loss': 0.6263, 'grad_norm': 0.20832839608192444, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6006295680999756, 'eval_runtime': 6.6, 'eval_samples_per_second': 151.364, 'eval_steps_per_second': 9.545, 'epoch': 0.48}
{'loss': 0.608, 'grad_norm': 0.27652090787887573, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5653243660926819, 'eval_runtime': 6.5831, 'eval_samples_per_second': 151.751, 'eval_steps_per_second': 9.57, 'epoch': 0.52}
{'loss': 0.5822, 'grad_norm': 0.26971474289894104, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5369804501533508, 'eval_runtime': 6.5561, 'eval_samples_per_second': 152.377, 'eval_steps_per_second': 9.609, 'epoch': 0.56}
{'loss': 0.5583, 'grad_norm': 0.2204037755727768, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5025789141654968, 'eval_runtime': 6.5594, 'eval_samples_per_second': 152.3, 'eval_steps_per_second': 9.605, 'epoch': 0.6}
{'loss': 0.5494, 'grad_norm': 0.23903991281986237, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4808415472507477, 'eval_runtime': 6.558, 'eval_samples_per_second': 152.333, 'eval_steps_per_second': 9.607, 'epoch': 0.64}
{'loss': 0.5051, 'grad_norm': 0.3278745114803314, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.450864315032959, 'eval_runtime': 6.5597, 'eval_samples_per_second': 152.294, 'eval_steps_per_second': 9.604, 'epoch': 0.68}
{'loss': 0.4782, 'grad_norm': 0.3045964539051056, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.42461249232292175, 'eval_runtime': 6.566, 'eval_samples_per_second': 152.147, 'eval_steps_per_second': 9.595, 'epoch': 0.72}
{'loss': 0.4628, 'grad_norm': 0.2802411913871765, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.39952194690704346, 'eval_runtime': 6.5705, 'eval_samples_per_second': 152.044, 'eval_steps_per_second': 9.588, 'epoch': 0.76}
{'loss': 0.4137, 'grad_norm': 0.2766745090484619, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3755744397640228, 'eval_runtime': 6.6093, 'eval_samples_per_second': 151.15, 'eval_steps_per_second': 9.532, 'epoch': 0.8}
{'loss': 0.4428, 'grad_norm': 0.26351797580718994, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.36022746562957764, 'eval_runtime': 6.5961, 'eval_samples_per_second': 151.454, 'eval_steps_per_second': 9.551, 'epoch': 0.84}
{'loss': 0.4113, 'grad_norm': 0.36335158348083496, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.34195202589035034, 'eval_runtime': 6.6023, 'eval_samples_per_second': 151.311, 'eval_steps_per_second': 9.542, 'epoch': 0.88}
{'loss': 0.3895, 'grad_norm': 0.33768731355667114, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3285241425037384, 'eval_runtime': 6.6024, 'eval_samples_per_second': 151.308, 'eval_steps_per_second': 9.542, 'epoch': 0.92}
{'loss': 0.3886, 'grad_norm': 0.25168171525001526, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3195081651210785, 'eval_runtime': 6.6274, 'eval_samples_per_second': 150.739, 'eval_steps_per_second': 9.506, 'epoch': 0.96}
{'loss': 0.3572, 'grad_norm': 0.2895036041736603, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3159799575805664, 'eval_runtime': 6.5926, 'eval_samples_per_second': 151.533, 'eval_steps_per_second': 9.556, 'epoch': 1.0}
{'train_runtime': 388.7336, 'train_samples_per_second': 25.722, 'train_steps_per_second': 1.608, 'train_loss': 0.7872645843505859, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9600279331207275, 1.1587259769439697, 1.0452795028686523, 1.0060803890228271, 0.9705612659454346, 0.9276792407035828, 0.8714998960494995, 0.803246796131134, 0.759121298789978, 0.7038955688476562, 0.6456509232521057, 0.6006295680999756, 0.5653243660926819, 0.5369804501533508, 0.5025789141654968, 0.4808415472507477, 0.450864315032959, 0.42461249232292175, 0.39952194690704346, 0.3755744397640228, 0.36022746562957764, 0.34195202589035034, 0.3285241425037384, 0.3195081651210785, 0.3159799575805664], 'performance': [0.62, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:27,  3.59it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.60it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.18it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 66.84it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 71.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 69.48it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.1163580417633057
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.6405
BO observations:  [1.1595380306243896, 1.1163580417633057]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0409 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.051513431296535
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.9396355365824415
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.1268702628542508
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 1.1299104002291103
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 1.1299091801037868
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2603, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7397, dtype=torch.float64), 32, 1, 1, 1, 0, 0, 3, 0.0, 47.99999999999995, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(2.0910e-16, dtype=torch.float64), tensor(0.2603, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.7194e-16, dtype=torch.float64), tensor(4.2010e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7397, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0253, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.26
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.74

LoRA Parameters:
  lora_r: (3,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (47.99999999999995,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  3
lora dropout:  0.0
lora alpha:  47.99999999999995
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 3,047,424 || all params: 8,033,308,672 || trainable%: 0.0379
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.83it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 47.67it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 51.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.40it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 61.36it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 60.46it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 62.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 65.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 64.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 41.88it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.89it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.8315, 'grad_norm': 3.170449733734131, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.537196159362793, 'eval_runtime': 10.0669, 'eval_samples_per_second': 99.236, 'eval_steps_per_second': 6.258, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.50it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 26.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.32it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 45.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 57.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 61.38it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 63.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 66.78it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 65.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 65.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.97it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.3902, 'grad_norm': 1.5668025016784668, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.2881346940994263, 'eval_runtime': 10.0208, 'eval_samples_per_second': 99.692, 'eval_steps_per_second': 6.287, 'epoch': 0.08}
{'loss': 1.2197, 'grad_norm': 1.3414239883422852, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1864875555038452, 'eval_runtime': 10.0516, 'eval_samples_per_second': 99.387, 'eval_steps_per_second': 6.268, 'epoch': 0.12}
{'loss': 1.1625, 'grad_norm': 1.3597356081008911, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1130094528198242, 'eval_runtime': 10.098, 'eval_samples_per_second': 98.931, 'eval_steps_per_second': 6.239, 'epoch': 0.16}
{'loss': 1.0614, 'grad_norm': 1.3025561571121216, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0594453811645508, 'eval_runtime': 10.1145, 'eval_samples_per_second': 98.769, 'eval_steps_per_second': 6.229, 'epoch': 0.2}
{'loss': 1.0441, 'grad_norm': 1.4608412981033325, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0195972919464111, 'eval_runtime': 10.1404, 'eval_samples_per_second': 98.516, 'eval_steps_per_second': 6.213, 'epoch': 0.24}
{'loss': 0.9904, 'grad_norm': 1.4565515518188477, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.975623369216919, 'eval_runtime': 10.1385, 'eval_samples_per_second': 98.535, 'eval_steps_per_second': 6.214, 'epoch': 0.28}
{'loss': 1.0172, 'grad_norm': 1.4353244304656982, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9454672932624817, 'eval_runtime': 10.1352, 'eval_samples_per_second': 98.567, 'eval_steps_per_second': 6.216, 'epoch': 0.32}
{'loss': 0.9383, 'grad_norm': 1.5476895570755005, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.910256564617157, 'eval_runtime': 10.1592, 'eval_samples_per_second': 98.335, 'eval_steps_per_second': 6.201, 'epoch': 0.36}
{'loss': 0.915, 'grad_norm': 1.5512158870697021, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8690907955169678, 'eval_runtime': 10.1984, 'eval_samples_per_second': 97.957, 'eval_steps_per_second': 6.177, 'epoch': 0.4}
{'loss': 0.9233, 'grad_norm': 1.9448398351669312, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8381124138832092, 'eval_runtime': 10.152, 'eval_samples_per_second': 98.404, 'eval_steps_per_second': 6.206, 'epoch': 0.44}
{'loss': 0.8639, 'grad_norm': 1.6269012689590454, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8122451305389404, 'eval_runtime': 10.1526, 'eval_samples_per_second': 98.398, 'eval_steps_per_second': 6.205, 'epoch': 0.48}
{'loss': 0.9163, 'grad_norm': 1.7998998165130615, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7912803888320923, 'eval_runtime': 10.1384, 'eval_samples_per_second': 98.536, 'eval_steps_per_second': 6.214, 'epoch': 0.52}
{'loss': 0.8457, 'grad_norm': 2.0682942867279053, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7716127634048462, 'eval_runtime': 10.1205, 'eval_samples_per_second': 98.71, 'eval_steps_per_second': 6.225, 'epoch': 0.56}
{'loss': 0.8073, 'grad_norm': 1.7512121200561523, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7520002126693726, 'eval_runtime': 10.1254, 'eval_samples_per_second': 98.663, 'eval_steps_per_second': 6.222, 'epoch': 0.6}
{'loss': 0.8889, 'grad_norm': 2.11565899848938, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7362735271453857, 'eval_runtime': 10.1753, 'eval_samples_per_second': 98.179, 'eval_steps_per_second': 6.191, 'epoch': 0.64}
{'loss': 0.81, 'grad_norm': 1.6785935163497925, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7260391712188721, 'eval_runtime': 10.1605, 'eval_samples_per_second': 98.322, 'eval_steps_per_second': 6.201, 'epoch': 0.68}
{'loss': 0.8494, 'grad_norm': 1.543447494506836, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7131683230400085, 'eval_runtime': 10.1475, 'eval_samples_per_second': 98.448, 'eval_steps_per_second': 6.208, 'epoch': 0.72}
{'loss': 0.8571, 'grad_norm': 1.4748725891113281, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7052991986274719, 'eval_runtime': 10.1203, 'eval_samples_per_second': 98.713, 'eval_steps_per_second': 6.225, 'epoch': 0.76}
{'loss': 0.8036, 'grad_norm': 1.838582158088684, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6976190805435181, 'eval_runtime': 10.0838, 'eval_samples_per_second': 99.07, 'eval_steps_per_second': 6.248, 'epoch': 0.8}
{'loss': 0.8476, 'grad_norm': 1.5006322860717773, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.68746417760849, 'eval_runtime': 10.0873, 'eval_samples_per_second': 99.036, 'eval_steps_per_second': 6.245, 'epoch': 0.84}
{'loss': 0.7753, 'grad_norm': 1.6352084875106812, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6816914677619934, 'eval_runtime': 10.091, 'eval_samples_per_second': 98.999, 'eval_steps_per_second': 6.243, 'epoch': 0.88}
{'loss': 0.7489, 'grad_norm': 2.301278829574585, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6767372488975525, 'eval_runtime': 10.1067, 'eval_samples_per_second': 98.845, 'eval_steps_per_second': 6.233, 'epoch': 0.92}
{'loss': 0.7773, 'grad_norm': 1.5317946672439575, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6735994815826416, 'eval_runtime': 10.0876, 'eval_samples_per_second': 99.033, 'eval_steps_per_second': 6.245, 'epoch': 0.96}
{'loss': 0.6692, 'grad_norm': 1.187543511390686, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.671842098236084, 'eval_runtime': 10.1091, 'eval_samples_per_second': 98.822, 'eval_steps_per_second': 6.232, 'epoch': 1.0}
{'train_runtime': 541.4524, 'train_samples_per_second': 18.467, 'train_steps_per_second': 1.154, 'train_loss': 0.9981639892578125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.537196159362793, 1.2881346940994263, 1.1864875555038452, 1.1130094528198242, 1.0594453811645508, 1.0195972919464111, 0.975623369216919, 0.9454672932624817, 0.910256564617157, 0.8690907955169678, 0.8381124138832092, 0.8122451305389404, 0.7912803888320923, 0.7716127634048462, 0.7520002126693726, 0.7362735271453857, 0.7260391712188721, 0.7131683230400085, 0.7052991986274719, 0.6976190805435181, 0.68746417760849, 0.6816914677619934, 0.6767372488975525, 0.6735994815826416, 0.671842098236084], 'performance': [0.62, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:27,  3.65it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 67.17it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 71.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 69.72it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  0.9322792887687683
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.6405
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0523 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.9153944051590929
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.0622532150629902
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 1.0345882488487788
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.062908088096995
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.06287690559452
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(0.3335, dtype=torch.float64), tensor(0.6665, dtype=torch.float64), 32, 0, 1, 1, 1, 1, 128, 2.551643258082968e-19, 1.4800000190734908, 0]
normalized proposed parameters for next round by BO: [tensor(4.1031e-17, dtype=torch.float64), tensor(9.8945e-17, dtype=torch.float64), tensor(2.5435e-17, dtype=torch.float64), tensor(7.9889e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.1484e-17, dtype=torch.float64), tensor(1.2073e-16, dtype=torch.float64), tensor(0.3335, dtype=torch.float64), tensor(0.6665, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.5516e-18, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.333
  arc_challenge: 0.667

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.551643258082968e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (1.4800000190734908,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  2.551643258082968e-19
lora alpha:  1.4800000190734908
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 247,463,936 || all params: 8,277,725,184 || trainable%: 2.9895
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.95it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 24.83it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.04it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 39.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.68it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 45.40it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 41.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 45.58it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 44.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 46.69it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 48.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 44.02it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.5496, 'grad_norm': 0.20109188556671143, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.437872886657715, 'eval_runtime': 9.4771, 'eval_samples_per_second': 105.412, 'eval_steps_per_second': 6.648, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.13it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.93it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.85it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.75it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.26it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.48it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.81it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.13it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.11it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.37it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.65it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.71it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.6999, 'grad_norm': 0.15382668375968933, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.2623555660247803, 'eval_runtime': 9.4574, 'eval_samples_per_second': 105.631, 'eval_steps_per_second': 6.661, 'epoch': 0.08}
{'loss': 1.155, 'grad_norm': 0.0571519210934639, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1068395376205444, 'eval_runtime': 9.4548, 'eval_samples_per_second': 105.66, 'eval_steps_per_second': 6.663, 'epoch': 0.12}
{'loss': 1.1019, 'grad_norm': 0.04559873044490814, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0227607488632202, 'eval_runtime': 9.4627, 'eval_samples_per_second': 105.573, 'eval_steps_per_second': 6.658, 'epoch': 0.16}
{'loss': 1.004, 'grad_norm': 0.04287979006767273, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0010484457015991, 'eval_runtime': 9.4777, 'eval_samples_per_second': 105.405, 'eval_steps_per_second': 6.647, 'epoch': 0.2}
{'loss': 0.9619, 'grad_norm': 0.043253231793642044, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9794610142707825, 'eval_runtime': 9.4874, 'eval_samples_per_second': 105.298, 'eval_steps_per_second': 6.64, 'epoch': 0.24}
{'loss': 0.9429, 'grad_norm': 0.03955763950943947, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9614338278770447, 'eval_runtime': 9.5189, 'eval_samples_per_second': 104.949, 'eval_steps_per_second': 6.618, 'epoch': 0.28}
{'loss': 0.9695, 'grad_norm': 0.0521446131169796, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9436256289482117, 'eval_runtime': 9.5157, 'eval_samples_per_second': 104.984, 'eval_steps_per_second': 6.621, 'epoch': 0.32}
{'loss': 0.9309, 'grad_norm': 0.04475550726056099, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9291024804115295, 'eval_runtime': 9.5057, 'eval_samples_per_second': 105.095, 'eval_steps_per_second': 6.628, 'epoch': 0.36}
{'loss': 0.935, 'grad_norm': 0.047216080129146576, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9139131307601929, 'eval_runtime': 9.5394, 'eval_samples_per_second': 104.724, 'eval_steps_per_second': 6.604, 'epoch': 0.4}
{'loss': 0.9715, 'grad_norm': 0.05132720246911049, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8987276554107666, 'eval_runtime': 9.5638, 'eval_samples_per_second': 104.456, 'eval_steps_per_second': 6.587, 'epoch': 0.44}
{'loss': 0.9337, 'grad_norm': 0.05820214003324509, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8820629715919495, 'eval_runtime': 9.609, 'eval_samples_per_second': 103.965, 'eval_steps_per_second': 6.556, 'epoch': 0.48}
{'loss': 0.9877, 'grad_norm': 0.06718730181455612, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8705451488494873, 'eval_runtime': 9.5575, 'eval_samples_per_second': 104.525, 'eval_steps_per_second': 6.592, 'epoch': 0.52}
{'loss': 0.8701, 'grad_norm': 0.061814721673727036, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.852872371673584, 'eval_runtime': 9.5551, 'eval_samples_per_second': 104.552, 'eval_steps_per_second': 6.593, 'epoch': 0.56}
{'loss': 0.8625, 'grad_norm': 0.06376992911100388, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8361000418663025, 'eval_runtime': 9.5648, 'eval_samples_per_second': 104.446, 'eval_steps_per_second': 6.587, 'epoch': 0.6}
{'loss': 0.8954, 'grad_norm': 0.06831406056880951, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.81421959400177, 'eval_runtime': 9.5601, 'eval_samples_per_second': 104.497, 'eval_steps_per_second': 6.59, 'epoch': 0.64}
{'loss': 0.857, 'grad_norm': 0.10009365528821945, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7943979501724243, 'eval_runtime': 9.5471, 'eval_samples_per_second': 104.639, 'eval_steps_per_second': 6.599, 'epoch': 0.68}
{'loss': 0.8511, 'grad_norm': 0.07898062467575073, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.774994969367981, 'eval_runtime': 9.4948, 'eval_samples_per_second': 105.215, 'eval_steps_per_second': 6.635, 'epoch': 0.72}
{'loss': 0.8636, 'grad_norm': 0.08458837121725082, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7643584609031677, 'eval_runtime': 9.5072, 'eval_samples_per_second': 105.078, 'eval_steps_per_second': 6.627, 'epoch': 0.76}
{'loss': 0.7954, 'grad_norm': 0.10479271411895752, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7479516267776489, 'eval_runtime': 9.5067, 'eval_samples_per_second': 105.084, 'eval_steps_per_second': 6.627, 'epoch': 0.8}
{'loss': 0.8392, 'grad_norm': 0.12272783368825912, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7309807538986206, 'eval_runtime': 9.5153, 'eval_samples_per_second': 104.988, 'eval_steps_per_second': 6.621, 'epoch': 0.84}
{'loss': 0.8409, 'grad_norm': 0.11188092082738876, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7184422016143799, 'eval_runtime': 9.5245, 'eval_samples_per_second': 104.887, 'eval_steps_per_second': 6.615, 'epoch': 0.88}
{'loss': 0.7539, 'grad_norm': 0.1027960255742073, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7104962468147278, 'eval_runtime': 9.548, 'eval_samples_per_second': 104.629, 'eval_steps_per_second': 6.598, 'epoch': 0.92}
{'loss': 0.7765, 'grad_norm': 0.12359883636236191, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7042309045791626, 'eval_runtime': 9.5703, 'eval_samples_per_second': 104.385, 'eval_steps_per_second': 6.583, 'epoch': 0.96}
{'loss': 0.7552, 'grad_norm': 0.12567344307899475, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7018641233444214, 'eval_runtime': 9.5652, 'eval_samples_per_second': 104.441, 'eval_steps_per_second': 6.586, 'epoch': 1.0}
{'train_runtime': 517.7447, 'train_samples_per_second': 19.313, 'train_steps_per_second': 1.207, 'train_loss': 1.0441807403564454, 'epoch': 1.0}
train_results:  {'eval_loss': [2.437872886657715, 1.2623555660247803, 1.1068395376205444, 1.0227607488632202, 1.0010484457015991, 0.9794610142707825, 0.9614338278770447, 0.9436256289482117, 0.9291024804115295, 0.9139131307601929, 0.8987276554107666, 0.8820629715919495, 0.8705451488494873, 0.852872371673584, 0.8361000418663025, 0.81421959400177, 0.7943979501724243, 0.774994969367981, 0.7643584609031677, 0.7479516267776489, 0.7309807538986206, 0.7184422016143799, 0.7104962468147278, 0.7042309045791626, 0.7018641233444214], 'performance': [0.61, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.20it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.00it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.61it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 69.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.085510015487671
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.6405
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2524 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.0272813307688866
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.9885120029383078
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.0273312346867405
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.9922042817644112
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.9340439405689461
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.3185, dtype=torch.float64), 0, tensor(0.6815, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.09999999999999984, 47.999999998847706, 1]
normalized proposed parameters for next round by BO: [tensor(1.3178e-14, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.1620e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.5988e-14, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3185, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6815, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.318
  mmlu: 0
  arc_challenge: 0.682

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999984,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (47.999999998847706,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.09999999999999984
lora alpha:  47.999999998847706
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.57it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.73it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.68it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.00it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.81it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.21it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.98it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.01it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 59.39it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.10it/s]
Evaluation performance at step 25: 0.61
{'loss': 2.6355, 'grad_norm': 0.4835759997367859, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.5031105279922485, 'eval_runtime': 7.9945, 'eval_samples_per_second': 124.961, 'eval_steps_per_second': 7.88, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.45it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 25.91it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.43it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.01it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.63it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.77it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.01it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.92it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 58.53it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.45it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.3389, 'grad_norm': 0.23797006905078888, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.2644009590148926, 'eval_runtime': 7.9905, 'eval_samples_per_second': 125.024, 'eval_steps_per_second': 7.884, 'epoch': 0.08}
{'loss': 1.1852, 'grad_norm': 0.24596387147903442, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.155626893043518, 'eval_runtime': 8.039, 'eval_samples_per_second': 124.269, 'eval_steps_per_second': 7.837, 'epoch': 0.12}
{'loss': 1.1135, 'grad_norm': 0.23547302186489105, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0549753904342651, 'eval_runtime': 8.0718, 'eval_samples_per_second': 123.764, 'eval_steps_per_second': 7.805, 'epoch': 0.16}
{'loss': 0.973, 'grad_norm': 0.3027292490005493, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9666545391082764, 'eval_runtime': 8.0792, 'eval_samples_per_second': 123.65, 'eval_steps_per_second': 7.798, 'epoch': 0.2}
{'loss': 0.8629, 'grad_norm': 0.37229761481285095, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8959590792655945, 'eval_runtime': 8.0675, 'eval_samples_per_second': 123.83, 'eval_steps_per_second': 7.809, 'epoch': 0.24}
{'loss': 0.8726, 'grad_norm': 0.3292841911315918, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8353855609893799, 'eval_runtime': 8.0649, 'eval_samples_per_second': 123.87, 'eval_steps_per_second': 7.812, 'epoch': 0.28}
{'loss': 0.8147, 'grad_norm': 0.3493456542491913, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7783191800117493, 'eval_runtime': 8.0287, 'eval_samples_per_second': 124.429, 'eval_steps_per_second': 7.847, 'epoch': 0.32}
{'loss': 0.7517, 'grad_norm': 0.40776389837265015, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7330318689346313, 'eval_runtime': 8.0179, 'eval_samples_per_second': 124.597, 'eval_steps_per_second': 7.857, 'epoch': 0.36}
{'loss': 0.8059, 'grad_norm': 0.2889277935028076, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6881250739097595, 'eval_runtime': 8.0242, 'eval_samples_per_second': 124.499, 'eval_steps_per_second': 7.851, 'epoch': 0.4}
{'loss': 0.7219, 'grad_norm': 0.34448474645614624, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6457923054695129, 'eval_runtime': 8.0196, 'eval_samples_per_second': 124.57, 'eval_steps_per_second': 7.856, 'epoch': 0.44}
{'loss': 0.6658, 'grad_norm': 0.5134320259094238, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6095723509788513, 'eval_runtime': 8.0342, 'eval_samples_per_second': 124.344, 'eval_steps_per_second': 7.842, 'epoch': 0.48}
{'loss': 0.6952, 'grad_norm': 0.3685013949871063, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5852528214454651, 'eval_runtime': 8.0306, 'eval_samples_per_second': 124.4, 'eval_steps_per_second': 7.845, 'epoch': 0.52}
{'loss': 0.5962, 'grad_norm': 0.43650728464126587, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5550228953361511, 'eval_runtime': 8.0361, 'eval_samples_per_second': 124.314, 'eval_steps_per_second': 7.84, 'epoch': 0.56}
{'loss': 0.6294, 'grad_norm': 0.39314916729927063, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.531283974647522, 'eval_runtime': 8.0305, 'eval_samples_per_second': 124.401, 'eval_steps_per_second': 7.845, 'epoch': 0.6}
{'loss': 0.696, 'grad_norm': 0.42827585339546204, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5124339461326599, 'eval_runtime': 8.0369, 'eval_samples_per_second': 124.302, 'eval_steps_per_second': 7.839, 'epoch': 0.64}
{'loss': 0.5683, 'grad_norm': 0.5144149661064148, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.49407318234443665, 'eval_runtime': 8.0484, 'eval_samples_per_second': 124.124, 'eval_steps_per_second': 7.828, 'epoch': 0.68}
{'loss': 0.6568, 'grad_norm': 0.2833440899848938, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4768984019756317, 'eval_runtime': 8.0292, 'eval_samples_per_second': 124.421, 'eval_steps_per_second': 7.846, 'epoch': 0.72}
{'loss': 0.6433, 'grad_norm': 0.44648563861846924, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4674595594406128, 'eval_runtime': 8.0267, 'eval_samples_per_second': 124.459, 'eval_steps_per_second': 7.849, 'epoch': 0.76}
{'loss': 0.5423, 'grad_norm': 0.2303415983915329, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4555085599422455, 'eval_runtime': 8.0229, 'eval_samples_per_second': 124.519, 'eval_steps_per_second': 7.853, 'epoch': 0.8}
{'loss': 0.5761, 'grad_norm': 0.3836062550544739, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4460737705230713, 'eval_runtime': 8.0242, 'eval_samples_per_second': 124.498, 'eval_steps_per_second': 7.851, 'epoch': 0.84}
{'loss': 0.5925, 'grad_norm': 0.27996590733528137, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.4388292133808136, 'eval_runtime': 8.0346, 'eval_samples_per_second': 124.337, 'eval_steps_per_second': 7.841, 'epoch': 0.88}
{'loss': 0.547, 'grad_norm': 0.27490130066871643, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.43449434638023376, 'eval_runtime': 8.0207, 'eval_samples_per_second': 124.553, 'eval_steps_per_second': 7.855, 'epoch': 0.92}
{'loss': 0.5072, 'grad_norm': 0.33604320883750916, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.4317522943019867, 'eval_runtime': 8.0871, 'eval_samples_per_second': 123.53, 'eval_steps_per_second': 7.79, 'epoch': 0.96}
{'loss': 0.5164, 'grad_norm': 0.219456747174263, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.43026214838027954, 'eval_runtime': 8.0803, 'eval_samples_per_second': 123.634, 'eval_steps_per_second': 7.797, 'epoch': 1.0}
{'train_runtime': 444.0069, 'train_samples_per_second': 22.52, 'train_steps_per_second': 1.408, 'train_loss': 0.8203303192138672, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5031105279922485, 1.2644009590148926, 1.155626893043518, 1.0549753904342651, 0.9666545391082764, 0.8959590792655945, 0.8353855609893799, 0.7783191800117493, 0.7330318689346313, 0.6881250739097595, 0.6457923054695129, 0.6095723509788513, 0.5852528214454651, 0.5550228953361511, 0.531283974647522, 0.5124339461326599, 0.49407318234443665, 0.4768984019756317, 0.4674595594406128, 0.4555085599422455, 0.4460737705230713, 0.4388292133808136, 0.43449434638023376, 0.4317522943019867, 0.43026214838027954], 'performance': [0.61, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.82it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.35it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.82it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.48it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.82it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.2469189167022705
current iteration best possible performance (full train run):  0.651
max performance so far:  0.651
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4098 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.0292714078280627
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 1.0565365714235893
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 1.0565742738077497
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.8489534194106949
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.0540732856374113
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.2920, dtype=torch.float64), 0, tensor(0.7080, dtype=torch.float64), 10, 1, 0, 1, 0, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(4.4365e-17, dtype=torch.float64), tensor(8.0550e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.2728e-17, dtype=torch.float64), tensor(0.2920, dtype=torch.float64), tensor(8.0941e-17, dtype=torch.float64), tensor(0.7080, dtype=torch.float64), tensor(0.3187, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.292
  mmlu: 0
  arc_challenge: 0.708

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (10,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  10
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 57,671,680 || all params: 8,087,932,928 || trainable%: 0.7131
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.82it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 39.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 52.21it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 58.97it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 63.25it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 69.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 72.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 77.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 76.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 77.21it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 70.24it/s]
Evaluation performance at step 25: 0.62
{'loss': 3.1034, 'grad_norm': 0.8187827467918396, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.889741063117981, 'eval_runtime': 7.4286, 'eval_samples_per_second': 134.48, 'eval_steps_per_second': 8.481, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:14,  7.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 55.06it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 60.10it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 63.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 71.20it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:00<00:00, 73.68it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 77.70it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 77.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 77.26it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 72.56it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.5987, 'grad_norm': 0.23897847533226013, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.4561039209365845, 'eval_runtime': 7.4177, 'eval_samples_per_second': 134.678, 'eval_steps_per_second': 8.493, 'epoch': 0.08}
{'loss': 1.3035, 'grad_norm': 0.18966275453567505, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2933082580566406, 'eval_runtime': 7.4483, 'eval_samples_per_second': 134.125, 'eval_steps_per_second': 8.458, 'epoch': 0.12}
{'loss': 1.2567, 'grad_norm': 0.21290157735347748, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2132784128189087, 'eval_runtime': 7.4521, 'eval_samples_per_second': 134.057, 'eval_steps_per_second': 8.454, 'epoch': 0.16}
{'loss': 1.1465, 'grad_norm': 0.21650294959545135, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1697794198989868, 'eval_runtime': 7.4793, 'eval_samples_per_second': 133.569, 'eval_steps_per_second': 8.423, 'epoch': 0.2}
{'loss': 1.1183, 'grad_norm': 0.24789535999298096, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1433218717575073, 'eval_runtime': 7.4837, 'eval_samples_per_second': 133.489, 'eval_steps_per_second': 8.418, 'epoch': 0.24}
{'loss': 1.0587, 'grad_norm': 0.2882811725139618, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1144471168518066, 'eval_runtime': 7.5127, 'eval_samples_per_second': 132.975, 'eval_steps_per_second': 8.386, 'epoch': 0.28}
{'loss': 1.0828, 'grad_norm': 0.292344331741333, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0910593271255493, 'eval_runtime': 7.5143, 'eval_samples_per_second': 132.947, 'eval_steps_per_second': 8.384, 'epoch': 0.32}
{'loss': 1.0886, 'grad_norm': 0.28678515553474426, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.061482548713684, 'eval_runtime': 7.5529, 'eval_samples_per_second': 132.267, 'eval_steps_per_second': 8.341, 'epoch': 0.36}
{'loss': 1.0569, 'grad_norm': 0.26562732458114624, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0283479690551758, 'eval_runtime': 7.4838, 'eval_samples_per_second': 133.489, 'eval_steps_per_second': 8.418, 'epoch': 0.4}
{'loss': 1.026, 'grad_norm': 0.3752431571483612, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0024038553237915, 'eval_runtime': 7.4859, 'eval_samples_per_second': 133.452, 'eval_steps_per_second': 8.416, 'epoch': 0.44}
{'loss': 1.0239, 'grad_norm': 0.35503706336021423, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9828317761421204, 'eval_runtime': 7.449, 'eval_samples_per_second': 134.111, 'eval_steps_per_second': 8.457, 'epoch': 0.48}
{'loss': 1.048, 'grad_norm': 0.5238874554634094, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9492259621620178, 'eval_runtime': 7.4566, 'eval_samples_per_second': 133.976, 'eval_steps_per_second': 8.449, 'epoch': 0.52}
{'loss': 0.973, 'grad_norm': 0.546229362487793, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9198810458183289, 'eval_runtime': 7.4591, 'eval_samples_per_second': 133.93, 'eval_steps_per_second': 8.446, 'epoch': 0.56}
{'loss': 0.9171, 'grad_norm': 0.641802966594696, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.88897305727005, 'eval_runtime': 7.539, 'eval_samples_per_second': 132.51, 'eval_steps_per_second': 8.357, 'epoch': 0.6}
{'loss': 1.0115, 'grad_norm': 0.57450270652771, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8631174564361572, 'eval_runtime': 7.4972, 'eval_samples_per_second': 133.25, 'eval_steps_per_second': 8.403, 'epoch': 0.64}
{'loss': 0.9127, 'grad_norm': 0.6045541167259216, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8350412249565125, 'eval_runtime': 7.4877, 'eval_samples_per_second': 133.418, 'eval_steps_per_second': 8.414, 'epoch': 0.68}
{'loss': 0.9075, 'grad_norm': 0.5713976621627808, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8148507475852966, 'eval_runtime': 7.4915, 'eval_samples_per_second': 133.351, 'eval_steps_per_second': 8.41, 'epoch': 0.72}
{'loss': 0.9409, 'grad_norm': 0.9118421673774719, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7905004620552063, 'eval_runtime': 7.5021, 'eval_samples_per_second': 133.162, 'eval_steps_per_second': 8.398, 'epoch': 0.76}
{'loss': 0.8358, 'grad_norm': 0.5877645015716553, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.772888720035553, 'eval_runtime': 7.4925, 'eval_samples_per_second': 133.333, 'eval_steps_per_second': 8.408, 'epoch': 0.8}
{'loss': 0.835, 'grad_norm': 0.8367207050323486, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7453267574310303, 'eval_runtime': 7.468, 'eval_samples_per_second': 133.771, 'eval_steps_per_second': 8.436, 'epoch': 0.84}
{'loss': 0.7505, 'grad_norm': 0.9142866730690002, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7252992987632751, 'eval_runtime': 7.4695, 'eval_samples_per_second': 133.743, 'eval_steps_per_second': 8.434, 'epoch': 0.88}
{'loss': 0.7437, 'grad_norm': 0.9355564713478088, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7049340605735779, 'eval_runtime': 7.4975, 'eval_samples_per_second': 133.245, 'eval_steps_per_second': 8.403, 'epoch': 0.92}
{'loss': 0.7569, 'grad_norm': 1.144594430923462, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6905264854431152, 'eval_runtime': 7.5181, 'eval_samples_per_second': 132.88, 'eval_steps_per_second': 8.38, 'epoch': 0.96}
{'loss': 0.7335, 'grad_norm': 0.7442262768745422, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6837870478630066, 'eval_runtime': 7.4923, 'eval_samples_per_second': 133.336, 'eval_steps_per_second': 8.409, 'epoch': 1.0}
{'train_runtime': 411.2424, 'train_samples_per_second': 24.314, 'train_steps_per_second': 1.52, 'train_loss': 1.0892073211669921, 'epoch': 1.0}
train_results:  {'eval_loss': [1.889741063117981, 1.4561039209365845, 1.2933082580566406, 1.2132784128189087, 1.1697794198989868, 1.1433218717575073, 1.1144471168518066, 1.0910593271255493, 1.061482548713684, 1.0283479690551758, 1.0024038553237915, 0.9828317761421204, 0.9492259621620178, 0.9198810458183289, 0.88897305727005, 0.8631174564361572, 0.8350412249565125, 0.8148507475852966, 0.7905004620552063, 0.772888720035553, 0.7453267574310303, 0.7252992987632751, 0.7049340605735779, 0.6905264854431152, 0.6837870478630066], 'performance': [0.62, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:25,  3.94it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 45.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 60.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 68.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 76.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 74.80it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.1680582761764526
current iteration best possible performance (full train run):  0.5355000000000001
max performance so far:  0.651
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5226 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.0704238999589113
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 1.0177361332411792
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.0444150772820777
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 1.0425923986561108
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 0.97186382996327
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.3126, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.6874, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.1, 47.99999999999995, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(6.0910e-17, dtype=torch.float64), tensor(0.3126, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6448e-17, dtype=torch.float64), tensor(6.3828e-17, dtype=torch.float64), tensor(2.2846e-16, dtype=torch.float64), tensor(0.6874, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.313
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.687

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.99999999999995,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  47.99999999999995
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.71it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.82it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.74it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 41.96it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 47.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.47it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.27it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.32it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 50.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 49.34it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 48.33it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 54.82it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.05it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.6958, 'grad_norm': 0.43235257267951965, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.5147062540054321, 'eval_runtime': 10.9517, 'eval_samples_per_second': 91.218, 'eval_steps_per_second': 5.753, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.92it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:06, 15.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 24.69it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 31.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 36.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 42.90it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.52it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 51.31it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.92it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.05it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.90it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 52.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 44.69it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.342, 'grad_norm': 0.3010179400444031, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.2676026821136475, 'eval_runtime': 10.8619, 'eval_samples_per_second': 91.973, 'eval_steps_per_second': 5.8, 'epoch': 0.08}
{'loss': 1.22, 'grad_norm': 0.2531704306602478, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2073718309402466, 'eval_runtime': 10.8981, 'eval_samples_per_second': 91.668, 'eval_steps_per_second': 5.781, 'epoch': 0.12}
{'loss': 1.2159, 'grad_norm': 0.1923498809337616, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1687372922897339, 'eval_runtime': 10.9102, 'eval_samples_per_second': 91.566, 'eval_steps_per_second': 5.774, 'epoch': 0.16}
{'loss': 1.1533, 'grad_norm': 0.2167719304561615, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.123472809791565, 'eval_runtime': 10.9342, 'eval_samples_per_second': 91.365, 'eval_steps_per_second': 5.762, 'epoch': 0.2}
{'loss': 1.0542, 'grad_norm': 0.3225939869880676, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0742520093917847, 'eval_runtime': 10.9557, 'eval_samples_per_second': 91.185, 'eval_steps_per_second': 5.75, 'epoch': 0.24}
{'loss': 1.0103, 'grad_norm': 0.26051774621009827, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0303715467453003, 'eval_runtime': 10.9904, 'eval_samples_per_second': 90.897, 'eval_steps_per_second': 5.732, 'epoch': 0.28}
{'loss': 1.0499, 'grad_norm': 0.3358774483203888, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9958673119544983, 'eval_runtime': 10.9867, 'eval_samples_per_second': 90.928, 'eval_steps_per_second': 5.734, 'epoch': 0.32}
{'loss': 0.9687, 'grad_norm': 0.33242446184158325, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9576843976974487, 'eval_runtime': 10.9515, 'eval_samples_per_second': 91.22, 'eval_steps_per_second': 5.753, 'epoch': 0.36}
{'loss': 0.9714, 'grad_norm': 0.23963189125061035, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9240846633911133, 'eval_runtime': 10.9152, 'eval_samples_per_second': 91.524, 'eval_steps_per_second': 5.772, 'epoch': 0.4}
{'loss': 0.9878, 'grad_norm': 0.29014667868614197, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9040431976318359, 'eval_runtime': 10.9197, 'eval_samples_per_second': 91.486, 'eval_steps_per_second': 5.769, 'epoch': 0.44}
{'loss': 0.9286, 'grad_norm': 0.2631320059299469, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8729506134986877, 'eval_runtime': 10.8996, 'eval_samples_per_second': 91.655, 'eval_steps_per_second': 5.78, 'epoch': 0.48}
{'loss': 1.0021, 'grad_norm': 0.24994952976703644, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8552770018577576, 'eval_runtime': 10.8854, 'eval_samples_per_second': 91.774, 'eval_steps_per_second': 5.788, 'epoch': 0.52}
{'loss': 0.9008, 'grad_norm': 0.24385876953601837, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8374416828155518, 'eval_runtime': 10.9469, 'eval_samples_per_second': 91.258, 'eval_steps_per_second': 5.755, 'epoch': 0.56}
{'loss': 0.8701, 'grad_norm': 0.24779662489891052, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8209311366081238, 'eval_runtime': 10.9752, 'eval_samples_per_second': 91.023, 'eval_steps_per_second': 5.74, 'epoch': 0.6}
{'loss': 0.9422, 'grad_norm': 0.23074795305728912, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8085718750953674, 'eval_runtime': 10.9591, 'eval_samples_per_second': 91.157, 'eval_steps_per_second': 5.749, 'epoch': 0.64}
{'loss': 0.8556, 'grad_norm': 0.21552932262420654, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8007158041000366, 'eval_runtime': 10.9017, 'eval_samples_per_second': 91.637, 'eval_steps_per_second': 5.779, 'epoch': 0.68}
{'loss': 0.9175, 'grad_norm': 0.22579514980316162, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7893092036247253, 'eval_runtime': 10.8934, 'eval_samples_per_second': 91.707, 'eval_steps_per_second': 5.783, 'epoch': 0.72}
{'loss': 0.8785, 'grad_norm': 0.1486283391714096, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.780551016330719, 'eval_runtime': 10.8976, 'eval_samples_per_second': 91.671, 'eval_steps_per_second': 5.781, 'epoch': 0.76}
{'loss': 0.8296, 'grad_norm': 0.30745553970336914, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7729451656341553, 'eval_runtime': 10.8921, 'eval_samples_per_second': 91.718, 'eval_steps_per_second': 5.784, 'epoch': 0.8}
{'loss': 0.9203, 'grad_norm': 0.18863627314567566, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7695680260658264, 'eval_runtime': 10.8984, 'eval_samples_per_second': 91.665, 'eval_steps_per_second': 5.781, 'epoch': 0.84}
{'loss': 0.878, 'grad_norm': 0.19410084187984467, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7625576257705688, 'eval_runtime': 10.9052, 'eval_samples_per_second': 91.608, 'eval_steps_per_second': 5.777, 'epoch': 0.88}
{'loss': 0.8326, 'grad_norm': 0.20636963844299316, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7573310732841492, 'eval_runtime': 10.9197, 'eval_samples_per_second': 91.486, 'eval_steps_per_second': 5.769, 'epoch': 0.92}
{'loss': 0.843, 'grad_norm': 0.17160771787166595, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7536224126815796, 'eval_runtime': 10.9431, 'eval_samples_per_second': 91.29, 'eval_steps_per_second': 5.757, 'epoch': 0.96}
{'loss': 0.7793, 'grad_norm': 0.2711922526359558, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7524436116218567, 'eval_runtime': 10.999, 'eval_samples_per_second': 90.826, 'eval_steps_per_second': 5.728, 'epoch': 1.0}
{'train_runtime': 589.9557, 'train_samples_per_second': 16.949, 'train_steps_per_second': 1.059, 'train_loss': 1.0419010864257812, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5147062540054321, 1.2676026821136475, 1.2073718309402466, 1.1687372922897339, 1.123472809791565, 1.0742520093917847, 1.0303715467453003, 0.9958673119544983, 0.9576843976974487, 0.9240846633911133, 0.9040431976318359, 0.8729506134986877, 0.8552770018577576, 0.8374416828155518, 0.8209311366081238, 0.8085718750953674, 0.8007158041000366, 0.7893092036247253, 0.780551016330719, 0.7729451656341553, 0.7695680260658264, 0.7625576257705688, 0.7573310732841492, 0.7536224126815796, 0.7524436116218567], 'performance': [0.62, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:34,  2.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.93it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 42.80it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.74it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.55it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 65.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.94it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2458372116088867
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.651
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7492 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.0227434086403628
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.9532406435678643
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.0227421893169544
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.9566362134331952
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 1.0163079292134798
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3242, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6758, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 128, 0.1, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(2.4993e-16, dtype=torch.float64), tensor(1.1994e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3242, dtype=torch.float64), tensor(2.3045e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.1452e-15, dtype=torch.float64), tensor(0.6758, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.324
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.676

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Using the latest cached version of the dataset since trivia_qa couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'rc.nocontext' at /home/alfred/.cache/huggingface/datasets/trivia_qa/rc.nocontext/0.0.0/0f7faf33a3908546c6fd5b73a660e0f8ff173c2f (last modified on Fri Dec 12 10:26:21 2025).
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.13it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.61it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.80it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.27it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.46it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.06it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.78it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.16it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.28it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.5402, 'grad_norm': 0.48706498742103577, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.1144428253173828, 'eval_runtime': 6.9719, 'eval_samples_per_second': 143.288, 'eval_steps_per_second': 9.036, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.30it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.38it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.97it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.13it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.41it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.44it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 58.03it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.81it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 56.10it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.37it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.0204, 'grad_norm': 0.2587583661079407, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 0.927787184715271, 'eval_runtime': 6.9368, 'eval_samples_per_second': 144.015, 'eval_steps_per_second': 9.082, 'epoch': 0.08}
{'loss': 0.8586, 'grad_norm': 0.26954740285873413, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.772388219833374, 'eval_runtime': 6.9612, 'eval_samples_per_second': 143.51, 'eval_steps_per_second': 9.05, 'epoch': 0.12}
{'loss': 0.7414, 'grad_norm': 0.3158050775527954, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6892240047454834, 'eval_runtime': 6.9691, 'eval_samples_per_second': 143.347, 'eval_steps_per_second': 9.04, 'epoch': 0.16}
{'loss': 0.7029, 'grad_norm': 0.3241090774536133, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6314793229103088, 'eval_runtime': 6.9656, 'eval_samples_per_second': 143.419, 'eval_steps_per_second': 9.044, 'epoch': 0.2}
{'loss': 0.6646, 'grad_norm': 0.3126239776611328, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.564478874206543, 'eval_runtime': 6.957, 'eval_samples_per_second': 143.596, 'eval_steps_per_second': 9.056, 'epoch': 0.24}
{'loss': 0.5951, 'grad_norm': 0.3160964250564575, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5192660093307495, 'eval_runtime': 6.9751, 'eval_samples_per_second': 143.224, 'eval_steps_per_second': 9.032, 'epoch': 0.28}
{'loss': 0.5734, 'grad_norm': 0.33868494629859924, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.46699196100234985, 'eval_runtime': 7.0041, 'eval_samples_per_second': 142.631, 'eval_steps_per_second': 8.995, 'epoch': 0.32}
{'loss': 0.4506, 'grad_norm': 0.3634958863258362, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4297623038291931, 'eval_runtime': 7.0747, 'eval_samples_per_second': 141.207, 'eval_steps_per_second': 8.905, 'epoch': 0.36}
{'loss': 0.495, 'grad_norm': 0.2971700429916382, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.3891719579696655, 'eval_runtime': 7.0907, 'eval_samples_per_second': 140.889, 'eval_steps_per_second': 8.885, 'epoch': 0.4}
{'loss': 0.4357, 'grad_norm': 0.32770100235939026, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.35145074129104614, 'eval_runtime': 7.0638, 'eval_samples_per_second': 141.426, 'eval_steps_per_second': 8.919, 'epoch': 0.44}
{'loss': 0.413, 'grad_norm': 0.33333149552345276, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3209875524044037, 'eval_runtime': 7.0822, 'eval_samples_per_second': 141.057, 'eval_steps_per_second': 8.896, 'epoch': 0.48}
{'loss': 0.3997, 'grad_norm': 0.24666208028793335, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.29713886976242065, 'eval_runtime': 7.0277, 'eval_samples_per_second': 142.153, 'eval_steps_per_second': 8.965, 'epoch': 0.52}
{'loss': 0.3506, 'grad_norm': 0.36989331245422363, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.27781301736831665, 'eval_runtime': 7.012, 'eval_samples_per_second': 142.471, 'eval_steps_per_second': 8.985, 'epoch': 0.56}
{'loss': 0.31, 'grad_norm': 0.24553987383842468, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2593285143375397, 'eval_runtime': 6.9934, 'eval_samples_per_second': 142.85, 'eval_steps_per_second': 9.009, 'epoch': 0.6}
{'loss': 0.3313, 'grad_norm': 0.26108673214912415, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.24386747181415558, 'eval_runtime': 7.0016, 'eval_samples_per_second': 142.681, 'eval_steps_per_second': 8.998, 'epoch': 0.64}
{'loss': 0.2835, 'grad_norm': 0.29380735754966736, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.22927957773208618, 'eval_runtime': 6.97, 'eval_samples_per_second': 143.329, 'eval_steps_per_second': 9.039, 'epoch': 0.68}
{'loss': 0.3076, 'grad_norm': 0.26308026909828186, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.21593576669692993, 'eval_runtime': 6.96, 'eval_samples_per_second': 143.534, 'eval_steps_per_second': 9.052, 'epoch': 0.72}
{'loss': 0.2851, 'grad_norm': 0.201638326048851, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2086392641067505, 'eval_runtime': 7.0186, 'eval_samples_per_second': 142.336, 'eval_steps_per_second': 8.976, 'epoch': 0.76}
{'loss': 0.2571, 'grad_norm': 0.3731008768081665, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.19958122074604034, 'eval_runtime': 7.003, 'eval_samples_per_second': 142.653, 'eval_steps_per_second': 8.996, 'epoch': 0.8}
{'loss': 0.2551, 'grad_norm': 0.16231395304203033, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.19145961105823517, 'eval_runtime': 7.0183, 'eval_samples_per_second': 142.342, 'eval_steps_per_second': 8.976, 'epoch': 0.84}
{'loss': 0.2611, 'grad_norm': 0.4019549489021301, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.18812516331672668, 'eval_runtime': 7.0226, 'eval_samples_per_second': 142.255, 'eval_steps_per_second': 8.971, 'epoch': 0.88}
{'loss': 0.2286, 'grad_norm': 0.18522244691848755, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.1810377538204193, 'eval_runtime': 7.0064, 'eval_samples_per_second': 142.583, 'eval_steps_per_second': 8.992, 'epoch': 0.92}
{'loss': 0.2476, 'grad_norm': 0.28916335105895996, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.17866018414497375, 'eval_runtime': 7.0124, 'eval_samples_per_second': 142.462, 'eval_steps_per_second': 8.984, 'epoch': 0.96}
{'loss': 0.2183, 'grad_norm': 0.16655130684375763, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.1775704026222229, 'eval_runtime': 7.0241, 'eval_samples_per_second': 142.225, 'eval_steps_per_second': 8.969, 'epoch': 1.0}
{'train_runtime': 413.1981, 'train_samples_per_second': 24.199, 'train_steps_per_second': 1.513, 'train_loss': 0.5290542022705078, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1144428253173828, 0.927787184715271, 0.772388219833374, 0.6892240047454834, 0.6314793229103088, 0.564478874206543, 0.5192660093307495, 0.46699196100234985, 0.4297623038291931, 0.3891719579696655, 0.35145074129104614, 0.3209875524044037, 0.29713886976242065, 0.27781301736831665, 0.2593285143375397, 0.24386747181415558, 0.22927957773208618, 0.21593576669692993, 0.2086392641067505, 0.19958122074604034, 0.19145961105823517, 0.18812516331672668, 0.1810377538204193, 0.17866018414497375, 0.1775704026222229], 'performance': [0.65, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.46it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.58it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.96it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 58.90it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.245718240737915
current iteration best possible performance (full train run):  0.63
max performance so far:  0.651
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2211 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.9309117048192572
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.9453120745930771
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.0110675120710244
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.0110622764544224
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.0110675142124674
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3182, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.6818, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1.5540e-16, dtype=torch.float64), tensor(0.3182, dtype=torch.float64), tensor(2.7939e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.6649e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1042e-16, dtype=torch.float64), tensor(0.6818, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.318
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.682

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.80it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 28.75it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.36it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 39.64it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.22it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 45.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.00it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 50.21it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.36it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 51.74it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 50.99it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 50.28it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 50.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 46.83it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.0641, 'grad_norm': 0.3648355007171631, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.0367668867111206, 'eval_runtime': 10.4048, 'eval_samples_per_second': 96.014, 'eval_steps_per_second': 6.055, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.72it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 28.52it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.21it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.96it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 46.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 45.37it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 50.15it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 50.04it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 49.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.03it/s]
Evaluation performance at step 50: 0.56
{'loss': 0.9181, 'grad_norm': 0.32062360644340515, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 0.8586626052856445, 'eval_runtime': 10.3998, 'eval_samples_per_second': 96.06, 'eval_steps_per_second': 6.058, 'epoch': 0.08}
{'loss': 0.8253, 'grad_norm': 0.24204504489898682, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.795652449131012, 'eval_runtime': 10.4436, 'eval_samples_per_second': 95.656, 'eval_steps_per_second': 6.032, 'epoch': 0.12}
{'loss': 0.7564, 'grad_norm': 0.245091512799263, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7496962547302246, 'eval_runtime': 10.4558, 'eval_samples_per_second': 95.545, 'eval_steps_per_second': 6.025, 'epoch': 0.16}
{'loss': 0.7635, 'grad_norm': 0.2465251386165619, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7017130255699158, 'eval_runtime': 10.4944, 'eval_samples_per_second': 95.193, 'eval_steps_per_second': 6.003, 'epoch': 0.2}
{'loss': 0.6694, 'grad_norm': 0.28219839930534363, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6520337462425232, 'eval_runtime': 10.5044, 'eval_samples_per_second': 95.103, 'eval_steps_per_second': 5.998, 'epoch': 0.24}
{'loss': 0.6539, 'grad_norm': 0.3104071021080017, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6045038104057312, 'eval_runtime': 10.4871, 'eval_samples_per_second': 95.26, 'eval_steps_per_second': 6.007, 'epoch': 0.28}
{'loss': 0.6577, 'grad_norm': 0.29957345128059387, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.561679482460022, 'eval_runtime': 10.4937, 'eval_samples_per_second': 95.2, 'eval_steps_per_second': 6.004, 'epoch': 0.32}
{'loss': 0.5646, 'grad_norm': 0.25942346453666687, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5295714139938354, 'eval_runtime': 10.4357, 'eval_samples_per_second': 95.729, 'eval_steps_per_second': 6.037, 'epoch': 0.36}
{'loss': 0.5935, 'grad_norm': 0.2375294417142868, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.498899906873703, 'eval_runtime': 10.435, 'eval_samples_per_second': 95.736, 'eval_steps_per_second': 6.037, 'epoch': 0.4}
{'loss': 0.543, 'grad_norm': 0.25975340604782104, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.47270604968070984, 'eval_runtime': 10.4315, 'eval_samples_per_second': 95.767, 'eval_steps_per_second': 6.039, 'epoch': 0.44}
{'loss': 0.5414, 'grad_norm': 0.30166095495224, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4538283348083496, 'eval_runtime': 10.4413, 'eval_samples_per_second': 95.678, 'eval_steps_per_second': 6.034, 'epoch': 0.48}
{'loss': 0.515, 'grad_norm': 0.2798759639263153, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4316413998603821, 'eval_runtime': 10.4309, 'eval_samples_per_second': 95.773, 'eval_steps_per_second': 6.04, 'epoch': 0.52}
{'loss': 0.4755, 'grad_norm': 0.24761083722114563, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4145580530166626, 'eval_runtime': 10.4436, 'eval_samples_per_second': 95.656, 'eval_steps_per_second': 6.032, 'epoch': 0.56}
{'loss': 0.4544, 'grad_norm': 0.21801702678203583, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3966027498245239, 'eval_runtime': 10.4675, 'eval_samples_per_second': 95.438, 'eval_steps_per_second': 6.019, 'epoch': 0.6}
{'loss': 0.4786, 'grad_norm': 0.3177286684513092, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.37897273898124695, 'eval_runtime': 10.4321, 'eval_samples_per_second': 95.762, 'eval_steps_per_second': 6.039, 'epoch': 0.64}
{'loss': 0.4315, 'grad_norm': 0.35321080684661865, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.36789071559906006, 'eval_runtime': 10.4398, 'eval_samples_per_second': 95.692, 'eval_steps_per_second': 6.035, 'epoch': 0.68}
{'loss': 0.4503, 'grad_norm': 0.20091567933559418, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.35993707180023193, 'eval_runtime': 10.4221, 'eval_samples_per_second': 95.854, 'eval_steps_per_second': 6.045, 'epoch': 0.72}
{'loss': 0.4373, 'grad_norm': 0.2068171501159668, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.35007116198539734, 'eval_runtime': 10.4314, 'eval_samples_per_second': 95.769, 'eval_steps_per_second': 6.039, 'epoch': 0.76}
{'loss': 0.3915, 'grad_norm': 0.19676843285560608, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3432905673980713, 'eval_runtime': 10.4156, 'eval_samples_per_second': 95.914, 'eval_steps_per_second': 6.049, 'epoch': 0.8}
{'loss': 0.4246, 'grad_norm': 0.15488561987876892, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.33684614300727844, 'eval_runtime': 10.4146, 'eval_samples_per_second': 95.923, 'eval_steps_per_second': 6.049, 'epoch': 0.84}
{'loss': 0.4261, 'grad_norm': 0.21739435195922852, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.329834908246994, 'eval_runtime': 10.4817, 'eval_samples_per_second': 95.309, 'eval_steps_per_second': 6.01, 'epoch': 0.88}
{'loss': 0.356, 'grad_norm': 0.17186331748962402, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3242408037185669, 'eval_runtime': 10.5489, 'eval_samples_per_second': 94.702, 'eval_steps_per_second': 5.972, 'epoch': 0.92}
{'loss': 0.3808, 'grad_norm': 0.1701555699110031, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3220469653606415, 'eval_runtime': 10.5442, 'eval_samples_per_second': 94.744, 'eval_steps_per_second': 5.975, 'epoch': 0.96}
{'loss': 0.366, 'grad_norm': 0.19942598044872284, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3210185170173645, 'eval_runtime': 10.5031, 'eval_samples_per_second': 95.114, 'eval_steps_per_second': 5.998, 'epoch': 1.0}
{'train_runtime': 571.3832, 'train_samples_per_second': 17.5, 'train_steps_per_second': 1.094, 'train_loss': 0.6055446456909179, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0367668867111206, 0.8586626052856445, 0.795652449131012, 0.7496962547302246, 0.7017130255699158, 0.6520337462425232, 0.6045038104057312, 0.561679482460022, 0.5295714139938354, 0.498899906873703, 0.47270604968070984, 0.4538283348083496, 0.4316413998603821, 0.4145580530166626, 0.3966027498245239, 0.37897273898124695, 0.36789071559906006, 0.35993707180023193, 0.35007116198539734, 0.3432905673980713, 0.33684614300727844, 0.329834908246994, 0.3242408037185669, 0.3220469653606415, 0.3210185170173645], 'performance': [0.63, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:42,  2.34it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 28.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 37.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 41.23it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.69it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 66.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.73it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.2479344606399536
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.651
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6536 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 0.9957745429422877
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9886777351582337
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 0.9959557244363226
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7059700898148921
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.9909489544151519
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2842, dtype=torch.float64), 0, 0, tensor(0.0115, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6899, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2842, dtype=torch.float64), tensor(0.0088, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0115, dtype=torch.float64), tensor(4.0902e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0056, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6899, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.284
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.012
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.69

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9856
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  985
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.84it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.39it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.37it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.54it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.60it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.58it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.23it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 51.27it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.84it/s]
Evaluation performance at step 25: 0.66
{'loss': 2.3664, 'grad_norm': 0.4760686457157135, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.66}
{'eval_loss': 1.0542131662368774, 'eval_runtime': 8.189, 'eval_samples_per_second': 120.283, 'eval_steps_per_second': 7.571, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.58it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:08, 10.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 19.16it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:02, 26.72it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 32.04it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 37.26it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 41.56it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 46.73it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 45.15it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:02<00:00, 49.92it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 49.72it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 49.49it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 39.56it/s]
Evaluation performance at step 50: 0.6
{'loss': 0.9431, 'grad_norm': 0.9322682023048401, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 0.8440060615539551, 'eval_runtime': 7.4364, 'eval_samples_per_second': 132.457, 'eval_steps_per_second': 8.337, 'epoch': 0.08}
{'loss': 0.8074, 'grad_norm': 0.2769834101200104, 'learning_rate': 0.0002872791519434629, 'epoch': 0.12}
{'eval_loss': 0.7840808629989624, 'eval_runtime': 7.4667, 'eval_samples_per_second': 131.918, 'eval_steps_per_second': 8.303, 'epoch': 0.12}
{'loss': 0.7773, 'grad_norm': 0.27635809779167175, 'learning_rate': 0.00027402826855123675, 'epoch': 0.16}
{'eval_loss': 0.7220045328140259, 'eval_runtime': 7.4637, 'eval_samples_per_second': 131.972, 'eval_steps_per_second': 8.307, 'epoch': 0.16}
{'loss': 0.7372, 'grad_norm': 0.2651572823524475, 'learning_rate': 0.00026077738515901055, 'epoch': 0.2}
{'eval_loss': 0.6690880656242371, 'eval_runtime': 7.4707, 'eval_samples_per_second': 131.849, 'eval_steps_per_second': 8.299, 'epoch': 0.2}
{'loss': 0.6793, 'grad_norm': 0.2840079963207245, 'learning_rate': 0.0002475265017667844, 'epoch': 0.24}
{'eval_loss': 0.6083289384841919, 'eval_runtime': 7.4768, 'eval_samples_per_second': 131.741, 'eval_steps_per_second': 8.292, 'epoch': 0.24}
{'loss': 0.6353, 'grad_norm': 0.38845619559288025, 'learning_rate': 0.00023427561837455828, 'epoch': 0.28}
{'eval_loss': 0.553900420665741, 'eval_runtime': 7.4729, 'eval_samples_per_second': 131.81, 'eval_steps_per_second': 8.297, 'epoch': 0.28}
{'loss': 0.5902, 'grad_norm': 0.2640211284160614, 'learning_rate': 0.00022102473498233213, 'epoch': 0.32}
{'eval_loss': 0.5182704925537109, 'eval_runtime': 7.4771, 'eval_samples_per_second': 131.735, 'eval_steps_per_second': 8.292, 'epoch': 0.32}
{'loss': 0.545, 'grad_norm': 0.35150033235549927, 'learning_rate': 0.00020777385159010599, 'epoch': 0.37}
{'eval_loss': 0.4756622016429901, 'eval_runtime': 7.4727, 'eval_samples_per_second': 131.814, 'eval_steps_per_second': 8.297, 'epoch': 0.37}
{'loss': 0.5508, 'grad_norm': 0.3117190897464752, 'learning_rate': 0.00019452296819787987, 'epoch': 0.41}
{'eval_loss': 0.44503360986709595, 'eval_runtime': 7.4591, 'eval_samples_per_second': 132.054, 'eval_steps_per_second': 8.312, 'epoch': 0.41}
{'loss': 0.5137, 'grad_norm': 0.31323832273483276, 'learning_rate': 0.0001812720848056537, 'epoch': 0.45}
{'eval_loss': 0.41716575622558594, 'eval_runtime': 7.4402, 'eval_samples_per_second': 132.389, 'eval_steps_per_second': 8.333, 'epoch': 0.45}
{'loss': 0.4642, 'grad_norm': 0.3492448329925537, 'learning_rate': 0.00016802120141342754, 'epoch': 0.49}
{'eval_loss': 0.3928965628147125, 'eval_runtime': 7.4547, 'eval_samples_per_second': 132.131, 'eval_steps_per_second': 8.317, 'epoch': 0.49}
{'loss': 0.4145, 'grad_norm': 0.401351660490036, 'learning_rate': 0.0001547703180212014, 'epoch': 0.53}
{'eval_loss': 0.36894410848617554, 'eval_runtime': 7.5501, 'eval_samples_per_second': 130.462, 'eval_steps_per_second': 8.212, 'epoch': 0.53}
{'loss': 0.3938, 'grad_norm': 0.37002861499786377, 'learning_rate': 0.00014151943462897525, 'epoch': 0.57}
{'eval_loss': 0.3518000543117523, 'eval_runtime': 7.5672, 'eval_samples_per_second': 130.168, 'eval_steps_per_second': 8.193, 'epoch': 0.57}
{'loss': 0.3966, 'grad_norm': 0.25004804134368896, 'learning_rate': 0.0001282685512367491, 'epoch': 0.61}
{'eval_loss': 0.3353579640388489, 'eval_runtime': 7.5659, 'eval_samples_per_second': 130.189, 'eval_steps_per_second': 8.195, 'epoch': 0.61}
{'loss': 0.3471, 'grad_norm': 0.21772651374340057, 'learning_rate': 0.00011501766784452296, 'epoch': 0.65}
{'eval_loss': 0.31650787591934204, 'eval_runtime': 7.5542, 'eval_samples_per_second': 130.39, 'eval_steps_per_second': 8.207, 'epoch': 0.65}
{'loss': 0.3671, 'grad_norm': 0.2581116259098053, 'learning_rate': 0.00010176678445229682, 'epoch': 0.69}
{'eval_loss': 0.30587419867515564, 'eval_runtime': 7.5443, 'eval_samples_per_second': 130.563, 'eval_steps_per_second': 8.218, 'epoch': 0.69}
{'loss': 0.3788, 'grad_norm': 0.3481748104095459, 'learning_rate': 8.851590106007066e-05, 'epoch': 0.73}
{'eval_loss': 0.2926578223705292, 'eval_runtime': 7.5787, 'eval_samples_per_second': 129.97, 'eval_steps_per_second': 8.181, 'epoch': 0.73}
{'loss': 0.358, 'grad_norm': 0.2750762701034546, 'learning_rate': 7.526501766784451e-05, 'epoch': 0.77}
{'eval_loss': 0.28546765446662903, 'eval_runtime': 7.5737, 'eval_samples_per_second': 130.056, 'eval_steps_per_second': 8.186, 'epoch': 0.77}
{'loss': 0.3127, 'grad_norm': 0.29782378673553467, 'learning_rate': 6.201413427561837e-05, 'epoch': 0.81}
{'eval_loss': 0.27951258420944214, 'eval_runtime': 7.5783, 'eval_samples_per_second': 129.977, 'eval_steps_per_second': 8.181, 'epoch': 0.81}
{'loss': 0.3268, 'grad_norm': 0.1943923830986023, 'learning_rate': 4.876325088339222e-05, 'epoch': 0.85}
{'eval_loss': 0.27215898036956787, 'eval_runtime': 7.5761, 'eval_samples_per_second': 130.014, 'eval_steps_per_second': 8.184, 'epoch': 0.85}
{'loss': 0.3106, 'grad_norm': 0.23063620924949646, 'learning_rate': 3.551236749116607e-05, 'epoch': 0.89}
{'eval_loss': 0.2664276957511902, 'eval_runtime': 7.5727, 'eval_samples_per_second': 130.072, 'eval_steps_per_second': 8.187, 'epoch': 0.89}
{'loss': 0.3122, 'grad_norm': 0.22200676798820496, 'learning_rate': 2.2261484098939926e-05, 'epoch': 0.93}
{'eval_loss': 0.2642945647239685, 'eval_runtime': 7.6009, 'eval_samples_per_second': 129.59, 'eval_steps_per_second': 8.157, 'epoch': 0.93}
{'loss': 0.289, 'grad_norm': 0.19954325258731842, 'learning_rate': 9.010600706713779e-06, 'epoch': 0.97}
{'eval_loss': 0.26214316487312317, 'eval_runtime': 7.5871, 'eval_samples_per_second': 129.826, 'eval_steps_per_second': 8.172, 'epoch': 0.97}
{'train_runtime': 422.7259, 'train_samples_per_second': 23.315, 'train_steps_per_second': 1.457, 'train_loss': 0.5687567266550931, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0542131662368774, 0.8440060615539551, 0.7840808629989624, 0.7220045328140259, 0.6690880656242371, 0.6083289384841919, 0.553900420665741, 0.5182704925537109, 0.4756622016429901, 0.44503360986709595, 0.41716575622558594, 0.3928965628147125, 0.36894410848617554, 0.3518000543117523, 0.3353579640388489, 0.31650787591934204, 0.30587419867515564, 0.2926578223705292, 0.28546765446662903, 0.27951258420944214, 0.27215898036956787, 0.2664276957511902, 0.2642945647239685, 0.26214316487312317], 'performance': [0.66, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:40,  2.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 25.92it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 33.42it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 34.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 38.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 41.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 53.94it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 41.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.66, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2451167106628418
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.6615000000000001
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7406 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.8975696010574759
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.7424385890574561
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.9924087183230061
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.9002365725833203
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.9794047337810137
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.2930, dtype=torch.float64), 0, 0, tensor(0.7070, dtype=torch.float64), 32, 0, 1, 1, 1, 0, 128, 0.09999999999999999, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.9619e-18, dtype=torch.float64), tensor(4.3923e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2930, dtype=torch.float64), tensor(3.6672e-17, dtype=torch.float64), tensor(8.0100e-17, dtype=torch.float64), tensor(0.7070, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.293
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.707

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999999,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.09999999999999999
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.59it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 45.15it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 49.05it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 51.44it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 55.18it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 57.90it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 62.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.84it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 62.68it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.14it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 57.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.97it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.4644, 'grad_norm': 0.49463197588920593, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.0179195404052734, 'eval_runtime': 6.6436, 'eval_samples_per_second': 150.37, 'eval_steps_per_second': 9.483, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.64it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 22.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.71it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 48.24it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.37it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.79it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.09it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.16it/s]
Evaluation performance at step 50: 0.63
{'loss': 0.8909, 'grad_norm': 0.4012615978717804, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 0.8037864565849304, 'eval_runtime': 6.6662, 'eval_samples_per_second': 149.861, 'eval_steps_per_second': 9.451, 'epoch': 0.08}
{'loss': 0.7838, 'grad_norm': 0.25808224081993103, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7527758479118347, 'eval_runtime': 6.6737, 'eval_samples_per_second': 149.692, 'eval_steps_per_second': 9.44, 'epoch': 0.12}
{'loss': 0.7619, 'grad_norm': 0.2616489827632904, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7003868222236633, 'eval_runtime': 6.7014, 'eval_samples_per_second': 149.072, 'eval_steps_per_second': 9.401, 'epoch': 0.16}
{'loss': 0.701, 'grad_norm': 0.33305686712265015, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.627350926399231, 'eval_runtime': 6.6832, 'eval_samples_per_second': 149.479, 'eval_steps_per_second': 9.427, 'epoch': 0.2}
{'loss': 0.6515, 'grad_norm': 0.3185643255710602, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5747341513633728, 'eval_runtime': 6.6949, 'eval_samples_per_second': 149.217, 'eval_steps_per_second': 9.41, 'epoch': 0.24}
{'loss': 0.5777, 'grad_norm': 0.33961769938468933, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5065913200378418, 'eval_runtime': 6.6982, 'eval_samples_per_second': 149.144, 'eval_steps_per_second': 9.405, 'epoch': 0.28}
{'loss': 0.5185, 'grad_norm': 0.33713498711586, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.4570927023887634, 'eval_runtime': 6.7058, 'eval_samples_per_second': 148.975, 'eval_steps_per_second': 9.395, 'epoch': 0.32}
{'loss': 0.4799, 'grad_norm': 0.3518284857273102, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4074859917163849, 'eval_runtime': 6.714, 'eval_samples_per_second': 148.794, 'eval_steps_per_second': 9.383, 'epoch': 0.36}
{'loss': 0.4371, 'grad_norm': 0.4743369519710541, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.3567189872264862, 'eval_runtime': 6.7434, 'eval_samples_per_second': 148.145, 'eval_steps_per_second': 9.342, 'epoch': 0.4}
{'loss': 0.4081, 'grad_norm': 0.4478357434272766, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.32545459270477295, 'eval_runtime': 6.7824, 'eval_samples_per_second': 147.293, 'eval_steps_per_second': 9.289, 'epoch': 0.44}
{'loss': 0.3504, 'grad_norm': 0.3333369493484497, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.29015350341796875, 'eval_runtime': 6.7514, 'eval_samples_per_second': 147.97, 'eval_steps_per_second': 9.331, 'epoch': 0.48}
{'loss': 0.3317, 'grad_norm': 0.32290348410606384, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.265153706073761, 'eval_runtime': 6.7235, 'eval_samples_per_second': 148.583, 'eval_steps_per_second': 9.37, 'epoch': 0.52}
{'loss': 0.309, 'grad_norm': 0.47930973768234253, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.2390163242816925, 'eval_runtime': 6.7071, 'eval_samples_per_second': 148.946, 'eval_steps_per_second': 9.393, 'epoch': 0.56}
{'loss': 0.2792, 'grad_norm': 0.37576836347579956, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2103389948606491, 'eval_runtime': 6.7134, 'eval_samples_per_second': 148.807, 'eval_steps_per_second': 9.384, 'epoch': 0.6}
{'loss': 0.2512, 'grad_norm': 0.3035423457622528, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.19199438393115997, 'eval_runtime': 6.688, 'eval_samples_per_second': 149.372, 'eval_steps_per_second': 9.42, 'epoch': 0.64}
{'loss': 0.2291, 'grad_norm': 0.42531293630599976, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.1772627979516983, 'eval_runtime': 6.6972, 'eval_samples_per_second': 149.167, 'eval_steps_per_second': 9.407, 'epoch': 0.68}
{'loss': 0.191, 'grad_norm': 0.33553680777549744, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.16331012547016144, 'eval_runtime': 6.6918, 'eval_samples_per_second': 149.288, 'eval_steps_per_second': 9.415, 'epoch': 0.72}
{'loss': 0.1844, 'grad_norm': 0.28206226229667664, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.15396559238433838, 'eval_runtime': 6.6676, 'eval_samples_per_second': 149.83, 'eval_steps_per_second': 9.449, 'epoch': 0.76}
{'loss': 0.1738, 'grad_norm': 0.1547853797674179, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.14517508447170258, 'eval_runtime': 6.6673, 'eval_samples_per_second': 149.836, 'eval_steps_per_second': 9.449, 'epoch': 0.8}
{'loss': 0.167, 'grad_norm': 0.2764161229133606, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.13984254002571106, 'eval_runtime': 6.6653, 'eval_samples_per_second': 149.882, 'eval_steps_per_second': 9.452, 'epoch': 0.84}
{'loss': 0.1713, 'grad_norm': 0.25660690665245056, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.12906469404697418, 'eval_runtime': 6.6732, 'eval_samples_per_second': 149.703, 'eval_steps_per_second': 9.441, 'epoch': 0.88}
{'loss': 0.1412, 'grad_norm': 0.25126954913139343, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.1222749873995781, 'eval_runtime': 6.6731, 'eval_samples_per_second': 149.705, 'eval_steps_per_second': 9.441, 'epoch': 0.92}
{'loss': 0.1441, 'grad_norm': 0.23582442104816437, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.11808101832866669, 'eval_runtime': 6.6685, 'eval_samples_per_second': 149.808, 'eval_steps_per_second': 9.447, 'epoch': 0.96}
{'loss': 0.1431, 'grad_norm': 0.4841640591621399, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.11676536500453949, 'eval_runtime': 6.6864, 'eval_samples_per_second': 149.408, 'eval_steps_per_second': 9.422, 'epoch': 1.0}
{'train_runtime': 391.0652, 'train_samples_per_second': 25.569, 'train_steps_per_second': 1.598, 'train_loss': 0.4696505168914795, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0179195404052734, 0.8037864565849304, 0.7527758479118347, 0.7003868222236633, 0.627350926399231, 0.5747341513633728, 0.5065913200378418, 0.4570927023887634, 0.4074859917163849, 0.3567189872264862, 0.32545459270477295, 0.29015350341796875, 0.265153706073761, 0.2390163242816925, 0.2103389948606491, 0.19199438393115997, 0.1772627979516983, 0.16331012547016144, 0.15396559238433838, 0.14517508447170258, 0.13984254002571106, 0.12906469404697418, 0.1222749873995781, 0.11808101832866669, 0.11676536500453949], 'performance': [0.64, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.47it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.99it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.84it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 73.88it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.00it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.240851640701294
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.6615000000000001
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.6889 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.9834481624386124
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.965722363781345
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 1.100907825676877
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.9834709085076714
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.9834710285399483
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0746, dtype=torch.float64), 0, 0, tensor(0.1911, dtype=torch.float64), 0, 0, tensor(0.0200, dtype=torch.float64), 0, tensor(0.7143, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.024617057686215985, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(0.0746, dtype=torch.float64), tensor(7.7045e-17, dtype=torch.float64), tensor(1.0109e-17, dtype=torch.float64), tensor(0.1911, dtype=torch.float64), tensor(1.5019e-16, dtype=torch.float64), tensor(2.5567e-17, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), tensor(1.7117e-17, dtype=torch.float64), tensor(0.7143, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2462, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.075
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.191
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.02
  mmlu: 0
  arc_challenge: 0.714

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.024617057686215985,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.024617057686215985
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.86it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.27it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.85it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.09it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.44it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.90it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 50.11it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 46.80it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 51.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 50.37it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 47.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.97it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.3744, 'grad_norm': 0.4566997289657593, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.0663889646530151, 'eval_runtime': 7.8793, 'eval_samples_per_second': 126.787, 'eval_steps_per_second': 7.996, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.87it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.41it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.28it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.52it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.66it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.58it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.16it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 51.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.16it/s]
Evaluation performance at step 50: 0.6
{'loss': 0.9466, 'grad_norm': 0.6541130542755127, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 0.8655654788017273, 'eval_runtime': 7.6626, 'eval_samples_per_second': 130.374, 'eval_steps_per_second': 8.222, 'epoch': 0.08}
{'loss': 0.8137, 'grad_norm': 0.407050758600235, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7895177602767944, 'eval_runtime': 7.6723, 'eval_samples_per_second': 130.209, 'eval_steps_per_second': 8.211, 'epoch': 0.12}
{'loss': 0.7818, 'grad_norm': 0.3763469457626343, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7227262258529663, 'eval_runtime': 7.6953, 'eval_samples_per_second': 129.819, 'eval_steps_per_second': 8.187, 'epoch': 0.16}
{'loss': 0.699, 'grad_norm': 0.29641708731651306, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6514700055122375, 'eval_runtime': 7.6756, 'eval_samples_per_second': 130.152, 'eval_steps_per_second': 8.208, 'epoch': 0.2}
{'loss': 0.7104, 'grad_norm': 0.3171139657497406, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5919253826141357, 'eval_runtime': 7.6822, 'eval_samples_per_second': 130.041, 'eval_steps_per_second': 8.201, 'epoch': 0.24}
{'loss': 0.5808, 'grad_norm': 0.27994924783706665, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5418128371238708, 'eval_runtime': 7.6759, 'eval_samples_per_second': 130.148, 'eval_steps_per_second': 8.208, 'epoch': 0.28}
{'loss': 0.5544, 'grad_norm': 0.363991916179657, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5010823607444763, 'eval_runtime': 7.6828, 'eval_samples_per_second': 130.031, 'eval_steps_per_second': 8.2, 'epoch': 0.32}
{'loss': 0.5356, 'grad_norm': 0.40632644295692444, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.44623494148254395, 'eval_runtime': 7.6887, 'eval_samples_per_second': 129.931, 'eval_steps_per_second': 8.194, 'epoch': 0.36}
{'loss': 0.4757, 'grad_norm': 0.3771359920501709, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.4122733473777771, 'eval_runtime': 7.7052, 'eval_samples_per_second': 129.653, 'eval_steps_per_second': 8.176, 'epoch': 0.4}
{'loss': 0.4275, 'grad_norm': 0.28659600019454956, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3793252110481262, 'eval_runtime': 7.6957, 'eval_samples_per_second': 129.812, 'eval_steps_per_second': 8.186, 'epoch': 0.44}
{'loss': 0.3861, 'grad_norm': 0.2705605626106262, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3535560369491577, 'eval_runtime': 7.7044, 'eval_samples_per_second': 129.667, 'eval_steps_per_second': 8.177, 'epoch': 0.48}
{'loss': 0.403, 'grad_norm': 0.2660219967365265, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.33341026306152344, 'eval_runtime': 7.7286, 'eval_samples_per_second': 129.26, 'eval_steps_per_second': 8.152, 'epoch': 0.52}
{'loss': 0.3572, 'grad_norm': 0.26482564210891724, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3069038391113281, 'eval_runtime': 7.712, 'eval_samples_per_second': 129.538, 'eval_steps_per_second': 8.169, 'epoch': 0.56}
{'loss': 0.3804, 'grad_norm': 0.2647087574005127, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.28621894121170044, 'eval_runtime': 7.6932, 'eval_samples_per_second': 129.854, 'eval_steps_per_second': 8.189, 'epoch': 0.6}
{'loss': 0.3446, 'grad_norm': 0.17571178078651428, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.27132877707481384, 'eval_runtime': 7.7081, 'eval_samples_per_second': 129.604, 'eval_steps_per_second': 8.173, 'epoch': 0.64}
{'loss': 0.2972, 'grad_norm': 0.21396006643772125, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2602135241031647, 'eval_runtime': 7.6982, 'eval_samples_per_second': 129.77, 'eval_steps_per_second': 8.184, 'epoch': 0.68}
{'loss': 0.2764, 'grad_norm': 0.2914709448814392, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.24354326725006104, 'eval_runtime': 7.6835, 'eval_samples_per_second': 130.019, 'eval_steps_per_second': 8.199, 'epoch': 0.72}
{'loss': 0.2779, 'grad_norm': 0.2110956311225891, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.23518504202365875, 'eval_runtime': 7.7225, 'eval_samples_per_second': 129.362, 'eval_steps_per_second': 8.158, 'epoch': 0.76}
{'loss': 0.2785, 'grad_norm': 0.4219987094402313, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.22964000701904297, 'eval_runtime': 7.7033, 'eval_samples_per_second': 129.684, 'eval_steps_per_second': 8.178, 'epoch': 0.8}
{'loss': 0.2902, 'grad_norm': 0.18619850277900696, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.22427475452423096, 'eval_runtime': 7.6845, 'eval_samples_per_second': 130.001, 'eval_steps_per_second': 8.198, 'epoch': 0.84}
{'loss': 0.2704, 'grad_norm': 0.11598996073007584, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.21831737458705902, 'eval_runtime': 7.6863, 'eval_samples_per_second': 129.971, 'eval_steps_per_second': 8.196, 'epoch': 0.88}
{'loss': 0.2634, 'grad_norm': 0.20019765198230743, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2141599804162979, 'eval_runtime': 7.6897, 'eval_samples_per_second': 129.914, 'eval_steps_per_second': 8.193, 'epoch': 0.92}
{'loss': 0.2625, 'grad_norm': 0.19814378023147583, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.21227304637432098, 'eval_runtime': 7.7006, 'eval_samples_per_second': 129.731, 'eval_steps_per_second': 8.181, 'epoch': 0.96}
{'loss': 0.2557, 'grad_norm': 0.18419799208641052, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.21127542853355408, 'eval_runtime': 7.753, 'eval_samples_per_second': 128.854, 'eval_steps_per_second': 8.126, 'epoch': 1.0}
{'train_runtime': 444.3971, 'train_samples_per_second': 22.498, 'train_steps_per_second': 1.406, 'train_loss': 0.5297319793701172, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0663889646530151, 0.8655654788017273, 0.7895177602767944, 0.7227262258529663, 0.6514700055122375, 0.5919253826141357, 0.5418128371238708, 0.5010823607444763, 0.44623494148254395, 0.4122733473777771, 0.3793252110481262, 0.3535560369491577, 0.33341026306152344, 0.3069038391113281, 0.28621894121170044, 0.27132877707481384, 0.2602135241031647, 0.24354326725006104, 0.23518504202365875, 0.22964000701904297, 0.22427475452423096, 0.21831737458705902, 0.2141599804162979, 0.21227304637432098, 0.21127542853355408], 'performance': [0.64, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.29it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 42.76it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.34it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 61.90it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.96it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2448790073394775
current iteration best possible performance (full train run):  0.672
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9078 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.9751889037232325
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.9751921571429679
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.9751921532061001
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.9078830533156367
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.975192157104317
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2997, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.7003, dtype=torch.float64), 32, 0, 0, 1, 0, 1, 128, 5.063001287087161e-17, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.2997, dtype=torch.float64), tensor(2.1483e-16, dtype=torch.float64), tensor(6.8952e-17, dtype=torch.float64), tensor(2.7371e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9982e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7003, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(5.0630e-16, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.3
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.7

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (5.063001287087161e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 1]
lora rank:  128
lora dropout:  5.063001287087161e-17
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.93it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.47it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 46.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 51.67it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.78it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 59.06it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 59.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 62.77it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 66.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 66.11it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 66.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.90it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.2966, 'grad_norm': 0.39191704988479614, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.2088425159454346, 'eval_runtime': 9.2506, 'eval_samples_per_second': 107.993, 'eval_steps_per_second': 6.81, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.99it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 39.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 47.80it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 51.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.61it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 58.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 66.40it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 62.03it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 66.16it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 65.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 65.27it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.88it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.0763, 'grad_norm': 0.17140179872512817, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 0.9753530025482178, 'eval_runtime': 9.2366, 'eval_samples_per_second': 108.157, 'eval_steps_per_second': 6.821, 'epoch': 0.08}
{'loss': 0.9105, 'grad_norm': 0.1632816195487976, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.854942262172699, 'eval_runtime': 9.2882, 'eval_samples_per_second': 107.555, 'eval_steps_per_second': 6.783, 'epoch': 0.12}
{'loss': 0.8346, 'grad_norm': 0.1707068830728531, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7728500962257385, 'eval_runtime': 9.3036, 'eval_samples_per_second': 107.377, 'eval_steps_per_second': 6.772, 'epoch': 0.16}
{'loss': 0.7823, 'grad_norm': 0.2171088308095932, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7304964661598206, 'eval_runtime': 9.3568, 'eval_samples_per_second': 106.767, 'eval_steps_per_second': 6.733, 'epoch': 0.2}
{'loss': 0.7253, 'grad_norm': 0.23322749137878418, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.686378002166748, 'eval_runtime': 9.3827, 'eval_samples_per_second': 106.473, 'eval_steps_per_second': 6.714, 'epoch': 0.24}
{'loss': 0.672, 'grad_norm': 0.2731284499168396, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6417199373245239, 'eval_runtime': 9.3799, 'eval_samples_per_second': 106.504, 'eval_steps_per_second': 6.716, 'epoch': 0.28}
{'loss': 0.6693, 'grad_norm': 0.18372796475887299, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6174984574317932, 'eval_runtime': 9.4029, 'eval_samples_per_second': 106.244, 'eval_steps_per_second': 6.7, 'epoch': 0.32}
{'loss': 0.6505, 'grad_norm': 0.2223922163248062, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5706406831741333, 'eval_runtime': 9.3811, 'eval_samples_per_second': 106.491, 'eval_steps_per_second': 6.716, 'epoch': 0.36}
{'loss': 0.5857, 'grad_norm': 0.26618197560310364, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5295575857162476, 'eval_runtime': 9.4051, 'eval_samples_per_second': 106.218, 'eval_steps_per_second': 6.698, 'epoch': 0.4}
{'loss': 0.5716, 'grad_norm': 0.28602078557014465, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5041759610176086, 'eval_runtime': 9.3867, 'eval_samples_per_second': 106.428, 'eval_steps_per_second': 6.712, 'epoch': 0.44}
{'loss': 0.5404, 'grad_norm': 0.22114311158657074, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4810538589954376, 'eval_runtime': 9.3825, 'eval_samples_per_second': 106.475, 'eval_steps_per_second': 6.715, 'epoch': 0.48}
{'loss': 0.5525, 'grad_norm': 0.45509159564971924, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.45688945055007935, 'eval_runtime': 9.3534, 'eval_samples_per_second': 106.806, 'eval_steps_per_second': 6.736, 'epoch': 0.52}
{'loss': 0.4595, 'grad_norm': 0.3786991834640503, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.43962374329566956, 'eval_runtime': 9.3612, 'eval_samples_per_second': 106.717, 'eval_steps_per_second': 6.73, 'epoch': 0.56}
{'loss': 0.4681, 'grad_norm': 0.23344168066978455, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.41881614923477173, 'eval_runtime': 9.4067, 'eval_samples_per_second': 106.201, 'eval_steps_per_second': 6.697, 'epoch': 0.6}
{'loss': 0.5089, 'grad_norm': 0.18591849505901337, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.40513938665390015, 'eval_runtime': 9.3624, 'eval_samples_per_second': 106.703, 'eval_steps_per_second': 6.729, 'epoch': 0.64}
{'loss': 0.4358, 'grad_norm': 0.2641906440258026, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3899818956851959, 'eval_runtime': 9.4118, 'eval_samples_per_second': 106.144, 'eval_steps_per_second': 6.694, 'epoch': 0.68}
{'loss': 0.4562, 'grad_norm': 0.2054477483034134, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3802667558193207, 'eval_runtime': 9.4334, 'eval_samples_per_second': 105.901, 'eval_steps_per_second': 6.678, 'epoch': 0.72}
{'loss': 0.4717, 'grad_norm': 0.19222238659858704, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.36929404735565186, 'eval_runtime': 9.3761, 'eval_samples_per_second': 106.547, 'eval_steps_per_second': 6.719, 'epoch': 0.76}
{'loss': 0.414, 'grad_norm': 0.3270419239997864, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.36118659377098083, 'eval_runtime': 9.3807, 'eval_samples_per_second': 106.495, 'eval_steps_per_second': 6.716, 'epoch': 0.8}
{'loss': 0.4331, 'grad_norm': 0.26546159386634827, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.35407158732414246, 'eval_runtime': 9.3714, 'eval_samples_per_second': 106.601, 'eval_steps_per_second': 6.723, 'epoch': 0.84}
{'loss': 0.437, 'grad_norm': 0.18256518244743347, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3477713167667389, 'eval_runtime': 9.3717, 'eval_samples_per_second': 106.597, 'eval_steps_per_second': 6.722, 'epoch': 0.88}
{'loss': 0.4045, 'grad_norm': 0.22282586991786957, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.34364211559295654, 'eval_runtime': 9.3822, 'eval_samples_per_second': 106.478, 'eval_steps_per_second': 6.715, 'epoch': 0.92}
{'loss': 0.3967, 'grad_norm': 0.16854192316532135, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3405494689941406, 'eval_runtime': 9.3869, 'eval_samples_per_second': 106.425, 'eval_steps_per_second': 6.712, 'epoch': 0.96}
{'loss': 0.3778, 'grad_norm': 0.14876314997673035, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3394298851490021, 'eval_runtime': 9.3644, 'eval_samples_per_second': 106.681, 'eval_steps_per_second': 6.728, 'epoch': 1.0}
{'train_runtime': 498.9643, 'train_samples_per_second': 20.04, 'train_steps_per_second': 1.253, 'train_loss': 0.6452354461669921, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2088425159454346, 0.9753530025482178, 0.854942262172699, 0.7728500962257385, 0.7304964661598206, 0.686378002166748, 0.6417199373245239, 0.6174984574317932, 0.5706406831741333, 0.5295575857162476, 0.5041759610176086, 0.4810538589954376, 0.45688945055007935, 0.43962374329566956, 0.41881614923477173, 0.40513938665390015, 0.3899818956851959, 0.3802667558193207, 0.36929404735565186, 0.36118659377098083, 0.35407158732414246, 0.3477713167667389, 0.34364211559295654, 0.3405494689941406, 0.3394298851490021], 'performance': [0.62, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.17it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.56it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.04it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 63.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 67.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 64.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2403504848480225
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2041 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.8558876886923101
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.9136678110446195
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.9499758435336588
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.9574298847185642
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.970086452917995
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.2845, dtype=torch.float64), 0, tensor(0.7155, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.7774e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0793e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2845, dtype=torch.float64), tensor(4.6629e-17, dtype=torch.float64), tensor(0.7155, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.284
  mmlu: 0
  arc_challenge: 0.716

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.24it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.45it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 48.72it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 53.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 56.58it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 61.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 64.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 68.14it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 67.57it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 66.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.44it/s]
Evaluation performance at step 25: 0.65
{'loss': 2.6104, 'grad_norm': 0.6110796928405762, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.65}
{'eval_loss': 1.3519738912582397, 'eval_runtime': 8.1316, 'eval_samples_per_second': 122.853, 'eval_steps_per_second': 7.748, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.53it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 37.02it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 48.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 53.92it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 65.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 67.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 65.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 65.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 65.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 65.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.26it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.2181, 'grad_norm': 0.38138559460639954, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.1355537176132202, 'eval_runtime': 8.0843, 'eval_samples_per_second': 123.573, 'eval_steps_per_second': 7.793, 'epoch': 0.08}
{'loss': 1.0781, 'grad_norm': 0.3714373707771301, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0551402568817139, 'eval_runtime': 8.1289, 'eval_samples_per_second': 122.894, 'eval_steps_per_second': 7.75, 'epoch': 0.12}
{'loss': 1.0146, 'grad_norm': 0.27362021803855896, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0171737670898438, 'eval_runtime': 8.1171, 'eval_samples_per_second': 123.074, 'eval_steps_per_second': 7.761, 'epoch': 0.16}
{'loss': 0.9587, 'grad_norm': 0.2739352285861969, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9845753908157349, 'eval_runtime': 8.1079, 'eval_samples_per_second': 123.213, 'eval_steps_per_second': 7.77, 'epoch': 0.2}
{'loss': 0.9524, 'grad_norm': 0.3335270583629608, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9511789679527283, 'eval_runtime': 8.1124, 'eval_samples_per_second': 123.145, 'eval_steps_per_second': 7.766, 'epoch': 0.24}
{'loss': 0.926, 'grad_norm': 0.353325217962265, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9169284105300903, 'eval_runtime': 8.1114, 'eval_samples_per_second': 123.16, 'eval_steps_per_second': 7.767, 'epoch': 0.28}
{'loss': 0.9233, 'grad_norm': 0.39609360694885254, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8748621344566345, 'eval_runtime': 8.0876, 'eval_samples_per_second': 123.523, 'eval_steps_per_second': 7.79, 'epoch': 0.32}
{'loss': 0.8628, 'grad_norm': 0.38072484731674194, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8344569206237793, 'eval_runtime': 8.0883, 'eval_samples_per_second': 123.511, 'eval_steps_per_second': 7.789, 'epoch': 0.36}
{'loss': 0.7956, 'grad_norm': 0.45768800377845764, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7958859801292419, 'eval_runtime': 8.0843, 'eval_samples_per_second': 123.573, 'eval_steps_per_second': 7.793, 'epoch': 0.4}
{'loss': 0.8577, 'grad_norm': 0.4104301631450653, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7493991255760193, 'eval_runtime': 8.1101, 'eval_samples_per_second': 123.18, 'eval_steps_per_second': 7.768, 'epoch': 0.44}
{'loss': 0.8014, 'grad_norm': 0.4489932358264923, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7147984504699707, 'eval_runtime': 8.1385, 'eval_samples_per_second': 122.749, 'eval_steps_per_second': 7.741, 'epoch': 0.48}
{'loss': 0.7359, 'grad_norm': 0.4080008566379547, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6895516514778137, 'eval_runtime': 8.1817, 'eval_samples_per_second': 122.101, 'eval_steps_per_second': 7.7, 'epoch': 0.52}
{'loss': 0.7268, 'grad_norm': 0.4639705717563629, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6519055962562561, 'eval_runtime': 8.167, 'eval_samples_per_second': 122.321, 'eval_steps_per_second': 7.714, 'epoch': 0.56}
{'loss': 0.6905, 'grad_norm': 0.4909367263317108, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6221016645431519, 'eval_runtime': 8.1129, 'eval_samples_per_second': 123.137, 'eval_steps_per_second': 7.765, 'epoch': 0.6}
{'loss': 0.7636, 'grad_norm': 0.47582948207855225, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5934572219848633, 'eval_runtime': 8.1558, 'eval_samples_per_second': 122.49, 'eval_steps_per_second': 7.725, 'epoch': 0.64}
{'loss': 0.7091, 'grad_norm': 0.5957523584365845, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5658687949180603, 'eval_runtime': 8.1208, 'eval_samples_per_second': 123.017, 'eval_steps_per_second': 7.758, 'epoch': 0.68}
{'loss': 0.6398, 'grad_norm': 0.5186595916748047, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5461560487747192, 'eval_runtime': 8.1238, 'eval_samples_per_second': 122.973, 'eval_steps_per_second': 7.755, 'epoch': 0.72}
{'loss': 0.7018, 'grad_norm': 0.5551915764808655, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5268979072570801, 'eval_runtime': 8.1299, 'eval_samples_per_second': 122.88, 'eval_steps_per_second': 7.749, 'epoch': 0.76}
{'loss': 0.6202, 'grad_norm': 0.4592357575893402, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5070109367370605, 'eval_runtime': 8.1462, 'eval_samples_per_second': 122.634, 'eval_steps_per_second': 7.734, 'epoch': 0.8}
{'loss': 0.6329, 'grad_norm': 0.47094252705574036, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4878336489200592, 'eval_runtime': 8.1362, 'eval_samples_per_second': 122.784, 'eval_steps_per_second': 7.743, 'epoch': 0.84}
{'loss': 0.6182, 'grad_norm': 0.47972342371940613, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.47689154744148254, 'eval_runtime': 8.1322, 'eval_samples_per_second': 122.844, 'eval_steps_per_second': 7.747, 'epoch': 0.88}
{'loss': 0.4847, 'grad_norm': 0.540001630783081, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4687657058238983, 'eval_runtime': 8.1335, 'eval_samples_per_second': 122.825, 'eval_steps_per_second': 7.746, 'epoch': 0.92}
{'loss': 0.5351, 'grad_norm': 0.44594424962997437, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.4619971811771393, 'eval_runtime': 8.1387, 'eval_samples_per_second': 122.747, 'eval_steps_per_second': 7.741, 'epoch': 0.96}
{'loss': 0.5316, 'grad_norm': 0.466071218252182, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.4587697982788086, 'eval_runtime': 8.1184, 'eval_samples_per_second': 123.053, 'eval_steps_per_second': 7.76, 'epoch': 1.0}
{'train_runtime': 451.4497, 'train_samples_per_second': 22.149, 'train_steps_per_second': 1.384, 'train_loss': 0.8555778915405273, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3519738912582397, 1.1355537176132202, 1.0551402568817139, 1.0171737670898438, 0.9845753908157349, 0.9511789679527283, 0.9169284105300903, 0.8748621344566345, 0.8344569206237793, 0.7958859801292419, 0.7493991255760193, 0.7147984504699707, 0.6895516514778137, 0.6519055962562561, 0.6221016645431519, 0.5934572219848633, 0.5658687949180603, 0.5461560487747192, 0.5268979072570801, 0.5070109367370605, 0.4878336489200592, 0.47689154744148254, 0.4687657058238983, 0.4619971811771393, 0.4587697982788086], 'performance': [0.65, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:32,  3.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 48.06it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 60.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 67.15it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 68.67it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 66.39it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.65, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.2392396926879883
current iteration best possible performance (full train run):  0.546
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5137 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.861140889054948
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 1.0126760075820012
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.960826647717078
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.8781039225375309
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.9608208200682945
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0873, dtype=torch.float64), 0, 0, tensor(0.2250, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6877, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.1, 12.28932446667006, 1]
normalized proposed parameters for next round by BO: [tensor(0.0873, dtype=torch.float64), tensor(6.1263e-05, dtype=torch.float64), tensor(2.5936e-18, dtype=torch.float64), tensor(0.2250, dtype=torch.float64), tensor(2.6369e-16, dtype=torch.float64), tensor(1.4054e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.3394e-17, dtype=torch.float64), tensor(0.6877, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2560, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.087
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.225
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.688

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (12.28932446667006,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  12.28932446667006
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.03it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 30.01it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.09it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.46it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.97it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.20it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.79it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.69it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.59it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.08it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.1513, 'grad_norm': 0.7406081557273865, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.4342433214187622, 'eval_runtime': 7.4736, 'eval_samples_per_second': 133.671, 'eval_steps_per_second': 8.43, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.88it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.06it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.71it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.42it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.10it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 52.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.83it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 49.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.96it/s]
Evaluation performance at step 50: 0.65
{'loss': 1.1036, 'grad_norm': 0.2384663224220276, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.65}
{'eval_loss': 0.9497220516204834, 'eval_runtime': 7.4725, 'eval_samples_per_second': 133.691, 'eval_steps_per_second': 8.431, 'epoch': 0.08}
{'loss': 0.866, 'grad_norm': 0.16234169900417328, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8449198007583618, 'eval_runtime': 7.506, 'eval_samples_per_second': 133.093, 'eval_steps_per_second': 8.393, 'epoch': 0.12}
{'loss': 0.8241, 'grad_norm': 0.11731273680925369, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8113824129104614, 'eval_runtime': 7.5085, 'eval_samples_per_second': 133.049, 'eval_steps_per_second': 8.391, 'epoch': 0.16}
{'loss': 0.8066, 'grad_norm': 0.12183183431625366, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7810125350952148, 'eval_runtime': 7.5194, 'eval_samples_per_second': 132.857, 'eval_steps_per_second': 8.378, 'epoch': 0.2}
{'loss': 0.7586, 'grad_norm': 0.14032158255577087, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7428737282752991, 'eval_runtime': 7.4963, 'eval_samples_per_second': 133.265, 'eval_steps_per_second': 8.404, 'epoch': 0.24}
{'loss': 0.7497, 'grad_norm': 0.15810871124267578, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6922662258148193, 'eval_runtime': 7.504, 'eval_samples_per_second': 133.129, 'eval_steps_per_second': 8.396, 'epoch': 0.28}
{'loss': 0.7291, 'grad_norm': 0.18544550240039825, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.649019718170166, 'eval_runtime': 7.4953, 'eval_samples_per_second': 133.283, 'eval_steps_per_second': 8.405, 'epoch': 0.32}
{'loss': 0.655, 'grad_norm': 0.19332297146320343, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6059843301773071, 'eval_runtime': 7.4787, 'eval_samples_per_second': 133.58, 'eval_steps_per_second': 8.424, 'epoch': 0.36}
{'loss': 0.6301, 'grad_norm': 0.2679058015346527, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5657172203063965, 'eval_runtime': 7.4714, 'eval_samples_per_second': 133.71, 'eval_steps_per_second': 8.432, 'epoch': 0.4}
{'loss': 0.6025, 'grad_norm': 0.23277577757835388, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5335081815719604, 'eval_runtime': 7.4654, 'eval_samples_per_second': 133.818, 'eval_steps_per_second': 8.439, 'epoch': 0.44}
{'loss': 0.5843, 'grad_norm': 0.26620030403137207, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4998966455459595, 'eval_runtime': 7.4714, 'eval_samples_per_second': 133.709, 'eval_steps_per_second': 8.432, 'epoch': 0.48}
{'loss': 0.5124, 'grad_norm': 0.2912544310092926, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.47347593307495117, 'eval_runtime': 7.4734, 'eval_samples_per_second': 133.674, 'eval_steps_per_second': 8.43, 'epoch': 0.52}
{'loss': 0.4971, 'grad_norm': 0.21163296699523926, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.44933396577835083, 'eval_runtime': 7.507, 'eval_samples_per_second': 133.076, 'eval_steps_per_second': 8.392, 'epoch': 0.56}
{'loss': 0.4988, 'grad_norm': 0.2665371596813202, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4178854823112488, 'eval_runtime': 7.5011, 'eval_samples_per_second': 133.181, 'eval_steps_per_second': 8.399, 'epoch': 0.6}
{'loss': 0.4595, 'grad_norm': 0.35034823417663574, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.389975905418396, 'eval_runtime': 7.5568, 'eval_samples_per_second': 132.199, 'eval_steps_per_second': 8.337, 'epoch': 0.64}
{'loss': 0.4533, 'grad_norm': 0.26645413041114807, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.36683350801467896, 'eval_runtime': 7.5173, 'eval_samples_per_second': 132.893, 'eval_steps_per_second': 8.381, 'epoch': 0.68}
{'loss': 0.3969, 'grad_norm': 0.35726475715637207, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3481302857398987, 'eval_runtime': 7.5023, 'eval_samples_per_second': 133.159, 'eval_steps_per_second': 8.397, 'epoch': 0.72}
{'loss': 0.4071, 'grad_norm': 0.24339431524276733, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3277621269226074, 'eval_runtime': 7.4823, 'eval_samples_per_second': 133.516, 'eval_steps_per_second': 8.42, 'epoch': 0.76}
{'loss': 0.3791, 'grad_norm': 0.3112519383430481, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.31301799416542053, 'eval_runtime': 7.478, 'eval_samples_per_second': 133.592, 'eval_steps_per_second': 8.425, 'epoch': 0.8}
{'loss': 0.3555, 'grad_norm': 0.2701108455657959, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3002273142337799, 'eval_runtime': 7.5335, 'eval_samples_per_second': 132.608, 'eval_steps_per_second': 8.363, 'epoch': 0.84}
{'loss': 0.3306, 'grad_norm': 0.20447520911693573, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.29031071066856384, 'eval_runtime': 7.5655, 'eval_samples_per_second': 132.046, 'eval_steps_per_second': 8.327, 'epoch': 0.88}
{'loss': 0.3268, 'grad_norm': 0.2667433023452759, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.28252342343330383, 'eval_runtime': 7.5493, 'eval_samples_per_second': 132.331, 'eval_steps_per_second': 8.345, 'epoch': 0.92}
{'loss': 0.3127, 'grad_norm': 0.1973586082458496, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2764345109462738, 'eval_runtime': 7.5901, 'eval_samples_per_second': 131.618, 'eval_steps_per_second': 8.3, 'epoch': 0.96}
{'loss': 0.3135, 'grad_norm': 0.2770426869392395, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.27432113885879517, 'eval_runtime': 7.5554, 'eval_samples_per_second': 132.224, 'eval_steps_per_second': 8.338, 'epoch': 1.0}
{'train_runtime': 429.7184, 'train_samples_per_second': 23.266, 'train_steps_per_second': 1.454, 'train_loss': 0.6681771881103515, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4342433214187622, 0.9497220516204834, 0.8449198007583618, 0.8113824129104614, 0.7810125350952148, 0.7428737282752991, 0.6922662258148193, 0.649019718170166, 0.6059843301773071, 0.5657172203063965, 0.5335081815719604, 0.4998966455459595, 0.47347593307495117, 0.44933396577835083, 0.4178854823112488, 0.389975905418396, 0.36683350801467896, 0.3481302857398987, 0.3277621269226074, 0.31301799416542053, 0.3002273142337799, 0.29031071066856384, 0.28252342343330383, 0.2764345109462738, 0.27432113885879517], 'performance': [0.61, 0.65]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 29.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 37.91it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 45.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.52it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 68.02it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.65]
current iteration observed (possibly low-fid or predicted) performance:  1.1252690553665161
current iteration best possible performance (full train run):  0.609
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1924 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.9760286429142553
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.9525749743531002
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.9760316586969467
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.9760216865564948
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.5439708771196188
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(0.2639, dtype=torch.float64), tensor(0.7361, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.09868658504349756, 47.99999999999999, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.3137e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9780e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2639, dtype=torch.float64), tensor(0.7361, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9869, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.264
  arc_challenge: 0.736

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09868658504349756,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.09868658504349756
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.50it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.51it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.26it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 46.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.99it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.18it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.79it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.91it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.91it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.12it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 59.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.83it/s]
Evaluation performance at step 25: 0.61
{'loss': 2.5112, 'grad_norm': 0.5461364388465881, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.3116651773452759, 'eval_runtime': 8.6584, 'eval_samples_per_second': 115.379, 'eval_steps_per_second': 7.276, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:40,  4.65s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:35,  2.57it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:04<00:14,  5.56it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:08,  9.30it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:04, 13.76it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:03, 19.10it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:02, 25.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:01, 31.22it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:00, 35.41it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:05<00:00, 42.59it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:00, 46.34it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 49.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.02it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.1958, 'grad_norm': 0.1910552829504013, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.1154848337173462, 'eval_runtime': 8.6599, 'eval_samples_per_second': 115.359, 'eval_steps_per_second': 7.275, 'epoch': 0.08}
{'loss': 1.0204, 'grad_norm': 0.23647573590278625, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.970525324344635, 'eval_runtime': 8.7157, 'eval_samples_per_second': 114.621, 'eval_steps_per_second': 7.228, 'epoch': 0.12}
{'loss': 0.9232, 'grad_norm': 0.2125532478094101, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8661701083183289, 'eval_runtime': 8.74, 'eval_samples_per_second': 114.302, 'eval_steps_per_second': 7.208, 'epoch': 0.16}
{'loss': 0.8764, 'grad_norm': 0.3347947299480438, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8128837943077087, 'eval_runtime': 8.725, 'eval_samples_per_second': 114.499, 'eval_steps_per_second': 7.221, 'epoch': 0.2}
{'loss': 0.7687, 'grad_norm': 0.24697387218475342, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7564271688461304, 'eval_runtime': 8.7361, 'eval_samples_per_second': 114.353, 'eval_steps_per_second': 7.211, 'epoch': 0.24}
{'loss': 0.7493, 'grad_norm': 0.2839468717575073, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.702277421951294, 'eval_runtime': 8.7573, 'eval_samples_per_second': 114.077, 'eval_steps_per_second': 7.194, 'epoch': 0.28}
{'loss': 0.7465, 'grad_norm': 0.24157044291496277, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6642311215400696, 'eval_runtime': 8.7547, 'eval_samples_per_second': 114.11, 'eval_steps_per_second': 7.196, 'epoch': 0.32}
{'loss': 0.6495, 'grad_norm': 0.3506379723548889, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6178849935531616, 'eval_runtime': 8.7688, 'eval_samples_per_second': 113.927, 'eval_steps_per_second': 7.185, 'epoch': 0.36}
{'loss': 0.6959, 'grad_norm': 0.2662413716316223, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5782662630081177, 'eval_runtime': 8.7653, 'eval_samples_per_second': 113.972, 'eval_steps_per_second': 7.187, 'epoch': 0.4}
{'loss': 0.6347, 'grad_norm': 0.29733940958976746, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5310596823692322, 'eval_runtime': 8.7251, 'eval_samples_per_second': 114.497, 'eval_steps_per_second': 7.221, 'epoch': 0.44}
{'loss': 0.6106, 'grad_norm': 0.3706018328666687, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5025151371955872, 'eval_runtime': 8.712, 'eval_samples_per_second': 114.67, 'eval_steps_per_second': 7.231, 'epoch': 0.48}
{'loss': 0.5657, 'grad_norm': 0.4241391718387604, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4790394604206085, 'eval_runtime': 8.7224, 'eval_samples_per_second': 114.532, 'eval_steps_per_second': 7.223, 'epoch': 0.52}
{'loss': 0.5311, 'grad_norm': 0.3018372058868408, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.452853262424469, 'eval_runtime': 8.7206, 'eval_samples_per_second': 114.556, 'eval_steps_per_second': 7.224, 'epoch': 0.56}
{'loss': 0.5095, 'grad_norm': 0.27094361186027527, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.43134889006614685, 'eval_runtime': 8.7245, 'eval_samples_per_second': 114.506, 'eval_steps_per_second': 7.221, 'epoch': 0.6}
{'loss': 0.5782, 'grad_norm': 0.2246904820203781, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.413492888212204, 'eval_runtime': 8.7492, 'eval_samples_per_second': 114.182, 'eval_steps_per_second': 7.201, 'epoch': 0.64}
{'loss': 0.4518, 'grad_norm': 0.2207689881324768, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.39618954062461853, 'eval_runtime': 8.7213, 'eval_samples_per_second': 114.547, 'eval_steps_per_second': 7.224, 'epoch': 0.68}
{'loss': 0.473, 'grad_norm': 0.23971563577651978, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.38786032795906067, 'eval_runtime': 8.7766, 'eval_samples_per_second': 113.825, 'eval_steps_per_second': 7.178, 'epoch': 0.72}
{'loss': 0.4896, 'grad_norm': 0.20177456736564636, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3728425204753876, 'eval_runtime': 8.7875, 'eval_samples_per_second': 113.684, 'eval_steps_per_second': 7.169, 'epoch': 0.76}
{'loss': 0.4852, 'grad_norm': 0.37445998191833496, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3663904666900635, 'eval_runtime': 8.7843, 'eval_samples_per_second': 113.726, 'eval_steps_per_second': 7.172, 'epoch': 0.8}
{'loss': 0.5158, 'grad_norm': 0.318034291267395, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.35983899235725403, 'eval_runtime': 8.7802, 'eval_samples_per_second': 113.779, 'eval_steps_per_second': 7.175, 'epoch': 0.84}
{'loss': 0.468, 'grad_norm': 0.20033304393291473, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3547305464744568, 'eval_runtime': 8.7799, 'eval_samples_per_second': 113.783, 'eval_steps_per_second': 7.176, 'epoch': 0.88}
{'loss': 0.4196, 'grad_norm': 0.18546144664287567, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.35043656826019287, 'eval_runtime': 8.7702, 'eval_samples_per_second': 113.909, 'eval_steps_per_second': 7.183, 'epoch': 0.92}
{'loss': 0.3951, 'grad_norm': 0.1951938420534134, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3480943441390991, 'eval_runtime': 8.7626, 'eval_samples_per_second': 114.007, 'eval_steps_per_second': 7.19, 'epoch': 0.96}
{'loss': 0.4072, 'grad_norm': 0.26659095287323, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3472707271575928, 'eval_runtime': 8.7688, 'eval_samples_per_second': 113.927, 'eval_steps_per_second': 7.185, 'epoch': 1.0}
{'train_runtime': 478.4923, 'train_samples_per_second': 20.897, 'train_steps_per_second': 1.306, 'train_loss': 0.7068872024536133, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3116651773452759, 1.1154848337173462, 0.970525324344635, 0.8661701083183289, 0.8128837943077087, 0.7564271688461304, 0.702277421951294, 0.6642311215400696, 0.6178849935531616, 0.5782662630081177, 0.5310596823692322, 0.5025151371955872, 0.4790394604206085, 0.452853262424469, 0.43134889006614685, 0.413492888212204, 0.39618954062461853, 0.38786032795906067, 0.3728425204753876, 0.3663904666900635, 0.35983899235725403, 0.3547305464744568, 0.35043656826019287, 0.3480943441390991, 0.3472707271575928], 'performance': [0.61, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.86it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.20it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 59.21it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2454429864883423
current iteration best possible performance (full train run):  0.63
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.6819 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.9699720671972801
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.9699731734871425
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.8997835466285927
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.9590560105690461
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.9699731740367121
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2355, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.7645, dtype=torch.float64), 32, 1, 0, 0, 0, 0, 128, 0.08495139077558267, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0.2355, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0119e-16, dtype=torch.float64), tensor(6.0243e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7645, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8495, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.236
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.764

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.08495139077558267,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  128
lora dropout:  0.08495139077558267
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 33,554,432 || all params: 8,063,815,680 || trainable%: 0.4161
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:40,  2.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.93it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 51.11it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 64.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 70.28it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 76.33it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 76.55it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 76.76it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.29it/s]
Evaluation performance at step 25: 0.63
{'loss': 3.6362, 'grad_norm': 0.8669893741607666, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 2.5049033164978027, 'eval_runtime': 6.1128, 'eval_samples_per_second': 163.427, 'eval_steps_per_second': 10.306, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:13,  7.40it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.54it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 50.11it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 57.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 62.81it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 74.60it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 73.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 77.98it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 77.60it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 77.16it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 70.81it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.7685, 'grad_norm': 0.2395818680524826, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.3255562782287598, 'eval_runtime': 6.1041, 'eval_samples_per_second': 163.66, 'eval_steps_per_second': 10.321, 'epoch': 0.08}
{'loss': 1.2544, 'grad_norm': 0.14552414417266846, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2441298961639404, 'eval_runtime': 6.1621, 'eval_samples_per_second': 162.121, 'eval_steps_per_second': 10.224, 'epoch': 0.12}
{'loss': 1.2473, 'grad_norm': 0.10791496187448502, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2175729274749756, 'eval_runtime': 6.1651, 'eval_samples_per_second': 162.041, 'eval_steps_per_second': 10.219, 'epoch': 0.16}
{'loss': 1.1858, 'grad_norm': 0.10812320560216904, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2041925191879272, 'eval_runtime': 6.1592, 'eval_samples_per_second': 162.197, 'eval_steps_per_second': 10.229, 'epoch': 0.2}
{'loss': 1.1924, 'grad_norm': 0.11568854004144669, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1948268413543701, 'eval_runtime': 6.241, 'eval_samples_per_second': 160.071, 'eval_steps_per_second': 10.095, 'epoch': 0.24}
{'loss': 1.1915, 'grad_norm': 0.12814639508724213, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.184698224067688, 'eval_runtime': 6.2406, 'eval_samples_per_second': 160.081, 'eval_steps_per_second': 10.095, 'epoch': 0.28}
{'loss': 1.1635, 'grad_norm': 0.12257713824510574, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1756912469863892, 'eval_runtime': 6.2315, 'eval_samples_per_second': 160.314, 'eval_steps_per_second': 10.11, 'epoch': 0.32}
{'loss': 1.1722, 'grad_norm': 0.11265064030885696, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.1694518327713013, 'eval_runtime': 6.2288, 'eval_samples_per_second': 160.385, 'eval_steps_per_second': 10.114, 'epoch': 0.36}
{'loss': 1.1692, 'grad_norm': 0.12920770049095154, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.1586371660232544, 'eval_runtime': 6.2393, 'eval_samples_per_second': 160.113, 'eval_steps_per_second': 10.097, 'epoch': 0.4}
{'loss': 1.1535, 'grad_norm': 0.15087006986141205, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.1493184566497803, 'eval_runtime': 6.2492, 'eval_samples_per_second': 159.86, 'eval_steps_per_second': 10.081, 'epoch': 0.44}
{'loss': 1.1375, 'grad_norm': 0.13246838748455048, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.1398686170578003, 'eval_runtime': 6.2331, 'eval_samples_per_second': 160.274, 'eval_steps_per_second': 10.107, 'epoch': 0.48}
{'loss': 1.1401, 'grad_norm': 0.1413964331150055, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.134703516960144, 'eval_runtime': 6.2303, 'eval_samples_per_second': 160.346, 'eval_steps_per_second': 10.112, 'epoch': 0.52}
{'loss': 1.1395, 'grad_norm': 0.15097743272781372, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.125198483467102, 'eval_runtime': 6.2132, 'eval_samples_per_second': 160.788, 'eval_steps_per_second': 10.14, 'epoch': 0.56}
{'loss': 1.1341, 'grad_norm': 0.16444826126098633, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.1172133684158325, 'eval_runtime': 6.2361, 'eval_samples_per_second': 160.197, 'eval_steps_per_second': 10.103, 'epoch': 0.6}
{'loss': 1.133, 'grad_norm': 0.14406436681747437, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.1093225479125977, 'eval_runtime': 6.1993, 'eval_samples_per_second': 161.147, 'eval_steps_per_second': 10.162, 'epoch': 0.64}
{'loss': 1.1075, 'grad_norm': 0.17904312908649445, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.1023751497268677, 'eval_runtime': 6.1913, 'eval_samples_per_second': 161.354, 'eval_steps_per_second': 10.176, 'epoch': 0.68}
{'loss': 1.105, 'grad_norm': 0.18549898266792297, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.093774437904358, 'eval_runtime': 6.183, 'eval_samples_per_second': 161.573, 'eval_steps_per_second': 10.189, 'epoch': 0.72}
{'loss': 1.1082, 'grad_norm': 0.2123192995786667, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0871870517730713, 'eval_runtime': 6.1962, 'eval_samples_per_second': 161.229, 'eval_steps_per_second': 10.168, 'epoch': 0.76}
{'loss': 1.081, 'grad_norm': 0.19824908673763275, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0807335376739502, 'eval_runtime': 6.1821, 'eval_samples_per_second': 161.595, 'eval_steps_per_second': 10.191, 'epoch': 0.8}
{'loss': 1.1039, 'grad_norm': 0.2257155328989029, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.074228286743164, 'eval_runtime': 6.1728, 'eval_samples_per_second': 161.838, 'eval_steps_per_second': 10.206, 'epoch': 0.84}
{'loss': 1.081, 'grad_norm': 0.19058756530284882, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0683097839355469, 'eval_runtime': 6.1791, 'eval_samples_per_second': 161.674, 'eval_steps_per_second': 10.196, 'epoch': 0.88}
{'loss': 1.0873, 'grad_norm': 0.22112691402435303, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.064440369606018, 'eval_runtime': 6.193, 'eval_samples_per_second': 161.311, 'eval_steps_per_second': 10.173, 'epoch': 0.92}
{'loss': 1.0707, 'grad_norm': 0.19492535293102264, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.060997724533081, 'eval_runtime': 6.2035, 'eval_samples_per_second': 161.039, 'eval_steps_per_second': 10.156, 'epoch': 0.96}
{'loss': 1.0673, 'grad_norm': 0.22382351756095886, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.059743046760559, 'eval_runtime': 6.2052, 'eval_samples_per_second': 160.994, 'eval_steps_per_second': 10.153, 'epoch': 1.0}
{'train_runtime': 353.3592, 'train_samples_per_second': 28.297, 'train_steps_per_second': 1.769, 'train_loss': 1.2652232574462892, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5049033164978027, 1.3255562782287598, 1.2441298961639404, 1.2175729274749756, 1.2041925191879272, 1.1948268413543701, 1.184698224067688, 1.1756912469863892, 1.1694518327713013, 1.1586371660232544, 1.1493184566497803, 1.1398686170578003, 1.134703516960144, 1.125198483467102, 1.1172133684158325, 1.1093225479125977, 1.1023751497268677, 1.093774437904358, 1.0871870517730713, 1.0807335376739502, 1.074228286743164, 1.0683097839355469, 1.064440369606018, 1.060997724533081, 1.059743046760559], 'performance': [0.63, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:25,  3.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.22it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 58.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.23it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 70.00it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 77.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 75.21it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.2474339008331299
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4862 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.9650869753300098
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.9650640660949418
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.8208720581292025
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.9652489782037287
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.772450165996117
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2620, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7380, dtype=torch.float64), 32, 1, 1, 0, 0, 1, 128, 0.06470992576311428, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2620, dtype=torch.float64), tensor(6.2514e-17, dtype=torch.float64), tensor(4.6583e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5311e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7380, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6471, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.262
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.738

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06470992576311428,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 1]
lora rank:  128
lora dropout:  0.06470992576311428
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<01:54,  1.15s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:09,  9.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:04, 17.67it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:02, 25.41it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 32.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 39.39it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 43.96it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:00, 51.02it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:00, 50.83it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:02<00:00, 57.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 57.91it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 58.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 58.74it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 36.45it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.9515, 'grad_norm': 0.45807504653930664, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.6369292736053467, 'eval_runtime': 9.7831, 'eval_samples_per_second': 102.115, 'eval_steps_per_second': 6.44, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.78it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 37.81it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 46.20it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 50.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 52.74it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 56.16it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 59.12it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 61.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.76it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.57it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.48it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.87it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 57.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.37it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.4581, 'grad_norm': 0.24438682198524475, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.3269275426864624, 'eval_runtime': 9.7706, 'eval_samples_per_second': 102.246, 'eval_steps_per_second': 6.448, 'epoch': 0.08}
{'loss': 1.2605, 'grad_norm': 0.22531013190746307, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2145228385925293, 'eval_runtime': 9.7833, 'eval_samples_per_second': 102.113, 'eval_steps_per_second': 6.44, 'epoch': 0.12}
{'loss': 1.2013, 'grad_norm': 0.216024249792099, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1299209594726562, 'eval_runtime': 9.7636, 'eval_samples_per_second': 102.319, 'eval_steps_per_second': 6.453, 'epoch': 0.16}
{'loss': 1.1104, 'grad_norm': 0.20046931505203247, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0798121690750122, 'eval_runtime': 9.7992, 'eval_samples_per_second': 101.947, 'eval_steps_per_second': 6.429, 'epoch': 0.2}
{'loss': 1.0097, 'grad_norm': 0.2796797454357147, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0367686748504639, 'eval_runtime': 9.8259, 'eval_samples_per_second': 101.67, 'eval_steps_per_second': 6.412, 'epoch': 0.24}
{'loss': 0.9786, 'grad_norm': 0.28270435333251953, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9987770915031433, 'eval_runtime': 9.8364, 'eval_samples_per_second': 101.561, 'eval_steps_per_second': 6.405, 'epoch': 0.28}
{'loss': 1.0157, 'grad_norm': 0.2839741110801697, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9539873003959656, 'eval_runtime': 9.8402, 'eval_samples_per_second': 101.522, 'eval_steps_per_second': 6.402, 'epoch': 0.32}
{'loss': 0.9604, 'grad_norm': 0.2606087625026703, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9195817112922668, 'eval_runtime': 9.8771, 'eval_samples_per_second': 101.143, 'eval_steps_per_second': 6.378, 'epoch': 0.36}
{'loss': 0.9063, 'grad_norm': 0.2784262001514435, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8832741379737854, 'eval_runtime': 9.896, 'eval_samples_per_second': 100.95, 'eval_steps_per_second': 6.366, 'epoch': 0.4}
{'loss': 0.9, 'grad_norm': 0.374406635761261, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8466262817382812, 'eval_runtime': 9.9104, 'eval_samples_per_second': 100.803, 'eval_steps_per_second': 6.357, 'epoch': 0.44}
{'loss': 0.8838, 'grad_norm': 0.29800623655319214, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8246133327484131, 'eval_runtime': 9.9247, 'eval_samples_per_second': 100.658, 'eval_steps_per_second': 6.348, 'epoch': 0.48}
{'loss': 0.9573, 'grad_norm': 0.265648752450943, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7968688607215881, 'eval_runtime': 9.9404, 'eval_samples_per_second': 100.499, 'eval_steps_per_second': 6.338, 'epoch': 0.52}
{'loss': 0.8761, 'grad_norm': 0.2982081174850464, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7784969806671143, 'eval_runtime': 9.9132, 'eval_samples_per_second': 100.775, 'eval_steps_per_second': 6.355, 'epoch': 0.56}
{'loss': 0.8293, 'grad_norm': 0.36679574847221375, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7560959458351135, 'eval_runtime': 9.9452, 'eval_samples_per_second': 100.45, 'eval_steps_per_second': 6.335, 'epoch': 0.6}
{'loss': 0.8777, 'grad_norm': 0.3831203281879425, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7395113110542297, 'eval_runtime': 9.9164, 'eval_samples_per_second': 100.743, 'eval_steps_per_second': 6.353, 'epoch': 0.64}
{'loss': 0.8073, 'grad_norm': 0.34810858964920044, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7242828011512756, 'eval_runtime': 9.9041, 'eval_samples_per_second': 100.867, 'eval_steps_per_second': 6.361, 'epoch': 0.68}
{'loss': 0.8622, 'grad_norm': 0.29096123576164246, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7101213932037354, 'eval_runtime': 9.8989, 'eval_samples_per_second': 100.921, 'eval_steps_per_second': 6.364, 'epoch': 0.72}
{'loss': 0.8761, 'grad_norm': 0.2883909046649933, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.697730302810669, 'eval_runtime': 9.9026, 'eval_samples_per_second': 100.883, 'eval_steps_per_second': 6.362, 'epoch': 0.76}
{'loss': 0.7897, 'grad_norm': 0.21278542280197144, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6888492703437805, 'eval_runtime': 9.8923, 'eval_samples_per_second': 100.987, 'eval_steps_per_second': 6.369, 'epoch': 0.8}
{'loss': 0.8109, 'grad_norm': 0.24962274730205536, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6837078928947449, 'eval_runtime': 9.8924, 'eval_samples_per_second': 100.986, 'eval_steps_per_second': 6.368, 'epoch': 0.84}
{'loss': 0.7718, 'grad_norm': 0.1819087713956833, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6760419607162476, 'eval_runtime': 9.8335, 'eval_samples_per_second': 101.592, 'eval_steps_per_second': 6.407, 'epoch': 0.88}
{'loss': 0.7041, 'grad_norm': 0.28103795647621155, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.670688271522522, 'eval_runtime': 9.8292, 'eval_samples_per_second': 101.636, 'eval_steps_per_second': 6.409, 'epoch': 0.92}
{'loss': 0.7489, 'grad_norm': 0.18450002372264862, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6673933267593384, 'eval_runtime': 9.8982, 'eval_samples_per_second': 100.928, 'eval_steps_per_second': 6.365, 'epoch': 0.96}
{'loss': 0.6872, 'grad_norm': 0.25105559825897217, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6660950779914856, 'eval_runtime': 9.9014, 'eval_samples_per_second': 100.894, 'eval_steps_per_second': 6.363, 'epoch': 1.0}
{'train_runtime': 523.1375, 'train_samples_per_second': 19.114, 'train_steps_per_second': 1.195, 'train_loss': 1.009398583984375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6369292736053467, 1.3269275426864624, 1.2145228385925293, 1.1299209594726562, 1.0798121690750122, 1.0367686748504639, 0.9987770915031433, 0.9539873003959656, 0.9195817112922668, 0.8832741379737854, 0.8466262817382812, 0.8246133327484131, 0.7968688607215881, 0.7784969806671143, 0.7560959458351135, 0.7395113110542297, 0.7242828011512756, 0.7101213932037354, 0.697730302810669, 0.6888492703437805, 0.6837078928947449, 0.6760419607162476, 0.670688271522522, 0.6673933267593384, 0.6660950779914856], 'performance': [0.63, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:29,  3.40it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.63it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 65.19it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 76.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 61.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2465016841888428
current iteration best possible performance (full train run):  0.5775000000000001
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.3464 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.9635910246739674
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.9646299798648517
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.678519924680544
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.9646448795882107
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.9646420100750338
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0991, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.1954, dtype=torch.float64), tensor(0.7055, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 128, 0.05497660376432817, 47.999999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(0.0991, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.4386e-17, dtype=torch.float64), tensor(9.8221e-17, dtype=torch.float64), tensor(9.3147e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1954, dtype=torch.float64), tensor(0.7055, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.5498, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.099
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.195
  arc_challenge: 0.706

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05497660376432817,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (47.999999999999986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.05497660376432817
lora alpha:  47.999999999999986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:26,  5.12s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:38,  2.33it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:16,  5.08it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:08,  8.48it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:05, 12.53it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:03, 17.35it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:02, 22.28it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:01, 28.57it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:01, 32.36it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:00, 38.78it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:00, 42.21it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 45.05it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 46.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.42it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.506, 'grad_norm': 0.36717361211776733, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.2310950756072998, 'eval_runtime': 8.5121, 'eval_samples_per_second': 117.362, 'eval_steps_per_second': 7.401, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:24,  3.97it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.30it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.89it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.08it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.25it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.41it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.33it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 54.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.62it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.58it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.22it/s]
Evaluation performance at step 50: 0.63
{'loss': 1.1365, 'grad_norm': 0.24123989045619965, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 1.0660430192947388, 'eval_runtime': 8.5238, 'eval_samples_per_second': 117.201, 'eval_steps_per_second': 7.391, 'epoch': 0.08}
{'loss': 0.9917, 'grad_norm': 0.24682627618312836, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9240760803222656, 'eval_runtime': 8.5254, 'eval_samples_per_second': 117.179, 'eval_steps_per_second': 7.39, 'epoch': 0.12}
{'loss': 0.8882, 'grad_norm': 0.29329392313957214, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8313211798667908, 'eval_runtime': 8.5443, 'eval_samples_per_second': 116.92, 'eval_steps_per_second': 7.373, 'epoch': 0.16}
{'loss': 0.8116, 'grad_norm': 0.24800267815589905, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7843526601791382, 'eval_runtime': 8.6023, 'eval_samples_per_second': 116.132, 'eval_steps_per_second': 7.324, 'epoch': 0.2}
{'loss': 0.7448, 'grad_norm': 0.2898420989513397, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7307339310646057, 'eval_runtime': 8.5841, 'eval_samples_per_second': 116.378, 'eval_steps_per_second': 7.339, 'epoch': 0.24}
{'loss': 0.739, 'grad_norm': 0.2816010117530823, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6700530052185059, 'eval_runtime': 8.6265, 'eval_samples_per_second': 115.806, 'eval_steps_per_second': 7.303, 'epoch': 0.28}
{'loss': 0.7411, 'grad_norm': 0.25548020005226135, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6265034675598145, 'eval_runtime': 8.6254, 'eval_samples_per_second': 115.821, 'eval_steps_per_second': 7.304, 'epoch': 0.32}
{'loss': 0.6189, 'grad_norm': 0.2568657398223877, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5883136987686157, 'eval_runtime': 8.6281, 'eval_samples_per_second': 115.784, 'eval_steps_per_second': 7.302, 'epoch': 0.36}
{'loss': 0.6287, 'grad_norm': 0.30081748962402344, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5465419292449951, 'eval_runtime': 8.6002, 'eval_samples_per_second': 116.16, 'eval_steps_per_second': 7.325, 'epoch': 0.4}
{'loss': 0.6079, 'grad_norm': 0.31131595373153687, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5161098837852478, 'eval_runtime': 8.5946, 'eval_samples_per_second': 116.236, 'eval_steps_per_second': 7.33, 'epoch': 0.44}
{'loss': 0.5636, 'grad_norm': 0.3490172326564789, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4893931448459625, 'eval_runtime': 8.5818, 'eval_samples_per_second': 116.409, 'eval_steps_per_second': 7.341, 'epoch': 0.48}
{'loss': 0.6026, 'grad_norm': 0.3450310528278351, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4672962725162506, 'eval_runtime': 8.585, 'eval_samples_per_second': 116.366, 'eval_steps_per_second': 7.338, 'epoch': 0.52}
{'loss': 0.537, 'grad_norm': 0.29746749997138977, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.444821298122406, 'eval_runtime': 8.5849, 'eval_samples_per_second': 116.367, 'eval_steps_per_second': 7.338, 'epoch': 0.56}
{'loss': 0.4805, 'grad_norm': 0.3062799870967865, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4314313232898712, 'eval_runtime': 8.5897, 'eval_samples_per_second': 116.302, 'eval_steps_per_second': 7.334, 'epoch': 0.6}
{'loss': 0.5354, 'grad_norm': 0.22885066270828247, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4139384627342224, 'eval_runtime': 8.641, 'eval_samples_per_second': 115.612, 'eval_steps_per_second': 7.291, 'epoch': 0.64}
{'loss': 0.4975, 'grad_norm': 0.2563744783401489, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.40243521332740784, 'eval_runtime': 8.6203, 'eval_samples_per_second': 115.889, 'eval_steps_per_second': 7.308, 'epoch': 0.68}
{'loss': 0.4711, 'grad_norm': 0.23812274634838104, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.386751651763916, 'eval_runtime': 8.5936, 'eval_samples_per_second': 116.249, 'eval_steps_per_second': 7.331, 'epoch': 0.72}
{'loss': 0.4952, 'grad_norm': 0.22164565324783325, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.37844452261924744, 'eval_runtime': 8.5854, 'eval_samples_per_second': 116.361, 'eval_steps_per_second': 7.338, 'epoch': 0.76}
{'loss': 0.4613, 'grad_norm': 0.19161415100097656, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3689868152141571, 'eval_runtime': 8.6355, 'eval_samples_per_second': 115.686, 'eval_steps_per_second': 7.295, 'epoch': 0.8}
{'loss': 0.4756, 'grad_norm': 0.22327010333538055, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3606427311897278, 'eval_runtime': 8.5728, 'eval_samples_per_second': 116.531, 'eval_steps_per_second': 7.349, 'epoch': 0.84}
{'loss': 0.4069, 'grad_norm': 0.2628556489944458, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3538241386413574, 'eval_runtime': 8.5728, 'eval_samples_per_second': 116.531, 'eval_steps_per_second': 7.349, 'epoch': 0.88}
{'loss': 0.3861, 'grad_norm': 0.18754541873931885, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.35066723823547363, 'eval_runtime': 8.5653, 'eval_samples_per_second': 116.633, 'eval_steps_per_second': 7.355, 'epoch': 0.92}
{'loss': 0.4225, 'grad_norm': 0.2010468989610672, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.34795933961868286, 'eval_runtime': 8.5599, 'eval_samples_per_second': 116.707, 'eval_steps_per_second': 7.36, 'epoch': 0.96}
{'loss': 0.3734, 'grad_norm': 0.14027659595012665, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3474827706813812, 'eval_runtime': 8.5715, 'eval_samples_per_second': 116.549, 'eval_steps_per_second': 7.35, 'epoch': 1.0}
{'train_runtime': 476.7764, 'train_samples_per_second': 20.972, 'train_steps_per_second': 1.311, 'train_loss': 0.6849158309936524, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2310950756072998, 1.0660430192947388, 0.9240760803222656, 0.8313211798667908, 0.7843526601791382, 0.7307339310646057, 0.6700530052185059, 0.6265034675598145, 0.5883136987686157, 0.5465419292449951, 0.5161098837852478, 0.4893931448459625, 0.4672962725162506, 0.444821298122406, 0.4314313232898712, 0.4139384627342224, 0.40243521332740784, 0.386751651763916, 0.37844452261924744, 0.3689868152141571, 0.3606427311897278, 0.3538241386413574, 0.35066723823547363, 0.34795933961868286, 0.3474827706813812], 'performance': [0.62, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:32,  3.02it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.42it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.97it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.87it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 74.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.76it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.2478680610656738
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2095 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.764393004982454
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.9599362888625363
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.7538882488280365
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.9599313385354796
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.9511116598492146
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0438, dtype=torch.float64), tensor(0.2200, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.7363, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0438, dtype=torch.float64), tensor(0.2200, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3460e-16, dtype=torch.float64), tensor(2.5399e-17, dtype=torch.float64), tensor(2.9250e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7363, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.044
  gsm8k: 0.22
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.736

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.22it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.59it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.50it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.20it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.73it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.34it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.21it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.66it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.89it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.69it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.2858, 'grad_norm': 0.3390018045902252, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.1397275924682617, 'eval_runtime': 9.2104, 'eval_samples_per_second': 108.464, 'eval_steps_per_second': 6.84, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.26it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.03it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.40it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.45it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.15it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.86it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.40it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.0195, 'grad_norm': 0.22032859921455383, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 0.9516513347625732, 'eval_runtime': 9.2344, 'eval_samples_per_second': 108.182, 'eval_steps_per_second': 6.822, 'epoch': 0.08}
{'loss': 0.9093, 'grad_norm': 0.21765506267547607, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8308756947517395, 'eval_runtime': 9.2768, 'eval_samples_per_second': 107.688, 'eval_steps_per_second': 6.791, 'epoch': 0.12}
{'loss': 0.7993, 'grad_norm': 0.24912095069885254, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7492570281028748, 'eval_runtime': 9.2953, 'eval_samples_per_second': 107.473, 'eval_steps_per_second': 6.778, 'epoch': 0.16}
{'loss': 0.752, 'grad_norm': 0.24687787890434265, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6884158253669739, 'eval_runtime': 9.2935, 'eval_samples_per_second': 107.494, 'eval_steps_per_second': 6.779, 'epoch': 0.2}
{'loss': 0.6957, 'grad_norm': 0.23045910894870758, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6396487951278687, 'eval_runtime': 9.26, 'eval_samples_per_second': 107.883, 'eval_steps_per_second': 6.803, 'epoch': 0.24}
{'loss': 0.6542, 'grad_norm': 0.25533318519592285, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5941396951675415, 'eval_runtime': 9.2499, 'eval_samples_per_second': 108.001, 'eval_steps_per_second': 6.811, 'epoch': 0.28}
{'loss': 0.6301, 'grad_norm': 0.22216208279132843, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5516153573989868, 'eval_runtime': 9.2761, 'eval_samples_per_second': 107.696, 'eval_steps_per_second': 6.792, 'epoch': 0.32}
{'loss': 0.5984, 'grad_norm': 0.2599274814128876, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5155194997787476, 'eval_runtime': 9.3302, 'eval_samples_per_second': 107.072, 'eval_steps_per_second': 6.752, 'epoch': 0.36}
{'loss': 0.5807, 'grad_norm': 0.2996622323989868, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.479032963514328, 'eval_runtime': 9.385, 'eval_samples_per_second': 106.446, 'eval_steps_per_second': 6.713, 'epoch': 0.4}
{'loss': 0.5223, 'grad_norm': 0.23125040531158447, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.44871655106544495, 'eval_runtime': 9.3962, 'eval_samples_per_second': 106.32, 'eval_steps_per_second': 6.705, 'epoch': 0.44}
{'loss': 0.4892, 'grad_norm': 0.28513985872268677, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.42606890201568604, 'eval_runtime': 9.387, 'eval_samples_per_second': 106.424, 'eval_steps_per_second': 6.711, 'epoch': 0.48}
{'loss': 0.4771, 'grad_norm': 0.24547183513641357, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3981781601905823, 'eval_runtime': 9.3854, 'eval_samples_per_second': 106.442, 'eval_steps_per_second': 6.713, 'epoch': 0.52}
{'loss': 0.4304, 'grad_norm': 0.3185814321041107, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.38166481256484985, 'eval_runtime': 9.4026, 'eval_samples_per_second': 106.247, 'eval_steps_per_second': 6.7, 'epoch': 0.56}
{'loss': 0.4578, 'grad_norm': 0.322664350271225, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3655802309513092, 'eval_runtime': 9.3945, 'eval_samples_per_second': 106.339, 'eval_steps_per_second': 6.706, 'epoch': 0.6}
{'loss': 0.4074, 'grad_norm': 0.28851521015167236, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.3510383069515228, 'eval_runtime': 9.3864, 'eval_samples_per_second': 106.431, 'eval_steps_per_second': 6.712, 'epoch': 0.64}
{'loss': 0.4176, 'grad_norm': 0.2826285660266876, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3424118757247925, 'eval_runtime': 9.4196, 'eval_samples_per_second': 106.056, 'eval_steps_per_second': 6.688, 'epoch': 0.68}
{'loss': 0.3829, 'grad_norm': 0.4011927545070648, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3330196142196655, 'eval_runtime': 9.4098, 'eval_samples_per_second': 106.166, 'eval_steps_per_second': 6.695, 'epoch': 0.72}
{'loss': 0.3771, 'grad_norm': 0.17224465310573578, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3217729330062866, 'eval_runtime': 9.3788, 'eval_samples_per_second': 106.517, 'eval_steps_per_second': 6.717, 'epoch': 0.76}
{'loss': 0.3848, 'grad_norm': 0.2273358404636383, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3153320550918579, 'eval_runtime': 9.3715, 'eval_samples_per_second': 106.6, 'eval_steps_per_second': 6.722, 'epoch': 0.8}
{'loss': 0.3613, 'grad_norm': 0.20103085041046143, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.30842873454093933, 'eval_runtime': 9.3914, 'eval_samples_per_second': 106.374, 'eval_steps_per_second': 6.708, 'epoch': 0.84}
{'loss': 0.332, 'grad_norm': 0.2447057068347931, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3045409321784973, 'eval_runtime': 9.3321, 'eval_samples_per_second': 107.05, 'eval_steps_per_second': 6.751, 'epoch': 0.88}
{'loss': 0.3363, 'grad_norm': 0.15798090398311615, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.301137775182724, 'eval_runtime': 9.3315, 'eval_samples_per_second': 107.057, 'eval_steps_per_second': 6.751, 'epoch': 0.92}
{'loss': 0.3388, 'grad_norm': 0.22175894677639008, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.299064576625824, 'eval_runtime': 9.3292, 'eval_samples_per_second': 107.083, 'eval_steps_per_second': 6.753, 'epoch': 0.96}
{'loss': 0.3703, 'grad_norm': 0.247828409075737, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2983953058719635, 'eval_runtime': 9.3218, 'eval_samples_per_second': 107.168, 'eval_steps_per_second': 6.758, 'epoch': 1.0}
{'train_runtime': 516.0596, 'train_samples_per_second': 19.374, 'train_steps_per_second': 1.211, 'train_loss': 0.6004109954833985, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1397275924682617, 0.9516513347625732, 0.8308756947517395, 0.7492570281028748, 0.6884158253669739, 0.6396487951278687, 0.5941396951675415, 0.5516153573989868, 0.5155194997787476, 0.479032963514328, 0.44871655106544495, 0.42606890201568604, 0.3981781601905823, 0.38166481256484985, 0.3655802309513092, 0.3510383069515228, 0.3424118757247925, 0.3330196142196655, 0.3217729330062866, 0.3153320550918579, 0.30842873454093933, 0.3045409321784973, 0.301137775182724, 0.299064576625824, 0.2983953058719635], 'performance': [0.62, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.04it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.94it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.81it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.10it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2454702854156494
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0888 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.6288840402959825
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.9576999360563446
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.9575564171758728
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.9576868153628368
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.6272928354478964
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2402, dtype=torch.float64), 0, tensor(0.0417, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7181, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.04953754895750517, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.2402, dtype=torch.float64), tensor(2.8512e-16, dtype=torch.float64), tensor(0.0417, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5516e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7181, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4954, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.24
  gsm8k: 0
  rowan_hellaswag: 0.042
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.718

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04953754895750517,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.04953754895750517
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.62it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.94it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.84it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.50it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.41it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.53it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.37it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.56it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.62, 'grad_norm': 0.5026111006736755, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.349191665649414, 'eval_runtime': 7.9401, 'eval_samples_per_second': 125.817, 'eval_steps_per_second': 7.934, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.52it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 56.19it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 58.26it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 62.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.96it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 62.66it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 60.31it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 58.45it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.1995, 'grad_norm': 0.2523534893989563, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.1021031141281128, 'eval_runtime': 7.9335, 'eval_samples_per_second': 125.922, 'eval_steps_per_second': 7.941, 'epoch': 0.08}
{'loss': 1.0293, 'grad_norm': 0.19850100576877594, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9562824368476868, 'eval_runtime': 7.9806, 'eval_samples_per_second': 125.179, 'eval_steps_per_second': 7.894, 'epoch': 0.12}
{'loss': 0.9274, 'grad_norm': 0.23382394015789032, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8464967608451843, 'eval_runtime': 7.9757, 'eval_samples_per_second': 125.256, 'eval_steps_per_second': 7.899, 'epoch': 0.16}
{'loss': 0.7918, 'grad_norm': 0.2476913034915924, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.794775128364563, 'eval_runtime': 7.9955, 'eval_samples_per_second': 124.945, 'eval_steps_per_second': 7.879, 'epoch': 0.2}
{'loss': 0.7768, 'grad_norm': 0.22349905967712402, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7461941838264465, 'eval_runtime': 8.012, 'eval_samples_per_second': 124.688, 'eval_steps_per_second': 7.863, 'epoch': 0.24}
{'loss': 0.728, 'grad_norm': 0.300838828086853, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6903891563415527, 'eval_runtime': 8.0122, 'eval_samples_per_second': 124.685, 'eval_steps_per_second': 7.863, 'epoch': 0.28}
{'loss': 0.7308, 'grad_norm': 0.31864655017852783, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.649455189704895, 'eval_runtime': 8.023, 'eval_samples_per_second': 124.517, 'eval_steps_per_second': 7.852, 'epoch': 0.32}
{'loss': 0.6362, 'grad_norm': 0.30998411774635315, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6141497492790222, 'eval_runtime': 8.0159, 'eval_samples_per_second': 124.627, 'eval_steps_per_second': 7.859, 'epoch': 0.36}
{'loss': 0.6354, 'grad_norm': 0.35310986638069153, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5839124321937561, 'eval_runtime': 8.0257, 'eval_samples_per_second': 124.475, 'eval_steps_per_second': 7.85, 'epoch': 0.4}
{'loss': 0.5918, 'grad_norm': 0.292016863822937, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.545320987701416, 'eval_runtime': 8.0225, 'eval_samples_per_second': 124.524, 'eval_steps_per_second': 7.853, 'epoch': 0.44}
{'loss': 0.5889, 'grad_norm': 0.2925056219100952, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5178335905075073, 'eval_runtime': 8.0163, 'eval_samples_per_second': 124.621, 'eval_steps_per_second': 7.859, 'epoch': 0.48}
{'loss': 0.5538, 'grad_norm': 0.34271007776260376, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.48665815591812134, 'eval_runtime': 7.9945, 'eval_samples_per_second': 124.961, 'eval_steps_per_second': 7.88, 'epoch': 0.52}
{'loss': 0.4912, 'grad_norm': 0.3776763677597046, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.465847373008728, 'eval_runtime': 7.9593, 'eval_samples_per_second': 125.514, 'eval_steps_per_second': 7.915, 'epoch': 0.56}
{'loss': 0.5188, 'grad_norm': 0.46908268332481384, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4474044442176819, 'eval_runtime': 7.9712, 'eval_samples_per_second': 125.326, 'eval_steps_per_second': 7.903, 'epoch': 0.6}
{'loss': 0.4723, 'grad_norm': 0.30680209398269653, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.43163609504699707, 'eval_runtime': 7.9971, 'eval_samples_per_second': 124.921, 'eval_steps_per_second': 7.878, 'epoch': 0.64}
{'loss': 0.4393, 'grad_norm': 0.21351808309555054, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4163217842578888, 'eval_runtime': 7.9824, 'eval_samples_per_second': 125.15, 'eval_steps_per_second': 7.892, 'epoch': 0.68}
{'loss': 0.4269, 'grad_norm': 0.27920207381248474, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.40400680899620056, 'eval_runtime': 7.9677, 'eval_samples_per_second': 125.381, 'eval_steps_per_second': 7.907, 'epoch': 0.72}
{'loss': 0.3814, 'grad_norm': 0.2626647651195526, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3915984332561493, 'eval_runtime': 7.9652, 'eval_samples_per_second': 125.42, 'eval_steps_per_second': 7.909, 'epoch': 0.76}
{'loss': 0.3973, 'grad_norm': 0.29991233348846436, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.38356631994247437, 'eval_runtime': 7.963, 'eval_samples_per_second': 125.455, 'eval_steps_per_second': 7.912, 'epoch': 0.8}
{'loss': 0.4007, 'grad_norm': 0.228695347905159, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3763794004917145, 'eval_runtime': 7.9641, 'eval_samples_per_second': 125.438, 'eval_steps_per_second': 7.911, 'epoch': 0.84}
{'loss': 0.3599, 'grad_norm': 0.2567967176437378, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3714829087257385, 'eval_runtime': 7.9646, 'eval_samples_per_second': 125.43, 'eval_steps_per_second': 7.91, 'epoch': 0.88}
{'loss': 0.424, 'grad_norm': 0.1849784255027771, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3658371567726135, 'eval_runtime': 7.9647, 'eval_samples_per_second': 125.429, 'eval_steps_per_second': 7.91, 'epoch': 0.92}
{'loss': 0.3616, 'grad_norm': 0.22324538230895996, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.362994521856308, 'eval_runtime': 7.9661, 'eval_samples_per_second': 125.406, 'eval_steps_per_second': 7.909, 'epoch': 0.96}
{'loss': 0.3898, 'grad_norm': 0.1614259034395218, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3617762625217438, 'eval_runtime': 7.9662, 'eval_samples_per_second': 125.405, 'eval_steps_per_second': 7.908, 'epoch': 1.0}
{'train_runtime': 438.5678, 'train_samples_per_second': 22.797, 'train_steps_per_second': 1.425, 'train_loss': 0.6749292770385742, 'epoch': 1.0}
train_results:  {'eval_loss': [1.349191665649414, 1.1021031141281128, 0.9562824368476868, 0.8464967608451843, 0.794775128364563, 0.7461941838264465, 0.6903891563415527, 0.649455189704895, 0.6141497492790222, 0.5839124321937561, 0.545320987701416, 0.5178335905075073, 0.48665815591812134, 0.465847373008728, 0.4474044442176819, 0.43163609504699707, 0.4163217842578888, 0.40400680899620056, 0.3915984332561493, 0.38356631994247437, 0.3763794004917145, 0.3714829087257385, 0.3658371567726135, 0.362994521856308, 0.3617762625217438], 'performance': [0.62, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.81it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 43.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.54it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 69.13it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.02it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2468407154083252
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0703 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.9575028543125703
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.8050102854016484
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.959636794799456
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 1.095415538347468
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.7221809517560726
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [tensor(0.0924, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2017, dtype=torch.float64), tensor(0.7060, dtype=torch.float64), 32, 0, 0, 0, 0, 1, 128, 0.07832334642505484, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.0924, dtype=torch.float64), tensor(2.9222e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3816e-17, dtype=torch.float64), tensor(4.9669e-16, dtype=torch.float64), tensor(9.5768e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2017, dtype=torch.float64), tensor(0.7060, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7832, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.092
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.202
  arc_challenge: 0.706

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07832334642505484,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  0.07832334642505484
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.57it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 34.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 47.27it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 54.36it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 59.25it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 64.60it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 65.71it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 69.54it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 73.99it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 73.38it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 73.59it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 66.20it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.9309, 'grad_norm': 0.41405099630355835, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.5553171634674072, 'eval_runtime': 7.6819, 'eval_samples_per_second': 130.047, 'eval_steps_per_second': 8.201, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.55it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 40.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 51.82it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 57.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 60.84it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 65.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 66.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 69.50it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 73.79it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 73.07it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 73.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 67.37it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.299, 'grad_norm': 0.15014269948005676, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.1576234102249146, 'eval_runtime': 7.6871, 'eval_samples_per_second': 129.958, 'eval_steps_per_second': 8.196, 'epoch': 0.08}
{'loss': 1.1537, 'grad_norm': 0.1592317521572113, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0601444244384766, 'eval_runtime': 7.72, 'eval_samples_per_second': 129.404, 'eval_steps_per_second': 8.161, 'epoch': 0.12}
{'loss': 0.99, 'grad_norm': 0.18614207208156586, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9363377690315247, 'eval_runtime': 7.7345, 'eval_samples_per_second': 129.162, 'eval_steps_per_second': 8.145, 'epoch': 0.16}
{'loss': 0.9161, 'grad_norm': 0.19528594613075256, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8739497661590576, 'eval_runtime': 7.758, 'eval_samples_per_second': 128.77, 'eval_steps_per_second': 8.121, 'epoch': 0.2}
{'loss': 0.8619, 'grad_norm': 0.23281921446323395, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8367420434951782, 'eval_runtime': 7.7675, 'eval_samples_per_second': 128.613, 'eval_steps_per_second': 8.111, 'epoch': 0.24}
{'loss': 0.8402, 'grad_norm': 0.2711392343044281, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.798058807849884, 'eval_runtime': 7.7664, 'eval_samples_per_second': 128.631, 'eval_steps_per_second': 8.112, 'epoch': 0.28}
{'loss': 0.8522, 'grad_norm': 0.26297688484191895, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7652167677879333, 'eval_runtime': 7.7493, 'eval_samples_per_second': 128.915, 'eval_steps_per_second': 8.13, 'epoch': 0.32}
{'loss': 0.7883, 'grad_norm': 0.2840595543384552, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7302907705307007, 'eval_runtime': 7.7452, 'eval_samples_per_second': 128.983, 'eval_steps_per_second': 8.134, 'epoch': 0.36}
{'loss': 0.7717, 'grad_norm': 0.33518990874290466, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.699850857257843, 'eval_runtime': 7.7392, 'eval_samples_per_second': 129.083, 'eval_steps_per_second': 8.14, 'epoch': 0.4}
{'loss': 0.7252, 'grad_norm': 0.3731622099876404, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6684969067573547, 'eval_runtime': 7.743, 'eval_samples_per_second': 129.02, 'eval_steps_per_second': 8.136, 'epoch': 0.44}
{'loss': 0.7084, 'grad_norm': 0.36480408906936646, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6373088359832764, 'eval_runtime': 7.7383, 'eval_samples_per_second': 129.098, 'eval_steps_per_second': 8.141, 'epoch': 0.48}
{'loss': 0.705, 'grad_norm': 0.3632659614086151, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.614734411239624, 'eval_runtime': 7.73, 'eval_samples_per_second': 129.236, 'eval_steps_per_second': 8.15, 'epoch': 0.52}
{'loss': 0.6136, 'grad_norm': 0.30640316009521484, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5920595526695251, 'eval_runtime': 7.7367, 'eval_samples_per_second': 129.124, 'eval_steps_per_second': 8.143, 'epoch': 0.56}
{'loss': 0.64, 'grad_norm': 0.3470907509326935, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5678213834762573, 'eval_runtime': 7.7468, 'eval_samples_per_second': 128.956, 'eval_steps_per_second': 8.132, 'epoch': 0.6}
{'loss': 0.5948, 'grad_norm': 0.40389755368232727, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5428051352500916, 'eval_runtime': 7.7503, 'eval_samples_per_second': 128.898, 'eval_steps_per_second': 8.129, 'epoch': 0.64}
{'loss': 0.5806, 'grad_norm': 0.37206098437309265, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5183174014091492, 'eval_runtime': 7.8011, 'eval_samples_per_second': 128.059, 'eval_steps_per_second': 8.076, 'epoch': 0.68}
{'loss': 0.4922, 'grad_norm': 0.3878846764564514, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5001029968261719, 'eval_runtime': 7.8007, 'eval_samples_per_second': 128.065, 'eval_steps_per_second': 8.076, 'epoch': 0.72}
{'loss': 0.5475, 'grad_norm': 0.45857253670692444, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4858317971229553, 'eval_runtime': 7.7905, 'eval_samples_per_second': 128.232, 'eval_steps_per_second': 8.087, 'epoch': 0.76}
{'loss': 0.5352, 'grad_norm': 0.5046277642250061, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4696621596813202, 'eval_runtime': 7.7956, 'eval_samples_per_second': 128.15, 'eval_steps_per_second': 8.082, 'epoch': 0.8}
{'loss': 0.5204, 'grad_norm': 0.31303566694259644, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.45543932914733887, 'eval_runtime': 7.7913, 'eval_samples_per_second': 128.22, 'eval_steps_per_second': 8.086, 'epoch': 0.84}
{'loss': 0.469, 'grad_norm': 0.4355159401893616, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.445499062538147, 'eval_runtime': 7.7849, 'eval_samples_per_second': 128.325, 'eval_steps_per_second': 8.093, 'epoch': 0.88}
{'loss': 0.5284, 'grad_norm': 0.2903875410556793, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4358943998813629, 'eval_runtime': 7.7861, 'eval_samples_per_second': 128.306, 'eval_steps_per_second': 8.091, 'epoch': 0.92}
{'loss': 0.4366, 'grad_norm': 0.2971950173377991, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.431268572807312, 'eval_runtime': 7.7862, 'eval_samples_per_second': 128.305, 'eval_steps_per_second': 8.091, 'epoch': 0.96}
{'loss': 0.4943, 'grad_norm': 0.3769441843032837, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.42894619703292847, 'eval_runtime': 7.7865, 'eval_samples_per_second': 128.298, 'eval_steps_per_second': 8.091, 'epoch': 1.0}
{'train_runtime': 418.4016, 'train_samples_per_second': 23.896, 'train_steps_per_second': 1.494, 'train_loss': 0.7998060607910156, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5553171634674072, 1.1576234102249146, 1.0601444244384766, 0.9363377690315247, 0.8739497661590576, 0.8367420434951782, 0.798058807849884, 0.7652167677879333, 0.7302907705307007, 0.699850857257843, 0.6684969067573547, 0.6373088359832764, 0.614734411239624, 0.5920595526695251, 0.5678213834762573, 0.5428051352500916, 0.5183174014091492, 0.5001029968261719, 0.4858317971229553, 0.4696621596813202, 0.45543932914733887, 0.445499062538147, 0.4358943998813629, 0.431268572807312, 0.42894619703292847], 'performance': [0.62, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:26,  3.79it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.26it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 57.31it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 68.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 76.26it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 80.07it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 77.91it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2372987270355225
current iteration best possible performance (full train run):  0.651
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.4532 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 1.2127502703881083
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.9580737876349298
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.9580737811807408
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.9580737877685228
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.9327055197372148
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.2656, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7344, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.08319820024213594, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2656, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7344, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8320, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.266
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.734

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.08319820024213594,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.08319820024213594
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.53it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.67it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.73it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.18it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.92it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.28it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.39it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.04it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.23it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.38it/s]
Evaluation performance at step 25: 0.61
{'loss': 2.6164, 'grad_norm': 0.5498128533363342, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.1894762516021729, 'eval_runtime': 6.7007, 'eval_samples_per_second': 149.09, 'eval_steps_per_second': 9.402, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.48it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.25it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.33it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.56it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.84it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.41it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.19it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.84it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.32it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 59.52it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.01it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.0644, 'grad_norm': 0.20378394424915314, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 0.9771913290023804, 'eval_runtime': 6.7104, 'eval_samples_per_second': 148.874, 'eval_steps_per_second': 9.388, 'epoch': 0.08}
{'loss': 0.9187, 'grad_norm': 0.2440418154001236, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8227581977844238, 'eval_runtime': 6.7362, 'eval_samples_per_second': 148.303, 'eval_steps_per_second': 9.352, 'epoch': 0.12}
{'loss': 0.7527, 'grad_norm': 0.23696725070476532, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.706973671913147, 'eval_runtime': 6.7334, 'eval_samples_per_second': 148.366, 'eval_steps_per_second': 9.356, 'epoch': 0.16}
{'loss': 0.6951, 'grad_norm': 0.26490166783332825, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.653323233127594, 'eval_runtime': 6.7139, 'eval_samples_per_second': 148.795, 'eval_steps_per_second': 9.383, 'epoch': 0.2}
{'loss': 0.6484, 'grad_norm': 0.32662054896354675, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5801076889038086, 'eval_runtime': 6.7205, 'eval_samples_per_second': 148.65, 'eval_steps_per_second': 9.374, 'epoch': 0.24}
{'loss': 0.5805, 'grad_norm': 0.2755581736564636, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5370056629180908, 'eval_runtime': 6.7343, 'eval_samples_per_second': 148.346, 'eval_steps_per_second': 9.355, 'epoch': 0.28}
{'loss': 0.5141, 'grad_norm': 0.3038313090801239, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.4812939763069153, 'eval_runtime': 6.7301, 'eval_samples_per_second': 148.438, 'eval_steps_per_second': 9.361, 'epoch': 0.32}
{'loss': 0.4862, 'grad_norm': 0.29040011763572693, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4318528175354004, 'eval_runtime': 6.7309, 'eval_samples_per_second': 148.42, 'eval_steps_per_second': 9.36, 'epoch': 0.36}
{'loss': 0.4622, 'grad_norm': 0.32018718123435974, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.38679537177085876, 'eval_runtime': 6.7532, 'eval_samples_per_second': 147.929, 'eval_steps_per_second': 9.329, 'epoch': 0.4}
{'loss': 0.4157, 'grad_norm': 0.42230483889579773, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3453046679496765, 'eval_runtime': 6.7823, 'eval_samples_per_second': 147.296, 'eval_steps_per_second': 9.289, 'epoch': 0.44}
{'loss': 0.3976, 'grad_norm': 0.3284148871898651, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3095075786113739, 'eval_runtime': 6.7694, 'eval_samples_per_second': 147.577, 'eval_steps_per_second': 9.307, 'epoch': 0.48}
{'loss': 0.3628, 'grad_norm': 0.290232390165329, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.28618913888931274, 'eval_runtime': 6.7694, 'eval_samples_per_second': 147.575, 'eval_steps_per_second': 9.307, 'epoch': 0.52}
{'loss': 0.3325, 'grad_norm': 0.38799718022346497, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.2627433240413666, 'eval_runtime': 6.7728, 'eval_samples_per_second': 147.501, 'eval_steps_per_second': 9.302, 'epoch': 0.56}
{'loss': 0.3034, 'grad_norm': 0.3357483446598053, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2401188760995865, 'eval_runtime': 6.7659, 'eval_samples_per_second': 147.653, 'eval_steps_per_second': 9.311, 'epoch': 0.6}
{'loss': 0.2991, 'grad_norm': 0.2739484906196594, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.2252018004655838, 'eval_runtime': 6.773, 'eval_samples_per_second': 147.497, 'eval_steps_per_second': 9.302, 'epoch': 0.64}
{'loss': 0.2527, 'grad_norm': 0.2520045340061188, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2121201455593109, 'eval_runtime': 6.7847, 'eval_samples_per_second': 147.244, 'eval_steps_per_second': 9.286, 'epoch': 0.68}
{'loss': 0.2663, 'grad_norm': 0.2850569188594818, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.19711071252822876, 'eval_runtime': 6.7637, 'eval_samples_per_second': 147.7, 'eval_steps_per_second': 9.314, 'epoch': 0.72}
{'loss': 0.2646, 'grad_norm': 0.3116684556007385, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.1886027753353119, 'eval_runtime': 6.7693, 'eval_samples_per_second': 147.579, 'eval_steps_per_second': 9.307, 'epoch': 0.76}
{'loss': 0.2471, 'grad_norm': 0.19519196450710297, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.18085657060146332, 'eval_runtime': 6.7636, 'eval_samples_per_second': 147.703, 'eval_steps_per_second': 9.315, 'epoch': 0.8}
{'loss': 0.2495, 'grad_norm': 0.28792592883110046, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.17358966171741486, 'eval_runtime': 6.7581, 'eval_samples_per_second': 147.823, 'eval_steps_per_second': 9.322, 'epoch': 0.84}
{'loss': 0.2337, 'grad_norm': 0.21226340532302856, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.16851115226745605, 'eval_runtime': 6.7969, 'eval_samples_per_second': 146.979, 'eval_steps_per_second': 9.269, 'epoch': 0.88}
{'loss': 0.2056, 'grad_norm': 0.25233903527259827, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.16387653350830078, 'eval_runtime': 6.7703, 'eval_samples_per_second': 147.556, 'eval_steps_per_second': 9.305, 'epoch': 0.92}
{'loss': 0.2151, 'grad_norm': 0.16937430202960968, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.16112376749515533, 'eval_runtime': 6.7751, 'eval_samples_per_second': 147.452, 'eval_steps_per_second': 9.299, 'epoch': 0.96}
{'loss': 0.1918, 'grad_norm': 0.1972966194152832, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.15980063378810883, 'eval_runtime': 6.7631, 'eval_samples_per_second': 147.712, 'eval_steps_per_second': 9.315, 'epoch': 1.0}
{'train_runtime': 395.1299, 'train_samples_per_second': 25.306, 'train_steps_per_second': 1.582, 'train_loss': 0.5190436477661133, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1894762516021729, 0.9771913290023804, 0.8227581977844238, 0.706973671913147, 0.653323233127594, 0.5801076889038086, 0.5370056629180908, 0.4812939763069153, 0.4318528175354004, 0.38679537177085876, 0.3453046679496765, 0.3095075786113739, 0.28618913888931274, 0.2627433240413666, 0.2401188760995865, 0.2252018004655838, 0.2121201455593109, 0.19711071252822876, 0.1886027753353119, 0.18085657060146332, 0.17358966171741486, 0.16851115226745605, 0.16387653350830078, 0.16112376749515533, 0.15980063378810883], 'performance': [0.61, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.95it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.52it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.38it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.23it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2502251863479614
current iteration best possible performance (full train run):  0.651
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0572 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.9496929463044452
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.8397648591052737
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.9496959300777802
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.8363924120937687
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.9495577860599744
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.2722, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.7278, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.0772460697458496, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.9540e-16, dtype=torch.float64), tensor(0.2722, dtype=torch.float64), tensor(1.0703e-16, dtype=torch.float64), tensor(4.2902e-16, dtype=torch.float64), tensor(4.5810e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7278, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7725, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.272
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.728

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0772460697458496,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.0772460697458496
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.32it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.66it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 49.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 54.20it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 57.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 61.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 62.25it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:00<00:00, 66.73it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 66.31it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 66.31it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 66.93it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.04it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.4306, 'grad_norm': 0.5242950320243835, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.2968671321868896, 'eval_runtime': 8.8559, 'eval_samples_per_second': 112.806, 'eval_steps_per_second': 7.114, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  6.11it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 39.14it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 48.11it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 52.19it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 58.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 63.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 60.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 65.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 64.73it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 64.72it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.45it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.1092, 'grad_norm': 0.1738506704568863, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 0.998701274394989, 'eval_runtime': 8.8584, 'eval_samples_per_second': 112.774, 'eval_steps_per_second': 7.112, 'epoch': 0.08}
{'loss': 0.9799, 'grad_norm': 0.19379305839538574, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9251852631568909, 'eval_runtime': 8.9596, 'eval_samples_per_second': 111.5, 'eval_steps_per_second': 7.032, 'epoch': 0.12}
{'loss': 0.8836, 'grad_norm': 0.2031618356704712, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8461539149284363, 'eval_runtime': 8.9345, 'eval_samples_per_second': 111.813, 'eval_steps_per_second': 7.051, 'epoch': 0.16}
{'loss': 0.8401, 'grad_norm': 0.21252179145812988, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7847395539283752, 'eval_runtime': 8.942, 'eval_samples_per_second': 111.72, 'eval_steps_per_second': 7.045, 'epoch': 0.2}
{'loss': 0.8098, 'grad_norm': 0.2648485600948334, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7193183302879333, 'eval_runtime': 8.9123, 'eval_samples_per_second': 112.092, 'eval_steps_per_second': 7.069, 'epoch': 0.24}
{'loss': 0.7495, 'grad_norm': 0.2272546887397766, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6818189024925232, 'eval_runtime': 8.9059, 'eval_samples_per_second': 112.172, 'eval_steps_per_second': 7.074, 'epoch': 0.28}
{'loss': 0.7189, 'grad_norm': 0.25592121481895447, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6415195465087891, 'eval_runtime': 8.9201, 'eval_samples_per_second': 111.994, 'eval_steps_per_second': 7.063, 'epoch': 0.32}
{'loss': 0.6625, 'grad_norm': 0.30788764357566833, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5968088507652283, 'eval_runtime': 8.9255, 'eval_samples_per_second': 111.927, 'eval_steps_per_second': 7.058, 'epoch': 0.36}
{'loss': 0.6388, 'grad_norm': 0.2810983955860138, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5643215179443359, 'eval_runtime': 8.9105, 'eval_samples_per_second': 112.115, 'eval_steps_per_second': 7.07, 'epoch': 0.4}
{'loss': 0.6115, 'grad_norm': 0.38340312242507935, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.533909022808075, 'eval_runtime': 8.9567, 'eval_samples_per_second': 111.536, 'eval_steps_per_second': 7.034, 'epoch': 0.44}
{'loss': 0.572, 'grad_norm': 0.3670235574245453, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.502481997013092, 'eval_runtime': 8.9094, 'eval_samples_per_second': 112.129, 'eval_steps_per_second': 7.071, 'epoch': 0.48}
{'loss': 0.5691, 'grad_norm': 0.35142946243286133, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4777313470840454, 'eval_runtime': 8.9139, 'eval_samples_per_second': 112.072, 'eval_steps_per_second': 7.068, 'epoch': 0.52}
{'loss': 0.5308, 'grad_norm': 0.35077518224716187, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.45628148317337036, 'eval_runtime': 8.9375, 'eval_samples_per_second': 111.776, 'eval_steps_per_second': 7.049, 'epoch': 0.56}
{'loss': 0.4959, 'grad_norm': 0.2743924856185913, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4371267557144165, 'eval_runtime': 8.932, 'eval_samples_per_second': 111.845, 'eval_steps_per_second': 7.053, 'epoch': 0.6}
{'loss': 0.5239, 'grad_norm': 0.29178082942962646, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4166652262210846, 'eval_runtime': 8.9348, 'eval_samples_per_second': 111.81, 'eval_steps_per_second': 7.051, 'epoch': 0.64}
{'loss': 0.4758, 'grad_norm': 0.28364452719688416, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3968678116798401, 'eval_runtime': 8.9706, 'eval_samples_per_second': 111.364, 'eval_steps_per_second': 7.023, 'epoch': 0.68}
{'loss': 0.4772, 'grad_norm': 0.2452964186668396, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3810748755931854, 'eval_runtime': 8.9408, 'eval_samples_per_second': 111.735, 'eval_steps_per_second': 7.046, 'epoch': 0.72}
{'loss': 0.469, 'grad_norm': 0.3111012876033783, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.36918163299560547, 'eval_runtime': 8.9163, 'eval_samples_per_second': 112.043, 'eval_steps_per_second': 7.066, 'epoch': 0.76}
{'loss': 0.4313, 'grad_norm': 0.3356958329677582, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3570610284805298, 'eval_runtime': 8.926, 'eval_samples_per_second': 111.921, 'eval_steps_per_second': 7.058, 'epoch': 0.8}
{'loss': 0.42, 'grad_norm': 0.2632962167263031, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.34971359372138977, 'eval_runtime': 8.9192, 'eval_samples_per_second': 112.006, 'eval_steps_per_second': 7.063, 'epoch': 0.84}
{'loss': 0.4128, 'grad_norm': 0.23244686424732208, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3420494496822357, 'eval_runtime': 8.9284, 'eval_samples_per_second': 111.89, 'eval_steps_per_second': 7.056, 'epoch': 0.88}
{'loss': 0.3925, 'grad_norm': 0.19885113835334778, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.33662447333335876, 'eval_runtime': 8.9573, 'eval_samples_per_second': 111.53, 'eval_steps_per_second': 7.033, 'epoch': 0.92}
{'loss': 0.4042, 'grad_norm': 0.42917105555534363, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3322773575782776, 'eval_runtime': 8.9902, 'eval_samples_per_second': 111.121, 'eval_steps_per_second': 7.008, 'epoch': 0.96}
{'loss': 0.3514, 'grad_norm': 0.23947007954120636, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3306484520435333, 'eval_runtime': 8.9907, 'eval_samples_per_second': 111.114, 'eval_steps_per_second': 7.007, 'epoch': 1.0}
{'train_runtime': 476.1747, 'train_samples_per_second': 20.999, 'train_steps_per_second': 1.313, 'train_loss': 0.6784159286499023, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2968671321868896, 0.998701274394989, 0.9251852631568909, 0.8461539149284363, 0.7847395539283752, 0.7193183302879333, 0.6818189024925232, 0.6415195465087891, 0.5968088507652283, 0.5643215179443359, 0.533909022808075, 0.502481997013092, 0.4777313470840454, 0.45628148317337036, 0.4371267557144165, 0.4166652262210846, 0.3968678116798401, 0.3810748755931854, 0.36918163299560547, 0.3570610284805298, 0.34971359372138977, 0.3420494496822357, 0.33662447333335876, 0.3322773575782776, 0.3306484520435333], 'performance': [0.62, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:27,  3.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 55.25it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 61.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 67.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 66.18it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2460026741027832
current iteration best possible performance (full train run):  0.5775000000000001
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614, 1.2460026741027832]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1010 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.9444430986144376
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.9446658850253661
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.9448030586433633
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.9448030584784125
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.9447933535712107
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2796, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7204, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 128, 0.06682537767061379, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(2.7879e-16, dtype=torch.float64), tensor(1.4554e-16, dtype=torch.float64), tensor(0.2796, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.9345e-17, dtype=torch.float64), tensor(4.1435e-17, dtype=torch.float64), tensor(4.6635e-17, dtype=torch.float64), tensor(0.7204, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6683, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.28
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.72

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06682537767061379,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.06682537767061379
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.06it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.81it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.92it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.17it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.12it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.06it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.05it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.06it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.64it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.83it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 53.01it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.32it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.6984, 'grad_norm': 0.39136430621147156, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.5275557041168213, 'eval_runtime': 10.38, 'eval_samples_per_second': 96.243, 'eval_steps_per_second': 6.069, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.01it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.89it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.12it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.52it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.96it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.18it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.24it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 54.31it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.82it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.71it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.20it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.3982, 'grad_norm': 0.24529235064983368, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.3117440938949585, 'eval_runtime': 10.3491, 'eval_samples_per_second': 96.53, 'eval_steps_per_second': 6.088, 'epoch': 0.08}
{'loss': 1.2366, 'grad_norm': 0.200495645403862, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1948659420013428, 'eval_runtime': 10.355, 'eval_samples_per_second': 96.475, 'eval_steps_per_second': 6.084, 'epoch': 0.12}
{'loss': 1.1921, 'grad_norm': 0.21998733282089233, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1100034713745117, 'eval_runtime': 10.3851, 'eval_samples_per_second': 96.196, 'eval_steps_per_second': 6.066, 'epoch': 0.16}
{'loss': 1.0812, 'grad_norm': 0.22170038521289825, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0710867643356323, 'eval_runtime': 10.3848, 'eval_samples_per_second': 96.198, 'eval_steps_per_second': 6.067, 'epoch': 0.2}
{'loss': 1.0411, 'grad_norm': 0.2827291786670685, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0241082906723022, 'eval_runtime': 10.4719, 'eval_samples_per_second': 95.398, 'eval_steps_per_second': 6.016, 'epoch': 0.24}
{'loss': 0.9873, 'grad_norm': 0.27710580825805664, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9808261394500732, 'eval_runtime': 10.4572, 'eval_samples_per_second': 95.532, 'eval_steps_per_second': 6.025, 'epoch': 0.28}
{'loss': 0.9905, 'grad_norm': 0.24785493314266205, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9389670491218567, 'eval_runtime': 10.4613, 'eval_samples_per_second': 95.495, 'eval_steps_per_second': 6.022, 'epoch': 0.32}
{'loss': 0.9052, 'grad_norm': 0.2960086762905121, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8944977521896362, 'eval_runtime': 10.465, 'eval_samples_per_second': 95.461, 'eval_steps_per_second': 6.02, 'epoch': 0.36}
{'loss': 0.8973, 'grad_norm': 0.24121630191802979, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8676631450653076, 'eval_runtime': 10.4583, 'eval_samples_per_second': 95.522, 'eval_steps_per_second': 6.024, 'epoch': 0.4}
{'loss': 0.9504, 'grad_norm': 0.2718559503555298, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8336560726165771, 'eval_runtime': 10.458, 'eval_samples_per_second': 95.525, 'eval_steps_per_second': 6.024, 'epoch': 0.44}
{'loss': 0.8537, 'grad_norm': 0.26567211747169495, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8073625564575195, 'eval_runtime': 10.4612, 'eval_samples_per_second': 95.495, 'eval_steps_per_second': 6.022, 'epoch': 0.48}
{'loss': 0.9331, 'grad_norm': 0.29696282744407654, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7922211289405823, 'eval_runtime': 10.4846, 'eval_samples_per_second': 95.282, 'eval_steps_per_second': 6.009, 'epoch': 0.52}
{'loss': 0.837, 'grad_norm': 0.23832277953624725, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.771538257598877, 'eval_runtime': 10.4675, 'eval_samples_per_second': 95.439, 'eval_steps_per_second': 6.019, 'epoch': 0.56}
{'loss': 0.7811, 'grad_norm': 0.21861578524112701, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7564316987991333, 'eval_runtime': 10.4645, 'eval_samples_per_second': 95.465, 'eval_steps_per_second': 6.02, 'epoch': 0.6}
{'loss': 0.8914, 'grad_norm': 0.21621324121952057, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7416995167732239, 'eval_runtime': 10.4669, 'eval_samples_per_second': 95.444, 'eval_steps_per_second': 6.019, 'epoch': 0.64}
{'loss': 0.7938, 'grad_norm': 0.19289802014827728, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7324337959289551, 'eval_runtime': 10.4156, 'eval_samples_per_second': 95.913, 'eval_steps_per_second': 6.049, 'epoch': 0.68}
{'loss': 0.8581, 'grad_norm': 0.2265569120645523, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7227010130882263, 'eval_runtime': 10.4067, 'eval_samples_per_second': 95.996, 'eval_steps_per_second': 6.054, 'epoch': 0.72}
{'loss': 0.8804, 'grad_norm': 0.3348192572593689, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7145810127258301, 'eval_runtime': 10.4421, 'eval_samples_per_second': 95.671, 'eval_steps_per_second': 6.033, 'epoch': 0.76}
{'loss': 0.8018, 'grad_norm': 0.27398139238357544, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7061751484870911, 'eval_runtime': 10.4788, 'eval_samples_per_second': 95.335, 'eval_steps_per_second': 6.012, 'epoch': 0.8}
{'loss': 0.8593, 'grad_norm': 0.23512430489063263, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6994637846946716, 'eval_runtime': 10.5177, 'eval_samples_per_second': 94.982, 'eval_steps_per_second': 5.99, 'epoch': 0.84}
{'loss': 0.7996, 'grad_norm': 0.19543419778347015, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.693580150604248, 'eval_runtime': 10.5521, 'eval_samples_per_second': 94.673, 'eval_steps_per_second': 5.97, 'epoch': 0.88}
{'loss': 0.7264, 'grad_norm': 0.20800241827964783, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6876336932182312, 'eval_runtime': 10.538, 'eval_samples_per_second': 94.8, 'eval_steps_per_second': 5.978, 'epoch': 0.92}
{'loss': 0.7784, 'grad_norm': 0.14981558918952942, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.684990406036377, 'eval_runtime': 10.5098, 'eval_samples_per_second': 95.054, 'eval_steps_per_second': 5.994, 'epoch': 0.96}
{'loss': 0.7401, 'grad_norm': 0.17662261426448822, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6842046976089478, 'eval_runtime': 10.4581, 'eval_samples_per_second': 95.524, 'eval_steps_per_second': 6.024, 'epoch': 1.0}
{'train_runtime': 557.375, 'train_samples_per_second': 17.939, 'train_steps_per_second': 1.121, 'train_loss': 0.9964982482910156, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5275557041168213, 1.3117440938949585, 1.1948659420013428, 1.1100034713745117, 1.0710867643356323, 1.0241082906723022, 0.9808261394500732, 0.9389670491218567, 0.8944977521896362, 0.8676631450653076, 0.8336560726165771, 0.8073625564575195, 0.7922211289405823, 0.771538257598877, 0.7564316987991333, 0.7416995167732239, 0.7324337959289551, 0.7227010130882263, 0.7145810127258301, 0.7061751484870911, 0.6994637846946716, 0.693580150604248, 0.6876336932182312, 0.684990406036377, 0.6842046976089478], 'performance': [0.63, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:06<10:10,  6.17s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:22,  3.64it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:12<00:22,  3.04it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:12<00:09,  5.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:12<00:04,  8.37it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:13<00:01, 12.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2486201524734497
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614, 1.2460026741027832, 1.2486201524734497]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2526 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.9456976293426854
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.9457425535371716
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.9573744122491794
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.9457427319958676
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.7178982025065621
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0886, dtype=torch.float64), 0, 0, tensor(0.1825, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7289, dtype=torch.float64), 32, 1, 1, 1, 0, 0, 128, 0.09827806550566766, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0886, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1825, dtype=torch.float64), tensor(7.4704e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.8575e-16, dtype=torch.float64), tensor(0.7289, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9828, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.089
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.183
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.729

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09827806550566766,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  128
lora dropout:  0.09827806550566766
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.84it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.50it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 49.41it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 52.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 56.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 57.08it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 62.28it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.12it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 63.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 62.00it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 61.06it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.35it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.6626, 'grad_norm': 0.7481057643890381, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.18464994430542, 'eval_runtime': 6.7074, 'eval_samples_per_second': 148.94, 'eval_steps_per_second': 9.393, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.92it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.77it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.94it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 49.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 52.42it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 56.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.05it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.49it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 61.18it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 61.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.65it/s]
Evaluation performance at step 50: 0.66
{'loss': 1.0634, 'grad_norm': 0.2713809907436371, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.66}
{'eval_loss': 0.9968137741088867, 'eval_runtime': 6.7046, 'eval_samples_per_second': 149.002, 'eval_steps_per_second': 9.397, 'epoch': 0.08}
{'loss': 0.9264, 'grad_norm': 0.26202109456062317, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8769076466560364, 'eval_runtime': 6.7038, 'eval_samples_per_second': 149.021, 'eval_steps_per_second': 9.398, 'epoch': 0.12}
{'loss': 0.8496, 'grad_norm': 0.3226882815361023, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7945849895477295, 'eval_runtime': 6.7132, 'eval_samples_per_second': 148.81, 'eval_steps_per_second': 9.384, 'epoch': 0.16}
{'loss': 0.7596, 'grad_norm': 0.2867673933506012, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7281901240348816, 'eval_runtime': 6.7286, 'eval_samples_per_second': 148.472, 'eval_steps_per_second': 9.363, 'epoch': 0.2}
{'loss': 0.7245, 'grad_norm': 0.3026639223098755, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6763032674789429, 'eval_runtime': 6.7252, 'eval_samples_per_second': 148.546, 'eval_steps_per_second': 9.368, 'epoch': 0.24}
{'loss': 0.6772, 'grad_norm': 0.28389647603034973, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6181816458702087, 'eval_runtime': 6.7331, 'eval_samples_per_second': 148.371, 'eval_steps_per_second': 9.357, 'epoch': 0.28}
{'loss': 0.6025, 'grad_norm': 0.3009019196033478, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5632369518280029, 'eval_runtime': 6.7383, 'eval_samples_per_second': 148.256, 'eval_steps_per_second': 9.349, 'epoch': 0.32}
{'loss': 0.5495, 'grad_norm': 0.36743587255477905, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5139277577400208, 'eval_runtime': 6.7386, 'eval_samples_per_second': 148.249, 'eval_steps_per_second': 9.349, 'epoch': 0.36}
{'loss': 0.5427, 'grad_norm': 0.30146923661231995, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.4782581329345703, 'eval_runtime': 6.7605, 'eval_samples_per_second': 147.769, 'eval_steps_per_second': 9.319, 'epoch': 0.4}
{'loss': 0.481, 'grad_norm': 0.3555729389190674, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4273863732814789, 'eval_runtime': 6.7784, 'eval_samples_per_second': 147.38, 'eval_steps_per_second': 9.294, 'epoch': 0.44}
{'loss': 0.4583, 'grad_norm': 0.4172975420951843, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3949300944805145, 'eval_runtime': 6.7828, 'eval_samples_per_second': 147.285, 'eval_steps_per_second': 9.288, 'epoch': 0.48}
{'loss': 0.4325, 'grad_norm': 0.41699087619781494, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3718860447406769, 'eval_runtime': 6.787, 'eval_samples_per_second': 147.193, 'eval_steps_per_second': 9.282, 'epoch': 0.52}
{'loss': 0.4245, 'grad_norm': 0.5367618799209595, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3365339934825897, 'eval_runtime': 6.78, 'eval_samples_per_second': 147.345, 'eval_steps_per_second': 9.292, 'epoch': 0.56}
{'loss': 0.3715, 'grad_norm': 0.336262583732605, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3124535083770752, 'eval_runtime': 6.7956, 'eval_samples_per_second': 147.007, 'eval_steps_per_second': 9.271, 'epoch': 0.6}
{'loss': 0.3653, 'grad_norm': 0.28411102294921875, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.29203465580940247, 'eval_runtime': 6.7752, 'eval_samples_per_second': 147.449, 'eval_steps_per_second': 9.299, 'epoch': 0.64}
{'loss': 0.3374, 'grad_norm': 0.38552314043045044, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2741519510746002, 'eval_runtime': 6.7771, 'eval_samples_per_second': 147.409, 'eval_steps_per_second': 9.296, 'epoch': 0.68}
{'loss': 0.3187, 'grad_norm': 0.39007583260536194, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.26335111260414124, 'eval_runtime': 6.7631, 'eval_samples_per_second': 147.713, 'eval_steps_per_second': 9.315, 'epoch': 0.72}
{'loss': 0.3179, 'grad_norm': 0.26809170842170715, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.2516327500343323, 'eval_runtime': 6.7581, 'eval_samples_per_second': 147.822, 'eval_steps_per_second': 9.322, 'epoch': 0.76}
{'loss': 0.3005, 'grad_norm': 0.4650839865207672, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.24144460260868073, 'eval_runtime': 6.74, 'eval_samples_per_second': 148.22, 'eval_steps_per_second': 9.347, 'epoch': 0.8}
{'loss': 0.2955, 'grad_norm': 0.39443859457969666, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.23285506665706635, 'eval_runtime': 6.7469, 'eval_samples_per_second': 148.068, 'eval_steps_per_second': 9.338, 'epoch': 0.84}
{'loss': 0.2915, 'grad_norm': 0.2596200406551361, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.22528550028800964, 'eval_runtime': 6.744, 'eval_samples_per_second': 148.131, 'eval_steps_per_second': 9.342, 'epoch': 0.88}
{'loss': 0.2777, 'grad_norm': 0.2054840624332428, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.21803168952465057, 'eval_runtime': 6.7445, 'eval_samples_per_second': 148.122, 'eval_steps_per_second': 9.341, 'epoch': 0.92}
{'loss': 0.2784, 'grad_norm': 0.30246299505233765, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2132781594991684, 'eval_runtime': 6.7504, 'eval_samples_per_second': 147.991, 'eval_steps_per_second': 9.333, 'epoch': 0.96}
{'loss': 0.2365, 'grad_norm': 0.3642185628414154, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2118007242679596, 'eval_runtime': 6.8082, 'eval_samples_per_second': 146.735, 'eval_steps_per_second': 9.254, 'epoch': 1.0}
{'train_runtime': 377.0912, 'train_samples_per_second': 26.516, 'train_steps_per_second': 1.657, 'train_loss': 0.5818145706176758, 'epoch': 1.0}
train_results:  {'eval_loss': [1.18464994430542, 0.9968137741088867, 0.8769076466560364, 0.7945849895477295, 0.7281901240348816, 0.6763032674789429, 0.6181816458702087, 0.5632369518280029, 0.5139277577400208, 0.4782581329345703, 0.4273863732814789, 0.3949300944805145, 0.3718860447406769, 0.3365339934825897, 0.3124535083770752, 0.29203465580940247, 0.2741519510746002, 0.26335111260414124, 0.2516327500343323, 0.24144460260868073, 0.23285506665706635, 0.22528550028800964, 0.21803168952465057, 0.2132781594991684, 0.2118007242679596], 'performance': [0.63, 0.66]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:29,  3.40it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.64it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 62.22it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 66.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 65.16it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.66]
current iteration observed (possibly low-fid or predicted) performance:  1.2484309673309326
current iteration best possible performance (full train run):  0.651
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614, 1.2460026741027832, 1.2486201524734497, 1.2484309673309326]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4112 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.9497092667834166
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.9497564762540542
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.8858377334336811
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.9496641466877891
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.9612899143905868
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1125, dtype=torch.float64), 0, tensor(0.1648, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7227, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.1125, dtype=torch.float64), tensor(1.1515e-16, dtype=torch.float64), tensor(0.1648, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.1853e-17, dtype=torch.float64), tensor(3.4524e-17, dtype=torch.float64), tensor(6.2314e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7227, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.113
  gsm8k: 0
  rowan_hellaswag: 0.165
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.723

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.54it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.79it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.34it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.05it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.50it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.62it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.31it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.46it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.85it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.09it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.50it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.7705, 'grad_norm': 0.5650869607925415, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.504012107849121, 'eval_runtime': 9.401, 'eval_samples_per_second': 106.265, 'eval_steps_per_second': 6.701, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.61it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.55it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.96it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 51.35it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 57.54it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 59.60it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 63.89it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.97it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 63.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 62.17it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 61.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 61.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.77it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.3273, 'grad_norm': 0.2403070330619812, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.235573172569275, 'eval_runtime': 9.3791, 'eval_samples_per_second': 106.513, 'eval_steps_per_second': 6.717, 'epoch': 0.08}
{'loss': 1.1732, 'grad_norm': 0.2149936556816101, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0917283296585083, 'eval_runtime': 9.4024, 'eval_samples_per_second': 106.25, 'eval_steps_per_second': 6.7, 'epoch': 0.12}
{'loss': 1.0941, 'grad_norm': 0.20341582596302032, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0032117366790771, 'eval_runtime': 9.4151, 'eval_samples_per_second': 106.107, 'eval_steps_per_second': 6.691, 'epoch': 0.16}
{'loss': 0.9801, 'grad_norm': 0.20926643908023834, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9529885053634644, 'eval_runtime': 9.497, 'eval_samples_per_second': 105.191, 'eval_steps_per_second': 6.634, 'epoch': 0.2}
{'loss': 0.9132, 'grad_norm': 0.24318955838680267, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9001540541648865, 'eval_runtime': 9.4916, 'eval_samples_per_second': 105.251, 'eval_steps_per_second': 6.637, 'epoch': 0.24}
{'loss': 0.8984, 'grad_norm': 0.24673058092594147, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8503719568252563, 'eval_runtime': 9.4701, 'eval_samples_per_second': 105.489, 'eval_steps_per_second': 6.652, 'epoch': 0.28}
{'loss': 0.8808, 'grad_norm': 0.2398623377084732, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8092045783996582, 'eval_runtime': 9.5178, 'eval_samples_per_second': 104.961, 'eval_steps_per_second': 6.619, 'epoch': 0.32}
{'loss': 0.8331, 'grad_norm': 0.2462296187877655, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7710388898849487, 'eval_runtime': 9.5188, 'eval_samples_per_second': 104.95, 'eval_steps_per_second': 6.618, 'epoch': 0.36}
{'loss': 0.7844, 'grad_norm': 0.25894734263420105, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7344837188720703, 'eval_runtime': 9.5194, 'eval_samples_per_second': 104.943, 'eval_steps_per_second': 6.618, 'epoch': 0.4}
{'loss': 0.7499, 'grad_norm': 0.27816665172576904, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7035972476005554, 'eval_runtime': 9.516, 'eval_samples_per_second': 104.981, 'eval_steps_per_second': 6.62, 'epoch': 0.44}
{'loss': 0.7059, 'grad_norm': 0.2990683615207672, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.667902410030365, 'eval_runtime': 9.5205, 'eval_samples_per_second': 104.932, 'eval_steps_per_second': 6.617, 'epoch': 0.48}
{'loss': 0.7863, 'grad_norm': 0.27789151668548584, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6455353498458862, 'eval_runtime': 9.5231, 'eval_samples_per_second': 104.902, 'eval_steps_per_second': 6.615, 'epoch': 0.52}
{'loss': 0.7142, 'grad_norm': 0.28406450152397156, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.6200138330459595, 'eval_runtime': 9.4677, 'eval_samples_per_second': 105.517, 'eval_steps_per_second': 6.654, 'epoch': 0.56}
{'loss': 0.6605, 'grad_norm': 0.2214759737253189, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6072542667388916, 'eval_runtime': 9.4544, 'eval_samples_per_second': 105.665, 'eval_steps_per_second': 6.664, 'epoch': 0.6}
{'loss': 0.7186, 'grad_norm': 0.25314560532569885, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5980997085571289, 'eval_runtime': 9.4916, 'eval_samples_per_second': 105.251, 'eval_steps_per_second': 6.637, 'epoch': 0.64}
{'loss': 0.6885, 'grad_norm': 0.20385408401489258, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5864701271057129, 'eval_runtime': 9.4637, 'eval_samples_per_second': 105.562, 'eval_steps_per_second': 6.657, 'epoch': 0.68}
{'loss': 0.7425, 'grad_norm': 0.256211519241333, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5718961358070374, 'eval_runtime': 9.4652, 'eval_samples_per_second': 105.545, 'eval_steps_per_second': 6.656, 'epoch': 0.72}
{'loss': 0.6827, 'grad_norm': 0.29333385825157166, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5615316033363342, 'eval_runtime': 9.5055, 'eval_samples_per_second': 105.097, 'eval_steps_per_second': 6.628, 'epoch': 0.76}
{'loss': 0.6598, 'grad_norm': 0.26595786213874817, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5532581806182861, 'eval_runtime': 9.4633, 'eval_samples_per_second': 105.566, 'eval_steps_per_second': 6.657, 'epoch': 0.8}
{'loss': 0.67, 'grad_norm': 0.21854883432388306, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.5488656163215637, 'eval_runtime': 9.5247, 'eval_samples_per_second': 104.885, 'eval_steps_per_second': 6.614, 'epoch': 0.84}
{'loss': 0.6152, 'grad_norm': 0.24083907902240753, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.5444227457046509, 'eval_runtime': 9.5491, 'eval_samples_per_second': 104.617, 'eval_steps_per_second': 6.597, 'epoch': 0.88}
{'loss': 0.5692, 'grad_norm': 0.24721559882164001, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.5404561161994934, 'eval_runtime': 9.5792, 'eval_samples_per_second': 104.289, 'eval_steps_per_second': 6.577, 'epoch': 0.92}
{'loss': 0.6008, 'grad_norm': 0.15942761301994324, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5382018089294434, 'eval_runtime': 9.5998, 'eval_samples_per_second': 104.065, 'eval_steps_per_second': 6.563, 'epoch': 0.96}
{'loss': 0.5438, 'grad_norm': 0.24305370450019836, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5372994542121887, 'eval_runtime': 9.579, 'eval_samples_per_second': 104.291, 'eval_steps_per_second': 6.577, 'epoch': 1.0}
{'train_runtime': 512.3718, 'train_samples_per_second': 19.515, 'train_steps_per_second': 1.22, 'train_loss': 0.8705211135864258, 'epoch': 1.0}
train_results:  {'eval_loss': [1.504012107849121, 1.235573172569275, 1.0917283296585083, 1.0032117366790771, 0.9529885053634644, 0.9001540541648865, 0.8503719568252563, 0.8092045783996582, 0.7710388898849487, 0.7344837188720703, 0.7035972476005554, 0.667902410030365, 0.6455353498458862, 0.6200138330459595, 0.6072542667388916, 0.5980997085571289, 0.5864701271057129, 0.5718961358070374, 0.5615316033363342, 0.5532581806182861, 0.5488656163215637, 0.5444227457046509, 0.5404561161994934, 0.5382018089294434, 0.5372994542121887], 'performance': [0.63, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.58it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.17it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.30it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 73.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.32it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.250274896621704
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614, 1.2460026741027832, 1.2486201524734497, 1.2484309673309326, 1.250274896621704]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4040 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.87908976629434
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.9413474554078954
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.7009350775141214
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.8211684594202071
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.943416834957471
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.0357, dtype=torch.float64), tensor(0.2378, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7265, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1.3637e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0357, dtype=torch.float64), tensor(0.2378, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3836e-16, dtype=torch.float64), tensor(4.8242e-17, dtype=torch.float64), tensor(1.5126e-18, dtype=torch.float64), tensor(0.7265, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.036
  sciq: 0.238
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.726

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.57it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.14it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.19it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.93it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.95it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.35it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.50it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.44it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.19it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.33it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 59.53it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.34it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.6548, 'grad_norm': 0.9576507806777954, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.329704999923706, 'eval_runtime': 7.8188, 'eval_samples_per_second': 127.769, 'eval_steps_per_second': 8.057, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.59it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.42it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.71it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.62it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 51.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.81it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 57.63it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 62.28it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.96it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 62.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.48it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 60.54it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.86it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.95it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.1656, 'grad_norm': 0.21467889845371246, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.0833611488342285, 'eval_runtime': 7.8076, 'eval_samples_per_second': 127.952, 'eval_steps_per_second': 8.069, 'epoch': 0.08}
{'loss': 0.9859, 'grad_norm': 0.2492770552635193, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.925654411315918, 'eval_runtime': 7.8457, 'eval_samples_per_second': 127.33, 'eval_steps_per_second': 8.03, 'epoch': 0.12}
{'loss': 0.8508, 'grad_norm': 0.29628726840019226, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7988722324371338, 'eval_runtime': 7.8453, 'eval_samples_per_second': 127.338, 'eval_steps_per_second': 8.03, 'epoch': 0.16}
{'loss': 0.7893, 'grad_norm': 0.3023386299610138, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.734716534614563, 'eval_runtime': 7.8326, 'eval_samples_per_second': 127.543, 'eval_steps_per_second': 8.043, 'epoch': 0.2}
{'loss': 0.7316, 'grad_norm': 0.3010094165802002, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6782577037811279, 'eval_runtime': 7.8255, 'eval_samples_per_second': 127.659, 'eval_steps_per_second': 8.051, 'epoch': 0.24}
{'loss': 0.6915, 'grad_norm': 0.3102760910987854, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6368899345397949, 'eval_runtime': 7.8424, 'eval_samples_per_second': 127.384, 'eval_steps_per_second': 8.033, 'epoch': 0.28}
{'loss': 0.614, 'grad_norm': 0.421434611082077, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.580838680267334, 'eval_runtime': 7.8563, 'eval_samples_per_second': 127.16, 'eval_steps_per_second': 8.019, 'epoch': 0.32}
{'loss': 0.6107, 'grad_norm': 0.32997047901153564, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5363783836364746, 'eval_runtime': 7.8541, 'eval_samples_per_second': 127.195, 'eval_steps_per_second': 8.021, 'epoch': 0.36}
{'loss': 0.5652, 'grad_norm': 0.3579569458961487, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.4911231994628906, 'eval_runtime': 7.8548, 'eval_samples_per_second': 127.184, 'eval_steps_per_second': 8.021, 'epoch': 0.4}
{'loss': 0.511, 'grad_norm': 0.33795031905174255, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4682523310184479, 'eval_runtime': 7.8564, 'eval_samples_per_second': 127.158, 'eval_steps_per_second': 8.019, 'epoch': 0.44}
{'loss': 0.5279, 'grad_norm': 0.36119794845581055, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.43526437878608704, 'eval_runtime': 7.8499, 'eval_samples_per_second': 127.262, 'eval_steps_per_second': 8.026, 'epoch': 0.48}
{'loss': 0.4539, 'grad_norm': 0.43079641461372375, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.40797021985054016, 'eval_runtime': 7.88, 'eval_samples_per_second': 126.776, 'eval_steps_per_second': 7.995, 'epoch': 0.52}
{'loss': 0.3996, 'grad_norm': 0.3160441517829895, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3803721070289612, 'eval_runtime': 7.863, 'eval_samples_per_second': 127.051, 'eval_steps_per_second': 8.012, 'epoch': 0.56}
{'loss': 0.4198, 'grad_norm': 0.3198055028915405, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.35944655537605286, 'eval_runtime': 7.872, 'eval_samples_per_second': 126.905, 'eval_steps_per_second': 8.003, 'epoch': 0.6}
{'loss': 0.3372, 'grad_norm': 0.33814406394958496, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.34033483266830444, 'eval_runtime': 7.8594, 'eval_samples_per_second': 127.11, 'eval_steps_per_second': 8.016, 'epoch': 0.64}
{'loss': 0.4047, 'grad_norm': 0.3652496039867401, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.33080145716667175, 'eval_runtime': 7.846, 'eval_samples_per_second': 127.325, 'eval_steps_per_second': 8.03, 'epoch': 0.68}
{'loss': 0.3386, 'grad_norm': 0.32756656408309937, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.31067538261413574, 'eval_runtime': 7.8515, 'eval_samples_per_second': 127.236, 'eval_steps_per_second': 8.024, 'epoch': 0.72}
{'loss': 0.3385, 'grad_norm': 0.22579102218151093, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.29857873916625977, 'eval_runtime': 7.8532, 'eval_samples_per_second': 127.21, 'eval_steps_per_second': 8.022, 'epoch': 0.76}
{'loss': 0.3707, 'grad_norm': 0.3870943784713745, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.28818172216415405, 'eval_runtime': 7.8524, 'eval_samples_per_second': 127.223, 'eval_steps_per_second': 8.023, 'epoch': 0.8}
{'loss': 0.3069, 'grad_norm': 0.2121124267578125, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2810455560684204, 'eval_runtime': 7.8589, 'eval_samples_per_second': 127.118, 'eval_steps_per_second': 8.016, 'epoch': 0.84}
{'loss': 0.3128, 'grad_norm': 0.2011594921350479, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2748791575431824, 'eval_runtime': 7.8685, 'eval_samples_per_second': 126.962, 'eval_steps_per_second': 8.007, 'epoch': 0.88}
{'loss': 0.3652, 'grad_norm': 0.15208640694618225, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2710474133491516, 'eval_runtime': 7.8572, 'eval_samples_per_second': 127.144, 'eval_steps_per_second': 8.018, 'epoch': 0.92}
{'loss': 0.3034, 'grad_norm': 0.28165706992149353, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2681426703929901, 'eval_runtime': 7.8551, 'eval_samples_per_second': 127.179, 'eval_steps_per_second': 8.02, 'epoch': 0.96}
{'loss': 0.3024, 'grad_norm': 0.26846185326576233, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2673053443431854, 'eval_runtime': 7.8659, 'eval_samples_per_second': 127.004, 'eval_steps_per_second': 8.009, 'epoch': 1.0}
{'train_runtime': 434.9875, 'train_samples_per_second': 22.985, 'train_steps_per_second': 1.437, 'train_loss': 0.6140808822631836, 'epoch': 1.0}
train_results:  {'eval_loss': [1.329704999923706, 1.0833611488342285, 0.925654411315918, 0.7988722324371338, 0.734716534614563, 0.6782577037811279, 0.6368899345397949, 0.580838680267334, 0.5363783836364746, 0.4911231994628906, 0.4682523310184479, 0.43526437878608704, 0.40797021985054016, 0.3803721070289612, 0.35944655537605286, 0.34033483266830444, 0.33080145716667175, 0.31067538261413574, 0.29857873916625977, 0.28818172216415405, 0.2810455560684204, 0.2748791575431824, 0.2710474133491516, 0.2681426703929901, 0.2673053443431854], 'performance': [0.62, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.94it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.91it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.95it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 62.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 61.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2502437829971313
current iteration best possible performance (full train run):  0.63
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614, 1.2460026741027832, 1.2486201524734497, 1.2484309673309326, 1.250274896621704, 1.2502437829971313]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9702 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.9485133372558361
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.9427824068040773
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.9475485634805253
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.9485132622793535
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.9485124718967859
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0971, dtype=torch.float64), 0, 0, tensor(0.1833, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7196, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.07447085693152322, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0971, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1833, dtype=torch.float64), tensor(5.9530e-16, dtype=torch.float64), tensor(8.1347e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9155e-17, dtype=torch.float64), tensor(0.7196, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7447, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.097
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.183
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.72

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07447085693152322,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.07447085693152322
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.43it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.90it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 45.55it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 48.34it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 52.40it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.47it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 57.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 58.30it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.66it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.6432, 'grad_norm': 0.5731077790260315, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.2245594263076782, 'eval_runtime': 6.7524, 'eval_samples_per_second': 147.947, 'eval_steps_per_second': 9.33, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.50it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.91it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.44it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.21it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.69it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 55.59it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 57.86it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 61.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.00it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.51it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.26it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.52it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 58.66it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.60it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.0837, 'grad_norm': 0.2581859529018402, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.008122444152832, 'eval_runtime': 6.7769, 'eval_samples_per_second': 147.413, 'eval_steps_per_second': 9.296, 'epoch': 0.08}
{'loss': 0.9387, 'grad_norm': 0.23545975983142853, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8582473993301392, 'eval_runtime': 6.7988, 'eval_samples_per_second': 146.938, 'eval_steps_per_second': 9.266, 'epoch': 0.12}
{'loss': 0.7994, 'grad_norm': 0.2278270274400711, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7432541251182556, 'eval_runtime': 6.7962, 'eval_samples_per_second': 146.995, 'eval_steps_per_second': 9.27, 'epoch': 0.16}
{'loss': 0.7369, 'grad_norm': 0.27641984820365906, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6853697896003723, 'eval_runtime': 6.8199, 'eval_samples_per_second': 146.483, 'eval_steps_per_second': 9.238, 'epoch': 0.2}
{'loss': 0.6907, 'grad_norm': 0.2720950245857239, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6338309645652771, 'eval_runtime': 6.8163, 'eval_samples_per_second': 146.56, 'eval_steps_per_second': 9.243, 'epoch': 0.24}
{'loss': 0.627, 'grad_norm': 0.30271467566490173, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5645421147346497, 'eval_runtime': 6.8105, 'eval_samples_per_second': 146.686, 'eval_steps_per_second': 9.25, 'epoch': 0.28}
{'loss': 0.5956, 'grad_norm': 0.3502449095249176, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5086330771446228, 'eval_runtime': 6.8148, 'eval_samples_per_second': 146.594, 'eval_steps_per_second': 9.245, 'epoch': 0.32}
{'loss': 0.5211, 'grad_norm': 0.3301312327384949, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4553234875202179, 'eval_runtime': 6.8127, 'eval_samples_per_second': 146.638, 'eval_steps_per_second': 9.247, 'epoch': 0.36}
{'loss': 0.4976, 'grad_norm': 0.3082423508167267, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.4201522171497345, 'eval_runtime': 6.805, 'eval_samples_per_second': 146.805, 'eval_steps_per_second': 9.258, 'epoch': 0.4}
{'loss': 0.4483, 'grad_norm': 0.31723925471305847, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3857016861438751, 'eval_runtime': 6.7769, 'eval_samples_per_second': 147.413, 'eval_steps_per_second': 9.296, 'epoch': 0.44}
{'loss': 0.4498, 'grad_norm': 0.4179231822490692, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.355516254901886, 'eval_runtime': 6.7995, 'eval_samples_per_second': 146.922, 'eval_steps_per_second': 9.265, 'epoch': 0.48}
{'loss': 0.4028, 'grad_norm': 0.3471006155014038, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.33360064029693604, 'eval_runtime': 6.8185, 'eval_samples_per_second': 146.513, 'eval_steps_per_second': 9.24, 'epoch': 0.52}
{'loss': 0.3589, 'grad_norm': 0.3303576409816742, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3116741478443146, 'eval_runtime': 6.8438, 'eval_samples_per_second': 145.971, 'eval_steps_per_second': 9.205, 'epoch': 0.56}
{'loss': 0.3407, 'grad_norm': 0.3630932569503784, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.29022443294525146, 'eval_runtime': 6.8239, 'eval_samples_per_second': 146.398, 'eval_steps_per_second': 9.232, 'epoch': 0.6}
{'loss': 0.3482, 'grad_norm': 0.27851203083992004, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.27450600266456604, 'eval_runtime': 6.8228, 'eval_samples_per_second': 146.421, 'eval_steps_per_second': 9.234, 'epoch': 0.64}
{'loss': 0.3291, 'grad_norm': 0.327447772026062, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2567874789237976, 'eval_runtime': 6.826, 'eval_samples_per_second': 146.353, 'eval_steps_per_second': 9.229, 'epoch': 0.68}
{'loss': 0.3155, 'grad_norm': 0.25957873463630676, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.2466391921043396, 'eval_runtime': 6.8196, 'eval_samples_per_second': 146.489, 'eval_steps_per_second': 9.238, 'epoch': 0.72}
{'loss': 0.31, 'grad_norm': 0.3644479513168335, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.23814748227596283, 'eval_runtime': 6.8275, 'eval_samples_per_second': 146.319, 'eval_steps_per_second': 9.227, 'epoch': 0.76}
{'loss': 0.2853, 'grad_norm': 0.32933828234672546, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.23022031784057617, 'eval_runtime': 6.8204, 'eval_samples_per_second': 146.472, 'eval_steps_per_second': 9.237, 'epoch': 0.8}
{'loss': 0.3061, 'grad_norm': 0.2650512754917145, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2208835333585739, 'eval_runtime': 6.8179, 'eval_samples_per_second': 146.525, 'eval_steps_per_second': 9.24, 'epoch': 0.84}
{'loss': 0.2816, 'grad_norm': 0.2127482146024704, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.21685738861560822, 'eval_runtime': 6.8346, 'eval_samples_per_second': 146.168, 'eval_steps_per_second': 9.218, 'epoch': 0.88}
{'loss': 0.2724, 'grad_norm': 0.145756796002388, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.21230268478393555, 'eval_runtime': 6.8437, 'eval_samples_per_second': 145.975, 'eval_steps_per_second': 9.206, 'epoch': 0.92}
{'loss': 0.2455, 'grad_norm': 0.23456384241580963, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.20975437760353088, 'eval_runtime': 6.8059, 'eval_samples_per_second': 146.785, 'eval_steps_per_second': 9.257, 'epoch': 0.96}
{'loss': 0.2397, 'grad_norm': 0.2203637808561325, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.20902366936206818, 'eval_runtime': 6.7862, 'eval_samples_per_second': 147.211, 'eval_steps_per_second': 9.284, 'epoch': 1.0}
{'train_runtime': 385.9089, 'train_samples_per_second': 25.91, 'train_steps_per_second': 1.62, 'train_loss': 0.5627021385192871, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2245594263076782, 1.008122444152832, 0.8582473993301392, 0.7432541251182556, 0.6853697896003723, 0.6338309645652771, 0.5645421147346497, 0.5086330771446228, 0.4553234875202179, 0.4201522171497345, 0.3857016861438751, 0.355516254901886, 0.33360064029693604, 0.3116741478443146, 0.29022443294525146, 0.27450600266456604, 0.2567874789237976, 0.2466391921043396, 0.23814748227596283, 0.23022031784057617, 0.2208835333585739, 0.21685738861560822, 0.21230268478393555, 0.20975437760353088, 0.20902366936206818], 'performance': [0.63, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.80it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.84it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.07it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.73it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.51it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.24it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.28it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2503771781921387
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614, 1.2460026741027832, 1.2486201524734497, 1.2484309673309326, 1.250274896621704, 1.2502437829971313, 1.2503771781921387]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2670 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.9102102656856554
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.9556753550273269
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.9556748724251796
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.7920490946211709
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.9476295042524435
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0251, dtype=torch.float64), 0, tensor(0.1252, dtype=torch.float64), tensor(0.1383, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7115, dtype=torch.float64), 32, 1, 0, 1, 1, 0, 128, 0.06890455699892178, 47.999999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(0.0251, dtype=torch.float64), tensor(6.7074e-17, dtype=torch.float64), tensor(0.1252, dtype=torch.float64), tensor(0.1383, dtype=torch.float64), tensor(3.0150e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.4450e-16, dtype=torch.float64), tensor(0.7115, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.6890, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.025
  gsm8k: 0
  rowan_hellaswag: 0.125
  sciq: 0.138
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.711

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.06890455699892178,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 0],)
  lora_alpha: (47.999999999999986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 0]
lora rank:  128
lora dropout:  0.06890455699892178
lora alpha:  47.999999999999986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.56it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.32it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.55it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.71it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.42it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.97it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 56.58it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.66it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.65it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 57.65it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 61.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.36it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.647, 'grad_norm': 0.45712268352508545, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.3443889617919922, 'eval_runtime': 9.4029, 'eval_samples_per_second': 106.244, 'eval_steps_per_second': 6.7, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.51it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.91it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.00it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.96it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.36it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 56.71it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 59.08it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 63.29it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.27it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 62.81it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.40it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 60.17it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 58.59it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.1579, 'grad_norm': 0.4166897237300873, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.071563482284546, 'eval_runtime': 9.3985, 'eval_samples_per_second': 106.294, 'eval_steps_per_second': 6.703, 'epoch': 0.08}
{'loss': 1.0752, 'grad_norm': 0.20754870772361755, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0117204189300537, 'eval_runtime': 9.3944, 'eval_samples_per_second': 106.34, 'eval_steps_per_second': 6.706, 'epoch': 0.12}
{'loss': 0.9568, 'grad_norm': 0.25339752435684204, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9632164239883423, 'eval_runtime': 9.4056, 'eval_samples_per_second': 106.213, 'eval_steps_per_second': 6.698, 'epoch': 0.16}
{'loss': 0.8912, 'grad_norm': 0.25209447741508484, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9266694784164429, 'eval_runtime': 9.4259, 'eval_samples_per_second': 105.984, 'eval_steps_per_second': 6.684, 'epoch': 0.2}
{'loss': 0.8878, 'grad_norm': 0.2551063299179077, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8741520047187805, 'eval_runtime': 9.4005, 'eval_samples_per_second': 106.271, 'eval_steps_per_second': 6.702, 'epoch': 0.24}
{'loss': 0.8812, 'grad_norm': 0.28386572003364563, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8195693492889404, 'eval_runtime': 9.4016, 'eval_samples_per_second': 106.258, 'eval_steps_per_second': 6.701, 'epoch': 0.28}
{'loss': 0.8578, 'grad_norm': 0.3067331612110138, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7821318507194519, 'eval_runtime': 9.3829, 'eval_samples_per_second': 106.471, 'eval_steps_per_second': 6.714, 'epoch': 0.32}
{'loss': 0.8424, 'grad_norm': 0.317612886428833, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7478141188621521, 'eval_runtime': 9.3887, 'eval_samples_per_second': 106.405, 'eval_steps_per_second': 6.71, 'epoch': 0.36}
{'loss': 0.8304, 'grad_norm': 0.41498863697052, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6957924962043762, 'eval_runtime': 9.3833, 'eval_samples_per_second': 106.466, 'eval_steps_per_second': 6.714, 'epoch': 0.4}
{'loss': 0.6994, 'grad_norm': 0.3418397307395935, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6593605279922485, 'eval_runtime': 9.3908, 'eval_samples_per_second': 106.381, 'eval_steps_per_second': 6.709, 'epoch': 0.44}
{'loss': 0.724, 'grad_norm': 0.3630392551422119, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6300938725471497, 'eval_runtime': 9.404, 'eval_samples_per_second': 106.231, 'eval_steps_per_second': 6.699, 'epoch': 0.48}
{'loss': 0.6793, 'grad_norm': 0.3547889292240143, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6126763820648193, 'eval_runtime': 9.3969, 'eval_samples_per_second': 106.312, 'eval_steps_per_second': 6.704, 'epoch': 0.52}
{'loss': 0.5519, 'grad_norm': 0.35551708936691284, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5935404300689697, 'eval_runtime': 9.3949, 'eval_samples_per_second': 106.334, 'eval_steps_per_second': 6.706, 'epoch': 0.56}
{'loss': 0.6861, 'grad_norm': 0.42801013588905334, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5684551000595093, 'eval_runtime': 9.4073, 'eval_samples_per_second': 106.194, 'eval_steps_per_second': 6.697, 'epoch': 0.6}
{'loss': 0.6133, 'grad_norm': 0.32877570390701294, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5536133646965027, 'eval_runtime': 9.4014, 'eval_samples_per_second': 106.261, 'eval_steps_per_second': 6.701, 'epoch': 0.64}
{'loss': 0.6435, 'grad_norm': 0.28624701499938965, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5317134857177734, 'eval_runtime': 9.4, 'eval_samples_per_second': 106.277, 'eval_steps_per_second': 6.702, 'epoch': 0.68}
{'loss': 0.5361, 'grad_norm': 0.3301197290420532, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5161709785461426, 'eval_runtime': 9.4028, 'eval_samples_per_second': 106.245, 'eval_steps_per_second': 6.7, 'epoch': 0.72}
{'loss': 0.6309, 'grad_norm': 0.20777596533298492, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5031304359436035, 'eval_runtime': 9.4292, 'eval_samples_per_second': 105.947, 'eval_steps_per_second': 6.681, 'epoch': 0.76}
{'loss': 0.611, 'grad_norm': 0.28468483686447144, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4950554370880127, 'eval_runtime': 9.4603, 'eval_samples_per_second': 105.599, 'eval_steps_per_second': 6.659, 'epoch': 0.8}
{'loss': 0.5485, 'grad_norm': 0.2244090735912323, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4861682653427124, 'eval_runtime': 9.5187, 'eval_samples_per_second': 104.952, 'eval_steps_per_second': 6.619, 'epoch': 0.84}
{'loss': 0.5018, 'grad_norm': 0.2632492184638977, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.47843509912490845, 'eval_runtime': 9.5202, 'eval_samples_per_second': 104.935, 'eval_steps_per_second': 6.618, 'epoch': 0.88}
{'loss': 0.5399, 'grad_norm': 0.21954919397830963, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4731866121292114, 'eval_runtime': 9.5213, 'eval_samples_per_second': 104.923, 'eval_steps_per_second': 6.617, 'epoch': 0.92}
{'loss': 0.4982, 'grad_norm': 0.2058628499507904, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.47087761759757996, 'eval_runtime': 9.52, 'eval_samples_per_second': 104.937, 'eval_steps_per_second': 6.618, 'epoch': 0.96}
{'loss': 0.4749, 'grad_norm': 0.2342301458120346, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.46973034739494324, 'eval_runtime': 9.5095, 'eval_samples_per_second': 105.053, 'eval_steps_per_second': 6.625, 'epoch': 1.0}
{'train_runtime': 510.5785, 'train_samples_per_second': 19.582, 'train_steps_per_second': 1.224, 'train_loss': 0.7986559143066406, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3443889617919922, 1.071563482284546, 1.0117204189300537, 0.9632164239883423, 0.9266694784164429, 0.8741520047187805, 0.8195693492889404, 0.7821318507194519, 0.7478141188621521, 0.6957924962043762, 0.6593605279922485, 0.6300938725471497, 0.6126763820648193, 0.5935404300689697, 0.5684551000595093, 0.5536133646965027, 0.5317134857177734, 0.5161709785461426, 0.5031304359436035, 0.4950554370880127, 0.4861682653427124, 0.47843509912490845, 0.4731866121292114, 0.47087761759757996, 0.46973034739494324], 'performance': [0.62, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:30,  3.22it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.61it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.93it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 64.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.36it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2486722469329834
current iteration best possible performance (full train run):  0.651
max performance so far:  0.672
BO observations:  [1.1595380306243896, 1.1163580417633057, 0.9322792887687683, 1.085510015487671, 1.2469189167022705, 1.1680582761764526, 1.2458372116088867, 1.245718240737915, 1.2479344606399536, 1.2451167106628418, 1.240851640701294, 1.2448790073394775, 1.2403504848480225, 1.2392396926879883, 1.1252690553665161, 1.2454429864883423, 1.2474339008331299, 1.2465016841888428, 1.2478680610656738, 1.2454702854156494, 1.2468407154083252, 1.2372987270355225, 1.2502251863479614, 1.2460026741027832, 1.2486201524734497, 1.2484309673309326, 1.250274896621704, 1.2502437829971313, 1.2503771781921387, 1.2486722469329834]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5553 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.9316579945277116
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.717521166431872
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.9316580237535237
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.9316580072099917
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.9119341393547484
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1024, dtype=torch.float64), 0, 0, tensor(0.1752, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7223, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.08604752949935573, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.1024, dtype=torch.float64), tensor(1.6419e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1752, dtype=torch.float64), tensor(2.3353e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.9214e-17, dtype=torch.float64), tensor(1.5435e-16, dtype=torch.float64), tensor(0.7223, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8605, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.6405, 0.6405, 0.6405, 0.6405, 0.651, 0.651, 0.651, 0.651, 0.651, 0.6615000000000001, 0.6615000000000001, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/performance_H25_50_T625_curve/triviaqa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 2, 0, 0, 0, 1, 0, 29, 0.05396168787624587, 37, 1]
Checking history sample input_X_between_0_1:  [0.0025890410871690145, 0.13781070651430954, 0.02905723080608936, 0.19744438924302404, 0.013566899908320463, 0.13836243865941616, 0.1340311250444947, 0.16136200140000634, 0.18577616733717042, 0.0625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2265625, 0.5396168787624587, 0.7708333333333334, 1.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 7, 0, 0, 1, 0, 0, 101, 0.003930648435578799, 29, 1]
Checking history sample input_X_between_0_1:  [0.03545473243073316, 0.35349904465273074, 0.03332128122073966, 0.1808740505168125, 0.08187017732765006, 0.17466022659195388, 0.03460523705251845, 0.0510531257873455, 0.05466212441951601, 0.21875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7890625, 0.039306484355787985, 0.6041666666666666, 1.0]
Checking history sample performance at 625 steps:  0.56
Checking history sample input_X:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 22, 1, 0, 0, 1, 1, 3, 0.04742737743265794, 11, 1]
Checking history sample input_X_between_0_1:  [0.09756724004836205, 0.025805262942956896, 0.010953245500186082, 0.13157879221671273, 0.06187680540408013, 0.3486050332797147, 0.09325295299971124, 0.0375077220595116, 0.19285294554876461, 0.6875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0234375, 0.47427377432657936, 0.22916666666666666, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 13, 1, 0, 1, 1, 1, 123, 0.09371289547650785, 48, 0]
Checking history sample input_X_between_0_1:  [0.041299013800231515, 0.24333779014945742, 0.0773030088346414, 0.26146887695009063, 0.02481664082840471, 0.14290754990585744, 0.04402652425456659, 0.09397461893838965, 0.07086597633836066, 0.40625, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9609375, 0.9371289547650785, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.62
Checking history sample input_X:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 13, 0, 0, 1, 1, 0, 127, 0.09560839072699708, 9, 1]
Checking history sample input_X_between_0_1:  [0.13828807979823884, 0.23813371519660878, 0.09301426708805763, 0.05769474253953403, 0.005966749821967281, 0.02600228437820524, 0.3388153009459044, 0.0742899628886265, 0.027794897342857224, 0.40625, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9921875, 0.9560839072699707, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 9, 0, 0, 1, 1, 1, 16, 0.08052765798611784, 41, 1]
Checking history sample input_X_between_0_1:  [0.12030323199724466, 0.4143454145821148, 0.00177433710650165, 0.016164237120967498, 0.19634861200102968, 0.07350605644605224, 0.04855801953456344, 0.10089341393186044, 0.0281066772796657, 0.28125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.8052765798611784, 0.8541666666666666, 1.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 5, 0, 1, 0, 1, 0, 59, 0.05350476033435025, 24, 1]
Checking history sample input_X_between_0_1:  [0.08902921960761732, 0.26876453096262165, 0.03506751141466781, 0.07113052541738814, 0.022387203379859864, 0.031115262537760743, 0.13641625240563676, 0.3358156364623844, 0.010273857812063436, 0.15625, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4609375, 0.5350476033435024, 0.5, 1.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 2, 1, 0, 0, 1, 0, 52, 0.0964290393747826, 9, 0]
Checking history sample input_X_between_0_1:  [0.2524527684972841, 0.1744070581068035, 0.10461249445607207, 0.15440066352142304, 0.0700283884089386, 0.0049384966617366756, 0.113306568367722, 0.10296553927979063, 0.022888022700229552, 0.0625, 1.0, 0.0, 0.0, 1.0, 0.0, 0.40625, 0.964290393747826, 0.1875, 0.0]
Checking history sample performance at 625 steps:  0.44
Checking history sample input_X:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 7, 1, 1, 0, 0, 1, 121, 0.029877603091235272, 27, 0]
Checking history sample input_X_between_0_1:  [0.29232173269029876, 0.17693312913429507, 0.012858019852433568, 0.03286664379720002, 0.2625013596703321, 0.11047823026185605, 0.003382139414079334, 0.024505946890111277, 0.08415279828939379, 0.21875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9453125, 0.2987760309123527, 0.5625, 0.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 5, 1, 1, 0, 0, 0, 99, 0.012004498902397865, 15, 0]
Checking history sample input_X_between_0_1:  [0.020035337801137663, 0.10399700182048381, 0.3892321047123048, 0.06684496502695834, 0.06878002385488091, 0.10067783052619873, 0.017231237316925774, 0.09944853448860064, 0.13375296445250937, 0.15625, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7734375, 0.12004498902397864, 0.3125, 0.0]
Checking history sample performance at 625 steps:  0.57
Checking history sample input_X:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 24, 1, 0, 1, 1, 1, 39, 0.002627655727844236, 41, 0]
Checking history sample input_X_between_0_1:  [0.056946090015311174, 0.04767143759071871, 0.09148929761785743, 0.2225862197075886, 0.07769581649173836, 0.1157450381963471, 0.07010144250850459, 0.10265404297518957, 0.21511061489674455, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3046875, 0.02627655727844236, 0.8541666666666666, 0.0]
Checking history sample performance at 625 steps:  0.61
Checking history sample input_X:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 24, 0, 0, 0, 0, 1, 93, 0.03163409179138251, 48, 0]
Checking history sample input_X_between_0_1:  [0.08704919640423543, 0.31923775477359795, 0.14131022616782132, 0.03538458441113131, 0.09172757496103237, 0.13225452027430123, 0.08144481122647668, 0.002148850246311566, 0.10944248153509237, 0.75, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7265625, 0.31634091791382507, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 28, 1, 0, 0, 0, 0, 89, 0.0740409362882843, 48, 0]
Checking history sample input_X_between_0_1:  [0.1648606021324061, 0.1570907563130515, 0.08988652806102636, 0.06221194665716195, 0.08591082606981566, 0.030806588121181165, 0.09351413443319909, 0.05953940139785769, 0.2561792168143003, 0.875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.740409362882843, 1.0, 0.0]
Checking history sample performance at 625 steps:  0.63
Checking history sample input_X:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 28, 1, 0, 1, 1, 1, 2, 0.052382422456560135, 28, 1]
Checking history sample input_X_between_0_1:  [0.1857657841696568, 0.025715438342757906, 0.030940439858736887, 0.23695799540048462, 0.08906568554320403, 0.04092778609320657, 0.046529521338003, 0.32818853152172184, 0.01590881773222842, 0.875, 1.0, 0.0, 1.0, 1.0, 1.0, 0.015625, 0.5238242245656013, 0.5833333333333334, 1.0]
Checking history sample performance at 625 steps:  0.59
Checking history sample input_X:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 11, 0, 0, 0, 1, 1, 54, 0.06588137610749685, 20, 1]
Checking history sample input_X_between_0_1:  [0.008462868116396433, 0.001495956458368692, 0.02053395216629507, 0.14102167835393783, 0.10515248450290147, 0.5550628847832677, 0.02335912587225868, 0.1284865739116555, 0.01642447583491859, 0.34375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.421875, 0.6588137610749685, 0.4166666666666667, 1.0]
Checking history sample performance at 625 steps:  0.54
Checking history sample input_X:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 27, 0, 0, 1, 1, 1, 76, 0.04383658958637696, 17, 0]
Checking history sample input_X_between_0_1:  [0.019005511814349393, 0.006038376495934501, 0.05890668142836401, 0.23282069076884948, 0.14214561673825565, 0.16613235184687988, 0.3163373464332231, 0.010005137783184972, 0.04860828669095915, 0.84375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.59375, 0.4383658958637696, 0.3541666666666667, 0.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 29, 0, 1, 0, 0, 1, 82, 0.07326489367366754, 40, 1]
Checking history sample input_X_between_0_1:  [0.03377081240431872, 0.2030779231792611, 0.01563162213255898, 0.0038507163279195475, 0.1471217239280814, 0.16421055336580767, 0.011839623666048224, 0.14920665420613816, 0.27129037078986606, 0.90625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.640625, 0.7326489367366753, 0.8333333333333334, 1.0]
Checking history sample performance at 625 steps:  0.53
Checking history sample input_X:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 1, 0, 0, 0, 1, 0, 44, 0.09959850306478138, 19, 0]
Checking history sample input_X_between_0_1:  [0.0666348143121966, 0.0017807756282038033, 0.04992851923338283, 0.009631889464321607, 0.12776533059783662, 0.11217100841885941, 0.1345111613095854, 0.23778726850819906, 0.25978923252741454, 0.03125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.34375, 0.9959850306478137, 0.3958333333333333, 0.0]
Checking history sample performance at 625 steps:  0.52
Checking history sample input_X:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 26, 1, 1, 1, 0, 1, 67, 0.08108306564701856, 9, 1]
Checking history sample input_X_between_0_1:  [0.06325401662046672, 0.06711742573240631, 0.3644727115393719, 0.06545091227405714, 0.10357992429069907, 0.012703345727807019, 0.06847753867763928, 0.15250964020205912, 0.10243448493549337, 0.8125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5234375, 0.8108306564701856, 0.1875, 1.0]
Checking history sample performance at 625 steps:  0.6
Checking history sample input_X:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 29, 0, 0, 0, 1, 1, 29, 0.08410329802499458, 20, 0]
Checking history sample input_X_between_0_1:  [8.217148863957131e-05, 0.1147867028301568, 0.06094507328781116, 0.16196899984460916, 0.11096251293916237, 0.06328254290156585, 0.2071963441084196, 0.05558426297728622, 0.22519138962234916, 0.90625, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2265625, 0.8410329802499458, 0.4166666666666667, 0.0]
Checking history sample performance at 625 steps:  0.61
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7154 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9698254466056824, 0.6256901621818542, 0.12144768238067627, 0.9695647954940796, 0.3494873046875, 0.3794281482696533, 0.11738818883895874, 0.9692235589027405, 0.9673594832420349, 0.20154891908168793, 0.34403765201568604, 0.28768932819366455, 0.8336107730865479, 0.11505532264709473, 0.32231462001800537, 0.23259741067886353, 0.27493399381637573, 0.9495991468429565, 0.8650606870651245]  ‚Üí  acq = 0.7000464480432271
X = [0.11209297180175781, 0.5485275387763977, 0.45425790548324585, 0.5119083523750305, 0.48880404233932495, 0.5432374477386475, 0.8431985378265381, 0.8066792488098145, 0.455693781375885, 0.7645586729049683, 0.44062334299087524, 0.8993340134620667, 0.10184741020202637, 0.5459865927696228, 0.9886246919631958, 0.653795599937439, 0.9764446020126343, 0.3780645728111267, 0.9932035803794861]  ‚Üí  acq = 0.7000464480432271
X = [0.08119475841522217, 0.4292720556259155, 0.25442206859588623, 0.8463194370269775, 0.3760976791381836, 0.07603275775909424, 0.67503821849823, 0.743344783782959, 0.5346527695655823, 0.11102241277694702, 0.6170213222503662, 0.9881593585014343, 0.7411287426948547, 0.45153743028640747, 0.835287868976593, 0.4669220745563507, 0.48337090015411377, 0.17927710711956024, 0.738283634185791]  ‚Üí  acq = 0.7000464480432271
X = [0.39439094066619873, 0.44772785902023315, 0.8567600846290588, 0.6580475568771362, 0.471177875995636, 0.29246973991394043, 0.1875823736190796, 0.3965558409690857, 0.0722048282623291, 0.31250903010368347, 0.05333912372589111, 0.712388813495636, 0.44666004180908203, 0.12340092658996582, 0.20336157083511353, 0.40940308570861816, 0.4689701199531555, 0.15155306458473206, 0.00869441032409668]  ‚Üí  acq = 0.7000464480432271
X = [0.673606812953949, 0.9277408719062805, 0.35161590576171875, 0.7000433802604675, 0.8237881064414978, 0.7738629579544067, 0.06280273199081421, 0.6325397491455078, 0.7173249125480652, 0.0737546756863594, 0.6253786683082581, 0.7490109205245972, 0.6915088891983032, 0.32707828283309937, 0.7825837135314941, 0.040756650269031525, 0.17992275953292847, 0.934341549873352, 0.3486214876174927]  ‚Üí  acq = 0.7000464480432271
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.0778, dtype=torch.float64), tensor(0.0172, dtype=torch.float64), tensor(0.1145, dtype=torch.float64), 0, 0, tensor(0.1100, dtype=torch.float64), 0, tensor(0.0722, dtype=torch.float64), tensor(0.6015, dtype=torch.float64), 32, 1, 0, 0, 0, 0, 119, 0.018879750832746307, 31.43773300116672, 1]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.0778, dtype=torch.float64), tensor(0.0172, dtype=torch.float64), tensor(0.1145, dtype=torch.float64), tensor(2.8888e-19, dtype=torch.float64), tensor(0.0069, dtype=torch.float64), tensor(0.1100, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0722, dtype=torch.float64), tensor(0.6015, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.9294, dtype=torch.float64), tensor(0.1888, dtype=torch.float64), tensor(0.6550, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.078
  gsm8k: 0.017
  rowan_hellaswag: 0.114
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.11
  wikitext: 0
  mmlu: 0.072
  arc_challenge: 0.601

LoRA Parameters:
  lora_r: (119,)
  lora_dropout: (0.018879750832746307,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 0],)
  lora_alpha: (31.43773300116672,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 0]
lora rank:  119
lora dropout:  0.018879750832746307
lora alpha:  31.43773300116672
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 31,195,136 || all params: 8,061,456,384 || trainable%: 0.3870
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9927
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  992
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.27it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 41.03it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 54.18it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 60.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 64.14it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 70.44it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 73.38it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 76.76it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 76.18it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 75.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 70.49it/s]
Evaluation performance at step 25: 0.62
{'loss': 3.8079, 'grad_norm': 0.3902214765548706, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 2.8563172817230225, 'eval_runtime': 8.515, 'eval_samples_per_second': 116.501, 'eval_steps_per_second': 7.281, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<01:29,  1.10it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:08, 11.19it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:03, 21.40it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:02, 30.42it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 38.79it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 47.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.43it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 60.07it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 67.89it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 68.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 70.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 44.46it/s]
Evaluation performance at step 50: 0.62
{'loss': 2.1642, 'grad_norm': 0.21362020075321198, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.7288600206375122, 'eval_runtime': 8.5008, 'eval_samples_per_second': 116.694, 'eval_steps_per_second': 7.293, 'epoch': 0.08}
{'loss': 1.6166, 'grad_norm': 0.14733417332172394, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.5137883424758911, 'eval_runtime': 8.5674, 'eval_samples_per_second': 115.788, 'eval_steps_per_second': 7.237, 'epoch': 0.12}
{'loss': 1.5003, 'grad_norm': 0.15216663479804993, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.4674392938613892, 'eval_runtime': 8.5647, 'eval_samples_per_second': 115.824, 'eval_steps_per_second': 7.239, 'epoch': 0.16}
{'loss': 1.5035, 'grad_norm': 0.11951503157615662, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.440880537033081, 'eval_runtime': 8.6076, 'eval_samples_per_second': 115.248, 'eval_steps_per_second': 7.203, 'epoch': 0.2}
{'loss': 1.4583, 'grad_norm': 0.1378725916147232, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.4254571199417114, 'eval_runtime': 8.6091, 'eval_samples_per_second': 115.227, 'eval_steps_per_second': 7.202, 'epoch': 0.24}
{'loss': 1.4198, 'grad_norm': 0.1421234905719757, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.4134190082550049, 'eval_runtime': 8.6264, 'eval_samples_per_second': 114.996, 'eval_steps_per_second': 7.187, 'epoch': 0.28}
{'loss': 1.4346, 'grad_norm': 0.12068663537502289, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 1.4046744108200073, 'eval_runtime': 8.6121, 'eval_samples_per_second': 115.187, 'eval_steps_per_second': 7.199, 'epoch': 0.32}
{'loss': 1.3972, 'grad_norm': 0.14830926060676575, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 1.3958812952041626, 'eval_runtime': 8.6313, 'eval_samples_per_second': 114.93, 'eval_steps_per_second': 7.183, 'epoch': 0.36}
{'loss': 1.3706, 'grad_norm': 0.1469094604253769, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 1.3897696733474731, 'eval_runtime': 8.6278, 'eval_samples_per_second': 114.977, 'eval_steps_per_second': 7.186, 'epoch': 0.4}
{'loss': 1.3719, 'grad_norm': 0.11975415050983429, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 1.3833813667297363, 'eval_runtime': 8.6267, 'eval_samples_per_second': 114.991, 'eval_steps_per_second': 7.187, 'epoch': 0.44}
{'loss': 1.414, 'grad_norm': 0.13947470486164093, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 1.3755131959915161, 'eval_runtime': 8.6299, 'eval_samples_per_second': 114.949, 'eval_steps_per_second': 7.184, 'epoch': 0.48}
{'loss': 1.4019, 'grad_norm': 0.12098061293363571, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 1.372102975845337, 'eval_runtime': 8.6319, 'eval_samples_per_second': 114.922, 'eval_steps_per_second': 7.183, 'epoch': 0.52}
{'loss': 1.3751, 'grad_norm': 0.14025849103927612, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 1.3658655881881714, 'eval_runtime': 8.6292, 'eval_samples_per_second': 114.959, 'eval_steps_per_second': 7.185, 'epoch': 0.56}
{'loss': 1.3776, 'grad_norm': 0.1409805417060852, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 1.3592689037322998, 'eval_runtime': 8.6157, 'eval_samples_per_second': 115.139, 'eval_steps_per_second': 7.196, 'epoch': 0.6}
{'loss': 1.3758, 'grad_norm': 0.1244363933801651, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 1.355679988861084, 'eval_runtime': 8.6186, 'eval_samples_per_second': 115.099, 'eval_steps_per_second': 7.194, 'epoch': 0.64}
{'loss': 1.3506, 'grad_norm': 0.14262627065181732, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 1.354159951210022, 'eval_runtime': 8.5775, 'eval_samples_per_second': 115.652, 'eval_steps_per_second': 7.228, 'epoch': 0.68}
{'loss': 1.3712, 'grad_norm': 0.12517674267292023, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 1.3480780124664307, 'eval_runtime': 8.5595, 'eval_samples_per_second': 115.894, 'eval_steps_per_second': 7.243, 'epoch': 0.72}
{'loss': 1.3776, 'grad_norm': 0.13787764310836792, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 1.3452495336532593, 'eval_runtime': 8.5579, 'eval_samples_per_second': 115.916, 'eval_steps_per_second': 7.245, 'epoch': 0.76}
{'loss': 1.3822, 'grad_norm': 0.16784697771072388, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 1.34223473072052, 'eval_runtime': 8.5627, 'eval_samples_per_second': 115.852, 'eval_steps_per_second': 7.241, 'epoch': 0.81}
{'loss': 1.3778, 'grad_norm': 0.1438242495059967, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 1.3397977352142334, 'eval_runtime': 8.5692, 'eval_samples_per_second': 115.763, 'eval_steps_per_second': 7.235, 'epoch': 0.85}
{'loss': 1.3676, 'grad_norm': 0.15287993848323822, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 1.3372218608856201, 'eval_runtime': 8.5683, 'eval_samples_per_second': 115.776, 'eval_steps_per_second': 7.236, 'epoch': 0.89}
{'loss': 1.3814, 'grad_norm': 0.15167054533958435, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 1.335562825202942, 'eval_runtime': 8.5674, 'eval_samples_per_second': 115.787, 'eval_steps_per_second': 7.237, 'epoch': 0.93}
{'loss': 1.3518, 'grad_norm': 0.1410958468914032, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 1.3345218896865845, 'eval_runtime': 8.5837, 'eval_samples_per_second': 115.568, 'eval_steps_per_second': 7.223, 'epoch': 0.97}
{'train_runtime': 460.9282, 'train_samples_per_second': 21.537, 'train_steps_per_second': 1.347, 'train_loss': 1.5339531706535106, 'epoch': 1.0}
train_results:  {'eval_loss': [2.8563172817230225, 1.7288600206375122, 1.5137883424758911, 1.4674392938613892, 1.440880537033081, 1.4254571199417114, 1.4134190082550049, 1.4046744108200073, 1.3958812952041626, 1.3897696733474731, 1.3833813667297363, 1.3755131959915161, 1.372102975845337, 1.3658655881881714, 1.3592689037322998, 1.355679988861084, 1.354159951210022, 1.3480780124664307, 1.3452495336532593, 1.34223473072052, 1.3397977352142334, 1.3372218608856201, 1.335562825202942, 1.3345218896865845], 'performance': [0.62, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:25,  3.82it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 45.74it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 57.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 70.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 77.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 82.54it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 79.90it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.1595536470413208
current iteration best possible performance (full train run):  0.63
max performance so far:  0.63
BO observations:  [1.1595536470413208]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7545 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.053047776222229004, 0.6501649618148804, 0.9851829409599304, 0.7351623773574829, 0.7940095663070679, 0.28148186206817627, 0.549715518951416, 0.9452856779098511, 0.4469038248062134, 0.16022159159183502, 0.8710899353027344, 0.3339494466781616, 0.9363725781440735, 0.4698870778083801, 0.17289769649505615, 0.01995915174484253, 0.033189237117767334, 0.39389821887016296, 0.7928342223167419]  ‚Üí  acq = 1.1790715072436346
X = [0.5053534507751465, 0.928024172782898, 0.2433428168296814, 0.845051109790802, 0.9386757612228394, 0.6827583909034729, 0.483393132686615, 0.7821528911590576, 0.5030372738838196, 0.5764239430427551, 0.6028188467025757, 0.1286104917526245, 0.9507086277008057, 0.6810739040374756, 0.6294482350349426, 0.8688355684280396, 0.7706508636474609, 0.5831615924835205, 0.7437930703163147]  ‚Üí  acq = 1.1790726616188776
X = [0.9007059335708618, 0.8978631496429443, 0.8289351463317871, 0.6056347489356995, 0.015752553939819336, 0.9887630343437195, 0.7300342917442322, 0.865301251411438, 0.9738363027572632, 0.31270259618759155, 0.4015148878097534, 0.028795599937438965, 0.9707925915718079, 0.23026835918426514, 0.013322412967681885, 0.9969233870506287, 0.17718452215194702, 0.03839905560016632, 0.1501760482788086]  ‚Üí  acq = 1.2167515476857842
X = [0.4912058711051941, 0.39680856466293335, 0.8716861605644226, 0.3406755328178406, 0.4463224411010742, 0.2730293273925781, 0.43982428312301636, 0.4077645540237427, 0.6379868984222412, 0.8044995069503784, 0.3119833469390869, 0.8293644189834595, 0.8532388806343079, 0.5593993067741394, 0.008453845977783203, 0.4512398838996887, 0.3229691982269287, 0.9439021348953247, 0.323627769947052]  ‚Üí  acq = 1.169267678251488
X = [0.478571355342865, 0.6347243189811707, 0.782326340675354, 0.28465795516967773, 0.9082427620887756, 0.5039736032485962, 0.343906044960022, 0.32884907722473145, 0.8620960116386414, 0.4074193239212036, 0.7241823077201843, 0.315071702003479, 0.9074722528457642, 0.5193868279457092, 0.7839004397392273, 0.8629605770111084, 0.17728787660598755, 0.42011165618896484, 0.4722220301628113]  ‚Üí  acq = 1.1790726644614953
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1490, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.8510, dtype=torch.float64), 32, 1, 0, 1, 0, 0, 128, 0.005714892469065631, 3.5508452430905937, 0]
normalized proposed parameters for next round by BO: [tensor(0.1490, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.8305e-18, dtype=torch.float64), tensor(2.8552e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8510, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.0571, dtype=torch.float64), tensor(0.0740, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.149
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.851

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.005714892469065631,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (3.5508452430905937,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.005714892469065631
lora alpha:  3.5508452430905937
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:15,  6.20it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.49it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.40it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 49.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.08it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 59.15it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 60.40it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 63.77it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 67.93it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 67.45it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 67.27it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.73it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.7524, 'grad_norm': 0.23306846618652344, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.486140727996826, 'eval_runtime': 7.1019, 'eval_samples_per_second': 140.666, 'eval_steps_per_second': 8.871, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.73it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.47it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 45.95it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.12it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.40it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.38it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.59it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 62.78it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.80it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.74it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 59.37it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.25it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.6459, 'grad_norm': 0.10337807238101959, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.2373348474502563, 'eval_runtime': 6.735, 'eval_samples_per_second': 148.33, 'eval_steps_per_second': 9.354, 'epoch': 0.08}
{'loss': 1.1447, 'grad_norm': 0.08461544662714005, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0860540866851807, 'eval_runtime': 6.7816, 'eval_samples_per_second': 147.31, 'eval_steps_per_second': 9.29, 'epoch': 0.12}
{'loss': 1.0534, 'grad_norm': 0.05108138173818588, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0371756553649902, 'eval_runtime': 6.7664, 'eval_samples_per_second': 147.642, 'eval_steps_per_second': 9.311, 'epoch': 0.16}
{'loss': 1.0153, 'grad_norm': 0.05166246369481087, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0089517831802368, 'eval_runtime': 6.7569, 'eval_samples_per_second': 147.849, 'eval_steps_per_second': 9.324, 'epoch': 0.2}
{'loss': 0.9836, 'grad_norm': 0.06112562492489815, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9839843511581421, 'eval_runtime': 6.7933, 'eval_samples_per_second': 147.056, 'eval_steps_per_second': 9.274, 'epoch': 0.24}
{'loss': 0.9667, 'grad_norm': 0.06303362548351288, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9569984078407288, 'eval_runtime': 6.8399, 'eval_samples_per_second': 146.055, 'eval_steps_per_second': 9.211, 'epoch': 0.28}
{'loss': 0.9301, 'grad_norm': 0.06627730280160904, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9284593462944031, 'eval_runtime': 6.8054, 'eval_samples_per_second': 146.795, 'eval_steps_per_second': 9.257, 'epoch': 0.32}
{'loss': 0.9441, 'grad_norm': 0.07984428107738495, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8940315842628479, 'eval_runtime': 6.8006, 'eval_samples_per_second': 146.898, 'eval_steps_per_second': 9.264, 'epoch': 0.36}
{'loss': 0.9123, 'grad_norm': 0.10128746926784515, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8393767476081848, 'eval_runtime': 6.7943, 'eval_samples_per_second': 147.036, 'eval_steps_per_second': 9.273, 'epoch': 0.4}
{'loss': 0.8384, 'grad_norm': 0.09119075536727905, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.7881452441215515, 'eval_runtime': 6.7637, 'eval_samples_per_second': 147.701, 'eval_steps_per_second': 9.314, 'epoch': 0.44}
{'loss': 0.7929, 'grad_norm': 0.10186730325222015, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.7511471509933472, 'eval_runtime': 6.755, 'eval_samples_per_second': 147.89, 'eval_steps_per_second': 9.326, 'epoch': 0.48}
{'loss': 0.7709, 'grad_norm': 0.12129290401935577, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.7121495008468628, 'eval_runtime': 6.7893, 'eval_samples_per_second': 147.142, 'eval_steps_per_second': 9.279, 'epoch': 0.52}
{'loss': 0.7217, 'grad_norm': 0.12369756400585175, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.673369824886322, 'eval_runtime': 6.7423, 'eval_samples_per_second': 148.17, 'eval_steps_per_second': 9.344, 'epoch': 0.56}
{'loss': 0.6902, 'grad_norm': 0.14041338860988617, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.6262184977531433, 'eval_runtime': 6.7715, 'eval_samples_per_second': 147.53, 'eval_steps_per_second': 9.304, 'epoch': 0.6}
{'loss': 0.6462, 'grad_norm': 0.14478197693824768, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.6004619598388672, 'eval_runtime': 6.7369, 'eval_samples_per_second': 148.288, 'eval_steps_per_second': 9.351, 'epoch': 0.64}
{'loss': 0.6372, 'grad_norm': 0.19177477061748505, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5725149512290955, 'eval_runtime': 6.737, 'eval_samples_per_second': 148.285, 'eval_steps_per_second': 9.351, 'epoch': 0.68}
{'loss': 0.5885, 'grad_norm': 0.19835975766181946, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.5467209219932556, 'eval_runtime': 6.7052, 'eval_samples_per_second': 148.988, 'eval_steps_per_second': 9.396, 'epoch': 0.72}
{'loss': 0.588, 'grad_norm': 0.1646927446126938, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.5267514586448669, 'eval_runtime': 6.7167, 'eval_samples_per_second': 148.733, 'eval_steps_per_second': 9.38, 'epoch': 0.76}
{'loss': 0.5475, 'grad_norm': 0.2524373531341553, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.5081085562705994, 'eval_runtime': 6.7206, 'eval_samples_per_second': 148.648, 'eval_steps_per_second': 9.374, 'epoch': 0.8}
{'loss': 0.5799, 'grad_norm': 0.18617579340934753, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4876047372817993, 'eval_runtime': 6.7059, 'eval_samples_per_second': 148.974, 'eval_steps_per_second': 9.395, 'epoch': 0.84}
{'loss': 0.5199, 'grad_norm': 0.2591119110584259, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.46840909123420715, 'eval_runtime': 6.7273, 'eval_samples_per_second': 148.499, 'eval_steps_per_second': 9.365, 'epoch': 0.88}
{'loss': 0.5164, 'grad_norm': 0.2281205952167511, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4561622738838196, 'eval_runtime': 6.7309, 'eval_samples_per_second': 148.42, 'eval_steps_per_second': 9.36, 'epoch': 0.92}
{'loss': 0.5009, 'grad_norm': 0.2258959263563156, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.4468284547328949, 'eval_runtime': 6.7135, 'eval_samples_per_second': 148.805, 'eval_steps_per_second': 9.384, 'epoch': 0.96}
{'loss': 0.4741, 'grad_norm': 0.2341509312391281, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.4435895085334778, 'eval_runtime': 6.7381, 'eval_samples_per_second': 148.261, 'eval_steps_per_second': 9.35, 'epoch': 1.0}
{'train_runtime': 383.2684, 'train_samples_per_second': 26.089, 'train_steps_per_second': 1.631, 'train_loss': 0.9104555709838867, 'epoch': 1.0}
train_results:  {'eval_loss': [2.486140727996826, 1.2373348474502563, 1.0860540866851807, 1.0371756553649902, 1.0089517831802368, 0.9839843511581421, 0.9569984078407288, 0.9284593462944031, 0.8940315842628479, 0.8393767476081848, 0.7881452441215515, 0.7511471509933472, 0.7121495008468628, 0.673369824886322, 0.6262184977531433, 0.6004619598388672, 0.5725149512290955, 0.5467209219932556, 0.5267514586448669, 0.5081085562705994, 0.4876047372817993, 0.46840909123420715, 0.4561622738838196, 0.4468284547328949, 0.4435895085334778], 'performance': [0.61, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:27,  3.59it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 48.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 59.86it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 65.89it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 70.74it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 68.68it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.1007390022277832
current iteration best possible performance (full train run):  0.63
max performance so far:  0.63
BO observations:  [1.1595536470413208, 1.1007390022277832]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0952 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6386879682540894, 0.5824955701828003, 0.6064498424530029, 0.9670383334159851, 0.14824450016021729, 0.6293829083442688, 0.7138169407844543, 0.5305539965629578, 0.46020710468292236, 0.9323126077651978, 0.9317017197608948, 0.7528126239776611, 0.6931688785552979, 0.6732017397880554, 0.13743829727172852, 0.1290336698293686, 0.4142226576805115, 0.8972223997116089, 0.4694112539291382]  ‚Üí  acq = 1.0579079040017798
X = [0.550851583480835, 0.06749564409255981, 0.990001380443573, 0.757815420627594, 0.18911796808242798, 0.1985887885093689, 0.35938072204589844, 0.5434634685516357, 0.23156803846359253, 0.3823596239089966, 0.09104955196380615, 0.8203065991401672, 0.8704387545585632, 0.34861934185028076, 0.6640573740005493, 0.8307376503944397, 0.13255983591079712, 0.6894335746765137, 0.7202272415161133]  ‚Üí  acq = 0.9448339324163273
X = [0.9534384608268738, 0.7769880294799805, 0.34341365098953247, 0.4993631839752197, 0.4922976493835449, 0.9023944735527039, 0.9575459361076355, 0.844373345375061, 0.4032459855079651, 0.3424668312072754, 0.5942675471305847, 0.745133101940155, 0.06396245956420898, 0.24812442064285278, 0.41066014766693115, 0.5158007144927979, 0.2255229949951172, 0.0882030725479126, 0.7287709712982178]  ‚Üí  acq = 1.1246015918677768
X = [0.9387708902359009, 0.00998610258102417, 0.3234195113182068, 0.18031585216522217, 0.9887293577194214, 0.8305545449256897, 0.024687767028808594, 0.1710277795791626, 0.7048782110214233, 0.28048449754714966, 0.41500991582870483, 0.62555330991745, 0.307354211807251, 0.8576723337173462, 0.468417227268219, 0.6532734632492065, 0.2535977363586426, 0.6759016513824463, 0.09239798784255981]  ‚Üí  acq = 1.127313593044831
X = [0.314616322517395, 0.5990985631942749, 0.1349496841430664, 0.7717016339302063, 0.8139169216156006, 0.7022995352745056, 0.14085698127746582, 0.27958011627197266, 0.12301594018936157, 0.5556814074516296, 0.8490679860115051, 0.996526300907135, 0.8469864726066589, 0.2871156930923462, 0.3564026951789856, 0.40006667375564575, 0.28629976511001587, 0.651291012763977, 0.6519075036048889]  ‚Üí  acq = 1.1273122227298007
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2747, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7253, dtype=torch.float64), 32, 1, 1, 1, 0, 0, 7, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(3.4613e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2747, dtype=torch.float64), tensor(1.2324e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.4338e-17, dtype=torch.float64), tensor(3.4083e-17, dtype=torch.float64), tensor(3.6411e-17, dtype=torch.float64), tensor(0.7253, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0536, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.275
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.725

LoRA Parameters:
  lora_r: (7,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 0]
lora rank:  7
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 7,110,656 || all params: 8,037,371,904 || trainable%: 0.0885
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<07:33,  4.58s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:04<00:34,  2.61it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:04<00:14,  5.70it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:07,  9.57it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:04, 14.22it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:02, 19.84it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:01, 25.64it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:00, 36.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 45.74it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 48.70it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 13.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.67it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.838, 'grad_norm': 2.0601136684417725, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.550298810005188, 'eval_runtime': 10.1121, 'eval_samples_per_second': 98.793, 'eval_steps_per_second': 6.23, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.94it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.28it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 45.75it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 50.72it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 53.58it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 57.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 61.19it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.78it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 63.39it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 62.77it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 58.66it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.4124, 'grad_norm': 1.0168983936309814, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.296973466873169, 'eval_runtime': 10.0796, 'eval_samples_per_second': 99.111, 'eval_steps_per_second': 6.25, 'epoch': 0.08}
{'loss': 1.2309, 'grad_norm': 0.8499446511268616, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.204499363899231, 'eval_runtime': 10.0495, 'eval_samples_per_second': 99.408, 'eval_steps_per_second': 6.269, 'epoch': 0.12}
{'loss': 1.1826, 'grad_norm': 0.8964208364486694, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1226950883865356, 'eval_runtime': 10.0844, 'eval_samples_per_second': 99.064, 'eval_steps_per_second': 6.247, 'epoch': 0.16}
{'loss': 1.0934, 'grad_norm': 1.0499390363693237, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0777095556259155, 'eval_runtime': 10.1553, 'eval_samples_per_second': 98.372, 'eval_steps_per_second': 6.204, 'epoch': 0.2}
{'loss': 1.049, 'grad_norm': 1.0528507232666016, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0288453102111816, 'eval_runtime': 10.2592, 'eval_samples_per_second': 97.376, 'eval_steps_per_second': 6.141, 'epoch': 0.24}
{'loss': 0.9818, 'grad_norm': 1.0943701267242432, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9988153576850891, 'eval_runtime': 10.2595, 'eval_samples_per_second': 97.373, 'eval_steps_per_second': 6.141, 'epoch': 0.28}
{'loss': 0.9776, 'grad_norm': 1.4013895988464355, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9588804841041565, 'eval_runtime': 10.2617, 'eval_samples_per_second': 97.353, 'eval_steps_per_second': 6.139, 'epoch': 0.32}
{'loss': 0.9393, 'grad_norm': 1.5315687656402588, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9131391048431396, 'eval_runtime': 10.2845, 'eval_samples_per_second': 97.136, 'eval_steps_per_second': 6.126, 'epoch': 0.36}
{'loss': 0.9269, 'grad_norm': 1.120449185371399, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8822174072265625, 'eval_runtime': 10.2999, 'eval_samples_per_second': 96.991, 'eval_steps_per_second': 6.117, 'epoch': 0.4}
{'loss': 0.935, 'grad_norm': 1.179972529411316, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8465001583099365, 'eval_runtime': 10.2813, 'eval_samples_per_second': 97.167, 'eval_steps_per_second': 6.128, 'epoch': 0.44}
{'loss': 0.8898, 'grad_norm': 1.162131667137146, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.822737455368042, 'eval_runtime': 10.2433, 'eval_samples_per_second': 97.527, 'eval_steps_per_second': 6.15, 'epoch': 0.48}
{'loss': 0.941, 'grad_norm': 1.2514532804489136, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8074233531951904, 'eval_runtime': 10.2261, 'eval_samples_per_second': 97.691, 'eval_steps_per_second': 6.161, 'epoch': 0.52}
{'loss': 0.867, 'grad_norm': 1.396700382232666, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7904532551765442, 'eval_runtime': 10.2216, 'eval_samples_per_second': 97.734, 'eval_steps_per_second': 6.163, 'epoch': 0.56}
{'loss': 0.8045, 'grad_norm': 1.4468008279800415, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7711479663848877, 'eval_runtime': 10.2153, 'eval_samples_per_second': 97.795, 'eval_steps_per_second': 6.167, 'epoch': 0.6}
{'loss': 0.8994, 'grad_norm': 1.6218239068984985, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7551853656768799, 'eval_runtime': 10.2093, 'eval_samples_per_second': 97.852, 'eval_steps_per_second': 6.171, 'epoch': 0.64}
{'loss': 0.815, 'grad_norm': 1.1399956941604614, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7384257316589355, 'eval_runtime': 10.2079, 'eval_samples_per_second': 97.865, 'eval_steps_per_second': 6.172, 'epoch': 0.68}
{'loss': 0.8743, 'grad_norm': 1.4437932968139648, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7249249219894409, 'eval_runtime': 10.2136, 'eval_samples_per_second': 97.811, 'eval_steps_per_second': 6.168, 'epoch': 0.72}
{'loss': 0.9012, 'grad_norm': 1.799021601676941, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7174625992774963, 'eval_runtime': 10.201, 'eval_samples_per_second': 97.932, 'eval_steps_per_second': 6.176, 'epoch': 0.76}
{'loss': 0.7864, 'grad_norm': 0.9356193542480469, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7093263268470764, 'eval_runtime': 10.1327, 'eval_samples_per_second': 98.592, 'eval_steps_per_second': 6.217, 'epoch': 0.8}
{'loss': 0.8451, 'grad_norm': 1.727386474609375, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7042009830474854, 'eval_runtime': 10.138, 'eval_samples_per_second': 98.54, 'eval_steps_per_second': 6.214, 'epoch': 0.84}
{'loss': 0.8258, 'grad_norm': 1.2068055868148804, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.698354959487915, 'eval_runtime': 10.1417, 'eval_samples_per_second': 98.505, 'eval_steps_per_second': 6.212, 'epoch': 0.88}
{'loss': 0.7457, 'grad_norm': 0.7937256097793579, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.691088080406189, 'eval_runtime': 10.1397, 'eval_samples_per_second': 98.523, 'eval_steps_per_second': 6.213, 'epoch': 0.92}
{'loss': 0.7856, 'grad_norm': 0.6415189504623413, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6883499622344971, 'eval_runtime': 10.2162, 'eval_samples_per_second': 97.786, 'eval_steps_per_second': 6.167, 'epoch': 0.96}
{'loss': 0.7141, 'grad_norm': 0.7438695430755615, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6871074438095093, 'eval_runtime': 10.2514, 'eval_samples_per_second': 97.45, 'eval_steps_per_second': 6.145, 'epoch': 1.0}
{'train_runtime': 559.7982, 'train_samples_per_second': 17.862, 'train_steps_per_second': 1.116, 'train_loss': 1.0104693420410156, 'epoch': 1.0}
train_results:  {'eval_loss': [1.550298810005188, 1.296973466873169, 1.204499363899231, 1.1226950883865356, 1.0777095556259155, 1.0288453102111816, 0.9988153576850891, 0.9588804841041565, 0.9131391048431396, 0.8822174072265625, 0.8465001583099365, 0.822737455368042, 0.8074233531951904, 0.7904532551765442, 0.7711479663848877, 0.7551853656768799, 0.7384257316589355, 0.7249249219894409, 0.7174625992774963, 0.7093263268470764, 0.7042009830474854, 0.698354959487915, 0.691088080406189, 0.6883499622344971, 0.6871074438095093], 'performance': [0.64, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:13,  5.59s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:20,  4.04it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:11<00:19,  3.37it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:11<00:08,  5.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:16<00:08,  4.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:21<00:05,  3.78it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:21<00:00,  4.57it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  0.9383885860443115
current iteration best possible performance (full train run):  0.546
max performance so far:  0.63
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7108 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1849806308746338, 0.9466091394424438, 0.7312465906143188, 0.7103930711746216, 0.1768285036087036, 0.83840411901474, 0.5128150582313538, 0.06713700294494629, 0.33758246898651123, 0.142256960272789, 0.6739506721496582, 0.584257960319519, 0.7152610421180725, 0.6844035387039185, 0.4542079567909241, 0.8319082856178284, 0.6086143851280212, 0.2273647040128708, 0.19425541162490845]  ‚Üí  acq = 0.9141263101734316
X = [0.5230960845947266, 0.43956923484802246, 0.9579080939292908, 0.936005175113678, 0.5017755031585693, 0.431688129901886, 0.5646679401397705, 0.9847831726074219, 0.6754579544067383, 0.3724852502346039, 0.18684160709381104, 0.2433403730392456, 0.09801590442657471, 0.022879600524902344, 0.3853943943977356, 0.8563355207443237, 0.5143457055091858, 0.07274103164672852, 0.5320384502410889]  ‚Üí  acq = 1.0573444165629193
X = [0.32794177532196045, 0.5746227502822876, 0.9713163375854492, 0.12717437744140625, 0.029366135597229004, 0.14992880821228027, 0.7234503030776978, 0.4520750641822815, 0.47438526153564453, 0.5829265117645264, 0.22844940423965454, 0.25441646575927734, 0.954014778137207, 0.5760148763656616, 0.43397587537765503, 0.13782529532909393, 0.668753981590271, 0.851784348487854, 0.6729594469070435]  ‚Üí  acq = 1.0363685213491123
X = [0.2621985673904419, 0.843769907951355, 0.7792602181434631, 0.9600828886032104, 0.7925567030906677, 0.22771531343460083, 0.5612452030181885, 0.1398864984512329, 0.6504448056221008, 0.29328691959381104, 0.3110172748565674, 0.6285000443458557, 0.15306401252746582, 0.19779455661773682, 0.9369556903839111, 0.27714627981185913, 0.21384334564208984, 0.664448618888855, 0.6769750118255615]  ‚Üí  acq = 1.057940055744866
X = [0.34051525592803955, 0.08763033151626587, 0.05575340986251831, 0.9832366704940796, 0.6508274674415588, 0.4397357106208801, 0.7169950604438782, 0.729676365852356, 0.2878371477127075, 0.8126056790351868, 0.5228124260902405, 0.9665745496749878, 0.642060399055481, 0.7630205154418945, 0.08145463466644287, 0.5600201487541199, 0.38862085342407227, 0.739506721496582, 0.5106248259544373]  ‚Üí  acq = 1.0579103711926465
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(0.3487, dtype=torch.float64), tensor(0.6513, dtype=torch.float64), 32, 0, 1, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0]
normalized proposed parameters for next round by BO: [tensor(1.2588e-16, dtype=torch.float64), tensor(1.4230e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.9810e-16, dtype=torch.float64), tensor(8.9420e-17, dtype=torch.float64), tensor(0.3487, dtype=torch.float64), tensor(0.6513, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.349
  arc_challenge: 0.651

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (1.4800000190734863,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  1.4800000190734863
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 247,463,936 || all params: 8,277,725,184 || trainable%: 2.9895
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.10it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 25.73it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.39it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.81it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.22it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 48.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.24it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.29it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.99it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 54.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.57it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.5985, 'grad_norm': 0.2041381597518921, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.4443697929382324, 'eval_runtime': 9.3276, 'eval_samples_per_second': 107.101, 'eval_steps_per_second': 6.754, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.18it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.21it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.99it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.52it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 47.70it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.48it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.48it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 52.54it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.28it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 50.32it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 49.85it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.00it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.6991, 'grad_norm': 0.1921110600233078, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.2679086923599243, 'eval_runtime': 9.3788, 'eval_samples_per_second': 106.517, 'eval_steps_per_second': 6.717, 'epoch': 0.08}
{'loss': 1.2002, 'grad_norm': 0.05419175699353218, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1175572872161865, 'eval_runtime': 9.4244, 'eval_samples_per_second': 106.002, 'eval_steps_per_second': 6.685, 'epoch': 0.12}
{'loss': 1.1146, 'grad_norm': 0.044578395783901215, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0349594354629517, 'eval_runtime': 9.468, 'eval_samples_per_second': 105.513, 'eval_steps_per_second': 6.654, 'epoch': 0.16}
{'loss': 1.0121, 'grad_norm': 0.04398265853524208, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0135667324066162, 'eval_runtime': 9.476, 'eval_samples_per_second': 105.424, 'eval_steps_per_second': 6.648, 'epoch': 0.2}
{'loss': 0.9775, 'grad_norm': 0.03905492275953293, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9951313734054565, 'eval_runtime': 9.4993, 'eval_samples_per_second': 105.165, 'eval_steps_per_second': 6.632, 'epoch': 0.24}
{'loss': 0.9827, 'grad_norm': 0.04384743422269821, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9777169823646545, 'eval_runtime': 9.4939, 'eval_samples_per_second': 105.226, 'eval_steps_per_second': 6.636, 'epoch': 0.28}
{'loss': 1.0078, 'grad_norm': 0.044813353568315506, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9625303745269775, 'eval_runtime': 9.4477, 'eval_samples_per_second': 105.74, 'eval_steps_per_second': 6.668, 'epoch': 0.32}
{'loss': 0.9274, 'grad_norm': 0.04915594682097435, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9492764472961426, 'eval_runtime': 9.4335, 'eval_samples_per_second': 105.899, 'eval_steps_per_second': 6.678, 'epoch': 0.36}
{'loss': 0.9117, 'grad_norm': 0.05120537802577019, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9378076195716858, 'eval_runtime': 9.4288, 'eval_samples_per_second': 105.952, 'eval_steps_per_second': 6.682, 'epoch': 0.4}
{'loss': 0.9103, 'grad_norm': 0.04925309866666794, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9213108420372009, 'eval_runtime': 9.4299, 'eval_samples_per_second': 105.939, 'eval_steps_per_second': 6.681, 'epoch': 0.44}
{'loss': 0.9779, 'grad_norm': 0.052109912037849426, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9090694189071655, 'eval_runtime': 9.4249, 'eval_samples_per_second': 105.996, 'eval_steps_per_second': 6.684, 'epoch': 0.48}
{'loss': 0.9109, 'grad_norm': 0.06603565067052841, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8943971395492554, 'eval_runtime': 9.3973, 'eval_samples_per_second': 106.308, 'eval_steps_per_second': 6.704, 'epoch': 0.52}
{'loss': 0.916, 'grad_norm': 0.06157418712973595, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8787060976028442, 'eval_runtime': 9.3903, 'eval_samples_per_second': 106.386, 'eval_steps_per_second': 6.709, 'epoch': 0.56}
{'loss': 0.89, 'grad_norm': 0.062168221920728683, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8600834608078003, 'eval_runtime': 9.3831, 'eval_samples_per_second': 106.468, 'eval_steps_per_second': 6.714, 'epoch': 0.6}
{'loss': 0.8958, 'grad_norm': 0.06870163977146149, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.842851996421814, 'eval_runtime': 9.4299, 'eval_samples_per_second': 105.939, 'eval_steps_per_second': 6.681, 'epoch': 0.64}
{'loss': 0.8633, 'grad_norm': 0.07624614238739014, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8259232044219971, 'eval_runtime': 9.3821, 'eval_samples_per_second': 106.48, 'eval_steps_per_second': 6.715, 'epoch': 0.68}
{'loss': 0.8438, 'grad_norm': 0.09372662007808685, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8107163310050964, 'eval_runtime': 9.3805, 'eval_samples_per_second': 106.498, 'eval_steps_per_second': 6.716, 'epoch': 0.72}
{'loss': 0.9115, 'grad_norm': 0.08614036440849304, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8018590807914734, 'eval_runtime': 9.3793, 'eval_samples_per_second': 106.512, 'eval_steps_per_second': 6.717, 'epoch': 0.76}
{'loss': 0.8306, 'grad_norm': 0.09971199929714203, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7821869850158691, 'eval_runtime': 9.4183, 'eval_samples_per_second': 106.07, 'eval_steps_per_second': 6.689, 'epoch': 0.8}
{'loss': 0.8358, 'grad_norm': 0.09790902584791183, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7694530487060547, 'eval_runtime': 9.4492, 'eval_samples_per_second': 105.723, 'eval_steps_per_second': 6.667, 'epoch': 0.84}
{'loss': 0.8366, 'grad_norm': 0.10162532329559326, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7600519061088562, 'eval_runtime': 9.4398, 'eval_samples_per_second': 105.829, 'eval_steps_per_second': 6.674, 'epoch': 0.88}
{'loss': 0.8231, 'grad_norm': 0.10065694898366928, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7511481642723083, 'eval_runtime': 9.4399, 'eval_samples_per_second': 105.827, 'eval_steps_per_second': 6.674, 'epoch': 0.92}
{'loss': 0.8231, 'grad_norm': 0.09333527088165283, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7460930347442627, 'eval_runtime': 9.4367, 'eval_samples_per_second': 105.863, 'eval_steps_per_second': 6.676, 'epoch': 0.96}
{'loss': 0.7993, 'grad_norm': 0.11320675164461136, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7434767484664917, 'eval_runtime': 9.4206, 'eval_samples_per_second': 106.044, 'eval_steps_per_second': 6.687, 'epoch': 1.0}
{'train_runtime': 505.6895, 'train_samples_per_second': 19.773, 'train_steps_per_second': 1.236, 'train_loss': 1.059978594970703, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4443697929382324, 1.2679086923599243, 1.1175572872161865, 1.0349594354629517, 1.0135667324066162, 0.9951313734054565, 0.9777169823646545, 0.9625303745269775, 0.9492764472961426, 0.9378076195716858, 0.9213108420372009, 0.9090694189071655, 0.8943971395492554, 0.8787060976028442, 0.8600834608078003, 0.842851996421814, 0.8259232044219971, 0.8107163310050964, 0.8018590807914734, 0.7821869850158691, 0.7694530487060547, 0.7600519061088562, 0.7511481642723083, 0.7460930347442627, 0.7434767484664917], 'performance': [0.61, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.91it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.18it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.39it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.07it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.77it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 70.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.94it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.0854151248931885
current iteration best possible performance (full train run):  0.6405
max performance so far:  0.6405
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0660 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9992697238922119, 0.4815741181373596, 0.15013211965560913, 0.5617397427558899, 0.61008220911026, 0.9274998903274536, 0.7369027137756348, 0.5016750693321228, 0.027337193489074707, 0.6951549053192139, 0.10076373815536499, 0.8413710594177246, 0.02551424503326416, 0.5817463397979736, 0.19247043132781982, 0.460382878780365, 0.9088072776794434, 0.8689981698989868, 0.03067171573638916]  ‚Üí  acq = 1.025881390788654
X = [0.9607988595962524, 0.386763334274292, 0.9881590008735657, 0.47782617807388306, 0.2983860373497009, 0.6069186925888062, 0.46520036458969116, 0.3141617774963379, 0.24606788158416748, 0.8659851551055908, 0.28152352571487427, 0.37767112255096436, 0.35367393493652344, 0.6926108598709106, 0.8767876029014587, 0.9039384126663208, 0.9407015442848206, 0.2951793968677521, 0.26284271478652954]  ‚Üí  acq = 0.9876446382471125
X = [0.7857325077056885, 0.31991463899612427, 0.9785335659980774, 0.8994920253753662, 0.7722318172454834, 0.0979430079460144, 0.5216630697250366, 0.19692546129226685, 0.16780740022659302, 0.3873956501483917, 0.29857975244522095, 0.11939942836761475, 0.18671172857284546, 0.5084601640701294, 0.6497288346290588, 0.7585896253585815, 0.7465715408325195, 0.50398188829422, 0.6326173543930054]  ‚Üí  acq = 1.025934604093096
X = [0.0633084774017334, 0.7409090399742126, 0.38070547580718994, 0.1810343861579895, 0.2906460165977478, 0.795657753944397, 0.745154857635498, 0.8337947726249695, 0.08266621828079224, 0.07359226793050766, 0.6996797919273376, 0.7881956100463867, 0.007792532444000244, 0.6584209203720093, 0.9974734783172607, 0.4803454279899597, 0.32486361265182495, 0.6937472820281982, 0.3627607226371765]  ‚Üí  acq = 0.9869467301065835
X = [0.7081489562988281, 0.33821946382522583, 0.13111770153045654, 0.19059079885482788, 0.15817368030548096, 0.4174908995628357, 0.5595240592956543, 0.4828631281852722, 0.5316267609596252, 0.552460253238678, 0.40550071001052856, 0.2792316675186157, 0.8546876311302185, 0.014700174331665039, 0.6765121817588806, 0.26966333389282227, 0.6628555059432983, 0.46717262268066406, 0.7080737948417664]  ‚Üí  acq = 0.937915775953112
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.3267, dtype=torch.float64), 0, tensor(0.6733, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 1.9047263766225327e-16, 47.99999999756032, 1]
normalized proposed parameters for next round by BO: [tensor(2.7463e-14, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3536e-14, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.3723e-14, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3267, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6733, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.9047e-15, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.327
  mmlu: 0
  arc_challenge: 0.673

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.9047263766225327e-16,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (47.99999999756032,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  1.9047263766225327e-16
lora alpha:  47.99999999756032
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.60it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.94it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.13it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.41it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.53it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.40it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.50it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.77it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.12it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.65it/s]
Evaluation performance at step 25: 0.61
{'loss': 2.6259, 'grad_norm': 0.48776334524154663, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.486214518547058, 'eval_runtime': 8.2699, 'eval_samples_per_second': 120.799, 'eval_steps_per_second': 7.618, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.54it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.62it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.92it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.63it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.24it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.87it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.97it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.90it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.73it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.70it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.3294, 'grad_norm': 0.22125539183616638, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.2442348003387451, 'eval_runtime': 8.2425, 'eval_samples_per_second': 121.201, 'eval_steps_per_second': 7.643, 'epoch': 0.08}
{'loss': 1.189, 'grad_norm': 0.24093829095363617, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1224892139434814, 'eval_runtime': 8.2722, 'eval_samples_per_second': 120.767, 'eval_steps_per_second': 7.616, 'epoch': 0.12}
{'loss': 1.167, 'grad_norm': 0.2921881675720215, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0278728008270264, 'eval_runtime': 8.291, 'eval_samples_per_second': 120.492, 'eval_steps_per_second': 7.599, 'epoch': 0.16}
{'loss': 1.0513, 'grad_norm': 0.2696802020072937, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.926403820514679, 'eval_runtime': 8.3, 'eval_samples_per_second': 120.362, 'eval_steps_per_second': 7.59, 'epoch': 0.2}
{'loss': 0.8768, 'grad_norm': 0.36687296628952026, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8596011996269226, 'eval_runtime': 8.32, 'eval_samples_per_second': 120.072, 'eval_steps_per_second': 7.572, 'epoch': 0.24}
{'loss': 0.8687, 'grad_norm': 0.4832761883735657, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7918564081192017, 'eval_runtime': 8.3208, 'eval_samples_per_second': 120.06, 'eval_steps_per_second': 7.571, 'epoch': 0.28}
{'loss': 0.8323, 'grad_norm': 0.3348161578178406, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7347073554992676, 'eval_runtime': 8.3174, 'eval_samples_per_second': 120.109, 'eval_steps_per_second': 7.574, 'epoch': 0.32}
{'loss': 0.7549, 'grad_norm': 0.338267982006073, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6924254298210144, 'eval_runtime': 8.3253, 'eval_samples_per_second': 119.996, 'eval_steps_per_second': 7.567, 'epoch': 0.36}
{'loss': 0.7811, 'grad_norm': 0.3893367648124695, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.654240608215332, 'eval_runtime': 8.3361, 'eval_samples_per_second': 119.84, 'eval_steps_per_second': 7.557, 'epoch': 0.4}
{'loss': 0.7624, 'grad_norm': 0.504875659942627, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6177889704704285, 'eval_runtime': 8.3401, 'eval_samples_per_second': 119.782, 'eval_steps_per_second': 7.554, 'epoch': 0.44}
{'loss': 0.7138, 'grad_norm': 0.38566216826438904, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.588901937007904, 'eval_runtime': 8.342, 'eval_samples_per_second': 119.756, 'eval_steps_per_second': 7.552, 'epoch': 0.48}
{'loss': 0.8053, 'grad_norm': 0.3916040360927582, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5609220266342163, 'eval_runtime': 8.325, 'eval_samples_per_second': 120.0, 'eval_steps_per_second': 7.568, 'epoch': 0.52}
{'loss': 0.6871, 'grad_norm': 0.348798543214798, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5293870568275452, 'eval_runtime': 8.3238, 'eval_samples_per_second': 120.017, 'eval_steps_per_second': 7.569, 'epoch': 0.56}
{'loss': 0.6088, 'grad_norm': 0.3064918518066406, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5014681816101074, 'eval_runtime': 8.3653, 'eval_samples_per_second': 119.422, 'eval_steps_per_second': 7.531, 'epoch': 0.6}
{'loss': 0.6584, 'grad_norm': 0.23425157368183136, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4821445643901825, 'eval_runtime': 8.3304, 'eval_samples_per_second': 119.923, 'eval_steps_per_second': 7.563, 'epoch': 0.64}
{'loss': 0.6159, 'grad_norm': 0.3163949251174927, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4706570506095886, 'eval_runtime': 8.3832, 'eval_samples_per_second': 119.167, 'eval_steps_per_second': 7.515, 'epoch': 0.68}
{'loss': 0.6707, 'grad_norm': 0.2602248787879944, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4566192030906677, 'eval_runtime': 8.3523, 'eval_samples_per_second': 119.607, 'eval_steps_per_second': 7.543, 'epoch': 0.72}
{'loss': 0.7291, 'grad_norm': 0.28296616673469543, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4439610540866852, 'eval_runtime': 8.3429, 'eval_samples_per_second': 119.743, 'eval_steps_per_second': 7.551, 'epoch': 0.76}
{'loss': 0.5778, 'grad_norm': 0.2706325352191925, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.4332876205444336, 'eval_runtime': 8.3455, 'eval_samples_per_second': 119.705, 'eval_steps_per_second': 7.549, 'epoch': 0.8}
{'loss': 0.5652, 'grad_norm': 0.2917563021183014, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.42288124561309814, 'eval_runtime': 8.3499, 'eval_samples_per_second': 119.642, 'eval_steps_per_second': 7.545, 'epoch': 0.84}
{'loss': 0.6423, 'grad_norm': 0.24135659635066986, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.41724133491516113, 'eval_runtime': 8.3511, 'eval_samples_per_second': 119.625, 'eval_steps_per_second': 7.544, 'epoch': 0.88}
{'loss': 0.5883, 'grad_norm': 0.22621160745620728, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4122024178504944, 'eval_runtime': 8.3378, 'eval_samples_per_second': 119.816, 'eval_steps_per_second': 7.556, 'epoch': 0.92}
{'loss': 0.5771, 'grad_norm': 0.24608451128005981, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.4086972773075104, 'eval_runtime': 8.3371, 'eval_samples_per_second': 119.826, 'eval_steps_per_second': 7.557, 'epoch': 0.96}
{'loss': 0.5338, 'grad_norm': 0.22731703519821167, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.4073813557624817, 'eval_runtime': 8.3414, 'eval_samples_per_second': 119.764, 'eval_steps_per_second': 7.553, 'epoch': 1.0}
{'train_runtime': 457.8565, 'train_samples_per_second': 21.839, 'train_steps_per_second': 1.365, 'train_loss': 0.8484910934448242, 'epoch': 1.0}
train_results:  {'eval_loss': [1.486214518547058, 1.2442348003387451, 1.1224892139434814, 1.0278728008270264, 0.926403820514679, 0.8596011996269226, 0.7918564081192017, 0.7347073554992676, 0.6924254298210144, 0.654240608215332, 0.6177889704704285, 0.588901937007904, 0.5609220266342163, 0.5293870568275452, 0.5014681816101074, 0.4821445643901825, 0.4706570506095886, 0.4566192030906677, 0.4439610540866852, 0.4332876205444336, 0.42288124561309814, 0.41724133491516113, 0.4122024178504944, 0.4086972773075104, 0.4073813557624817], 'performance': [0.61, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.28it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 24.98it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 32.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 41.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 45.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 59.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 43.91it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2467622756958008
current iteration best possible performance (full train run):  0.651
max performance so far:  0.651
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7419 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9830232262611389, 0.7579970359802246, 0.7816738486289978, 0.9628047943115234, 0.2901614308357239, 0.6829801797866821, 0.6668112874031067, 0.4673480987548828, 0.32448810338974, 0.9360848665237427, 0.837611198425293, 0.527209460735321, 0.6708904504776001, 0.6879414319992065, 0.339047908782959, 0.024302393198013306, 0.4788960814476013, 0.11430045962333679, 0.08227097988128662]  ‚Üí  acq = 1.0285241013431659
X = [0.13892126083374023, 0.8641919493675232, 0.24125045537948608, 0.21967530250549316, 0.6614311337471008, 0.9048324227333069, 0.8978860974311829, 0.07337337732315063, 0.18301409482955933, 0.07685256004333496, 0.27726423740386963, 0.06260800361633301, 0.9963902831077576, 0.9966145157814026, 0.917843759059906, 0.3583885431289673, 0.21862637996673584, 0.21438340842723846, 0.3962606191635132]  ‚Üí  acq = 1.0562220538940088
X = [0.9604361653327942, 0.07694202661514282, 0.14082640409469604, 0.3031776547431946, 0.718339741230011, 0.5850151777267456, 0.2472417950630188, 0.6841635704040527, 0.7226089835166931, 0.9291759133338928, 0.3148321509361267, 0.9546623826026917, 0.23290318250656128, 0.7922331690788269, 0.5972667932510376, 0.250851571559906, 0.7106441855430603, 0.6392757892608643, 0.0201108455657959]  ‚Üí  acq = 1.0562556072962392
X = [0.015726923942565918, 0.5197004079818726, 0.9479424357414246, 0.14721781015396118, 0.07523757219314575, 0.015402495861053467, 0.4781879782676697, 0.3767808675765991, 0.07328468561172485, 0.18847940862178802, 0.12791645526885986, 0.9503196477890015, 0.2605670690536499, 0.02603590488433838, 0.8494945168495178, 0.8741697072982788, 0.8193966150283813, 0.5264592170715332, 0.011708974838256836]  ‚Üí  acq = 0.8360294552995207
X = [0.7478103637695312, 0.19503211975097656, 0.5493379235267639, 0.9836851954460144, 0.5114096403121948, 0.13825678825378418, 0.7392382025718689, 0.4126766324043274, 0.47528553009033203, 0.4978892505168915, 0.4488353729248047, 0.5503937602043152, 0.8845456838607788, 0.4540330171585083, 0.7512220740318298, 0.9761327505111694, 0.43995410203933716, 0.6715582609176636, 0.4389188885688782]  ‚Üí  acq = 1.0538685756327146
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.2927, dtype=torch.float64), 0, tensor(0.7073, dtype=torch.float64), 11, 1, 0, 1, 0, 1, 128, 1.214306433183765e-18, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(7.8329e-17, dtype=torch.float64), tensor(1.2785e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.6096e-16, dtype=torch.float64), tensor(0.2927, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7073, dtype=torch.float64), tensor(0.3428, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.2143e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.293
  mmlu: 0
  arc_challenge: 0.707

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.214306433183765e-18,)
  num_layers_to_apply: (11,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  11
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  1.214306433183765e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 63,438,848 || all params: 8,093,700,096 || trainable%: 0.7838
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.76it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.74it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 51.45it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 57.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 62.20it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 67.53it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 68.45it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 72.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 76.74it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 76.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 75.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 68.74it/s]
Evaluation performance at step 25: 0.59
{'loss': 2.9437, 'grad_norm': 1.8749314546585083, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.59}
{'eval_loss': 1.8416582345962524, 'eval_runtime': 7.7685, 'eval_samples_per_second': 128.597, 'eval_steps_per_second': 8.11, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:14,  6.82it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 42.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 53.71it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 59.37it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 63.02it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 68.13it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:00<00:00, 76.33it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 72.15it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 76.83it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 70.08it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 69.60it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.5121, 'grad_norm': 0.2558506727218628, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.4513580799102783, 'eval_runtime': 7.7753, 'eval_samples_per_second': 128.484, 'eval_steps_per_second': 8.103, 'epoch': 0.08}
{'loss': 1.3435, 'grad_norm': 0.22911939024925232, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2888941764831543, 'eval_runtime': 7.7902, 'eval_samples_per_second': 128.238, 'eval_steps_per_second': 8.087, 'epoch': 0.12}
{'loss': 1.3003, 'grad_norm': 0.20927850902080536, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2005274295806885, 'eval_runtime': 7.7907, 'eval_samples_per_second': 128.229, 'eval_steps_per_second': 8.087, 'epoch': 0.16}
{'loss': 1.0775, 'grad_norm': 0.22098016738891602, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1763889789581299, 'eval_runtime': 7.7945, 'eval_samples_per_second': 128.168, 'eval_steps_per_second': 8.083, 'epoch': 0.2}
{'loss': 1.1124, 'grad_norm': 0.20789852738380432, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1377109289169312, 'eval_runtime': 7.8124, 'eval_samples_per_second': 127.873, 'eval_steps_per_second': 8.064, 'epoch': 0.24}
{'loss': 1.0689, 'grad_norm': 0.2524901032447815, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1093510389328003, 'eval_runtime': 7.8107, 'eval_samples_per_second': 127.902, 'eval_steps_per_second': 8.066, 'epoch': 0.28}
{'loss': 1.1081, 'grad_norm': 0.2569519281387329, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.080125093460083, 'eval_runtime': 7.8216, 'eval_samples_per_second': 127.723, 'eval_steps_per_second': 8.055, 'epoch': 0.32}
{'loss': 1.069, 'grad_norm': 0.30418631434440613, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0613775253295898, 'eval_runtime': 7.8146, 'eval_samples_per_second': 127.838, 'eval_steps_per_second': 8.062, 'epoch': 0.36}
{'loss': 1.0312, 'grad_norm': 0.3222939372062683, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0300942659378052, 'eval_runtime': 7.8074, 'eval_samples_per_second': 127.956, 'eval_steps_per_second': 8.069, 'epoch': 0.4}
{'loss': 0.9913, 'grad_norm': 0.4145805537700653, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9992232322692871, 'eval_runtime': 7.8313, 'eval_samples_per_second': 127.565, 'eval_steps_per_second': 8.045, 'epoch': 0.44}
{'loss': 1.0601, 'grad_norm': 0.2767859995365143, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9632934927940369, 'eval_runtime': 7.8362, 'eval_samples_per_second': 127.486, 'eval_steps_per_second': 8.04, 'epoch': 0.48}
{'loss': 0.9853, 'grad_norm': 0.36341211199760437, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9383796453475952, 'eval_runtime': 7.832, 'eval_samples_per_second': 127.554, 'eval_steps_per_second': 8.044, 'epoch': 0.52}
{'loss': 1.0077, 'grad_norm': 0.4860176146030426, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.904374361038208, 'eval_runtime': 7.8358, 'eval_samples_per_second': 127.492, 'eval_steps_per_second': 8.04, 'epoch': 0.56}
{'loss': 0.9628, 'grad_norm': 0.5551852583885193, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8744234442710876, 'eval_runtime': 7.883, 'eval_samples_per_second': 126.728, 'eval_steps_per_second': 7.992, 'epoch': 0.6}
{'loss': 0.9648, 'grad_norm': 0.4990842938423157, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8518196940422058, 'eval_runtime': 7.8848, 'eval_samples_per_second': 126.7, 'eval_steps_per_second': 7.99, 'epoch': 0.64}
{'loss': 0.8692, 'grad_norm': 0.5600496530532837, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8155927062034607, 'eval_runtime': 7.8899, 'eval_samples_per_second': 126.618, 'eval_steps_per_second': 7.985, 'epoch': 0.68}
{'loss': 0.8959, 'grad_norm': 0.43245720863342285, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7885444164276123, 'eval_runtime': 7.8838, 'eval_samples_per_second': 126.716, 'eval_steps_per_second': 7.991, 'epoch': 0.72}
{'loss': 0.8806, 'grad_norm': 0.7230004668235779, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7633035778999329, 'eval_runtime': 7.8791, 'eval_samples_per_second': 126.791, 'eval_steps_per_second': 7.996, 'epoch': 0.76}
{'loss': 0.8131, 'grad_norm': 0.5614590048789978, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7375102043151855, 'eval_runtime': 7.881, 'eval_samples_per_second': 126.761, 'eval_steps_per_second': 7.994, 'epoch': 0.8}
{'loss': 0.8187, 'grad_norm': 0.7257513403892517, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7115446925163269, 'eval_runtime': 7.8932, 'eval_samples_per_second': 126.565, 'eval_steps_per_second': 7.982, 'epoch': 0.84}
{'loss': 0.7634, 'grad_norm': 0.5505520105361938, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6889477372169495, 'eval_runtime': 7.8699, 'eval_samples_per_second': 126.94, 'eval_steps_per_second': 8.005, 'epoch': 0.88}
{'loss': 0.7818, 'grad_norm': 0.6694589257240295, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6709675788879395, 'eval_runtime': 7.897, 'eval_samples_per_second': 126.504, 'eval_steps_per_second': 7.978, 'epoch': 0.92}
{'loss': 0.7548, 'grad_norm': 0.8400411009788513, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6551364660263062, 'eval_runtime': 7.8834, 'eval_samples_per_second': 126.722, 'eval_steps_per_second': 7.991, 'epoch': 0.96}
{'loss': 0.7097, 'grad_norm': 0.6517720222473145, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6486790776252747, 'eval_runtime': 7.8428, 'eval_samples_per_second': 127.378, 'eval_steps_per_second': 8.033, 'epoch': 1.0}
{'train_runtime': 418.7546, 'train_samples_per_second': 23.878, 'train_steps_per_second': 1.493, 'train_loss': 1.0730317413330077, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8416582345962524, 1.4513580799102783, 1.2888941764831543, 1.2005274295806885, 1.1763889789581299, 1.1377109289169312, 1.1093510389328003, 1.080125093460083, 1.0613775253295898, 1.0300942659378052, 0.9992232322692871, 0.9632934927940369, 0.9383796453475952, 0.904374361038208, 0.8744234442710876, 0.8518196940422058, 0.8155927062034607, 0.7885444164276123, 0.7633035778999329, 0.7375102043151855, 0.7115446925163269, 0.6889477372169495, 0.6709675788879395, 0.6551364660263062, 0.6486790776252747], 'performance': [0.59, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:27,  3.61it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.63it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 51.48it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 60.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 69.60it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 75.40it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 70.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.59, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.1713917255401611
current iteration best possible performance (full train run):  0.5565000000000001
max performance so far:  0.651
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.6585 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7171106934547424, 0.9397449493408203, 0.6566453576087952, 0.11273926496505737, 0.009976029396057129, 0.5448768734931946, 0.05566394329071045, 0.059625089168548584, 0.9285798072814941, 0.12913955748081207, 0.21951395273208618, 0.4179774522781372, 0.5759981274604797, 0.34611475467681885, 0.4967361092567444, 0.16718563437461853, 0.42890292406082153, 0.8727803230285645, 0.06831812858581543]  ‚Üí  acq = 1.0705457995902412
X = [0.4621891379356384, 0.567959189414978, 0.8868556618690491, 0.4724832773208618, 0.1118088960647583, 0.39940357208251953, 0.7038169503211975, 0.7469888925552368, 0.5486112236976624, 0.39387625455856323, 0.6448217630386353, 0.9641906023025513, 0.10746407508850098, 0.16918951272964478, 0.6621964573860168, 0.5928937792778015, 0.46829432249069214, 0.7630789279937744, 0.2845645546913147]  ‚Üí  acq = 1.01738307843312
X = [0.35225367546081543, 0.2633128762245178, 0.5183491110801697, 0.8707551956176758, 0.7476221323013306, 0.8758028149604797, 0.26998084783554077, 0.7914958000183105, 0.9188525080680847, 0.20107461512088776, 0.6752326488494873, 0.40350157022476196, 0.9553766846656799, 0.9859111309051514, 0.21206063032150269, 0.5049585103988647, 0.8507007360458374, 0.80156409740448, 0.31753742694854736]  ‚Üí  acq = 1.04433894273984
X = [0.5304156541824341, 0.4665030837059021, 0.22353124618530273, 0.2968805432319641, 0.44578659534454346, 0.515704870223999, 0.41556406021118164, 0.8232296705245972, 0.8956379890441895, 0.6189178824424744, 0.13748890161514282, 0.40618497133255005, 0.36923009157180786, 0.03654670715332031, 0.31714946031570435, 0.8253052234649658, 0.2909470796585083, 0.9229733943939209, 0.3883286714553833]  ‚Üí  acq = 1.042249439578693
X = [0.7428531646728516, 0.0048070549964904785, 0.9297398924827576, 0.4447299838066101, 0.2086504101753235, 0.8029792308807373, 0.7042059302330017, 0.015212178230285645, 0.43831586837768555, 0.1467318832874298, 0.00017964839935302734, 0.9899052977561951, 0.3860800266265869, 0.8397652506828308, 0.7205843329429626, 0.8989208340644836, 0.5881524682044983, 0.10077255964279175, 0.36803239583969116]  ‚Üí  acq = 0.9736051866294555
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.3115, dtype=torch.float64), 0, tensor(0.6885, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5329e-17, dtype=torch.float64), tensor(7.1147e-17, dtype=torch.float64), tensor(1.7660e-16, dtype=torch.float64), tensor(0.3115, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6885, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.311
  mmlu: 0
  arc_challenge: 0.689

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.73it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 28.62it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.22it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.09it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.48it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 45.84it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.33it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.81it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 52.94it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.83it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 51.06it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 51.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.56it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.3802, 'grad_norm': 0.3567447066307068, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.3331125974655151, 'eval_runtime': 9.065, 'eval_samples_per_second': 110.205, 'eval_steps_per_second': 6.95, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:08,  5.54s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:42,  2.12it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:19,  4.30it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:10,  7.23it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:06, 10.77it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:03, 15.06it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:02, 19.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:07<00:02, 20.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:07<00:01, 24.93it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:07<00:00, 30.84it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:07<00:00, 34.80it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:07<00:00, 38.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:07<00:00, 40.04it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.54it/s]
Evaluation performance at step 50: 0.52
{'loss': 1.2263, 'grad_norm': 0.34520164132118225, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.52}
{'eval_loss': 1.1153753995895386, 'eval_runtime': 9.0654, 'eval_samples_per_second': 110.2, 'eval_steps_per_second': 6.95, 'epoch': 0.08}
{'loss': 1.0666, 'grad_norm': 0.386074036359787, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0098850727081299, 'eval_runtime': 9.0809, 'eval_samples_per_second': 110.012, 'eval_steps_per_second': 6.938, 'epoch': 0.12}
{'loss': 0.9671, 'grad_norm': 0.31750166416168213, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9509983062744141, 'eval_runtime': 9.0553, 'eval_samples_per_second': 110.322, 'eval_steps_per_second': 6.957, 'epoch': 0.16}
{'loss': 0.8962, 'grad_norm': 0.4217011630535126, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8925836086273193, 'eval_runtime': 9.0558, 'eval_samples_per_second': 110.317, 'eval_steps_per_second': 6.957, 'epoch': 0.2}
{'loss': 0.818, 'grad_norm': 0.33723384141921997, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8452508449554443, 'eval_runtime': 9.0685, 'eval_samples_per_second': 110.162, 'eval_steps_per_second': 6.947, 'epoch': 0.24}
{'loss': 0.7735, 'grad_norm': 0.331851989030838, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7700563073158264, 'eval_runtime': 9.1271, 'eval_samples_per_second': 109.455, 'eval_steps_per_second': 6.903, 'epoch': 0.28}
{'loss': 0.8165, 'grad_norm': 0.35758650302886963, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.724462628364563, 'eval_runtime': 9.1219, 'eval_samples_per_second': 109.517, 'eval_steps_per_second': 6.906, 'epoch': 0.32}
{'loss': 0.7186, 'grad_norm': 0.4603923559188843, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6713351011276245, 'eval_runtime': 9.1211, 'eval_samples_per_second': 109.526, 'eval_steps_per_second': 6.907, 'epoch': 0.36}
{'loss': 0.71, 'grad_norm': 0.4450148046016693, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6254687309265137, 'eval_runtime': 9.1222, 'eval_samples_per_second': 109.512, 'eval_steps_per_second': 6.906, 'epoch': 0.4}
{'loss': 0.6896, 'grad_norm': 0.41253355145454407, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5950157642364502, 'eval_runtime': 9.119, 'eval_samples_per_second': 109.552, 'eval_steps_per_second': 6.909, 'epoch': 0.44}
{'loss': 0.6343, 'grad_norm': 0.39453524351119995, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.565406084060669, 'eval_runtime': 9.1171, 'eval_samples_per_second': 109.574, 'eval_steps_per_second': 6.91, 'epoch': 0.48}
{'loss': 0.7709, 'grad_norm': 0.34399136900901794, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5443488955497742, 'eval_runtime': 9.117, 'eval_samples_per_second': 109.576, 'eval_steps_per_second': 6.91, 'epoch': 0.52}
{'loss': 0.6214, 'grad_norm': 0.3651692271232605, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5148574113845825, 'eval_runtime': 9.1228, 'eval_samples_per_second': 109.506, 'eval_steps_per_second': 6.906, 'epoch': 0.56}
{'loss': 0.5906, 'grad_norm': 0.3570093512535095, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.49336498975753784, 'eval_runtime': 9.1214, 'eval_samples_per_second': 109.523, 'eval_steps_per_second': 6.907, 'epoch': 0.6}
{'loss': 0.6209, 'grad_norm': 0.3367766737937927, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4778592586517334, 'eval_runtime': 9.1228, 'eval_samples_per_second': 109.506, 'eval_steps_per_second': 6.906, 'epoch': 0.64}
{'loss': 0.5294, 'grad_norm': 0.2378324419260025, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.46307042241096497, 'eval_runtime': 9.1356, 'eval_samples_per_second': 109.353, 'eval_steps_per_second': 6.896, 'epoch': 0.68}
{'loss': 0.6015, 'grad_norm': 0.2865288257598877, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4470808804035187, 'eval_runtime': 9.132, 'eval_samples_per_second': 109.396, 'eval_steps_per_second': 6.899, 'epoch': 0.72}
{'loss': 0.6309, 'grad_norm': 0.46777182817459106, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.4379529654979706, 'eval_runtime': 9.1274, 'eval_samples_per_second': 109.45, 'eval_steps_per_second': 6.902, 'epoch': 0.76}
{'loss': 0.52, 'grad_norm': 0.3040222227573395, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.43024376034736633, 'eval_runtime': 9.1284, 'eval_samples_per_second': 109.439, 'eval_steps_per_second': 6.902, 'epoch': 0.8}
{'loss': 0.519, 'grad_norm': 0.2235889881849289, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4239579439163208, 'eval_runtime': 9.1429, 'eval_samples_per_second': 109.265, 'eval_steps_per_second': 6.891, 'epoch': 0.84}
{'loss': 0.5438, 'grad_norm': 0.31193265318870544, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.41947513818740845, 'eval_runtime': 9.1487, 'eval_samples_per_second': 109.196, 'eval_steps_per_second': 6.886, 'epoch': 0.88}
{'loss': 0.5368, 'grad_norm': 0.30939650535583496, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4157595634460449, 'eval_runtime': 9.1357, 'eval_samples_per_second': 109.352, 'eval_steps_per_second': 6.896, 'epoch': 0.92}
{'loss': 0.5627, 'grad_norm': 0.37954628467559814, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.41242632269859314, 'eval_runtime': 9.1457, 'eval_samples_per_second': 109.231, 'eval_steps_per_second': 6.888, 'epoch': 0.96}
{'loss': 0.4692, 'grad_norm': 0.21323193609714508, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.41119465231895447, 'eval_runtime': 9.145, 'eval_samples_per_second': 109.24, 'eval_steps_per_second': 6.889, 'epoch': 1.0}
{'train_runtime': 514.416, 'train_samples_per_second': 19.438, 'train_steps_per_second': 1.215, 'train_loss': 0.7685555908203126, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3331125974655151, 1.1153753995895386, 1.0098850727081299, 0.9509983062744141, 0.8925836086273193, 0.8452508449554443, 0.7700563073158264, 0.724462628364563, 0.6713351011276245, 0.6254687309265137, 0.5950157642364502, 0.565406084060669, 0.5443488955497742, 0.5148574113845825, 0.49336498975753784, 0.4778592586517334, 0.46307042241096497, 0.4470808804035187, 0.4379529654979706, 0.43024376034736633, 0.4239579439163208, 0.41947513818740845, 0.4157595634460449, 0.41242632269859314, 0.41119465231895447], 'performance': [0.63, 0.52]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.62it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 28.63it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 34.20it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 38.99it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 45.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 47.60it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 61.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.13it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.52]
current iteration observed (possibly low-fid or predicted) performance:  1.2482727766036987
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.651
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1853 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2676165699958801, 0.05009537935256958, 0.8128373026847839, 0.27514880895614624, 0.8756583333015442, 0.7410405278205872, 0.8690311908721924, 0.26918065547943115, 0.756043016910553, 0.8426547050476074, 0.018447041511535645, 0.10982537269592285, 0.21289664506912231, 0.8607468008995056, 0.08691489696502686, 0.45598283410072327, 0.8770661950111389, 0.6069663763046265, 0.7397691607475281]  ‚Üí  acq = 1.0289263412449325
X = [0.07060253620147705, 0.4947226643562317, 0.16237682104110718, 0.29813480377197266, 0.24806135892868042, 0.628314733505249, 0.7315069437026978, 0.895024299621582, 0.6736090183258057, 0.9001978039741516, 0.8788895606994629, 0.7353760004043579, 0.13589835166931152, 0.2516027092933655, 0.25681543350219727, 0.5458636283874512, 0.023227810859680176, 0.7328213453292847, 0.8322544097900391]  ‚Üí  acq = 0.9818620532725155
X = [0.6613595485687256, 0.887019693851471, 0.47657227516174316, 0.6411178708076477, 0.6944774389266968, 0.8365639448165894, 0.8021358251571655, 0.8192504048347473, 0.8177878260612488, 0.5932173132896423, 0.617336094379425, 0.6421382427215576, 0.11952698230743408, 0.04799288511276245, 0.2864319682121277, 0.17480668425559998, 0.9850505590438843, 0.9875184297561646, 0.2940676212310791]  ‚Üí  acq = 1.0289234766438111
X = [0.7306765913963318, 0.004298865795135498, 0.23774147033691406, 0.15573155879974365, 0.28925132751464844, 0.9238865971565247, 0.6522131562232971, 0.8452191948890686, 0.16257965564727783, 0.13547693192958832, 0.06015998125076294, 0.5453985333442688, 0.481606125831604, 0.472128689289093, 0.11913812160491943, 0.30026623606681824, 0.7941622734069824, 0.971887469291687, 0.3454384207725525]  ‚Üí  acq = 0.9717083290064229
X = [0.19791817665100098, 0.23941630125045776, 0.051687657833099365, 0.18921977281570435, 0.4253740906715393, 0.18525397777557373, 0.4007197618484497, 0.9029441475868225, 0.05244570970535278, 0.26204755902290344, 0.35965901613235474, 0.4472508430480957, 0.29924464225769043, 0.21894919872283936, 0.2961270213127136, 0.21767891943454742, 0.3993695378303528, 0.2953560948371887, 0.19076406955718994]  ‚Üí  acq = 1.0214074668875643
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.3255, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6745, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.09999999999999978, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(7.9874e-17, dtype=torch.float64), tensor(1.3423e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3255, dtype=torch.float64), tensor(3.1840e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6745, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.325
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.675

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999978,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.09999999999999978
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  6.15it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.41it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.68it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.60it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.08it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.01it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.24it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.73it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.62it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.56it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 58.79it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.23it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.6841, 'grad_norm': 0.5849509239196777, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.2097090482711792, 'eval_runtime': 7.1635, 'eval_samples_per_second': 139.458, 'eval_steps_per_second': 8.795, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.55it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.16it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.20it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 46.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.58it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.33it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.03it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 57.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.93it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.90it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 60.14it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.29it/s]
Evaluation performance at step 50: 0.63
{'loss': 1.0731, 'grad_norm': 0.22702832520008087, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 0.9903919696807861, 'eval_runtime': 6.9196, 'eval_samples_per_second': 144.372, 'eval_steps_per_second': 9.105, 'epoch': 0.08}
{'loss': 0.9373, 'grad_norm': 0.24076303839683533, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8288996815681458, 'eval_runtime': 6.9463, 'eval_samples_per_second': 143.818, 'eval_steps_per_second': 9.07, 'epoch': 0.12}
{'loss': 0.7922, 'grad_norm': 0.2397221326828003, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7099189162254333, 'eval_runtime': 7.0139, 'eval_samples_per_second': 142.432, 'eval_steps_per_second': 8.982, 'epoch': 0.16}
{'loss': 0.7102, 'grad_norm': 0.2632772922515869, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6537412405014038, 'eval_runtime': 7.0434, 'eval_samples_per_second': 141.835, 'eval_steps_per_second': 8.945, 'epoch': 0.2}
{'loss': 0.6482, 'grad_norm': 0.3152729272842407, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5839094519615173, 'eval_runtime': 7.0591, 'eval_samples_per_second': 141.52, 'eval_steps_per_second': 8.925, 'epoch': 0.24}
{'loss': 0.6088, 'grad_norm': 0.3554995059967041, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5209255814552307, 'eval_runtime': 7.0397, 'eval_samples_per_second': 141.909, 'eval_steps_per_second': 8.949, 'epoch': 0.28}
{'loss': 0.5745, 'grad_norm': 0.33854204416275024, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.4722820818424225, 'eval_runtime': 7.031, 'eval_samples_per_second': 142.086, 'eval_steps_per_second': 8.96, 'epoch': 0.32}
{'loss': 0.5387, 'grad_norm': 0.3674258291721344, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.430074006319046, 'eval_runtime': 7.0198, 'eval_samples_per_second': 142.312, 'eval_steps_per_second': 8.975, 'epoch': 0.36}
{'loss': 0.4765, 'grad_norm': 0.3556935489177704, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.39142510294914246, 'eval_runtime': 6.9721, 'eval_samples_per_second': 143.286, 'eval_steps_per_second': 9.036, 'epoch': 0.4}
{'loss': 0.4642, 'grad_norm': 0.29659217596054077, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3585886061191559, 'eval_runtime': 6.973, 'eval_samples_per_second': 143.267, 'eval_steps_per_second': 9.035, 'epoch': 0.44}
{'loss': 0.3901, 'grad_norm': 0.34787797927856445, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3371209502220154, 'eval_runtime': 6.9738, 'eval_samples_per_second': 143.251, 'eval_steps_per_second': 9.034, 'epoch': 0.48}
{'loss': 0.3813, 'grad_norm': 0.2850714921951294, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.31726503372192383, 'eval_runtime': 6.9722, 'eval_samples_per_second': 143.283, 'eval_steps_per_second': 9.036, 'epoch': 0.52}
{'loss': 0.3404, 'grad_norm': 0.3568440079689026, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.298738956451416, 'eval_runtime': 6.9735, 'eval_samples_per_second': 143.257, 'eval_steps_per_second': 9.034, 'epoch': 0.56}
{'loss': 0.3203, 'grad_norm': 0.2593107223510742, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2780618667602539, 'eval_runtime': 6.974, 'eval_samples_per_second': 143.247, 'eval_steps_per_second': 9.034, 'epoch': 0.6}
{'loss': 0.3318, 'grad_norm': 0.35192638635635376, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.2665688693523407, 'eval_runtime': 6.975, 'eval_samples_per_second': 143.225, 'eval_steps_per_second': 9.032, 'epoch': 0.64}
{'loss': 0.3124, 'grad_norm': 0.2940467894077301, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.24993997812271118, 'eval_runtime': 6.9742, 'eval_samples_per_second': 143.242, 'eval_steps_per_second': 9.033, 'epoch': 0.68}
{'loss': 0.3262, 'grad_norm': 0.5093385577201843, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.2346794754266739, 'eval_runtime': 6.9782, 'eval_samples_per_second': 143.159, 'eval_steps_per_second': 9.028, 'epoch': 0.72}
{'loss': 0.2951, 'grad_norm': 0.31693002581596375, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.22255268692970276, 'eval_runtime': 6.9845, 'eval_samples_per_second': 143.03, 'eval_steps_per_second': 9.02, 'epoch': 0.76}
{'loss': 0.2931, 'grad_norm': 0.14626266062259674, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.20972678065299988, 'eval_runtime': 6.9843, 'eval_samples_per_second': 143.034, 'eval_steps_per_second': 9.02, 'epoch': 0.8}
{'loss': 0.2742, 'grad_norm': 0.25623050332069397, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.20229029655456543, 'eval_runtime': 6.9742, 'eval_samples_per_second': 143.242, 'eval_steps_per_second': 9.033, 'epoch': 0.84}
{'loss': 0.2737, 'grad_norm': 0.2858489751815796, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.19689184427261353, 'eval_runtime': 7.0082, 'eval_samples_per_second': 142.548, 'eval_steps_per_second': 8.99, 'epoch': 0.88}
{'loss': 0.2423, 'grad_norm': 0.23010997474193573, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.19349589943885803, 'eval_runtime': 7.0417, 'eval_samples_per_second': 141.87, 'eval_steps_per_second': 8.947, 'epoch': 0.92}
{'loss': 0.247, 'grad_norm': 0.34284690022468567, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.19000686705112457, 'eval_runtime': 7.0368, 'eval_samples_per_second': 141.967, 'eval_steps_per_second': 8.953, 'epoch': 0.96}
{'loss': 0.232, 'grad_norm': 0.2909955382347107, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.18874837458133698, 'eval_runtime': 7.0287, 'eval_samples_per_second': 142.132, 'eval_steps_per_second': 8.963, 'epoch': 1.0}
{'train_runtime': 395.3197, 'train_samples_per_second': 25.293, 'train_steps_per_second': 1.581, 'train_loss': 0.5507118202209472, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2097090482711792, 0.9903919696807861, 0.8288996815681458, 0.7099189162254333, 0.6537412405014038, 0.5839094519615173, 0.5209255814552307, 0.4722820818424225, 0.430074006319046, 0.39142510294914246, 0.3585886061191559, 0.3371209502220154, 0.31726503372192383, 0.298738956451416, 0.2780618667602539, 0.2665688693523407, 0.24993997812271118, 0.2346794754266739, 0.22255268692970276, 0.20972678065299988, 0.20229029655456543, 0.19689184427261353, 0.19349589943885803, 0.19000686705112457, 0.18874837458133698], 'performance': [0.62, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.83it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.67it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.44it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.23it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.00it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 58.81it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.24687922000885
current iteration best possible performance (full train run):  0.63
max performance so far:  0.651
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8881 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.40685564279556274, 0.39035719633102417, 0.6683285236358643, 0.17839092016220093, 0.16464561223983765, 0.4280046820640564, 0.6866235733032227, 0.5048015117645264, 0.6379832029342651, 0.548767626285553, 0.9746066331863403, 0.6363967657089233, 0.9073230028152466, 0.9121650457382202, 0.33419090509414673, 0.8987778425216675, 0.6975538730621338, 0.3334830105304718, 0.6953573226928711]  ‚Üí  acq = 0.9708488051008056
X = [0.4835927486419678, 0.05785036087036133, 0.9475119709968567, 0.8744463920593262, 0.24723225831985474, 0.8936115503311157, 0.9881628751754761, 0.9870502948760986, 0.5485019087791443, 0.7914798259735107, 0.36764925718307495, 0.6675997972488403, 0.8196915984153748, 0.7172710299491882, 0.5778520107269287, 0.9738675951957703, 0.8543980121612549, 0.9197123050689697, 0.6446119546890259]  ‚Üí  acq = 0.9797854411034765
X = [0.6578963398933411, 0.0816299319267273, 0.8131148815155029, 0.27954965829849243, 0.8601289987564087, 0.7604178786277771, 0.3440965414047241, 0.9159334301948547, 0.7187910676002502, 0.6201157569885254, 0.1766255497932434, 0.3155909776687622, 0.5532656908035278, 0.3574908375740051, 0.5299145579338074, 0.6330821514129639, 0.6014247536659241, 0.3344332277774811, 0.7641726732254028]  ‚Üí  acq = 1.0206209831617592
X = [0.9344323873519897, 0.3524037003517151, 0.6896010041236877, 0.1377587914466858, 0.6498231291770935, 0.8575630187988281, 0.7518943548202515, 0.9634361267089844, 0.743019700050354, 0.9762428402900696, 0.04415541887283325, 0.8341125845909119, 0.6749036908149719, 0.01980435848236084, 0.673465371131897, 0.034642890095710754, 0.34327733516693115, 0.588912844657898, 0.8255391120910645]  ‚Üí  acq = 1.020607160637709
X = [0.4430288076400757, 0.6287723183631897, 0.727653980255127, 0.4034571051597595, 0.9500873684883118, 0.19143694639205933, 0.5471545457839966, 0.7528753876686096, 0.12118703126907349, 0.9224622249603271, 0.30433475971221924, 0.3606336712837219, 0.5619364380836487, 0.4938642978668213, 0.6687572598457336, 0.3765754997730255, 0.5833379030227661, 0.11373197287321091, 0.23658573627471924]  ‚Üí  acq = 1.0206209976115455
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.2994, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.7006, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.09999999999999998, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(8.6789e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2994, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9897e-16, dtype=torch.float64), tensor(4.0725e-17, dtype=torch.float64), tensor(7.4482e-17, dtype=torch.float64), tensor(1.7793e-17, dtype=torch.float64), tensor(0.7006, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.299
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.701

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999998,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.09999999999999998
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.53it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.85it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.97it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.27it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.71it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.94it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.65it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.76it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.86it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 60.21it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 60.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 55.80it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.8609, 'grad_norm': 0.4515427350997925, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.6839009523391724, 'eval_runtime': 10.1407, 'eval_samples_per_second': 98.514, 'eval_steps_per_second': 6.213, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.65it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 36.60it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.98it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 48.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 51.34it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 55.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 57.91it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 62.65it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.38it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 63.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.96it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 61.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 61.33it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.34it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.4842, 'grad_norm': 0.21662293374538422, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.394868016242981, 'eval_runtime': 10.1343, 'eval_samples_per_second': 98.576, 'eval_steps_per_second': 6.217, 'epoch': 0.08}
{'loss': 1.3384, 'grad_norm': 0.18446078896522522, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2688238620758057, 'eval_runtime': 10.228, 'eval_samples_per_second': 97.674, 'eval_steps_per_second': 6.16, 'epoch': 0.12}
{'loss': 1.2495, 'grad_norm': 0.2137136459350586, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1785852909088135, 'eval_runtime': 10.2921, 'eval_samples_per_second': 97.065, 'eval_steps_per_second': 6.121, 'epoch': 0.16}
{'loss': 1.1252, 'grad_norm': 0.23017750680446625, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.141605257987976, 'eval_runtime': 10.2735, 'eval_samples_per_second': 97.24, 'eval_steps_per_second': 6.132, 'epoch': 0.2}
{'loss': 1.0527, 'grad_norm': 0.214908167719841, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1019623279571533, 'eval_runtime': 10.3312, 'eval_samples_per_second': 96.697, 'eval_steps_per_second': 6.098, 'epoch': 0.24}
{'loss': 0.9913, 'grad_norm': 0.282177597284317, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0605934858322144, 'eval_runtime': 10.35, 'eval_samples_per_second': 96.522, 'eval_steps_per_second': 6.087, 'epoch': 0.28}
{'loss': 1.0443, 'grad_norm': 0.28154274821281433, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0242035388946533, 'eval_runtime': 10.3179, 'eval_samples_per_second': 96.822, 'eval_steps_per_second': 6.106, 'epoch': 0.32}
{'loss': 0.9665, 'grad_norm': 0.3728768825531006, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9875528216362, 'eval_runtime': 10.3208, 'eval_samples_per_second': 96.795, 'eval_steps_per_second': 6.104, 'epoch': 0.36}
{'loss': 0.9725, 'grad_norm': 0.234596386551857, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.959716796875, 'eval_runtime': 10.2822, 'eval_samples_per_second': 97.158, 'eval_steps_per_second': 6.127, 'epoch': 0.4}
{'loss': 0.9781, 'grad_norm': 0.26448673009872437, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9273736476898193, 'eval_runtime': 10.3508, 'eval_samples_per_second': 96.514, 'eval_steps_per_second': 6.086, 'epoch': 0.44}
{'loss': 0.9548, 'grad_norm': 0.30768007040023804, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9026837944984436, 'eval_runtime': 10.3325, 'eval_samples_per_second': 96.685, 'eval_steps_per_second': 6.097, 'epoch': 0.48}
{'loss': 1.0053, 'grad_norm': 0.22798633575439453, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8811408281326294, 'eval_runtime': 10.3349, 'eval_samples_per_second': 96.663, 'eval_steps_per_second': 6.096, 'epoch': 0.52}
{'loss': 0.8648, 'grad_norm': 0.2399609386920929, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8600037097930908, 'eval_runtime': 10.3584, 'eval_samples_per_second': 96.443, 'eval_steps_per_second': 6.082, 'epoch': 0.56}
{'loss': 0.8752, 'grad_norm': 0.26903632283210754, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.841247022151947, 'eval_runtime': 10.3227, 'eval_samples_per_second': 96.777, 'eval_steps_per_second': 6.103, 'epoch': 0.6}
{'loss': 0.9158, 'grad_norm': 0.2407732456922531, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8225461840629578, 'eval_runtime': 10.359, 'eval_samples_per_second': 96.438, 'eval_steps_per_second': 6.082, 'epoch': 0.64}
{'loss': 0.844, 'grad_norm': 0.17107334733009338, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8095003962516785, 'eval_runtime': 10.2968, 'eval_samples_per_second': 97.02, 'eval_steps_per_second': 6.118, 'epoch': 0.68}
{'loss': 0.8994, 'grad_norm': 0.32517191767692566, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7930344939231873, 'eval_runtime': 10.2704, 'eval_samples_per_second': 97.27, 'eval_steps_per_second': 6.134, 'epoch': 0.72}
{'loss': 0.8988, 'grad_norm': 0.27498534321784973, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7832192778587341, 'eval_runtime': 10.2716, 'eval_samples_per_second': 97.258, 'eval_steps_per_second': 6.133, 'epoch': 0.76}
{'loss': 0.8297, 'grad_norm': 0.31733354926109314, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7727853059768677, 'eval_runtime': 10.2693, 'eval_samples_per_second': 97.28, 'eval_steps_per_second': 6.135, 'epoch': 0.8}
{'loss': 0.9064, 'grad_norm': 0.16207866370677948, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7643386125564575, 'eval_runtime': 10.291, 'eval_samples_per_second': 97.075, 'eval_steps_per_second': 6.122, 'epoch': 0.84}
{'loss': 0.8638, 'grad_norm': 0.1658865362405777, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.758173942565918, 'eval_runtime': 10.2687, 'eval_samples_per_second': 97.286, 'eval_steps_per_second': 6.135, 'epoch': 0.88}
{'loss': 0.768, 'grad_norm': 0.18068481981754303, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7550046443939209, 'eval_runtime': 10.2473, 'eval_samples_per_second': 97.489, 'eval_steps_per_second': 6.148, 'epoch': 0.92}
{'loss': 0.8587, 'grad_norm': 0.18575452268123627, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7523728609085083, 'eval_runtime': 10.2552, 'eval_samples_per_second': 97.414, 'eval_steps_per_second': 6.143, 'epoch': 0.96}
{'loss': 0.7481, 'grad_norm': 0.19858728349208832, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7515508532524109, 'eval_runtime': 10.2742, 'eval_samples_per_second': 97.234, 'eval_steps_per_second': 6.132, 'epoch': 1.0}
{'train_runtime': 544.5957, 'train_samples_per_second': 18.36, 'train_steps_per_second': 1.148, 'train_loss': 1.0518516021728515, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6839009523391724, 1.394868016242981, 1.2688238620758057, 1.1785852909088135, 1.141605257987976, 1.1019623279571533, 1.0605934858322144, 1.0242035388946533, 0.9875528216362, 0.959716796875, 0.9273736476898193, 0.9026837944984436, 0.8811408281326294, 0.8600037097930908, 0.841247022151947, 0.8225461840629578, 0.8095003962516785, 0.7930344939231873, 0.7832192778587341, 0.7727853059768677, 0.7643386125564575, 0.758173942565918, 0.7550046443939209, 0.7523728609085083, 0.7515508532524109], 'performance': [0.63, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.78it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.67it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 52.97it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.44it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.49it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2465814352035522
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.651
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.7346 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9319775104522705, 0.3655719757080078, 0.6493487358093262, 0.7032051682472229, 0.542920708656311, 0.16185641288757324, 0.36940109729766846, 0.793797492980957, 0.05289262533187866, 0.0712268278002739, 0.21700918674468994, 0.5615952014923096, 0.08549737930297852, 0.2054244875907898, 0.38690412044525146, 0.6032564043998718, 0.9026855826377869, 0.7471753358840942, 0.6020525693893433]  ‚Üí  acq = 1.003651348673256
X = [0.5037892460823059, 0.2463057041168213, 0.7810913920402527, 0.2668842077255249, 0.44900304079055786, 0.22197848558425903, 0.7925740480422974, 0.2286773920059204, 0.41979897022247314, 0.35291001200675964, 0.062223970890045166, 0.029854297637939453, 0.5561855435371399, 0.4943855404853821, 0.8535159826278687, 0.05418674647808075, 0.25151777267456055, 0.06507664918899536, 0.17633581161499023]  ‚Üí  acq = 0.9954469270587427
X = [0.022059857845306396, 0.7768075466156006, 0.5663529634475708, 0.8611503839492798, 0.8474230170249939, 0.3139118552207947, 0.059894859790802, 0.42257463932037354, 0.6395968794822693, 0.5212297439575195, 0.7591906785964966, 0.33218294382095337, 0.24012255668640137, 0.9893919229507446, 0.6086559891700745, 0.9990507364273071, 0.01348966360092163, 0.09252259135246277, 0.823517918586731]  ‚Üí  acq = 1.0037975815458378
X = [0.5340758562088013, 0.4371677041053772, 0.31703656911849976, 0.12611567974090576, 0.18851327896118164, 0.12940073013305664, 0.3762919306755066, 0.47999441623687744, 0.261313259601593, 0.4031679332256317, 0.02085179090499878, 0.8304163217544556, 0.3032383918762207, 0.6169120669364929, 0.361413836479187, 0.6508481502532959, 0.1964874267578125, 0.4624755382537842, 0.9065936803817749]  ‚Üí  acq = 0.7076169440239497
X = [0.6505399346351624, 0.7485781311988831, 0.48586922883987427, 0.06686937808990479, 0.4750337600708008, 0.14166080951690674, 0.5646201968193054, 0.3027225732803345, 0.3331472873687744, 0.8013460636138916, 0.6811515092849731, 0.08358621597290039, 0.9963775277137756, 0.5006908774375916, 0.7250810265541077, 0.19186821579933167, 0.014074325561523438, 0.748652458190918, 0.16274040937423706]  ‚Üí  acq = 0.9982840448136765
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3003, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.6997, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.07302511996015221, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.3003, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.8015e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6997, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7303, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.3
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.7

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.07302511996015221,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.07302511996015221
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.07it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.86it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.73it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.13it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.28it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.30it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.23it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.54it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.03it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.54it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.49it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.1563, 'grad_norm': 0.3643677234649658, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.0386407375335693, 'eval_runtime': 10.2959, 'eval_samples_per_second': 97.029, 'eval_steps_per_second': 6.119, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:22,  4.43it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.79it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.57it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.38it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 42.82it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.21it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.30it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.23it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.68it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.00it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.99it/s]
Evaluation performance at step 50: 0.57
{'loss': 0.9395, 'grad_norm': 0.34667614102363586, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 0.8440341949462891, 'eval_runtime': 10.297, 'eval_samples_per_second': 97.019, 'eval_steps_per_second': 6.118, 'epoch': 0.08}
{'loss': 0.8218, 'grad_norm': 0.22573910653591156, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7824283242225647, 'eval_runtime': 10.2664, 'eval_samples_per_second': 97.308, 'eval_steps_per_second': 6.137, 'epoch': 0.12}
{'loss': 0.8092, 'grad_norm': 0.21662864089012146, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7341558933258057, 'eval_runtime': 10.2793, 'eval_samples_per_second': 97.185, 'eval_steps_per_second': 6.129, 'epoch': 0.16}
{'loss': 0.7366, 'grad_norm': 0.23338302969932556, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6920531392097473, 'eval_runtime': 10.2975, 'eval_samples_per_second': 97.014, 'eval_steps_per_second': 6.118, 'epoch': 0.2}
{'loss': 0.6979, 'grad_norm': 0.3006191849708557, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6461511850357056, 'eval_runtime': 10.3162, 'eval_samples_per_second': 96.838, 'eval_steps_per_second': 6.107, 'epoch': 0.24}
{'loss': 0.6655, 'grad_norm': 0.2763894498348236, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6046127080917358, 'eval_runtime': 10.3242, 'eval_samples_per_second': 96.763, 'eval_steps_per_second': 6.102, 'epoch': 0.28}
{'loss': 0.5975, 'grad_norm': 0.2504698634147644, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5625515580177307, 'eval_runtime': 10.3729, 'eval_samples_per_second': 96.308, 'eval_steps_per_second': 6.073, 'epoch': 0.32}
{'loss': 0.5678, 'grad_norm': 0.40647879242897034, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.520196795463562, 'eval_runtime': 10.3832, 'eval_samples_per_second': 96.214, 'eval_steps_per_second': 6.068, 'epoch': 0.36}
{'loss': 0.5469, 'grad_norm': 0.2732391953468323, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.4904663562774658, 'eval_runtime': 10.4522, 'eval_samples_per_second': 95.578, 'eval_steps_per_second': 6.027, 'epoch': 0.4}
{'loss': 0.5306, 'grad_norm': 0.2861871123313904, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4645601511001587, 'eval_runtime': 10.4264, 'eval_samples_per_second': 95.815, 'eval_steps_per_second': 6.042, 'epoch': 0.44}
{'loss': 0.5055, 'grad_norm': 0.25257185101509094, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.4405668079853058, 'eval_runtime': 10.4026, 'eval_samples_per_second': 96.034, 'eval_steps_per_second': 6.056, 'epoch': 0.48}
{'loss': 0.5159, 'grad_norm': 0.3010236322879791, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.4227217137813568, 'eval_runtime': 10.4147, 'eval_samples_per_second': 95.922, 'eval_steps_per_second': 6.049, 'epoch': 0.52}
{'loss': 0.4434, 'grad_norm': 0.2519568204879761, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4036821722984314, 'eval_runtime': 10.4089, 'eval_samples_per_second': 95.975, 'eval_steps_per_second': 6.052, 'epoch': 0.56}
{'loss': 0.4503, 'grad_norm': 0.24887141585350037, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.38906148076057434, 'eval_runtime': 10.4044, 'eval_samples_per_second': 96.017, 'eval_steps_per_second': 6.055, 'epoch': 0.6}
{'loss': 0.466, 'grad_norm': 0.21310904622077942, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.3750995695590973, 'eval_runtime': 10.3993, 'eval_samples_per_second': 96.064, 'eval_steps_per_second': 6.058, 'epoch': 0.64}
{'loss': 0.4317, 'grad_norm': 0.30129000544548035, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3627917468547821, 'eval_runtime': 10.454, 'eval_samples_per_second': 95.561, 'eval_steps_per_second': 6.026, 'epoch': 0.68}
{'loss': 0.4563, 'grad_norm': 0.2014409899711609, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3512100279331207, 'eval_runtime': 10.4514, 'eval_samples_per_second': 95.586, 'eval_steps_per_second': 6.028, 'epoch': 0.72}
{'loss': 0.445, 'grad_norm': 0.19576555490493774, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.34608709812164307, 'eval_runtime': 10.4041, 'eval_samples_per_second': 96.02, 'eval_steps_per_second': 6.055, 'epoch': 0.76}
{'loss': 0.4008, 'grad_norm': 0.24891771376132965, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.33752888441085815, 'eval_runtime': 10.3822, 'eval_samples_per_second': 96.222, 'eval_steps_per_second': 6.068, 'epoch': 0.8}
{'loss': 0.4322, 'grad_norm': 0.20942340791225433, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3290244936943054, 'eval_runtime': 10.3963, 'eval_samples_per_second': 96.092, 'eval_steps_per_second': 6.06, 'epoch': 0.84}
{'loss': 0.4119, 'grad_norm': 0.17558181285858154, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.32353317737579346, 'eval_runtime': 10.3976, 'eval_samples_per_second': 96.08, 'eval_steps_per_second': 6.059, 'epoch': 0.88}
{'loss': 0.3726, 'grad_norm': 0.1651138812303543, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.31903108954429626, 'eval_runtime': 10.4214, 'eval_samples_per_second': 95.86, 'eval_steps_per_second': 6.045, 'epoch': 0.92}
{'loss': 0.3901, 'grad_norm': 0.22249750792980194, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.316744863986969, 'eval_runtime': 10.3785, 'eval_samples_per_second': 96.257, 'eval_steps_per_second': 6.07, 'epoch': 0.96}
{'loss': 0.3496, 'grad_norm': 0.15324491262435913, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.31606626510620117, 'eval_runtime': 10.391, 'eval_samples_per_second': 96.141, 'eval_steps_per_second': 6.063, 'epoch': 1.0}
{'train_runtime': 558.4362, 'train_samples_per_second': 17.905, 'train_steps_per_second': 1.119, 'train_loss': 0.6056292327880859, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0386407375335693, 0.8440341949462891, 0.7824283242225647, 0.7341558933258057, 0.6920531392097473, 0.6461511850357056, 0.6046127080917358, 0.5625515580177307, 0.520196795463562, 0.4904663562774658, 0.4645601511001587, 0.4405668079853058, 0.4227217137813568, 0.4036821722984314, 0.38906148076057434, 0.3750995695590973, 0.3627917468547821, 0.3512100279331207, 0.34608709812164307, 0.33752888441085815, 0.3290244936943054, 0.32353317737579346, 0.31903108954429626, 0.316744863986969, 0.31606626510620117], 'performance': [0.62, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:38,  2.57it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.16it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.65it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 44.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.08it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.23it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 70.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.29it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.2491205930709839
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.651
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3199 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5838610529899597, 0.04236328601837158, 0.19560039043426514, 0.4905102252960205, 0.30678439140319824, 0.14869606494903564, 0.5454267263412476, 0.7882201671600342, 0.46907639503479004, 0.8025528192520142, 0.29663074016571045, 0.34233689308166504, 0.6710034608840942, 0.8255642652511597, 0.286285400390625, 0.5991491079330444, 0.8582577109336853, 0.044964198023080826, 0.07679176330566406]  ‚Üí  acq = 0.9051580910842385
X = [0.5152655243873596, 0.10480529069900513, 0.6655109524726868, 0.03623384237289429, 0.10131490230560303, 0.07930868864059448, 0.8228660821914673, 0.7990092635154724, 0.1491125226020813, 0.12989474833011627, 0.30011963844299316, 0.562957763671875, 0.7295524477958679, 0.6549836993217468, 0.6571943163871765, 0.35044625401496887, 0.48587602376937866, 0.11221357434988022, 0.15928804874420166]  ‚Üí  acq = 0.7482076475540966
X = [0.5368649363517761, 0.3982643485069275, 0.6292279362678528, 0.7810671925544739, 0.5709779858589172, 0.10295450687408447, 0.7676753997802734, 0.7117535471916199, 0.9774518013000488, 0.23157523572444916, 0.0538865327835083, 0.9648066759109497, 0.640056312084198, 0.12956231832504272, 0.4334040880203247, 0.595800518989563, 0.27445727586746216, 0.732178807258606, 0.9177902340888977]  ‚Üí  acq = 0.9877533442550275
X = [0.05928230285644531, 0.5111358165740967, 0.6208975315093994, 0.7416911721229553, 0.13495999574661255, 0.5329247117042542, 0.3060787320137024, 0.39218050241470337, 0.5444698929786682, 0.43418654799461365, 0.7832498550415039, 0.15273773670196533, 0.77518230676651, 0.4962908625602722, 0.41853803396224976, 0.986393392086029, 0.15150392055511475, 0.059195347130298615, 0.10973715782165527]  ‚Üí  acq = 0.8875669881885463
X = [0.8117028474807739, 0.5006781220436096, 0.6540417671203613, 0.2978166341781616, 0.2760578393936157, 0.11399728059768677, 0.36787599325180054, 0.904978334903717, 0.9091209769248962, 0.3706066906452179, 0.067501962184906, 0.48582929372787476, 0.5383182168006897, 0.979578971862793, 0.7421678900718689, 0.9020864963531494, 0.17683923244476318, 0.6355545520782471, 0.41553884744644165]  ‚Üí  acq = 0.9768449443281235
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.2755, dtype=torch.float64), 0, 0, tensor(0.7245, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1.6925e-17, dtype=torch.float64), tensor(1.2453e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(6.3119e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2755, dtype=torch.float64), tensor(7.1170e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7245, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.275
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.725

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Using the latest cached version of the dataset since trivia_qa couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'rc.nocontext' at /home/alfred/.cache/huggingface/datasets/trivia_qa/rc.nocontext/0.0.0/0f7faf33a3908546c6fd5b73a660e0f8ff173c2f (last modified on Fri Dec 12 10:26:21 2025).
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.91it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.33it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.15it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.42it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.22it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.57it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.58it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 49.81it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.43it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.3631, 'grad_norm': 0.4723665118217468, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.0066890716552734, 'eval_runtime': 7.4678, 'eval_samples_per_second': 133.775, 'eval_steps_per_second': 8.436, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  4.95it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.50it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.48it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.30it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.05it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.79it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.15it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 44.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 44.58it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 49.70it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 50.10it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 50.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.38it/s]
Evaluation performance at step 50: 0.57
{'loss': 0.891, 'grad_norm': 0.4488487243652344, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 0.7933356761932373, 'eval_runtime': 7.4747, 'eval_samples_per_second': 133.651, 'eval_steps_per_second': 8.428, 'epoch': 0.08}
{'loss': 0.7639, 'grad_norm': 0.2576988935470581, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.708206057548523, 'eval_runtime': 7.5037, 'eval_samples_per_second': 133.135, 'eval_steps_per_second': 8.396, 'epoch': 0.12}
{'loss': 0.6908, 'grad_norm': 0.3150218427181244, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6459921598434448, 'eval_runtime': 7.4909, 'eval_samples_per_second': 133.361, 'eval_steps_per_second': 8.41, 'epoch': 0.16}
{'loss': 0.639, 'grad_norm': 0.31163570284843445, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.5901537537574768, 'eval_runtime': 7.5138, 'eval_samples_per_second': 132.956, 'eval_steps_per_second': 8.385, 'epoch': 0.2}
{'loss': 0.5777, 'grad_norm': 0.34080442786216736, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5114919543266296, 'eval_runtime': 7.5088, 'eval_samples_per_second': 133.044, 'eval_steps_per_second': 8.39, 'epoch': 0.24}
{'loss': 0.5076, 'grad_norm': 0.2895725965499878, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.4401705265045166, 'eval_runtime': 7.5117, 'eval_samples_per_second': 132.993, 'eval_steps_per_second': 8.387, 'epoch': 0.28}
{'loss': 0.4547, 'grad_norm': 0.3241710364818573, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.3824383318424225, 'eval_runtime': 7.4884, 'eval_samples_per_second': 133.406, 'eval_steps_per_second': 8.413, 'epoch': 0.32}
{'loss': 0.3776, 'grad_norm': 0.28705036640167236, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.3456924855709076, 'eval_runtime': 7.4847, 'eval_samples_per_second': 133.473, 'eval_steps_per_second': 8.417, 'epoch': 0.36}
{'loss': 0.3861, 'grad_norm': 0.4118798077106476, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.3005146384239197, 'eval_runtime': 7.479, 'eval_samples_per_second': 133.574, 'eval_steps_per_second': 8.424, 'epoch': 0.4}
{'loss': 0.3505, 'grad_norm': 0.3456379771232605, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.2709561884403229, 'eval_runtime': 7.484, 'eval_samples_per_second': 133.485, 'eval_steps_per_second': 8.418, 'epoch': 0.44}
{'loss': 0.3092, 'grad_norm': 0.3324752449989319, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.23357142508029938, 'eval_runtime': 7.48, 'eval_samples_per_second': 133.556, 'eval_steps_per_second': 8.422, 'epoch': 0.48}
{'loss': 0.271, 'grad_norm': 0.33928486704826355, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.21217961609363556, 'eval_runtime': 7.497, 'eval_samples_per_second': 133.253, 'eval_steps_per_second': 8.403, 'epoch': 0.52}
{'loss': 0.2626, 'grad_norm': 0.3378584384918213, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.18862102925777435, 'eval_runtime': 7.4931, 'eval_samples_per_second': 133.322, 'eval_steps_per_second': 8.408, 'epoch': 0.56}
{'loss': 0.2353, 'grad_norm': 0.28765949606895447, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.17123936116695404, 'eval_runtime': 7.5408, 'eval_samples_per_second': 132.479, 'eval_steps_per_second': 8.355, 'epoch': 0.6}
{'loss': 0.2152, 'grad_norm': 0.31097927689552307, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.1605531871318817, 'eval_runtime': 7.5541, 'eval_samples_per_second': 132.246, 'eval_steps_per_second': 8.34, 'epoch': 0.64}
{'loss': 0.1926, 'grad_norm': 0.269015371799469, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.1479489803314209, 'eval_runtime': 7.5537, 'eval_samples_per_second': 132.252, 'eval_steps_per_second': 8.34, 'epoch': 0.68}
{'loss': 0.1948, 'grad_norm': 0.20700938999652863, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.13958454132080078, 'eval_runtime': 7.5526, 'eval_samples_per_second': 132.273, 'eval_steps_per_second': 8.342, 'epoch': 0.72}
{'loss': 0.1803, 'grad_norm': 0.3334724009037018, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.13091634213924408, 'eval_runtime': 7.5713, 'eval_samples_per_second': 131.946, 'eval_steps_per_second': 8.321, 'epoch': 0.76}
{'loss': 0.1537, 'grad_norm': 0.20168811082839966, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.1242000088095665, 'eval_runtime': 7.5682, 'eval_samples_per_second': 132.0, 'eval_steps_per_second': 8.324, 'epoch': 0.8}
{'loss': 0.1356, 'grad_norm': 0.28211644291877747, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.11918116360902786, 'eval_runtime': 7.5939, 'eval_samples_per_second': 131.552, 'eval_steps_per_second': 8.296, 'epoch': 0.84}
{'loss': 0.1368, 'grad_norm': 0.15339329838752747, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.1138334572315216, 'eval_runtime': 7.5966, 'eval_samples_per_second': 131.507, 'eval_steps_per_second': 8.293, 'epoch': 0.88}
{'loss': 0.1321, 'grad_norm': 0.1990293562412262, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.1100328117609024, 'eval_runtime': 7.5545, 'eval_samples_per_second': 132.238, 'eval_steps_per_second': 8.339, 'epoch': 0.92}
{'loss': 0.1255, 'grad_norm': 0.26345017552375793, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.10731393098831177, 'eval_runtime': 7.5361, 'eval_samples_per_second': 132.562, 'eval_steps_per_second': 8.36, 'epoch': 0.96}
{'loss': 0.1279, 'grad_norm': 0.15865230560302734, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.1060904785990715, 'eval_runtime': 7.5386, 'eval_samples_per_second': 132.518, 'eval_steps_per_second': 8.357, 'epoch': 1.0}
{'train_runtime': 448.3608, 'train_samples_per_second': 22.301, 'train_steps_per_second': 1.394, 'train_loss': 0.4269864673614502, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0066890716552734, 0.7933356761932373, 0.708206057548523, 0.6459921598434448, 0.5901537537574768, 0.5114919543266296, 0.4401705265045166, 0.3824383318424225, 0.3456924855709076, 0.3005146384239197, 0.2709561884403229, 0.23357142508029938, 0.21217961609363556, 0.18862102925777435, 0.17123936116695404, 0.1605531871318817, 0.1479489803314209, 0.13958454132080078, 0.13091634213924408, 0.1242000088095665, 0.11918116360902786, 0.1138334572315216, 0.1100328117609024, 0.10731393098831177, 0.1060904785990715], 'performance': [0.63, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 29.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 38.51it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 38.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 45.02it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 48.86it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 63.28it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.51it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.247354507446289
current iteration best possible performance (full train run):  0.5880000000000001
max performance so far:  0.651
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3916 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6781049966812134, 0.3479852080345154, 0.5102622509002686, 0.8038994669914246, 0.6442047953605652, 0.3868564963340759, 0.32666367292404175, 0.5092322826385498, 0.4259360432624817, 0.20281469821929932, 0.09597671031951904, 0.19993919134140015, 0.06522715091705322, 0.4566006064414978, 0.005524754524230957, 0.0486307293176651, 0.5814146399497986, 0.8280243873596191, 0.5397253632545471]  ‚Üí  acq = 0.9813019653506997
X = [0.5562145113945007, 0.8155552744865417, 0.6676850318908691, 0.28831934928894043, 0.38356202840805054, 0.7610248327255249, 0.6004678606987, 0.3595930337905884, 0.7299065589904785, 0.7022190690040588, 0.19405782222747803, 0.4393198490142822, 0.3637639284133911, 0.8109979033470154, 0.7511748671531677, 0.7375595569610596, 0.08848613500595093, 0.20459695160388947, 0.8890791535377502]  ‚Üí  acq = 0.961858717337067
X = [0.09274232387542725, 0.6872765421867371, 0.5184670686721802, 0.3195956349372864, 0.03150308132171631, 0.8577396869659424, 0.5955685377120972, 0.07632237672805786, 0.6101509928703308, 0.7571964859962463, 0.9813784956932068, 0.6416856050491333, 0.5271301865577698, 0.6430464386940002, 0.4891604781150818, 0.42948290705680847, 0.614726185798645, 0.12887290120124817, 0.13427436351776123]  ‚Üí  acq = 1.0596778103777784
X = [0.42897307872772217, 0.8907100558280945, 0.9058293700218201, 0.5923592448234558, 0.7527062892913818, 0.24076086282730103, 0.947281002998352, 0.8487843871116638, 0.8092248439788818, 0.5392772555351257, 0.8249647617340088, 0.5184991955757141, 0.7692602872848511, 0.5707024335861206, 0.6885986328125, 0.6543653607368469, 0.49551016092300415, 0.32401242852211, 0.5228598713874817]  ‚Üí  acq = 0.9813216369327422
X = [0.03539532423019409, 0.6096476316452026, 0.6978389620780945, 0.864052414894104, 0.840135395526886, 0.6226072311401367, 0.6537296175956726, 0.43565839529037476, 0.5983365178108215, 0.30314064025878906, 0.9303818345069885, 0.7893745303153992, 0.4542014002799988, 0.2215697169303894, 0.3052491545677185, 0.35358837246894836, 0.6937093734741211, 0.3355548679828644, 0.5179179310798645]  ‚Üí  acq = 0.9813218458114816
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, tensor(0.2753, dtype=torch.float64), 0, 0, 0, 0, tensor(0.7247, dtype=torch.float64), 32, 0, 1, 1, 1, 0, 128, 0.0, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(3.9258e-17, dtype=torch.float64), tensor(3.8804e-17, dtype=torch.float64), tensor(0.2753, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3351e-16, dtype=torch.float64), tensor(1.7756e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7247, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.275
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.725

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 171,966,464 || all params: 8,202,227,712 || trainable%: 2.0966
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.77it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.16it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.53it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 49.25it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 52.16it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 56.19it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 59.19it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 64.03it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 59.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.73it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 62.83it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 59.97it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.4545, 'grad_norm': 0.48446008563041687, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.010255217552185, 'eval_runtime': 7.0406, 'eval_samples_per_second': 141.891, 'eval_steps_per_second': 8.948, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.86it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 38.19it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 49.51it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 52.53it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.36it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 57.89it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 60.29it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 65.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 60.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 64.34it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 63.38it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 61.55it/s]
Evaluation performance at step 50: 0.62
{'loss': 0.9124, 'grad_norm': 0.49859410524368286, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 0.8000925779342651, 'eval_runtime': 6.8295, 'eval_samples_per_second': 146.277, 'eval_steps_per_second': 9.225, 'epoch': 0.08}
{'loss': 0.8059, 'grad_norm': 0.2895510196685791, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.746900200843811, 'eval_runtime': 6.8302, 'eval_samples_per_second': 146.261, 'eval_steps_per_second': 9.224, 'epoch': 0.12}
{'loss': 0.7295, 'grad_norm': 0.2526407539844513, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6915762424468994, 'eval_runtime': 6.8576, 'eval_samples_per_second': 145.678, 'eval_steps_per_second': 9.187, 'epoch': 0.16}
{'loss': 0.6826, 'grad_norm': 0.3390180766582489, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.631845235824585, 'eval_runtime': 6.8687, 'eval_samples_per_second': 145.443, 'eval_steps_per_second': 9.172, 'epoch': 0.2}
{'loss': 0.6548, 'grad_norm': 0.32108837366104126, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5768163204193115, 'eval_runtime': 6.8726, 'eval_samples_per_second': 145.359, 'eval_steps_per_second': 9.167, 'epoch': 0.24}
{'loss': 0.581, 'grad_norm': 0.32215237617492676, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5165767669677734, 'eval_runtime': 6.8801, 'eval_samples_per_second': 145.201, 'eval_steps_per_second': 9.157, 'epoch': 0.28}
{'loss': 0.5466, 'grad_norm': 0.38940688967704773, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.46606194972991943, 'eval_runtime': 6.8681, 'eval_samples_per_second': 145.456, 'eval_steps_per_second': 9.173, 'epoch': 0.32}
{'loss': 0.499, 'grad_norm': 0.379151850938797, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4202030301094055, 'eval_runtime': 6.8742, 'eval_samples_per_second': 145.327, 'eval_steps_per_second': 9.165, 'epoch': 0.36}
{'loss': 0.4641, 'grad_norm': 0.3459300100803375, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.3784242868423462, 'eval_runtime': 6.9088, 'eval_samples_per_second': 144.598, 'eval_steps_per_second': 9.119, 'epoch': 0.4}
{'loss': 0.4137, 'grad_norm': 0.41472941637039185, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3426462709903717, 'eval_runtime': 6.9539, 'eval_samples_per_second': 143.66, 'eval_steps_per_second': 9.06, 'epoch': 0.44}
{'loss': 0.386, 'grad_norm': 0.3344016373157501, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3156912326812744, 'eval_runtime': 6.9641, 'eval_samples_per_second': 143.45, 'eval_steps_per_second': 9.046, 'epoch': 0.48}
{'loss': 0.3928, 'grad_norm': 0.3526744544506073, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.28750380873680115, 'eval_runtime': 6.947, 'eval_samples_per_second': 143.804, 'eval_steps_per_second': 9.069, 'epoch': 0.52}
{'loss': 0.3274, 'grad_norm': 0.2571222186088562, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.271335244178772, 'eval_runtime': 6.9495, 'eval_samples_per_second': 143.752, 'eval_steps_per_second': 9.065, 'epoch': 0.56}
{'loss': 0.295, 'grad_norm': 0.37155720591545105, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2488582879304886, 'eval_runtime': 6.8925, 'eval_samples_per_second': 144.941, 'eval_steps_per_second': 9.14, 'epoch': 0.6}
{'loss': 0.3105, 'grad_norm': 0.26688504219055176, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.23367959260940552, 'eval_runtime': 6.893, 'eval_samples_per_second': 144.93, 'eval_steps_per_second': 9.14, 'epoch': 0.64}
{'loss': 0.2573, 'grad_norm': 0.31605198979377747, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2216423749923706, 'eval_runtime': 6.8881, 'eval_samples_per_second': 145.033, 'eval_steps_per_second': 9.146, 'epoch': 0.68}
{'loss': 0.2843, 'grad_norm': 0.2608387768268585, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.20827104151248932, 'eval_runtime': 6.8648, 'eval_samples_per_second': 145.526, 'eval_steps_per_second': 9.177, 'epoch': 0.72}
{'loss': 0.2659, 'grad_norm': 0.42040061950683594, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.19564716517925262, 'eval_runtime': 6.8472, 'eval_samples_per_second': 145.899, 'eval_steps_per_second': 9.201, 'epoch': 0.76}
{'loss': 0.243, 'grad_norm': 0.34593504667282104, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.18661510944366455, 'eval_runtime': 6.8479, 'eval_samples_per_second': 145.884, 'eval_steps_per_second': 9.2, 'epoch': 0.8}
{'loss': 0.2411, 'grad_norm': 0.3540589511394501, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.18005646765232086, 'eval_runtime': 6.8665, 'eval_samples_per_second': 145.489, 'eval_steps_per_second': 9.175, 'epoch': 0.84}
{'loss': 0.2217, 'grad_norm': 0.20173248648643494, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.17332638800144196, 'eval_runtime': 6.8552, 'eval_samples_per_second': 145.728, 'eval_steps_per_second': 9.19, 'epoch': 0.88}
{'loss': 0.199, 'grad_norm': 0.2183588743209839, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.16846424341201782, 'eval_runtime': 6.861, 'eval_samples_per_second': 145.606, 'eval_steps_per_second': 9.182, 'epoch': 0.92}
{'loss': 0.214, 'grad_norm': 0.20339907705783844, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.16483436524868011, 'eval_runtime': 6.8683, 'eval_samples_per_second': 145.45, 'eval_steps_per_second': 9.173, 'epoch': 0.96}
{'loss': 0.1939, 'grad_norm': 0.4447533190250397, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.1633722335100174, 'eval_runtime': 6.8904, 'eval_samples_per_second': 144.985, 'eval_steps_per_second': 9.143, 'epoch': 1.0}
{'train_runtime': 395.4207, 'train_samples_per_second': 25.287, 'train_steps_per_second': 1.581, 'train_loss': 0.503040200805664, 'epoch': 1.0}
train_results:  {'eval_loss': [1.010255217552185, 0.8000925779342651, 0.746900200843811, 0.6915762424468994, 0.631845235824585, 0.5768163204193115, 0.5165767669677734, 0.46606194972991943, 0.4202030301094055, 0.3784242868423462, 0.3426462709903717, 0.3156912326812744, 0.28750380873680115, 0.271335244178772, 0.2488582879304886, 0.23367959260940552, 0.2216423749923706, 0.20827104151248932, 0.19564716517925262, 0.18661510944366455, 0.18005646765232086, 0.17332638800144196, 0.16846424341201782, 0.16483436524868011, 0.1633722335100174], 'performance': [0.64, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.11it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.01it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 58.58it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 63.36it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 61.44it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2381198406219482
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8186 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6492231488227844, 0.9983226656913757, 0.673710286617279, 0.1485937237739563, 0.7315465807914734, 0.2771422863006592, 0.03631800413131714, 0.7695220708847046, 0.5843249559402466, 0.7599004507064819, 0.5244206190109253, 0.9095444083213806, 0.4426685571670532, 0.5239457488059998, 0.3788059949874878, 0.509009838104248, 0.40622180700302124, 0.5757241249084473, 0.7128728628158569]  ‚Üí  acq = 0.9712047100695504
X = [0.7540649771690369, 0.3823252320289612, 0.04571259021759033, 0.1636398434638977, 0.9895616769790649, 0.7139806151390076, 0.05001175403594971, 0.269914448261261, 0.28755849599838257, 0.39333900809288025, 0.4149136543273926, 0.6843322515487671, 0.17388463020324707, 0.7400295734405518, 0.3803022503852844, 0.31922104954719543, 0.5046421885490417, 0.6274806261062622, 0.29626554250717163]  ‚Üí  acq = 0.9712058124926243
X = [0.43393421173095703, 0.9981526732444763, 0.8115408420562744, 0.10006868839263916, 0.9020599126815796, 0.7348343729972839, 0.2914268970489502, 0.0011762380599975586, 0.34477972984313965, 0.27221548557281494, 0.8593361377716064, 0.6359610557556152, 0.798437237739563, 0.9982348084449768, 0.7336147427558899, 0.953912615776062, 0.5569700002670288, 0.17710663378238678, 0.20784378051757812]  ‚Üí  acq = 0.971205809068111
X = [0.3381749987602234, 0.09763985872268677, 0.6359526515007019, 0.9373623728752136, 0.2528315782546997, 0.41271740198135376, 0.35478633642196655, 0.8600528836250305, 0.010003805160522461, 0.9256587028503418, 0.2860068678855896, 0.230150043964386, 0.74116051197052, 0.0828816294670105, 0.5050832033157349, 0.32037585973739624, 0.8059919476509094, 0.7319818735122681, 0.16218972206115723]  ‚Üí  acq = 0.8972025900878814
X = [0.31952589750289917, 0.20342183113098145, 0.9620497822761536, 0.9745755791664124, 0.9704625010490417, 0.6154479384422302, 0.5957858562469482, 0.5695112347602844, 0.578803300857544, 0.18084470927715302, 0.5776962637901306, 0.0926276445388794, 0.38126450777053833, 0.6921922564506531, 0.9467645883560181, 0.026990920305252075, 0.9116214513778687, 0.8134325742721558, 0.05813318490982056]  ‚Üí  acq = 0.9712058125034082
proposed candidate layer mask is:  tensor([0., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, tensor(0.3051, dtype=torch.float64), 0, 0, tensor(0.6949, dtype=torch.float64), 32, 0, 0, 1, 0, 0, 128, 0.09999999999999948, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.6386e-17, dtype=torch.float64), tensor(3.0476e-17, dtype=torch.float64), tensor(6.1028e-18, dtype=torch.float64), tensor(4.8388e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3051, dtype=torch.float64), tensor(1.6127e-16, dtype=torch.float64), tensor(1.1899e-19, dtype=torch.float64), tensor(0.6949, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.305
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.695

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999948,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 0, 0]
lora rank:  128
lora dropout:  0.09999999999999948
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:14,  6.83it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 41.61it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 53.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 58.75it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 62.16it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 67.32it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 68.22it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 71.45it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 75.93it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 73.99it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 74.06it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 68.82it/s]
Evaluation performance at step 25: 0.61
{'loss': 2.8553, 'grad_norm': 0.6574135422706604, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.3377435207366943, 'eval_runtime': 7.184, 'eval_samples_per_second': 139.06, 'eval_steps_per_second': 8.77, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:14,  6.97it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 53.95it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 58.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 62.09it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 71.96it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 71.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 75.22it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 74.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 72.03it/s]
Evaluation performance at step 50: 0.63
{'loss': 1.127, 'grad_norm': 0.23909346759319305, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 1.0214351415634155, 'eval_runtime': 6.4491, 'eval_samples_per_second': 154.905, 'eval_steps_per_second': 9.769, 'epoch': 0.08}
{'loss': 0.9781, 'grad_norm': 0.24328286945819855, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9221876263618469, 'eval_runtime': 6.5271, 'eval_samples_per_second': 153.053, 'eval_steps_per_second': 9.652, 'epoch': 0.12}
{'loss': 0.8644, 'grad_norm': 0.25859689712524414, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8230190873146057, 'eval_runtime': 6.5606, 'eval_samples_per_second': 152.272, 'eval_steps_per_second': 9.603, 'epoch': 0.16}
{'loss': 0.8092, 'grad_norm': 0.2530558407306671, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7282238006591797, 'eval_runtime': 6.5549, 'eval_samples_per_second': 152.405, 'eval_steps_per_second': 9.611, 'epoch': 0.2}
{'loss': 0.7105, 'grad_norm': 0.3109409511089325, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6720735430717468, 'eval_runtime': 6.5812, 'eval_samples_per_second': 151.797, 'eval_steps_per_second': 9.573, 'epoch': 0.24}
{'loss': 0.6583, 'grad_norm': 0.2951122224330902, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6200118660926819, 'eval_runtime': 6.537, 'eval_samples_per_second': 152.822, 'eval_steps_per_second': 9.637, 'epoch': 0.28}
{'loss': 0.6102, 'grad_norm': 0.44930970668792725, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5674533247947693, 'eval_runtime': 6.5358, 'eval_samples_per_second': 152.851, 'eval_steps_per_second': 9.639, 'epoch': 0.32}
{'loss': 0.6111, 'grad_norm': 0.3249798119068146, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5244656205177307, 'eval_runtime': 6.5275, 'eval_samples_per_second': 153.044, 'eval_steps_per_second': 9.651, 'epoch': 0.36}
{'loss': 0.5868, 'grad_norm': 0.42704886198043823, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.46739688515663147, 'eval_runtime': 6.5193, 'eval_samples_per_second': 153.237, 'eval_steps_per_second': 9.664, 'epoch': 0.4}
{'loss': 0.4961, 'grad_norm': 0.344705194234848, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.42074689269065857, 'eval_runtime': 6.5135, 'eval_samples_per_second': 153.375, 'eval_steps_per_second': 9.672, 'epoch': 0.44}
{'loss': 0.4705, 'grad_norm': 0.44073155522346497, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.38651227951049805, 'eval_runtime': 6.5268, 'eval_samples_per_second': 153.062, 'eval_steps_per_second': 9.653, 'epoch': 0.48}
{'loss': 0.4724, 'grad_norm': 0.38480934500694275, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3559633493423462, 'eval_runtime': 6.5277, 'eval_samples_per_second': 153.04, 'eval_steps_per_second': 9.651, 'epoch': 0.52}
{'loss': 0.4064, 'grad_norm': 0.4264247715473175, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.3242521286010742, 'eval_runtime': 6.5183, 'eval_samples_per_second': 153.26, 'eval_steps_per_second': 9.665, 'epoch': 0.56}
{'loss': 0.3573, 'grad_norm': 0.43990370631217957, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2872726321220398, 'eval_runtime': 6.526, 'eval_samples_per_second': 153.081, 'eval_steps_per_second': 9.654, 'epoch': 0.6}
{'loss': 0.3593, 'grad_norm': 0.482654869556427, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.26422834396362305, 'eval_runtime': 6.5288, 'eval_samples_per_second': 153.014, 'eval_steps_per_second': 9.65, 'epoch': 0.64}
{'loss': 0.3221, 'grad_norm': 0.46137937903404236, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.24008847773075104, 'eval_runtime': 6.5347, 'eval_samples_per_second': 152.877, 'eval_steps_per_second': 9.641, 'epoch': 0.68}
{'loss': 0.2753, 'grad_norm': 0.8071245551109314, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.21831949055194855, 'eval_runtime': 6.5352, 'eval_samples_per_second': 152.865, 'eval_steps_per_second': 9.64, 'epoch': 0.72}
{'loss': 0.2644, 'grad_norm': 0.3336108922958374, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.19872090220451355, 'eval_runtime': 6.5411, 'eval_samples_per_second': 152.727, 'eval_steps_per_second': 9.631, 'epoch': 0.76}
{'loss': 0.2365, 'grad_norm': 0.3795028030872345, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.1850619912147522, 'eval_runtime': 6.5394, 'eval_samples_per_second': 152.767, 'eval_steps_per_second': 9.634, 'epoch': 0.8}
{'loss': 0.2142, 'grad_norm': 0.4068325459957123, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.17336350679397583, 'eval_runtime': 6.5393, 'eval_samples_per_second': 152.77, 'eval_steps_per_second': 9.634, 'epoch': 0.84}
{'loss': 0.2152, 'grad_norm': 0.5301358699798584, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.1586834192276001, 'eval_runtime': 6.5373, 'eval_samples_per_second': 152.814, 'eval_steps_per_second': 9.637, 'epoch': 0.88}
{'loss': 0.1909, 'grad_norm': 0.24952103197574615, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.14835022389888763, 'eval_runtime': 6.5263, 'eval_samples_per_second': 153.073, 'eval_steps_per_second': 9.653, 'epoch': 0.92}
{'loss': 0.1941, 'grad_norm': 0.4118269085884094, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.1434718519449234, 'eval_runtime': 6.5082, 'eval_samples_per_second': 153.498, 'eval_steps_per_second': 9.68, 'epoch': 0.96}
{'loss': 0.1786, 'grad_norm': 0.2510567009449005, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.14129363000392914, 'eval_runtime': 6.507, 'eval_samples_per_second': 153.526, 'eval_steps_per_second': 9.682, 'epoch': 1.0}
{'train_runtime': 354.5859, 'train_samples_per_second': 28.199, 'train_steps_per_second': 1.763, 'train_loss': 0.5785766799926758, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3377435207366943, 1.0214351415634155, 0.9221876263618469, 0.8230190873146057, 0.7282238006591797, 0.6720735430717468, 0.6200118660926819, 0.5674533247947693, 0.5244656205177307, 0.46739688515663147, 0.42074689269065857, 0.38651227951049805, 0.3559633493423462, 0.3242521286010742, 0.2872726321220398, 0.26422834396362305, 0.24008847773075104, 0.21831949055194855, 0.19872090220451355, 0.1850619912147522, 0.17336350679397583, 0.1586834192276001, 0.14835022389888763, 0.1434718519449234, 0.14129363000392914], 'performance': [0.61, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:25,  3.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.71it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 55.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 67.42it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 73.33it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 75.85it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 75.58it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.2393970489501953
current iteration best possible performance (full train run):  0.5775000000000001
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9028 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5739718675613403, 0.5709601044654846, 0.38029056787490845, 0.26085174083709717, 0.28395169973373413, 0.35340577363967896, 0.4210600256919861, 0.5623074173927307, 0.06520140171051025, 0.9181063771247864, 0.8713845610618591, 0.005934298038482666, 0.11926496028900146, 0.7074142098426819, 0.762404203414917, 0.38688263297080994, 0.38453209400177, 0.04352915287017822, 0.9305654168128967]  ‚Üí  acq = 0.8678991061356561
X = [0.5857617855072021, 0.9708753228187561, 0.019611060619354248, 0.9366970062255859, 0.36372125148773193, 0.6767957806587219, 0.16598302125930786, 0.20697879791259766, 0.4871063828468323, 0.05680856108665466, 0.8059588074684143, 0.617242693901062, 0.8529475331306458, 0.982242226600647, 0.9818074703216553, 0.5557505488395691, 0.050306856632232666, 0.27563905715942383, 0.9853177070617676]  ‚Üí  acq = 0.9218470791026678
X = [0.10851812362670898, 0.4584953188896179, 0.2716476321220398, 0.664991021156311, 0.1964511275291443, 0.14080750942230225, 0.9493184685707092, 0.4104118347167969, 0.8133276700973511, 0.2914159595966339, 0.9679337739944458, 0.7077615261077881, 0.41401785612106323, 0.5853195190429688, 0.26299411058425903, 0.5999746918678284, 0.39580798149108887, 0.34744322299957275, 0.8053473234176636]  ‚Üí  acq = 0.9333339440976227
X = [0.153448224067688, 0.6746816039085388, 0.30124956369400024, 0.4827815294265747, 0.4474642872810364, 0.562120258808136, 0.8065040111541748, 0.7575616240501404, 0.5161547660827637, 0.8054929971694946, 0.6323732137680054, 0.7544072270393372, 0.6885694861412048, 0.8983424305915833, 0.3208085894584656, 0.5376754999160767, 0.012106716632843018, 0.791178822517395, 0.2458704113960266]  ‚Üí  acq = 0.9546870374725329
X = [0.3873633146286011, 0.9739648103713989, 0.1253301501274109, 0.9906280040740967, 0.7129344940185547, 0.9920598268508911, 0.47453564405441284, 0.4164969325065613, 0.0932343602180481, 0.8094826340675354, 0.4448079466819763, 0.7297403812408447, 0.40292978286743164, 0.8027117252349854, 0.1436668038368225, 0.5480492115020752, 0.34515559673309326, 0.8542231321334839, 0.29525285959243774]  ‚Üí  acq = 0.9645348538683862
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.3121, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.6879, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 128, 2.4986263158116406e-16, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.3121, dtype=torch.float64), tensor(1.8002e-16, dtype=torch.float64), tensor(3.1370e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(8.2658e-17, dtype=torch.float64), tensor(1.0602e-16, dtype=torch.float64), tensor(9.3037e-17, dtype=torch.float64), tensor(0.6879, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.4986e-15, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.312
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.688

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.4986263158116406e-16,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  2.4986263158116406e-16
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.63it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 27.15it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.54it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 38.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 38.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 39.76it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 41.24it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 46.12it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 43.45it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 47.50it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 46.07it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 44.71it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 44.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 42.56it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.1824, 'grad_norm': 0.3308691680431366, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.130470871925354, 'eval_runtime': 9.8346, 'eval_samples_per_second': 101.58, 'eval_steps_per_second': 6.406, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.23it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 26.10it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 35.80it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.66it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.63it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 47.78it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.54it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.56it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.77it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 54.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.18it/s]
Evaluation performance at step 50: 0.63
{'loss': 1.0191, 'grad_norm': 0.23199430108070374, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.63}
{'eval_loss': 0.9541458487510681, 'eval_runtime': 9.7722, 'eval_samples_per_second': 102.228, 'eval_steps_per_second': 6.447, 'epoch': 0.08}
{'loss': 0.8939, 'grad_norm': 0.23391544818878174, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8456385135650635, 'eval_runtime': 9.8071, 'eval_samples_per_second': 101.865, 'eval_steps_per_second': 6.424, 'epoch': 0.12}
{'loss': 0.8236, 'grad_norm': 0.2088036686182022, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7698004841804504, 'eval_runtime': 9.8266, 'eval_samples_per_second': 101.662, 'eval_steps_per_second': 6.411, 'epoch': 0.16}
{'loss': 0.7514, 'grad_norm': 0.21286998689174652, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7207000255584717, 'eval_runtime': 9.8518, 'eval_samples_per_second': 101.403, 'eval_steps_per_second': 6.395, 'epoch': 0.2}
{'loss': 0.7312, 'grad_norm': 0.23883464932441711, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6784868240356445, 'eval_runtime': 9.8634, 'eval_samples_per_second': 101.283, 'eval_steps_per_second': 6.387, 'epoch': 0.24}
{'loss': 0.6682, 'grad_norm': 0.2592604458332062, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6299066543579102, 'eval_runtime': 9.8758, 'eval_samples_per_second': 101.157, 'eval_steps_per_second': 6.379, 'epoch': 0.28}
{'loss': 0.6479, 'grad_norm': 0.24712617695331573, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.5855716466903687, 'eval_runtime': 9.8819, 'eval_samples_per_second': 101.093, 'eval_steps_per_second': 6.375, 'epoch': 0.32}
{'loss': 0.636, 'grad_norm': 0.30514150857925415, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5423377752304077, 'eval_runtime': 9.8801, 'eval_samples_per_second': 101.112, 'eval_steps_per_second': 6.376, 'epoch': 0.36}
{'loss': 0.5671, 'grad_norm': 0.2571815252304077, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5049718022346497, 'eval_runtime': 9.8753, 'eval_samples_per_second': 101.161, 'eval_steps_per_second': 6.38, 'epoch': 0.4}
{'loss': 0.5607, 'grad_norm': 0.4081783592700958, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.4760363698005676, 'eval_runtime': 9.8769, 'eval_samples_per_second': 101.145, 'eval_steps_per_second': 6.378, 'epoch': 0.44}
{'loss': 0.5088, 'grad_norm': 0.21101440489292145, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.45464518666267395, 'eval_runtime': 9.8541, 'eval_samples_per_second': 101.379, 'eval_steps_per_second': 6.393, 'epoch': 0.48}
{'loss': 0.5305, 'grad_norm': 0.2720031440258026, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.43865638971328735, 'eval_runtime': 9.8218, 'eval_samples_per_second': 101.712, 'eval_steps_per_second': 6.414, 'epoch': 0.52}
{'loss': 0.4793, 'grad_norm': 0.2726703882217407, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4255843460559845, 'eval_runtime': 9.8222, 'eval_samples_per_second': 101.708, 'eval_steps_per_second': 6.414, 'epoch': 0.56}
{'loss': 0.4561, 'grad_norm': 0.23209868371486664, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.40896499156951904, 'eval_runtime': 9.8638, 'eval_samples_per_second': 101.279, 'eval_steps_per_second': 6.387, 'epoch': 0.6}
{'loss': 0.481, 'grad_norm': 0.28115007281303406, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.3928758502006531, 'eval_runtime': 9.868, 'eval_samples_per_second': 101.236, 'eval_steps_per_second': 6.384, 'epoch': 0.64}
{'loss': 0.4349, 'grad_norm': 0.26166847348213196, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3845657706260681, 'eval_runtime': 9.8488, 'eval_samples_per_second': 101.434, 'eval_steps_per_second': 6.397, 'epoch': 0.68}
{'loss': 0.4607, 'grad_norm': 0.18486405909061432, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.3739311695098877, 'eval_runtime': 9.8509, 'eval_samples_per_second': 101.412, 'eval_steps_per_second': 6.395, 'epoch': 0.72}
{'loss': 0.4527, 'grad_norm': 0.19415371119976044, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.365904837846756, 'eval_runtime': 9.8706, 'eval_samples_per_second': 101.209, 'eval_steps_per_second': 6.383, 'epoch': 0.76}
{'loss': 0.4125, 'grad_norm': 0.18508638441562653, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3564383387565613, 'eval_runtime': 9.8748, 'eval_samples_per_second': 101.166, 'eval_steps_per_second': 6.38, 'epoch': 0.8}
{'loss': 0.4206, 'grad_norm': 0.18343211710453033, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3489503860473633, 'eval_runtime': 9.8864, 'eval_samples_per_second': 101.048, 'eval_steps_per_second': 6.372, 'epoch': 0.84}
{'loss': 0.4166, 'grad_norm': 0.2378871589899063, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3438129723072052, 'eval_runtime': 9.9015, 'eval_samples_per_second': 100.894, 'eval_steps_per_second': 6.363, 'epoch': 0.88}
{'loss': 0.4004, 'grad_norm': 0.21143069863319397, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.33958059549331665, 'eval_runtime': 9.88, 'eval_samples_per_second': 101.114, 'eval_steps_per_second': 6.377, 'epoch': 0.92}
{'loss': 0.406, 'grad_norm': 0.1678769290447235, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.33754366636276245, 'eval_runtime': 9.8844, 'eval_samples_per_second': 101.068, 'eval_steps_per_second': 6.374, 'epoch': 0.96}
{'loss': 0.3702, 'grad_norm': 0.10472934693098068, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3365149199962616, 'eval_runtime': 9.878, 'eval_samples_per_second': 101.134, 'eval_steps_per_second': 6.378, 'epoch': 1.0}
{'train_runtime': 546.8909, 'train_samples_per_second': 18.283, 'train_steps_per_second': 1.143, 'train_loss': 0.6284615478515625, 'epoch': 1.0}
train_results:  {'eval_loss': [1.130470871925354, 0.9541458487510681, 0.8456385135650635, 0.7698004841804504, 0.7207000255584717, 0.6784868240356445, 0.6299066543579102, 0.5855716466903687, 0.5423377752304077, 0.5049718022346497, 0.4760363698005676, 0.45464518666267395, 0.43865638971328735, 0.4255843460559845, 0.40896499156951904, 0.3928758502006531, 0.3845657706260681, 0.3739311695098877, 0.365904837846756, 0.3564383387565613, 0.3489503860473633, 0.3438129723072052, 0.33958059549331665, 0.33754366636276245, 0.3365149199962616], 'performance': [0.63, 0.63]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:30,  3.26it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 27.51it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 42.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 50.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.78it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.63]
current iteration observed (possibly low-fid or predicted) performance:  1.245240569114685
current iteration best possible performance (full train run):  0.546
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6474 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1986006498336792, 0.658925473690033, 0.33023494482040405, 0.43129462003707886, 0.10898596048355103, 0.583984911441803, 0.5397793054580688, 0.21894419193267822, 0.5292921662330627, 0.5853065252304077, 0.4954812526702881, 0.008728981018066406, 0.9791633486747742, 0.27319079637527466, 0.7329716682434082, 0.2307266741991043, 0.4934024214744568, 0.15419214963912964, 0.7088090181350708]  ‚Üí  acq = 0.833823807220878
X = [0.18454229831695557, 0.31489676237106323, 0.6861894130706787, 0.28850221633911133, 0.014378130435943604, 0.8424021005630493, 0.034187257289886475, 0.7496437430381775, 0.7763248682022095, 0.9211031794548035, 0.599116861820221, 0.5625665783882141, 0.3067396283149719, 0.9767115712165833, 0.836753785610199, 0.5788933634757996, 0.6569864153862, 0.8010284900665283, 0.6757482886314392]  ‚Üí  acq = 1.0295325624486282
X = [0.3972335457801819, 0.5151011347770691, 0.2893524169921875, 0.5536059737205505, 0.689430832862854, 0.9548563957214355, 0.7161945104598999, 0.3470768928527832, 0.9469635486602783, 0.392242431640625, 0.009788751602172852, 0.9775137901306152, 0.8900309801101685, 0.4049222469329834, 0.35626769065856934, 0.3026502728462219, 0.8998419046401978, 0.04215187951922417, 0.3992973566055298]  ‚Üí  acq = 0.95468491412721
X = [0.6954328417778015, 0.2076748013496399, 0.08545547723770142, 0.44661808013916016, 0.3233661651611328, 0.31079787015914917, 0.16175484657287598, 0.44474542140960693, 0.5780788660049438, 0.4546978771686554, 0.8899770379066467, 0.7039777040481567, 0.36670982837677, 0.3001217246055603, 0.25116783380508423, 0.675288200378418, 0.6150220632553101, 0.4984583854675293, 0.12079465389251709]  ‚Üí  acq = 0.8919359570929271
X = [0.7088842988014221, 0.946155309677124, 0.010708928108215332, 0.06199228763580322, 0.6765848398208618, 0.8559234738349915, 0.47451168298721313, 0.049057602882385254, 0.1052057147026062, 0.617496132850647, 0.05649161338806152, 0.3228719234466553, 0.5422348380088806, 0.7937590479850769, 0.779019832611084, 0.9603983759880066, 0.7421700954437256, 0.18496841192245483, 0.41646718978881836]  ‚Üí  acq = 0.954669663063832
proposed candidate layer mask is:  tensor([1., 0., 0., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.2869, dtype=torch.float64), 0, tensor(0.7131, dtype=torch.float64), 32, 1, 0, 0, 0, 1, 128, 0.09999999999999999, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(1.1446e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.0065e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.0800e-16, dtype=torch.float64), tensor(1.4676e-17, dtype=torch.float64), tensor(0.2869, dtype=torch.float64), tensor(4.5056e-17, dtype=torch.float64), tensor(0.7131, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.287
  mmlu: 0
  arc_challenge: 0.713

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.09999999999999999,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 0, 1]
lora rank:  128
lora dropout:  0.09999999999999999
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:50,  1.96it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:04, 18.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 29.51it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 37.39it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 42.54it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 48.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 51.28it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.93it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 59.98it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 60.37it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.89it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.836, 'grad_norm': 0.32878145575523376, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.6164891719818115, 'eval_runtime': 7.8577, 'eval_samples_per_second': 127.137, 'eval_steps_per_second': 8.018, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.99it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.49it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.52it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.61it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.29it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.08it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 57.79it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 61.92it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 64.72it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 61.83it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 61.40it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 61.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.79it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.4205, 'grad_norm': 0.19239628314971924, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.2669569253921509, 'eval_runtime': 7.8087, 'eval_samples_per_second': 127.935, 'eval_steps_per_second': 8.068, 'epoch': 0.08}
{'loss': 1.2061, 'grad_norm': 0.19750474393367767, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1581887006759644, 'eval_runtime': 7.8072, 'eval_samples_per_second': 127.959, 'eval_steps_per_second': 8.069, 'epoch': 0.12}
{'loss': 1.0832, 'grad_norm': 0.27998629212379456, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0566266775131226, 'eval_runtime': 7.8439, 'eval_samples_per_second': 127.361, 'eval_steps_per_second': 8.032, 'epoch': 0.16}
{'loss': 1.0235, 'grad_norm': 0.30917808413505554, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9662759900093079, 'eval_runtime': 7.8411, 'eval_samples_per_second': 127.406, 'eval_steps_per_second': 8.035, 'epoch': 0.2}
{'loss': 0.9485, 'grad_norm': 0.33577999472618103, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.907886266708374, 'eval_runtime': 7.8571, 'eval_samples_per_second': 127.146, 'eval_steps_per_second': 8.018, 'epoch': 0.24}
{'loss': 0.8656, 'grad_norm': 0.3486829102039337, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8529151082038879, 'eval_runtime': 7.8668, 'eval_samples_per_second': 126.989, 'eval_steps_per_second': 8.008, 'epoch': 0.28}
{'loss': 0.892, 'grad_norm': 0.3555891513824463, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7893688082695007, 'eval_runtime': 7.8775, 'eval_samples_per_second': 126.817, 'eval_steps_per_second': 7.997, 'epoch': 0.32}
{'loss': 0.8008, 'grad_norm': 0.3937103748321533, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.7382863759994507, 'eval_runtime': 7.8707, 'eval_samples_per_second': 126.927, 'eval_steps_per_second': 8.004, 'epoch': 0.36}
{'loss': 0.7842, 'grad_norm': 0.3327423930168152, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.7039898037910461, 'eval_runtime': 7.8735, 'eval_samples_per_second': 126.881, 'eval_steps_per_second': 8.001, 'epoch': 0.4}
{'loss': 0.7783, 'grad_norm': 0.4117976129055023, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6653573513031006, 'eval_runtime': 7.8716, 'eval_samples_per_second': 126.912, 'eval_steps_per_second': 8.003, 'epoch': 0.44}
{'loss': 0.7435, 'grad_norm': 0.4866613447666168, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.6322360038757324, 'eval_runtime': 7.8711, 'eval_samples_per_second': 126.92, 'eval_steps_per_second': 8.004, 'epoch': 0.48}
{'loss': 0.6948, 'grad_norm': 0.33412766456604004, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.6128111481666565, 'eval_runtime': 7.8721, 'eval_samples_per_second': 126.903, 'eval_steps_per_second': 8.003, 'epoch': 0.52}
{'loss': 0.6991, 'grad_norm': 0.3753865361213684, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5814623832702637, 'eval_runtime': 7.8606, 'eval_samples_per_second': 127.09, 'eval_steps_per_second': 8.015, 'epoch': 0.56}
{'loss': 0.5905, 'grad_norm': 0.3238797187805176, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.5609903931617737, 'eval_runtime': 7.8397, 'eval_samples_per_second': 127.428, 'eval_steps_per_second': 8.036, 'epoch': 0.6}
{'loss': 0.6901, 'grad_norm': 0.3918713331222534, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.5362083315849304, 'eval_runtime': 7.8461, 'eval_samples_per_second': 127.325, 'eval_steps_per_second': 8.03, 'epoch': 0.64}
{'loss': 0.5852, 'grad_norm': 0.33574095368385315, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.5156020522117615, 'eval_runtime': 7.8468, 'eval_samples_per_second': 127.312, 'eval_steps_per_second': 8.029, 'epoch': 0.68}
{'loss': 0.5876, 'grad_norm': 0.33055078983306885, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.4958915412425995, 'eval_runtime': 7.8392, 'eval_samples_per_second': 127.436, 'eval_steps_per_second': 8.036, 'epoch': 0.72}
{'loss': 0.6584, 'grad_norm': 0.24256764352321625, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.48390284180641174, 'eval_runtime': 7.8332, 'eval_samples_per_second': 127.534, 'eval_steps_per_second': 8.043, 'epoch': 0.76}
{'loss': 0.6161, 'grad_norm': 0.38030144572257996, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.47016292810440063, 'eval_runtime': 7.8387, 'eval_samples_per_second': 127.445, 'eval_steps_per_second': 8.037, 'epoch': 0.8}
{'loss': 0.5742, 'grad_norm': 0.21655282378196716, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.45893919467926025, 'eval_runtime': 7.8405, 'eval_samples_per_second': 127.416, 'eval_steps_per_second': 8.035, 'epoch': 0.84}
{'loss': 0.5786, 'grad_norm': 0.23010143637657166, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.45388659834861755, 'eval_runtime': 7.8319, 'eval_samples_per_second': 127.555, 'eval_steps_per_second': 8.044, 'epoch': 0.88}
{'loss': 0.5365, 'grad_norm': 0.43755027651786804, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.44660502672195435, 'eval_runtime': 7.8563, 'eval_samples_per_second': 127.16, 'eval_steps_per_second': 8.019, 'epoch': 0.92}
{'loss': 0.5732, 'grad_norm': 0.23399651050567627, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.4421771764755249, 'eval_runtime': 7.9064, 'eval_samples_per_second': 126.353, 'eval_steps_per_second': 7.968, 'epoch': 0.96}
{'loss': 0.4837, 'grad_norm': 0.4769726097583771, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.44028085470199585, 'eval_runtime': 7.8734, 'eval_samples_per_second': 126.882, 'eval_steps_per_second': 8.002, 'epoch': 1.0}
{'train_runtime': 437.8543, 'train_samples_per_second': 22.836, 'train_steps_per_second': 1.427, 'train_loss': 0.8500166198730469, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6164891719818115, 1.2669569253921509, 1.1581887006759644, 1.0566266775131226, 0.9662759900093079, 0.907886266708374, 0.8529151082038879, 0.7893688082695007, 0.7382863759994507, 0.7039898037910461, 0.6653573513031006, 0.6322360038757324, 0.6128111481666565, 0.5814623832702637, 0.5609903931617737, 0.5362083315849304, 0.5156020522117615, 0.4958915412425995, 0.48390284180641174, 0.47016292810440063, 0.45893919467926025, 0.45388659834861755, 0.44660502672195435, 0.4421771764755249, 0.44028085470199585], 'performance': [0.62, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:33,  2.99it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.93it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 29.38it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 35.40it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 42.56it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 47.89it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 60.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 46.02it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2445502281188965
current iteration best possible performance (full train run):  0.651
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2783 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6075543761253357, 0.14837175607681274, 0.732477605342865, 0.10297280550003052, 0.6975349187850952, 0.6701392531394958, 0.006200850009918213, 0.481764018535614, 0.6693928837776184, 0.19618308544158936, 0.7129403352737427, 0.17861396074295044, 0.15123003721237183, 0.8201956152915955, 0.5237151980400085, 0.4465259909629822, 0.07063072919845581, 0.2065262496471405, 0.25144654512405396]  ‚Üí  acq = 0.9505477862941722
X = [0.4101046919822693, 0.07919567823410034, 0.6155613660812378, 0.8227524161338806, 0.22096192836761475, 0.32699787616729736, 0.23508185148239136, 0.6298256516456604, 0.9492553472518921, 0.8328825235366821, 0.7630447745323181, 0.8959949612617493, 0.45415228605270386, 0.3766198754310608, 0.05817210674285889, 0.8461902737617493, 0.887573778629303, 0.20201367139816284, 0.16899991035461426]  ‚Üí  acq = 0.9371162014948065
X = [0.23369938135147095, 0.14073032140731812, 0.18362760543823242, 0.9396267533302307, 0.9560254812240601, 0.6832883954048157, 0.016032755374908447, 0.46766507625579834, 0.8738095164299011, 0.4231224060058594, 0.9265170693397522, 0.05219513177871704, 0.44860947132110596, 0.5099880695343018, 0.6339606642723083, 0.047696445137262344, 0.8641273379325867, 0.14121320843696594, 0.9284361600875854]  ‚Üí  acq = 0.9505514331908703
X = [0.1743742823600769, 0.611535370349884, 0.3855054974555969, 0.7766180038452148, 0.6872783303260803, 0.3083743453025818, 0.5316891074180603, 0.1909458041191101, 0.08776223659515381, 0.7930710315704346, 0.4191736578941345, 0.46188974380493164, 0.18484169244766235, 0.3694537281990051, 0.4039697051048279, 0.35729196667671204, 0.1838701367378235, 0.539975643157959, 0.8428286910057068]  ‚Üí  acq = 0.9505448536511851
X = [0.3569604754447937, 0.16039127111434937, 0.08819741010665894, 0.25184160470962524, 0.07300418615341187, 0.09784871339797974, 0.17070966958999634, 0.5472376346588135, 0.08003222942352295, 0.19520200788974762, 0.3191884160041809, 0.5763203501701355, 0.7595819234848022, 0.7259084582328796, 0.5696749091148376, 0.33646926283836365, 0.8923987150192261, 0.9271215200424194, 0.8581407070159912]  ‚Üí  acq = 0.6053325343561952
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.3117, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.6883, dtype=torch.float64), 32, 1, 1, 0, 1, 1, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(1.2606e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3117, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.4955e-16, dtype=torch.float64), tensor(1.1131e-16, dtype=torch.float64), tensor(1.6411e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6883, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.312
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.688

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.31it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 34.40it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 42.14it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 45.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.84it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.26it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.74it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.04it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.72it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 58.34it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 57.07it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.94it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 55.80it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.93it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.7581, 'grad_norm': 0.521210789680481, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.5161393880844116, 'eval_runtime': 10.5811, 'eval_samples_per_second': 94.414, 'eval_steps_per_second': 5.954, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.29it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.76it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.39it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.35it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.70it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 52.49it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.49it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.44it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.99it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.30it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.37it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.93it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.3633, 'grad_norm': 0.42205432057380676, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.2715513706207275, 'eval_runtime': 10.588, 'eval_samples_per_second': 94.352, 'eval_steps_per_second': 5.95, 'epoch': 0.08}
{'loss': 1.2327, 'grad_norm': 0.21479174494743347, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2080674171447754, 'eval_runtime': 10.5812, 'eval_samples_per_second': 94.413, 'eval_steps_per_second': 5.954, 'epoch': 0.12}
{'loss': 1.219, 'grad_norm': 0.22040006518363953, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.171059250831604, 'eval_runtime': 10.5764, 'eval_samples_per_second': 94.456, 'eval_steps_per_second': 5.957, 'epoch': 0.16}
{'loss': 1.1396, 'grad_norm': 0.24453206360340118, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.131328821182251, 'eval_runtime': 10.6696, 'eval_samples_per_second': 93.63, 'eval_steps_per_second': 5.905, 'epoch': 0.2}
{'loss': 1.0621, 'grad_norm': 0.29273298382759094, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0899693965911865, 'eval_runtime': 10.672, 'eval_samples_per_second': 93.609, 'eval_steps_per_second': 5.903, 'epoch': 0.24}
{'loss': 1.0406, 'grad_norm': 0.2528626024723053, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0466783046722412, 'eval_runtime': 10.6764, 'eval_samples_per_second': 93.571, 'eval_steps_per_second': 5.901, 'epoch': 0.28}
{'loss': 1.0907, 'grad_norm': 0.27133768796920776, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.0057138204574585, 'eval_runtime': 10.648, 'eval_samples_per_second': 93.821, 'eval_steps_per_second': 5.917, 'epoch': 0.32}
{'loss': 1.0089, 'grad_norm': 0.2730843722820282, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9732885360717773, 'eval_runtime': 10.6241, 'eval_samples_per_second': 94.032, 'eval_steps_per_second': 5.93, 'epoch': 0.36}
{'loss': 0.9834, 'grad_norm': 0.29829859733581543, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.940779983997345, 'eval_runtime': 10.5924, 'eval_samples_per_second': 94.313, 'eval_steps_per_second': 5.948, 'epoch': 0.4}
{'loss': 1.0002, 'grad_norm': 0.2736237347126007, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9118543863296509, 'eval_runtime': 10.5956, 'eval_samples_per_second': 94.285, 'eval_steps_per_second': 5.946, 'epoch': 0.44}
{'loss': 0.9259, 'grad_norm': 0.2820451557636261, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8852111101150513, 'eval_runtime': 10.6024, 'eval_samples_per_second': 94.224, 'eval_steps_per_second': 5.942, 'epoch': 0.48}
{'loss': 0.9918, 'grad_norm': 0.2414170801639557, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8660945892333984, 'eval_runtime': 10.6055, 'eval_samples_per_second': 94.196, 'eval_steps_per_second': 5.94, 'epoch': 0.52}
{'loss': 0.88, 'grad_norm': 0.2502038776874542, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8505442142486572, 'eval_runtime': 10.6029, 'eval_samples_per_second': 94.22, 'eval_steps_per_second': 5.942, 'epoch': 0.56}
{'loss': 0.8851, 'grad_norm': 0.247917041182518, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8320484757423401, 'eval_runtime': 10.5975, 'eval_samples_per_second': 94.268, 'eval_steps_per_second': 5.945, 'epoch': 0.6}
{'loss': 0.9597, 'grad_norm': 0.2973754107952118, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8136956095695496, 'eval_runtime': 10.6071, 'eval_samples_per_second': 94.182, 'eval_steps_per_second': 5.939, 'epoch': 0.64}
{'loss': 0.8467, 'grad_norm': 0.3478447198867798, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8017660975456238, 'eval_runtime': 10.6156, 'eval_samples_per_second': 94.107, 'eval_steps_per_second': 5.935, 'epoch': 0.68}
{'loss': 0.9513, 'grad_norm': 0.19815705716609955, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7918698787689209, 'eval_runtime': 10.6091, 'eval_samples_per_second': 94.165, 'eval_steps_per_second': 5.938, 'epoch': 0.72}
{'loss': 0.9288, 'grad_norm': 0.2438317835330963, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7817232608795166, 'eval_runtime': 10.6107, 'eval_samples_per_second': 94.15, 'eval_steps_per_second': 5.937, 'epoch': 0.76}
{'loss': 0.865, 'grad_norm': 0.31575289368629456, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7758359909057617, 'eval_runtime': 10.6187, 'eval_samples_per_second': 94.079, 'eval_steps_per_second': 5.933, 'epoch': 0.8}
{'loss': 0.8976, 'grad_norm': 0.15708723664283752, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7655630707740784, 'eval_runtime': 10.6383, 'eval_samples_per_second': 93.906, 'eval_steps_per_second': 5.922, 'epoch': 0.84}
{'loss': 0.8334, 'grad_norm': 0.19011764228343964, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7607000470161438, 'eval_runtime': 10.668, 'eval_samples_per_second': 93.645, 'eval_steps_per_second': 5.906, 'epoch': 0.88}
{'loss': 0.8318, 'grad_norm': 0.20311030745506287, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.7572081089019775, 'eval_runtime': 10.7425, 'eval_samples_per_second': 92.995, 'eval_steps_per_second': 5.865, 'epoch': 0.92}
{'loss': 0.8436, 'grad_norm': 0.19338048994541168, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.7543668150901794, 'eval_runtime': 10.7874, 'eval_samples_per_second': 92.608, 'eval_steps_per_second': 5.84, 'epoch': 0.96}
{'loss': 0.7769, 'grad_norm': 0.2114456593990326, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.7530363202095032, 'eval_runtime': 10.7833, 'eval_samples_per_second': 92.644, 'eval_steps_per_second': 5.842, 'epoch': 1.0}
{'train_runtime': 571.0738, 'train_samples_per_second': 17.509, 'train_steps_per_second': 1.094, 'train_loss': 1.0526528869628906, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5161393880844116, 1.2715513706207275, 1.2080674171447754, 1.171059250831604, 1.131328821182251, 1.0899693965911865, 1.0466783046722412, 1.0057138204574585, 0.9732885360717773, 0.940779983997345, 0.9118543863296509, 0.8852111101150513, 0.8660945892333984, 0.8505442142486572, 0.8320484757423401, 0.8136956095695496, 0.8017660975456238, 0.7918698787689209, 0.7817232608795166, 0.7758359909057617, 0.7655630707740784, 0.7607000470161438, 0.7572081089019775, 0.7543668150901794, 0.7530363202095032], 'performance': [0.62, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.64it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.22it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:07<00:17,  3.86it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:07<00:07,  6.47it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:08<00:03,  9.96it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:08<00:01, 14.09it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:08<00:00, 20.02it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.62it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2452890872955322
current iteration best possible performance (full train run):  0.63
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8344 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9267662763595581, 0.6323869824409485, 0.03701817989349365, 0.2569465637207031, 0.7195242047309875, 0.9788793325424194, 0.40876734256744385, 0.3967401385307312, 0.6073604822158813, 0.7326551079750061, 0.05768805742263794, 0.8464401364326477, 0.8111549615859985, 0.007667481899261475, 0.5063362121582031, 0.12010469287633896, 0.09157788753509521, 0.9313844442367554, 0.5546473264694214]  ‚Üí  acq = 0.9457924002503031
X = [0.2164575457572937, 0.014774143695831299, 0.516578197479248, 0.8359770178794861, 0.911345899105072, 0.22782862186431885, 0.49688708782196045, 0.17356735467910767, 0.08762586116790771, 0.3763657510280609, 0.558472752571106, 0.10966932773590088, 0.7067957520484924, 0.7259911298751831, 0.6226925253868103, 0.8368377685546875, 0.3364759683609009, 0.9566441774368286, 0.7511152029037476]  ‚Üí  acq = 0.9457938188718247
X = [0.7601731419563293, 0.715640664100647, 0.8452125787734985, 0.22607356309890747, 0.325300931930542, 0.4255574941635132, 0.8374440670013428, 0.2966812252998352, 0.3716070055961609, 0.5829849243164062, 0.8713144659996033, 0.7256700992584229, 0.09617900848388672, 0.03426504135131836, 0.248884916305542, 0.06910471618175507, 0.10646206140518188, 0.709165096282959, 0.4470563530921936]  ‚Üí  acq = 0.888299036743588
X = [0.6006543040275574, 0.6757649779319763, 0.051483750343322754, 0.11303436756134033, 0.4676780104637146, 0.0591389536857605, 0.1288602352142334, 0.47553199529647827, 0.2644087076187134, 0.9349585175514221, 0.15225332975387573, 0.7299689650535583, 0.9327967762947083, 0.2641924023628235, 0.3350575566291809, 0.9100606441497803, 0.44801563024520874, 0.6643359661102295, 0.6305233836174011]  ‚Üí  acq = 0.9357199769229337
X = [0.6030850410461426, 0.15685224533081055, 0.8442235589027405, 0.8902444839477539, 0.9480109810829163, 0.12873172760009766, 0.3460739850997925, 0.28718000650405884, 0.12468445301055908, 0.22467079758644104, 0.4612269401550293, 0.33926481008529663, 0.6126793622970581, 0.81937575340271, 0.8662098050117493, 0.6643738150596619, 0.09768640995025635, 0.8709299564361572, 0.6897938847541809]  ‚Üí  acq = 0.9457938219468557
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, tensor(0.3049, dtype=torch.float64), 0, tensor(0.6951, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.0, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(1.1141e-17, dtype=torch.float64), tensor(3.5946e-15, dtype=torch.float64), tensor(1.2932e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5350e-17, dtype=torch.float64), tensor(0.3049, dtype=torch.float64), tensor(9.3859e-17, dtype=torch.float64), tensor(0.6951, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.305
  mmlu: 0
  arc_challenge: 0.695

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  4.96it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 30.00it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.07it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.44it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.05it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.39it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.05it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 50.58it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 55.29it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.94it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.36it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 53.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.78it/s]
Evaluation performance at step 25: 0.6
{'loss': 2.4581, 'grad_norm': 0.395605206489563, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.6}
{'eval_loss': 1.3363497257232666, 'eval_runtime': 8.8612, 'eval_samples_per_second': 112.739, 'eval_steps_per_second': 7.11, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<08:14,  5.00s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:38,  2.36it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:16,  5.18it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:08,  8.65it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:05, 12.64it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:03, 17.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:02, 22.97it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:01, 29.27it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:01, 33.01it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:00, 39.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:00, 42.80it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 45.47it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.84it/s]
Evaluation performance at step 50: 0.58
{'loss': 1.2043, 'grad_norm': 0.30674615502357483, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.58}
{'eval_loss': 1.118674635887146, 'eval_runtime': 8.8138, 'eval_samples_per_second': 113.345, 'eval_steps_per_second': 7.148, 'epoch': 0.08}
{'loss': 1.0638, 'grad_norm': 0.293468713760376, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0266709327697754, 'eval_runtime': 8.8432, 'eval_samples_per_second': 112.969, 'eval_steps_per_second': 7.124, 'epoch': 0.12}
{'loss': 0.9767, 'grad_norm': 0.3165384531021118, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9585356116294861, 'eval_runtime': 8.8458, 'eval_samples_per_second': 112.935, 'eval_steps_per_second': 7.122, 'epoch': 0.16}
{'loss': 1.011, 'grad_norm': 0.3383055031299591, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8863662481307983, 'eval_runtime': 8.8764, 'eval_samples_per_second': 112.546, 'eval_steps_per_second': 7.097, 'epoch': 0.2}
{'loss': 0.8471, 'grad_norm': 0.5877726674079895, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8293032646179199, 'eval_runtime': 8.8784, 'eval_samples_per_second': 112.521, 'eval_steps_per_second': 7.096, 'epoch': 0.24}
{'loss': 0.7801, 'grad_norm': 0.3866352438926697, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7594913244247437, 'eval_runtime': 8.8513, 'eval_samples_per_second': 112.865, 'eval_steps_per_second': 7.118, 'epoch': 0.28}
{'loss': 0.7554, 'grad_norm': 0.443805456161499, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.7159856557846069, 'eval_runtime': 8.8645, 'eval_samples_per_second': 112.697, 'eval_steps_per_second': 7.107, 'epoch': 0.32}
{'loss': 0.7108, 'grad_norm': 0.46229270100593567, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6697019338607788, 'eval_runtime': 8.8722, 'eval_samples_per_second': 112.599, 'eval_steps_per_second': 7.101, 'epoch': 0.36}
{'loss': 0.6952, 'grad_norm': 0.5365508198738098, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6299333572387695, 'eval_runtime': 8.8769, 'eval_samples_per_second': 112.539, 'eval_steps_per_second': 7.097, 'epoch': 0.4}
{'loss': 0.6835, 'grad_norm': 0.4887620508670807, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.6011520624160767, 'eval_runtime': 8.8674, 'eval_samples_per_second': 112.659, 'eval_steps_per_second': 7.105, 'epoch': 0.44}
{'loss': 0.6163, 'grad_norm': 0.38525068759918213, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5688514709472656, 'eval_runtime': 8.8965, 'eval_samples_per_second': 112.291, 'eval_steps_per_second': 7.081, 'epoch': 0.48}
{'loss': 0.6953, 'grad_norm': 0.41757240891456604, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.537723958492279, 'eval_runtime': 8.9067, 'eval_samples_per_second': 112.163, 'eval_steps_per_second': 7.073, 'epoch': 0.52}
{'loss': 0.6166, 'grad_norm': 0.25694918632507324, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.5186492800712585, 'eval_runtime': 8.9016, 'eval_samples_per_second': 112.228, 'eval_steps_per_second': 7.077, 'epoch': 0.56}
{'loss': 0.5995, 'grad_norm': 0.3460618257522583, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4959854781627655, 'eval_runtime': 8.898, 'eval_samples_per_second': 112.273, 'eval_steps_per_second': 7.08, 'epoch': 0.6}
{'loss': 0.6916, 'grad_norm': 0.36845529079437256, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4823533892631531, 'eval_runtime': 8.9292, 'eval_samples_per_second': 111.881, 'eval_steps_per_second': 7.056, 'epoch': 0.64}
{'loss': 0.5435, 'grad_norm': 0.2377532422542572, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4664907157421112, 'eval_runtime': 8.8728, 'eval_samples_per_second': 112.591, 'eval_steps_per_second': 7.1, 'epoch': 0.68}
{'loss': 0.5967, 'grad_norm': 0.2720773220062256, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.45338600873947144, 'eval_runtime': 8.8775, 'eval_samples_per_second': 112.532, 'eval_steps_per_second': 7.097, 'epoch': 0.72}
{'loss': 0.6244, 'grad_norm': 0.23383277654647827, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.44332507252693176, 'eval_runtime': 8.8615, 'eval_samples_per_second': 112.735, 'eval_steps_per_second': 7.109, 'epoch': 0.76}
{'loss': 0.5524, 'grad_norm': 0.2874424457550049, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.43409785628318787, 'eval_runtime': 8.8655, 'eval_samples_per_second': 112.684, 'eval_steps_per_second': 7.106, 'epoch': 0.8}
{'loss': 0.571, 'grad_norm': 0.222861185669899, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.4230775535106659, 'eval_runtime': 8.8721, 'eval_samples_per_second': 112.601, 'eval_steps_per_second': 7.101, 'epoch': 0.84}
{'loss': 0.5418, 'grad_norm': 0.22225499153137207, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.4198111891746521, 'eval_runtime': 8.8836, 'eval_samples_per_second': 112.455, 'eval_steps_per_second': 7.092, 'epoch': 0.88}
{'loss': 0.4797, 'grad_norm': 0.27980101108551025, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.4150603115558624, 'eval_runtime': 8.89, 'eval_samples_per_second': 112.374, 'eval_steps_per_second': 7.087, 'epoch': 0.92}
{'loss': 0.5288, 'grad_norm': 0.2697223126888275, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.41270652413368225, 'eval_runtime': 8.885, 'eval_samples_per_second': 112.436, 'eval_steps_per_second': 7.091, 'epoch': 0.96}
{'loss': 0.3692, 'grad_norm': 0.23502591252326965, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.4117109477519989, 'eval_runtime': 8.9146, 'eval_samples_per_second': 112.063, 'eval_steps_per_second': 7.067, 'epoch': 1.0}
{'train_runtime': 484.5401, 'train_samples_per_second': 20.636, 'train_steps_per_second': 1.29, 'train_loss': 0.768507829284668, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3363497257232666, 1.118674635887146, 1.0266709327697754, 0.9585356116294861, 0.8863662481307983, 0.8293032646179199, 0.7594913244247437, 0.7159856557846069, 0.6697019338607788, 0.6299333572387695, 0.6011520624160767, 0.5688514709472656, 0.537723958492279, 0.5186492800712585, 0.4959854781627655, 0.4823533892631531, 0.4664907157421112, 0.45338600873947144, 0.44332507252693176, 0.43409785628318787, 0.4230775535106659, 0.4198111891746521, 0.4150603115558624, 0.41270652413368225, 0.4117109477519989], 'performance': [0.6, 0.58]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.65it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 15.44it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 24.82it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 31.87it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:00, 38.70it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 41.48it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 54.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 38.34it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.6, 0.58]
current iteration observed (possibly low-fid or predicted) performance:  1.249295711517334
current iteration best possible performance (full train run):  0.63
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.2313 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9850060939788818, 0.21512335538864136, 0.22177475690841675, 0.3456599712371826, 0.5804547667503357, 0.11632239818572998, 0.8791298866271973, 0.8092666864395142, 0.6203228235244751, 0.40812158584594727, 0.2055094838142395, 0.3788498640060425, 0.4518037438392639, 0.6825863718986511, 0.08049261569976807, 0.9624916911125183, 0.6498667001724243, 0.8193689584732056, 0.9904505014419556]  ‚Üí  acq = 0.9379947692999298
X = [0.49302464723587036, 0.1924196481704712, 0.5456835031509399, 0.36026960611343384, 0.6007611155509949, 0.32364416122436523, 0.069821298122406, 0.1105462908744812, 0.12792342901229858, 0.9814503192901611, 0.9233092069625854, 0.02941352128982544, 0.5441425442695618, 0.5786149501800537, 0.0419391393661499, 0.796787679195404, 0.30965495109558105, 0.29677143692970276, 0.939351499080658]  ‚Üí  acq = 0.9380624672939508
X = [0.07044440507888794, 0.8796802759170532, 0.594001829624176, 0.1995164155960083, 0.31244778633117676, 0.8665319681167603, 0.29118210077285767, 0.43379831314086914, 0.33467382192611694, 0.812040388584137, 0.08510172367095947, 0.8186143636703491, 0.8260303735733032, 0.747736930847168, 0.7611817121505737, 0.9319164752960205, 0.7998526692390442, 0.04624009504914284, 0.3688918948173523]  ‚Üí  acq = 0.7599442038402722
X = [0.6825830936431885, 0.7683879733085632, 0.2085895538330078, 0.7832455635070801, 0.6396840214729309, 0.48884671926498413, 0.4876680374145508, 0.7495908737182617, 0.48719942569732666, 0.3905937671661377, 0.9323699474334717, 0.23341262340545654, 0.8777536153793335, 0.5088933706283569, 0.3512105345726013, 0.15802353620529175, 0.9819060564041138, 0.7213280200958252, 0.6990442276000977]  ‚Üí  acq = 0.9381218234863924
X = [0.2843392491340637, 0.6341145038604736, 0.29735326766967773, 0.700494110584259, 0.2039269208908081, 0.9184576272964478, 0.2842642664909363, 0.936412513256073, 0.40833091735839844, 0.2766789197921753, 0.616297721862793, 0.6844825744628906, 0.060568273067474365, 0.9679666757583618, 0.5745741724967957, 0.11610714346170425, 0.2534680962562561, 0.25819867849349976, 0.7940090894699097]  ‚Üí  acq = 0.8206361006925312
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.0736, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2376, dtype=torch.float64), 0, tensor(0.6888, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.021164265239932825, 1.480000019073488, 0]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0.0736, dtype=torch.float64), tensor(6.5837e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5548e-16, dtype=torch.float64), tensor(0.2376, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6888, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2116, dtype=torch.float64), tensor(0.0308, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.074
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.238
  mmlu: 0
  arc_challenge: 0.689

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.021164265239932825,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (1.480000019073488,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.021164265239932825
lora alpha:  1.480000019073488
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.61it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 23.23it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.13it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 36.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.92it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 43.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 43.66it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 47.53it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 44.58it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 48.71it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 48.07it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 48.04it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 48.58it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 43.95it/s]
Evaluation performance at step 25: 0.61
{'loss': 3.4428, 'grad_norm': 0.16582205891609192, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 2.470590114593506, 'eval_runtime': 9.5043, 'eval_samples_per_second': 105.11, 'eval_steps_per_second': 6.629, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:23,  4.21it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 25.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 32.44it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:02, 35.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 37.85it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 40.93it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 41.23it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 45.32it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 42.26it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 46.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 46.55it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 47.01it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 47.74it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 42.65it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.8272, 'grad_norm': 0.19484852254390717, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.3858243227005005, 'eval_runtime': 9.48, 'eval_samples_per_second': 105.38, 'eval_steps_per_second': 6.646, 'epoch': 0.08}
{'loss': 1.2974, 'grad_norm': 0.0696176216006279, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.2121425867080688, 'eval_runtime': 9.517, 'eval_samples_per_second': 104.97, 'eval_steps_per_second': 6.62, 'epoch': 0.12}
{'loss': 1.104, 'grad_norm': 0.05256561189889908, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1411844491958618, 'eval_runtime': 9.5301, 'eval_samples_per_second': 104.826, 'eval_steps_per_second': 6.611, 'epoch': 0.16}
{'loss': 1.0966, 'grad_norm': 0.05830398574471474, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0844777822494507, 'eval_runtime': 9.5387, 'eval_samples_per_second': 104.731, 'eval_steps_per_second': 6.605, 'epoch': 0.2}
{'loss': 1.0098, 'grad_norm': 0.052452731877565384, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0515104532241821, 'eval_runtime': 9.5392, 'eval_samples_per_second': 104.726, 'eval_steps_per_second': 6.604, 'epoch': 0.24}
{'loss': 1.0152, 'grad_norm': 0.04801298305392265, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0284976959228516, 'eval_runtime': 9.5468, 'eval_samples_per_second': 104.642, 'eval_steps_per_second': 6.599, 'epoch': 0.28}
{'loss': 1.0432, 'grad_norm': 0.05303896218538284, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.008981466293335, 'eval_runtime': 9.5474, 'eval_samples_per_second': 104.636, 'eval_steps_per_second': 6.599, 'epoch': 0.32}
{'loss': 0.9978, 'grad_norm': 0.052957452833652496, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9913727045059204, 'eval_runtime': 9.5527, 'eval_samples_per_second': 104.578, 'eval_steps_per_second': 6.595, 'epoch': 0.36}
{'loss': 1.0151, 'grad_norm': 0.0724387913942337, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9710760712623596, 'eval_runtime': 9.5443, 'eval_samples_per_second': 104.67, 'eval_steps_per_second': 6.601, 'epoch': 0.4}
{'loss': 0.9485, 'grad_norm': 0.0629923939704895, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9535447955131531, 'eval_runtime': 9.5515, 'eval_samples_per_second': 104.591, 'eval_steps_per_second': 6.596, 'epoch': 0.44}
{'loss': 0.9559, 'grad_norm': 0.06386590749025345, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9324939846992493, 'eval_runtime': 9.6265, 'eval_samples_per_second': 103.776, 'eval_steps_per_second': 6.544, 'epoch': 0.48}
{'loss': 0.9142, 'grad_norm': 0.08666189759969711, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9066427946090698, 'eval_runtime': 9.6245, 'eval_samples_per_second': 103.798, 'eval_steps_per_second': 6.546, 'epoch': 0.52}
{'loss': 0.8963, 'grad_norm': 0.07845994830131531, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8803755044937134, 'eval_runtime': 9.607, 'eval_samples_per_second': 103.987, 'eval_steps_per_second': 6.558, 'epoch': 0.56}
{'loss': 0.9088, 'grad_norm': 0.08050903677940369, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8546446561813354, 'eval_runtime': 9.5762, 'eval_samples_per_second': 104.321, 'eval_steps_per_second': 6.579, 'epoch': 0.6}
{'loss': 0.8952, 'grad_norm': 0.09061916917562485, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8308210372924805, 'eval_runtime': 9.541, 'eval_samples_per_second': 104.706, 'eval_steps_per_second': 6.603, 'epoch': 0.64}
{'loss': 0.8324, 'grad_norm': 0.0922873243689537, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8092976808547974, 'eval_runtime': 9.5411, 'eval_samples_per_second': 104.705, 'eval_steps_per_second': 6.603, 'epoch': 0.68}
{'loss': 0.8276, 'grad_norm': 0.13715380430221558, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.783340334892273, 'eval_runtime': 9.5507, 'eval_samples_per_second': 104.6, 'eval_steps_per_second': 6.596, 'epoch': 0.72}
{'loss': 0.7955, 'grad_norm': 0.09841068089008331, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.7608932256698608, 'eval_runtime': 9.5724, 'eval_samples_per_second': 104.363, 'eval_steps_per_second': 6.581, 'epoch': 0.76}
{'loss': 0.8165, 'grad_norm': 0.11976698040962219, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.7412824034690857, 'eval_runtime': 9.5404, 'eval_samples_per_second': 104.712, 'eval_steps_per_second': 6.603, 'epoch': 0.8}
{'loss': 0.7888, 'grad_norm': 0.13703081011772156, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.7225810885429382, 'eval_runtime': 9.5507, 'eval_samples_per_second': 104.6, 'eval_steps_per_second': 6.596, 'epoch': 0.84}
{'loss': 0.7294, 'grad_norm': 0.1492948681116104, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.7063579559326172, 'eval_runtime': 9.5466, 'eval_samples_per_second': 104.645, 'eval_steps_per_second': 6.599, 'epoch': 0.88}
{'loss': 0.8338, 'grad_norm': 0.11934348195791245, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6950930953025818, 'eval_runtime': 9.5481, 'eval_samples_per_second': 104.628, 'eval_steps_per_second': 6.598, 'epoch': 0.92}
{'loss': 0.7316, 'grad_norm': 0.15017399191856384, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6850709319114685, 'eval_runtime': 9.5456, 'eval_samples_per_second': 104.656, 'eval_steps_per_second': 6.6, 'epoch': 0.96}
{'loss': 0.7265, 'grad_norm': 0.1609441339969635, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6825650334358215, 'eval_runtime': 9.5463, 'eval_samples_per_second': 104.648, 'eval_steps_per_second': 6.599, 'epoch': 1.0}
{'train_runtime': 543.2711, 'train_samples_per_second': 18.403, 'train_steps_per_second': 1.15, 'train_loss': 1.0580061279296875, 'epoch': 1.0}
train_results:  {'eval_loss': [2.470590114593506, 1.3858243227005005, 1.2121425867080688, 1.1411844491958618, 1.0844777822494507, 1.0515104532241821, 1.0284976959228516, 1.008981466293335, 0.9913727045059204, 0.9710760712623596, 0.9535447955131531, 0.9324939846992493, 0.9066427946090698, 0.8803755044937134, 0.8546446561813354, 0.8308210372924805, 0.8092976808547974, 0.783340334892273, 0.7608932256698608, 0.7412824034690857, 0.7225810885429382, 0.7063579559326172, 0.6950930953025818, 0.6850709319114685, 0.6825650334358215], 'performance': [0.61, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:40,  2.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 23.49it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 34.46it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 39.20it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 45.32it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 50.27it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 64.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.36it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.0923447608947754
current iteration best possible performance (full train run):  0.5984999999999999
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.8139 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5942257046699524, 0.6277637481689453, 0.38782650232315063, 0.37862110137939453, 0.5409438014030457, 0.6521950960159302, 0.07144474983215332, 0.6736050844192505, 0.1644742488861084, 0.6644530296325684, 0.9253227114677429, 0.7674115896224976, 0.778812050819397, 0.761591911315918, 0.13799923658370972, 0.5030709505081177, 0.7795954942703247, 0.23601557314395905, 0.6689286828041077]  ‚Üí  acq = 0.950258012850347
X = [0.45098429918289185, 0.6110503673553467, 0.28571975231170654, 0.8852470517158508, 0.6684074401855469, 0.5147650241851807, 0.517264187335968, 0.3443108797073364, 0.4686200022697449, 0.20916521549224854, 0.027361571788787842, 0.5054267644882202, 0.20162492990493774, 0.9628875851631165, 0.7232235074043274, 0.5497549176216125, 0.4938083291053772, 0.8217053413391113, 0.4559321403503418]  ‚Üí  acq = 0.9514495147978045
X = [0.9705662131309509, 0.9069650769233704, 0.2665117383003235, 0.011962831020355225, 0.06834208965301514, 0.7829591631889343, 0.9718748927116394, 0.1570678949356079, 0.1520630121231079, 0.6370755434036255, 0.7694988250732422, 0.053691208362579346, 0.5897045731544495, 0.9527956247329712, 0.660004734992981, 0.4609135687351227, 0.2216120958328247, 0.5080620646476746, 0.07429951429367065]  ‚Üí  acq = 0.6943669870255886
X = [0.20480990409851074, 0.6335836052894592, 0.7611386775970459, 0.17331886291503906, 0.8485125303268433, 0.09243136644363403, 0.5311341881752014, 0.994128942489624, 0.4272412061691284, 0.32003167271614075, 0.272697389125824, 0.8221282362937927, 0.05794215202331543, 0.7704386711120605, 0.18258321285247803, 0.6486541628837585, 0.41115450859069824, 0.33196502923965454, 0.31577277183532715]  ‚Üí  acq = 0.951473836664624
X = [0.9830282330513, 0.6886211037635803, 0.319507360458374, 0.7606837153434753, 0.7066754698753357, 0.9192408919334412, 0.3608999252319336, 0.4348216652870178, 0.08913511037826538, 0.9961743354797363, 0.38848674297332764, 0.03277784585952759, 0.3889153003692627, 0.7702159285545349, 0.528216540813446, 0.11223944276571274, 0.590921938419342, 0.5302199125289917, 0.08998453617095947]  ‚Üí  acq = 0.95146961330531
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2816, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.7184, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.053139098946126756, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2816, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0564e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7184, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.5314, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.282
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.718

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.053139098946126756,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.053139098946126756
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.79it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.08it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 36.85it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.78it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.18it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.92it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.45it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.61it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.53it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 51.78it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 51.98it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.27it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.4017, 'grad_norm': 0.48357805609703064, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.0534638166427612, 'eval_runtime': 7.5731, 'eval_samples_per_second': 131.915, 'eval_steps_per_second': 8.319, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.80it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.91it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.24it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.58it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 47.34it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.38it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.69it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.98it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.07it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.89it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 50.24it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 56.50it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.58it/s]
Evaluation performance at step 50: 0.62
{'loss': 0.909, 'grad_norm': 0.45459017157554626, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 0.8417803049087524, 'eval_runtime': 7.4712, 'eval_samples_per_second': 133.713, 'eval_steps_per_second': 8.432, 'epoch': 0.08}
{'loss': 0.8241, 'grad_norm': 0.25668227672576904, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7704935073852539, 'eval_runtime': 7.5178, 'eval_samples_per_second': 132.885, 'eval_steps_per_second': 8.38, 'epoch': 0.12}
{'loss': 0.7701, 'grad_norm': 0.2697813808917999, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7110252380371094, 'eval_runtime': 7.5082, 'eval_samples_per_second': 133.054, 'eval_steps_per_second': 8.391, 'epoch': 0.16}
{'loss': 0.7216, 'grad_norm': 0.32350197434425354, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.649408757686615, 'eval_runtime': 7.5261, 'eval_samples_per_second': 132.739, 'eval_steps_per_second': 8.371, 'epoch': 0.2}
{'loss': 0.6409, 'grad_norm': 0.3140104413032532, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5862286686897278, 'eval_runtime': 7.5446, 'eval_samples_per_second': 132.413, 'eval_steps_per_second': 8.35, 'epoch': 0.24}
{'loss': 0.5896, 'grad_norm': 0.3617285490036011, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5275576710700989, 'eval_runtime': 7.518, 'eval_samples_per_second': 132.882, 'eval_steps_per_second': 8.38, 'epoch': 0.28}
{'loss': 0.5397, 'grad_norm': 0.3165048062801361, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.4854881167411804, 'eval_runtime': 7.5489, 'eval_samples_per_second': 132.338, 'eval_steps_per_second': 8.346, 'epoch': 0.32}
{'loss': 0.5118, 'grad_norm': 0.2993847131729126, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.44585981965065, 'eval_runtime': 7.5646, 'eval_samples_per_second': 132.063, 'eval_steps_per_second': 8.328, 'epoch': 0.36}
{'loss': 0.4897, 'grad_norm': 0.2773626744747162, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.411384642124176, 'eval_runtime': 7.55, 'eval_samples_per_second': 132.317, 'eval_steps_per_second': 8.344, 'epoch': 0.4}
{'loss': 0.4379, 'grad_norm': 0.430103063583374, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.38121289014816284, 'eval_runtime': 7.5107, 'eval_samples_per_second': 133.011, 'eval_steps_per_second': 8.388, 'epoch': 0.44}
{'loss': 0.4342, 'grad_norm': 0.2790282964706421, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3568838834762573, 'eval_runtime': 7.5051, 'eval_samples_per_second': 133.109, 'eval_steps_per_second': 8.394, 'epoch': 0.48}
{'loss': 0.4102, 'grad_norm': 0.24975436925888062, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3370925188064575, 'eval_runtime': 7.5001, 'eval_samples_per_second': 133.199, 'eval_steps_per_second': 8.4, 'epoch': 0.52}
{'loss': 0.3876, 'grad_norm': 0.22692282497882843, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.31769075989723206, 'eval_runtime': 7.4766, 'eval_samples_per_second': 133.616, 'eval_steps_per_second': 8.426, 'epoch': 0.56}
{'loss': 0.3436, 'grad_norm': 0.2348731905221939, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.30243173241615295, 'eval_runtime': 7.4805, 'eval_samples_per_second': 133.547, 'eval_steps_per_second': 8.422, 'epoch': 0.6}
{'loss': 0.3857, 'grad_norm': 0.2613047659397125, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.29014742374420166, 'eval_runtime': 7.4878, 'eval_samples_per_second': 133.418, 'eval_steps_per_second': 8.414, 'epoch': 0.64}
{'loss': 0.354, 'grad_norm': 0.2469835728406906, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2795076072216034, 'eval_runtime': 7.5102, 'eval_samples_per_second': 133.019, 'eval_steps_per_second': 8.389, 'epoch': 0.68}
{'loss': 0.3536, 'grad_norm': 0.24994249641895294, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.2707371711730957, 'eval_runtime': 7.5397, 'eval_samples_per_second': 132.499, 'eval_steps_per_second': 8.356, 'epoch': 0.72}
{'loss': 0.363, 'grad_norm': 0.13325916230678558, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.26435616612434387, 'eval_runtime': 7.5479, 'eval_samples_per_second': 132.355, 'eval_steps_per_second': 8.347, 'epoch': 0.76}
{'loss': 0.3333, 'grad_norm': 0.2569166421890259, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.2545309364795685, 'eval_runtime': 7.5376, 'eval_samples_per_second': 132.535, 'eval_steps_per_second': 8.358, 'epoch': 0.8}
{'loss': 0.331, 'grad_norm': 0.21569286286830902, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.24843966960906982, 'eval_runtime': 7.5456, 'eval_samples_per_second': 132.396, 'eval_steps_per_second': 8.349, 'epoch': 0.84}
{'loss': 0.3034, 'grad_norm': 0.3216404318809509, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.24296456575393677, 'eval_runtime': 7.5182, 'eval_samples_per_second': 132.878, 'eval_steps_per_second': 8.38, 'epoch': 0.88}
{'loss': 0.2848, 'grad_norm': 0.2635857164859772, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.23944024741649628, 'eval_runtime': 7.5031, 'eval_samples_per_second': 133.146, 'eval_steps_per_second': 8.397, 'epoch': 0.92}
{'loss': 0.2999, 'grad_norm': 0.2908797562122345, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2379409223794937, 'eval_runtime': 7.5454, 'eval_samples_per_second': 132.399, 'eval_steps_per_second': 8.349, 'epoch': 0.96}
{'loss': 0.2645, 'grad_norm': 0.2058311104774475, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.23673731088638306, 'eval_runtime': 7.5095, 'eval_samples_per_second': 133.032, 'eval_steps_per_second': 8.389, 'epoch': 1.0}
{'train_runtime': 444.3907, 'train_samples_per_second': 22.5, 'train_steps_per_second': 1.406, 'train_loss': 0.5473888633728027, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0534638166427612, 0.8417803049087524, 0.7704935073852539, 0.7110252380371094, 0.649408757686615, 0.5862286686897278, 0.5275576710700989, 0.4854881167411804, 0.44585981965065, 0.411384642124176, 0.38121289014816284, 0.3568838834762573, 0.3370925188064575, 0.31769075989723206, 0.30243173241615295, 0.29014742374420166, 0.2795076072216034, 0.2707371711730957, 0.26435616612434387, 0.2545309364795685, 0.24843966960906982, 0.24296456575393677, 0.23944024741649628, 0.2379409223794937, 0.23673731088638306], 'performance': [0.63, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:40,  2.46it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 28.88it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 37.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 43.70it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.80it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 49.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 58.62it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.95it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2482903003692627
current iteration best possible performance (full train run):  0.63
max performance so far:  0.6615000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.9077 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.38405847549438477, 0.1316918134689331, 0.7304181456565857, 0.5353239178657532, 0.08240920305252075, 0.9292183518409729, 0.7614168524742126, 0.7895469069480896, 0.3474884629249573, 0.9557192921638489, 0.46338582038879395, 0.9888672232627869, 0.6890408992767334, 0.7924329042434692, 0.7154673337936401, 0.23412743210792542, 0.590995728969574, 0.9054816961288452, 0.5934849977493286]  ‚Üí  acq = 0.7698496668943722
X = [0.8939239978790283, 0.876083254814148, 0.10797595977783203, 0.22261542081832886, 0.737383246421814, 0.04969698190689087, 0.653698205947876, 0.49867141246795654, 0.46078193187713623, 0.6302239894866943, 0.5711628794670105, 0.6976407170295715, 0.09897392988204956, 0.19291263818740845, 0.08603078126907349, 0.2380482256412506, 0.005324244499206543, 0.050192270427942276, 0.015405476093292236]  ‚Üí  acq = 0.9499949705642652
X = [0.7619262337684631, 0.018857181072235107, 0.22052574157714844, 0.7179930806159973, 0.2547808289527893, 0.724411129951477, 0.4576507806777954, 0.1481790542602539, 0.1156415343284607, 0.6303877830505371, 0.8160991072654724, 0.27281343936920166, 0.5587962865829468, 0.45700669288635254, 0.648169994354248, 0.445888876914978, 0.5946420431137085, 0.3346695601940155, 0.91709303855896]  ‚Üí  acq = 0.7646573530608431
X = [0.1632022261619568, 0.49762779474258423, 0.20292824506759644, 0.5262919664382935, 0.6746607422828674, 0.9906603693962097, 0.6390199661254883, 0.04488229751586914, 0.5747889876365662, 0.8407934308052063, 0.1890905499458313, 0.15865761041641235, 0.9959678649902344, 0.8584550619125366, 0.6812216639518738, 0.019973671063780785, 0.03730529546737671, 0.7104324102401733, 0.32633787393569946]  ‚Üí  acq = 0.9499866176996935
X = [0.24437397718429565, 0.5571206212043762, 0.5199233889579773, 0.975454568862915, 0.3963009715080261, 0.7427161335945129, 0.18841809034347534, 0.7938576936721802, 0.8716811537742615, 0.3823332190513611, 0.8872495293617249, 0.9062683582305908, 0.3490223288536072, 0.42994165420532227, 0.19652289152145386, 0.832382082939148, 0.9906535744667053, 0.10609808564186096, 0.8738296031951904]  ‚Üí  acq = 0.9414145268698891
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0646, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.2248, dtype=torch.float64), tensor(0.7106, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.05763182710689677, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0.0646, dtype=torch.float64), tensor(8.1357e-16, dtype=torch.float64), tensor(9.4891e-16, dtype=torch.float64), tensor(8.5933e-17, dtype=torch.float64), tensor(3.6977e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2248, dtype=torch.float64), tensor(0.7106, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5763, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.065
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.225
  arc_challenge: 0.711

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05763182710689677,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.05763182710689677
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.15it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.22it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.68it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.83it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.41it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.59it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.81it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.11it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.54it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.43it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.70it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.40it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.3581, 'grad_norm': 0.4918513000011444, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.1773170232772827, 'eval_runtime': 9.1718, 'eval_samples_per_second': 108.921, 'eval_steps_per_second': 6.869, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:01<02:06,  1.27s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:01<00:10,  8.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:01<00:05, 16.13it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:03, 23.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 29.33it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:02<00:01, 36.66it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:01, 41.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:00, 47.84it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:00, 47.28it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:02<00:00, 52.50it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:02<00:00, 52.76it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 52.47it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 34.03it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.0262, 'grad_norm': 0.49004361033439636, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 0.9846286177635193, 'eval_runtime': 9.1687, 'eval_samples_per_second': 108.958, 'eval_steps_per_second': 6.871, 'epoch': 0.08}
{'loss': 0.9929, 'grad_norm': 0.28619080781936646, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9008037447929382, 'eval_runtime': 9.187, 'eval_samples_per_second': 108.741, 'eval_steps_per_second': 6.858, 'epoch': 0.12}
{'loss': 0.8622, 'grad_norm': 0.2686029374599457, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8478626608848572, 'eval_runtime': 9.2138, 'eval_samples_per_second': 108.425, 'eval_steps_per_second': 6.838, 'epoch': 0.16}
{'loss': 0.8388, 'grad_norm': 0.28193122148513794, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7916426062583923, 'eval_runtime': 9.2813, 'eval_samples_per_second': 107.635, 'eval_steps_per_second': 6.788, 'epoch': 0.2}
{'loss': 0.7688, 'grad_norm': 0.36976954340934753, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7277235984802246, 'eval_runtime': 9.2888, 'eval_samples_per_second': 107.549, 'eval_steps_per_second': 6.782, 'epoch': 0.24}
{'loss': 0.7594, 'grad_norm': 0.2989051043987274, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6829017996788025, 'eval_runtime': 9.3208, 'eval_samples_per_second': 107.18, 'eval_steps_per_second': 6.759, 'epoch': 0.28}
{'loss': 0.7422, 'grad_norm': 0.2565610408782959, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6447572112083435, 'eval_runtime': 9.3073, 'eval_samples_per_second': 107.335, 'eval_steps_per_second': 6.769, 'epoch': 0.32}
{'loss': 0.6619, 'grad_norm': 0.3537180721759796, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.599918007850647, 'eval_runtime': 9.3059, 'eval_samples_per_second': 107.351, 'eval_steps_per_second': 6.77, 'epoch': 0.36}
{'loss': 0.6598, 'grad_norm': 0.2952064871788025, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5643877983093262, 'eval_runtime': 9.273, 'eval_samples_per_second': 107.732, 'eval_steps_per_second': 6.794, 'epoch': 0.4}
{'loss': 0.586, 'grad_norm': 0.27778443694114685, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.531427264213562, 'eval_runtime': 9.3118, 'eval_samples_per_second': 107.283, 'eval_steps_per_second': 6.766, 'epoch': 0.44}
{'loss': 0.5983, 'grad_norm': 0.38747480511665344, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5044164061546326, 'eval_runtime': 9.2774, 'eval_samples_per_second': 107.681, 'eval_steps_per_second': 6.791, 'epoch': 0.48}
{'loss': 0.5202, 'grad_norm': 0.26941972970962524, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.47562679648399353, 'eval_runtime': 9.2804, 'eval_samples_per_second': 107.646, 'eval_steps_per_second': 6.789, 'epoch': 0.52}
{'loss': 0.5364, 'grad_norm': 0.35581597685813904, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4572523832321167, 'eval_runtime': 9.2836, 'eval_samples_per_second': 107.61, 'eval_steps_per_second': 6.786, 'epoch': 0.56}
{'loss': 0.512, 'grad_norm': 0.29075509309768677, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4425366222858429, 'eval_runtime': 9.2742, 'eval_samples_per_second': 107.718, 'eval_steps_per_second': 6.793, 'epoch': 0.6}
{'loss': 0.4963, 'grad_norm': 0.31962499022483826, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.42115485668182373, 'eval_runtime': 9.2586, 'eval_samples_per_second': 107.899, 'eval_steps_per_second': 6.804, 'epoch': 0.64}
{'loss': 0.4482, 'grad_norm': 0.21793395280838013, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.40615493059158325, 'eval_runtime': 9.2543, 'eval_samples_per_second': 107.949, 'eval_steps_per_second': 6.808, 'epoch': 0.68}
{'loss': 0.4422, 'grad_norm': 0.35277777910232544, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.39672234654426575, 'eval_runtime': 9.2817, 'eval_samples_per_second': 107.631, 'eval_steps_per_second': 6.788, 'epoch': 0.72}
{'loss': 0.4266, 'grad_norm': 0.2665283679962158, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3854379653930664, 'eval_runtime': 9.268, 'eval_samples_per_second': 107.79, 'eval_steps_per_second': 6.798, 'epoch': 0.76}
{'loss': 0.4714, 'grad_norm': 0.30845925211906433, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.3760961890220642, 'eval_runtime': 9.2398, 'eval_samples_per_second': 108.119, 'eval_steps_per_second': 6.818, 'epoch': 0.8}
{'loss': 0.4416, 'grad_norm': 0.19603155553340912, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3707561790943146, 'eval_runtime': 9.2652, 'eval_samples_per_second': 107.823, 'eval_steps_per_second': 6.8, 'epoch': 0.84}
{'loss': 0.4233, 'grad_norm': 0.20774662494659424, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.3651565611362457, 'eval_runtime': 9.2049, 'eval_samples_per_second': 108.529, 'eval_steps_per_second': 6.844, 'epoch': 0.88}
{'loss': 0.4344, 'grad_norm': 0.14572377502918243, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3620089888572693, 'eval_runtime': 9.2035, 'eval_samples_per_second': 108.545, 'eval_steps_per_second': 6.845, 'epoch': 0.92}
{'loss': 0.4503, 'grad_norm': 0.23382426798343658, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3581845462322235, 'eval_runtime': 9.241, 'eval_samples_per_second': 108.105, 'eval_steps_per_second': 6.817, 'epoch': 0.96}
{'loss': 0.4203, 'grad_norm': 0.140471413731575, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3568270206451416, 'eval_runtime': 9.2562, 'eval_samples_per_second': 107.928, 'eval_steps_per_second': 6.806, 'epoch': 1.0}
{'train_runtime': 506.9355, 'train_samples_per_second': 19.722, 'train_steps_per_second': 1.233, 'train_loss': 0.6751140396118164, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1773170232772827, 0.9846286177635193, 0.9008037447929382, 0.8478626608848572, 0.7916426062583923, 0.7277235984802246, 0.6829017996788025, 0.6447572112083435, 0.599918007850647, 0.5643877983093262, 0.531427264213562, 0.5044164061546326, 0.47562679648399353, 0.4572523832321167, 0.4425366222858429, 0.42115485668182373, 0.40615493059158325, 0.39672234654426575, 0.3854379653930664, 0.3760961890220642, 0.3707561790943146, 0.3651565611362457, 0.3620089888572693, 0.3581845462322235, 0.3568270206451416], 'performance': [0.62, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:34,  2.86it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 31.68it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 41.69it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 43.53it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.24it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.92it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 69.22it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.07it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2481999397277832
current iteration best possible performance (full train run):  0.6825000000000001
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5390 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.16739577054977417, 0.36976462602615356, 0.23139983415603638, 0.3812927007675171, 0.1881958246231079, 0.9429587125778198, 0.509323239326477, 0.2771714925765991, 0.30021870136260986, 0.74922776222229, 0.7460530400276184, 0.6484355926513672, 0.5752243399620056, 0.7358518838882446, 0.6738536357879639, 0.5535141229629517, 0.8008444309234619, 0.35139355063438416, 0.5993521809577942]  ‚Üí  acq = 0.6086124019609798
X = [0.7996418476104736, 0.001956641674041748, 0.0696491003036499, 0.15393584966659546, 0.9670318961143494, 0.7709032297134399, 0.24105119705200195, 0.697994589805603, 0.5109493732452393, 0.6219257712364197, 0.5899301767349243, 0.06563746929168701, 0.8877817392349243, 0.14481014013290405, 0.3469420075416565, 0.4816727638244629, 0.20394617319107056, 0.7075672149658203, 0.19621777534484863]  ‚Üí  acq = 0.9516253934125924
X = [0.1467341184616089, 0.018912076950073242, 0.2558440566062927, 0.5099181532859802, 0.6023241281509399, 0.7492532134056091, 0.0489506721496582, 0.7076557874679565, 0.47296130657196045, 0.5495465397834778, 0.34539294242858887, 0.35538214445114136, 0.08530306816101074, 0.9088041186332703, 0.878498911857605, 0.8205994963645935, 0.2606879472732544, 0.9892069101333618, 0.9987426400184631]  ‚Üí  acq = 0.9514099172070553
X = [0.8021048307418823, 0.03217673301696777, 0.3597593903541565, 0.2451736330986023, 0.6577511429786682, 0.7617989182472229, 0.755630373954773, 0.3598412275314331, 0.5294933319091797, 0.8140339851379395, 0.15254467725753784, 0.3463197946548462, 0.7070716619491577, 0.8607667684555054, 0.4309294819831848, 0.28376853466033936, 0.2066451907157898, 0.3385169208049774, 0.5878193378448486]  ‚Üí  acq = 0.9516055160594683
X = [0.6887049674987793, 0.1450744867324829, 0.29398250579833984, 0.9280814528465271, 0.126276433467865, 0.24283063411712646, 0.9612183570861816, 0.5084037184715271, 0.09574753046035767, 0.41849055886268616, 0.7182619571685791, 0.6204363703727722, 0.5924060344696045, 0.8438572287559509, 0.5270520448684692, 0.03937872499227524, 0.44906359910964966, 0.9233269691467285, 0.21459579467773438]  ‚Üí  acq = 0.621844991365232
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1745, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.0921, dtype=torch.float64), 0, tensor(0.7334, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.05260373633217113, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1745, dtype=torch.float64), tensor(1.9162e-16, dtype=torch.float64), tensor(2.0546e-17, dtype=torch.float64), tensor(5.4080e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0921, dtype=torch.float64), tensor(1.1603e-16, dtype=torch.float64), tensor(0.7334, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5260, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.175
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.092
  mmlu: 0
  arc_challenge: 0.733

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05260373633217113,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.05260373633217113
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.12it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 30.31it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.17it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.12it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.23it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 48.88it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.41it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.85it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.42it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.12it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.06it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 48.35it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 47.41it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.59it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.5864, 'grad_norm': 0.5454382300376892, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.2896571159362793, 'eval_runtime': 7.274, 'eval_samples_per_second': 137.338, 'eval_steps_per_second': 8.661, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.90it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.68it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.68it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.72it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.03it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 47.77it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.45it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.44it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.34it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.15it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.20it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 52.67it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 53.09it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.69it/s]
Evaluation performance at step 50: 0.59
{'loss': 1.1632, 'grad_norm': 0.1938997507095337, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 1.074475646018982, 'eval_runtime': 7.2672, 'eval_samples_per_second': 137.468, 'eval_steps_per_second': 8.669, 'epoch': 0.08}
{'loss': 1.0695, 'grad_norm': 0.1952110230922699, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9300296306610107, 'eval_runtime': 7.3217, 'eval_samples_per_second': 136.443, 'eval_steps_per_second': 8.605, 'epoch': 0.12}
{'loss': 0.901, 'grad_norm': 0.22320692241191864, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8135349750518799, 'eval_runtime': 7.3064, 'eval_samples_per_second': 136.73, 'eval_steps_per_second': 8.623, 'epoch': 0.16}
{'loss': 0.8116, 'grad_norm': 0.22748775780200958, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7703667879104614, 'eval_runtime': 7.2772, 'eval_samples_per_second': 137.278, 'eval_steps_per_second': 8.657, 'epoch': 0.2}
{'loss': 0.7402, 'grad_norm': 0.2895280718803406, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.6978543996810913, 'eval_runtime': 7.3127, 'eval_samples_per_second': 136.612, 'eval_steps_per_second': 8.615, 'epoch': 0.24}
{'loss': 0.7109, 'grad_norm': 0.28776782751083374, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6286085844039917, 'eval_runtime': 7.335, 'eval_samples_per_second': 136.197, 'eval_steps_per_second': 8.589, 'epoch': 0.28}
{'loss': 0.6993, 'grad_norm': 0.30755695700645447, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.585634171962738, 'eval_runtime': 7.3016, 'eval_samples_per_second': 136.818, 'eval_steps_per_second': 8.628, 'epoch': 0.32}
{'loss': 0.619, 'grad_norm': 0.3324708342552185, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.5371457934379578, 'eval_runtime': 7.2642, 'eval_samples_per_second': 137.525, 'eval_steps_per_second': 8.673, 'epoch': 0.36}
{'loss': 0.5519, 'grad_norm': 0.3324393928050995, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.48742035031318665, 'eval_runtime': 7.2611, 'eval_samples_per_second': 137.582, 'eval_steps_per_second': 8.676, 'epoch': 0.4}
{'loss': 0.515, 'grad_norm': 0.33643999695777893, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.45770418643951416, 'eval_runtime': 7.2647, 'eval_samples_per_second': 137.513, 'eval_steps_per_second': 8.672, 'epoch': 0.44}
{'loss': 0.5123, 'grad_norm': 0.3265155255794525, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.42791029810905457, 'eval_runtime': 7.2658, 'eval_samples_per_second': 137.494, 'eval_steps_per_second': 8.671, 'epoch': 0.48}
{'loss': 0.5018, 'grad_norm': 0.27627572417259216, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3976013660430908, 'eval_runtime': 7.2663, 'eval_samples_per_second': 137.484, 'eval_steps_per_second': 8.67, 'epoch': 0.52}
{'loss': 0.5074, 'grad_norm': 0.45197415351867676, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.379803866147995, 'eval_runtime': 7.2657, 'eval_samples_per_second': 137.495, 'eval_steps_per_second': 8.671, 'epoch': 0.56}
{'loss': 0.3894, 'grad_norm': 0.34621331095695496, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.3586362302303314, 'eval_runtime': 7.2711, 'eval_samples_per_second': 137.394, 'eval_steps_per_second': 8.664, 'epoch': 0.6}
{'loss': 0.4801, 'grad_norm': 0.3008841872215271, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.34172070026397705, 'eval_runtime': 7.2543, 'eval_samples_per_second': 137.711, 'eval_steps_per_second': 8.684, 'epoch': 0.64}
{'loss': 0.4276, 'grad_norm': 0.23550979793071747, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.3310384750366211, 'eval_runtime': 7.2527, 'eval_samples_per_second': 137.741, 'eval_steps_per_second': 8.686, 'epoch': 0.68}
{'loss': 0.4173, 'grad_norm': 0.27017998695373535, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.31753066182136536, 'eval_runtime': 7.2243, 'eval_samples_per_second': 138.283, 'eval_steps_per_second': 8.721, 'epoch': 0.72}
{'loss': 0.4168, 'grad_norm': 0.3980797231197357, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.3081400990486145, 'eval_runtime': 7.2199, 'eval_samples_per_second': 138.368, 'eval_steps_per_second': 8.726, 'epoch': 0.76}
{'loss': 0.4153, 'grad_norm': 0.3103046119213104, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.29765304923057556, 'eval_runtime': 7.2212, 'eval_samples_per_second': 138.342, 'eval_steps_per_second': 8.724, 'epoch': 0.8}
{'loss': 0.4484, 'grad_norm': 0.3199097812175751, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.2922084629535675, 'eval_runtime': 7.2221, 'eval_samples_per_second': 138.325, 'eval_steps_per_second': 8.723, 'epoch': 0.84}
{'loss': 0.3802, 'grad_norm': 0.17741736769676208, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.2856103181838989, 'eval_runtime': 7.2272, 'eval_samples_per_second': 138.228, 'eval_steps_per_second': 8.717, 'epoch': 0.88}
{'loss': 0.3296, 'grad_norm': 0.17410583794116974, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.2828199565410614, 'eval_runtime': 7.2278, 'eval_samples_per_second': 138.217, 'eval_steps_per_second': 8.716, 'epoch': 0.92}
{'loss': 0.3571, 'grad_norm': 0.37620383501052856, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.2787718176841736, 'eval_runtime': 7.2291, 'eval_samples_per_second': 138.191, 'eval_steps_per_second': 8.715, 'epoch': 0.96}
{'loss': 0.37, 'grad_norm': 0.24494783580303192, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.2775968015193939, 'eval_runtime': 7.2248, 'eval_samples_per_second': 138.274, 'eval_steps_per_second': 8.72, 'epoch': 1.0}
{'train_runtime': 415.8362, 'train_samples_per_second': 24.046, 'train_steps_per_second': 1.503, 'train_loss': 0.6528532440185547, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2896571159362793, 1.074475646018982, 0.9300296306610107, 0.8135349750518799, 0.7703667879104614, 0.6978543996810913, 0.6286085844039917, 0.585634171962738, 0.5371457934379578, 0.48742035031318665, 0.45770418643951416, 0.42791029810905457, 0.3976013660430908, 0.379803866147995, 0.3586362302303314, 0.34172070026397705, 0.3310384750366211, 0.31753066182136536, 0.3081400990486145, 0.29765304923057556, 0.2922084629535675, 0.2856103181838989, 0.2828199565410614, 0.2787718176841736, 0.2775968015193939], 'performance': [0.62, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.81it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 33.03it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.61it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.34it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.66it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.73it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 57.98it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.2468851804733276
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0013 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8712601065635681, 0.8196947574615479, 0.7311946153640747, 0.7045624256134033, 0.4803314208984375, 0.6012762188911438, 0.6369251608848572, 0.4473382234573364, 0.7306644916534424, 0.09855876863002777, 0.46514928340911865, 0.8539637327194214, 0.30570054054260254, 0.6082969903945923, 0.8093753457069397, 0.7440342307090759, 0.06490623950958252, 0.6199778318405151, 0.28617286682128906]  ‚Üí  acq = 0.9447021660670456
X = [0.7078545093536377, 0.9687197804450989, 0.5070731043815613, 0.9358494877815247, 0.22656601667404175, 0.05863767862319946, 0.3034905791282654, 0.7667337656021118, 0.5845226049423218, 0.5281763076782227, 0.8164713978767395, 0.9555569291114807, 0.4661250710487366, 0.2086886763572693, 0.728631317615509, 0.5902503728866577, 0.12672573328018188, 0.2925393879413605, 0.26510387659072876]  ‚Üí  acq = 0.8074924358511809
X = [0.8317387700080872, 0.43990039825439453, 0.6161847114562988, 0.5862241387367249, 0.9106979370117188, 0.8128601908683777, 0.9238333702087402, 0.2191341519355774, 0.1549028754234314, 0.27802133560180664, 0.10315525531768799, 0.3643144369125366, 0.2537804841995239, 0.3651861548423767, 0.671696662902832, 0.9828110337257385, 0.8630008697509766, 0.2270936667919159, 0.3998618721961975]  ‚Üí  acq = 0.946983132478026
X = [0.6585469245910645, 0.7723504304885864, 0.7728214859962463, 0.9251718521118164, 0.0540165901184082, 0.04994511604309082, 0.27446258068084717, 0.12176203727722168, 0.7579965591430664, 0.796631395816803, 0.6418143510818481, 0.6715606451034546, 0.7116429805755615, 0.41234850883483887, 0.15167266130447388, 0.7224836349487305, 0.7496002912521362, 0.2402583211660385, 0.5742266178131104]  ‚Üí  acq = 1.09755444778443
X = [0.24483734369277954, 0.312344491481781, 0.9795759320259094, 0.18757259845733643, 0.19529318809509277, 0.40900158882141113, 0.6854859590530396, 0.735378086566925, 0.5221658945083618, 0.48829546570777893, 0.3720560073852539, 0.9484366178512573, 0.3853077292442322, 0.08313095569610596, 0.7150333523750305, 0.9783208966255188, 0.4894517660140991, 0.7654321193695068, 0.3974494934082031]  ‚Üí  acq = 0.7408167040940898
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1841, dtype=torch.float64), 0, tensor(0.0875, dtype=torch.float64), 0, 0, 0, tensor(0.0327, dtype=torch.float64), 0, tensor(0.6957, dtype=torch.float64), 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.1841, dtype=torch.float64), tensor(7.5876e-17, dtype=torch.float64), tensor(0.0875, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0327, dtype=torch.float64), tensor(1.6694e-16, dtype=torch.float64), tensor(0.6957, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.184
  gsm8k: 0
  rowan_hellaswag: 0.088
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.033
  mmlu: 0
  arc_challenge: 0.696

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.30it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 35.76it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 44.80it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 53.55it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.09it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 59.12it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.20it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 60.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.38it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 56.15it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 57.03it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.34it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.9339, 'grad_norm': 0.6061190962791443, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.5251020193099976, 'eval_runtime': 8.852, 'eval_samples_per_second': 112.855, 'eval_steps_per_second': 7.117, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.38it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:06, 13.39it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 23.24it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:02, 30.70it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:01, 38.10it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.29it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.80it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 54.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.04it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.94it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.46it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:02<00:00, 58.76it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 46.91it/s]
Evaluation performance at step 50: 0.57
{'loss': 1.2452, 'grad_norm': 0.25883081555366516, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.57}
{'eval_loss': 1.1310697793960571, 'eval_runtime': 8.8735, 'eval_samples_per_second': 112.582, 'eval_steps_per_second': 7.1, 'epoch': 0.08}
{'loss': 1.057, 'grad_norm': 0.23671196401119232, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0713332891464233, 'eval_runtime': 8.9394, 'eval_samples_per_second': 111.752, 'eval_steps_per_second': 7.047, 'epoch': 0.12}
{'loss': 1.018, 'grad_norm': 0.22310109436511993, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.038116216659546, 'eval_runtime': 8.9353, 'eval_samples_per_second': 111.804, 'eval_steps_per_second': 7.051, 'epoch': 0.16}
{'loss': 1.0374, 'grad_norm': 0.23829218745231628, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.018660306930542, 'eval_runtime': 8.966, 'eval_samples_per_second': 111.421, 'eval_steps_per_second': 7.027, 'epoch': 0.2}
{'loss': 0.994, 'grad_norm': 0.2129754275083542, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9999253153800964, 'eval_runtime': 8.9293, 'eval_samples_per_second': 111.879, 'eval_steps_per_second': 7.055, 'epoch': 0.24}
{'loss': 0.9419, 'grad_norm': 0.2591678202152252, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9749343991279602, 'eval_runtime': 8.8993, 'eval_samples_per_second': 112.256, 'eval_steps_per_second': 7.079, 'epoch': 0.28}
{'loss': 0.9797, 'grad_norm': 0.24996991455554962, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9507179260253906, 'eval_runtime': 8.8914, 'eval_samples_per_second': 112.355, 'eval_steps_per_second': 7.085, 'epoch': 0.32}
{'loss': 0.9004, 'grad_norm': 0.27962037920951843, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9219052195549011, 'eval_runtime': 8.8949, 'eval_samples_per_second': 112.312, 'eval_steps_per_second': 7.083, 'epoch': 0.36}
{'loss': 0.8992, 'grad_norm': 0.2584090530872345, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8983995914459229, 'eval_runtime': 8.8953, 'eval_samples_per_second': 112.307, 'eval_steps_per_second': 7.082, 'epoch': 0.4}
{'loss': 0.8858, 'grad_norm': 0.3081818222999573, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8707932233810425, 'eval_runtime': 8.9367, 'eval_samples_per_second': 111.787, 'eval_steps_per_second': 7.05, 'epoch': 0.44}
{'loss': 0.8724, 'grad_norm': 0.3210267722606659, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8481488227844238, 'eval_runtime': 8.9034, 'eval_samples_per_second': 112.204, 'eval_steps_per_second': 7.076, 'epoch': 0.48}
{'loss': 0.7894, 'grad_norm': 0.34033873677253723, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8193154335021973, 'eval_runtime': 8.9224, 'eval_samples_per_second': 111.965, 'eval_steps_per_second': 7.061, 'epoch': 0.52}
{'loss': 0.8388, 'grad_norm': 0.36642083525657654, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7896741032600403, 'eval_runtime': 8.9378, 'eval_samples_per_second': 111.772, 'eval_steps_per_second': 7.049, 'epoch': 0.56}
{'loss': 0.8031, 'grad_norm': 0.4183429181575775, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7643959522247314, 'eval_runtime': 8.8969, 'eval_samples_per_second': 112.286, 'eval_steps_per_second': 7.081, 'epoch': 0.6}
{'loss': 0.7775, 'grad_norm': 0.42438384890556335, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7405074834823608, 'eval_runtime': 8.8897, 'eval_samples_per_second': 112.378, 'eval_steps_per_second': 7.087, 'epoch': 0.64}
{'loss': 0.669, 'grad_norm': 0.49231502413749695, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7103007435798645, 'eval_runtime': 8.9018, 'eval_samples_per_second': 112.225, 'eval_steps_per_second': 7.077, 'epoch': 0.68}
{'loss': 0.6879, 'grad_norm': 0.29597699642181396, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.6851313710212708, 'eval_runtime': 8.8836, 'eval_samples_per_second': 112.454, 'eval_steps_per_second': 7.092, 'epoch': 0.72}
{'loss': 0.6538, 'grad_norm': 0.38874372839927673, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6660052537918091, 'eval_runtime': 8.8781, 'eval_samples_per_second': 112.524, 'eval_steps_per_second': 7.096, 'epoch': 0.76}
{'loss': 0.6619, 'grad_norm': 0.5688433647155762, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6496956944465637, 'eval_runtime': 8.889, 'eval_samples_per_second': 112.386, 'eval_steps_per_second': 7.087, 'epoch': 0.8}
{'loss': 0.6712, 'grad_norm': 0.34040385484695435, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6318598985671997, 'eval_runtime': 8.8951, 'eval_samples_per_second': 112.309, 'eval_steps_per_second': 7.083, 'epoch': 0.84}
{'loss': 0.6212, 'grad_norm': 0.43779292702674866, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6164968609809875, 'eval_runtime': 8.8864, 'eval_samples_per_second': 112.419, 'eval_steps_per_second': 7.089, 'epoch': 0.88}
{'loss': 0.6655, 'grad_norm': 0.3402670621871948, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6067243814468384, 'eval_runtime': 8.8916, 'eval_samples_per_second': 112.353, 'eval_steps_per_second': 7.085, 'epoch': 0.92}
{'loss': 0.5976, 'grad_norm': 0.34767475724220276, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.5995789766311646, 'eval_runtime': 8.8413, 'eval_samples_per_second': 112.993, 'eval_steps_per_second': 7.126, 'epoch': 0.96}
{'loss': 0.6749, 'grad_norm': 0.38377225399017334, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.5972398519515991, 'eval_runtime': 8.8757, 'eval_samples_per_second': 112.555, 'eval_steps_per_second': 7.098, 'epoch': 1.0}
{'train_runtime': 469.504, 'train_samples_per_second': 21.295, 'train_steps_per_second': 1.331, 'train_loss': 0.9150647491455078, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5251020193099976, 1.1310697793960571, 1.0713332891464233, 1.038116216659546, 1.018660306930542, 0.9999253153800964, 0.9749343991279602, 0.9507179260253906, 0.9219052195549011, 0.8983995914459229, 0.8707932233810425, 0.8481488227844238, 0.8193154335021973, 0.7896741032600403, 0.7643959522247314, 0.7405074834823608, 0.7103007435798645, 0.6851313710212708, 0.6660052537918091, 0.6496956944465637, 0.6318598985671997, 0.6164968609809875, 0.6067243814468384, 0.5995789766311646, 0.5972398519515991], 'performance': [0.62, 0.57]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:29,  3.36it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 51.73it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 58.51it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 66.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 72.32it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 69.66it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.57]
current iteration observed (possibly low-fid or predicted) performance:  1.2401230335235596
current iteration best possible performance (full train run):  0.5775000000000001
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.2106 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6335514783859253, 0.5040490031242371, 0.45311635732650757, 0.4010031819343567, 0.004598498344421387, 0.9344545006752014, 0.41451430320739746, 0.21771740913391113, 0.747117817401886, 0.8591722846031189, 0.39759647846221924, 0.6243959665298462, 0.5587756633758545, 0.7929685115814209, 0.49943798780441284, 0.7143707275390625, 0.5815694332122803, 0.8336887359619141, 0.8365764617919922]  ‚Üí  acq = 1.2031500728304145
X = [0.5906044840812683, 0.4299340844154358, 0.5475839972496033, 0.3389297127723694, 0.918976902961731, 0.07804858684539795, 0.7256600856781006, 0.8662558794021606, 0.20233333110809326, 0.29697945713996887, 0.9950025677680969, 0.7881516814231873, 0.9918608069419861, 0.9171490669250488, 0.11589950323104858, 0.9192702174186707, 0.5737680792808533, 0.4738119840621948, 0.8726815581321716]  ‚Üí  acq = 0.9420618450133823
X = [0.8528556823730469, 0.43263792991638184, 0.3814696669578552, 0.6907084584236145, 0.822929322719574, 0.18319422006607056, 0.14396119117736816, 0.9276436567306519, 0.8194877505302429, 0.4211726784706116, 0.6345648169517517, 0.9680798053741455, 0.04524320363998413, 0.39436888694763184, 0.1730237603187561, 0.9231046438217163, 0.9775515198707581, 0.3157180845737457, 0.14850884675979614]  ‚Üí  acq = 0.9420618384878743
X = [0.7461927533149719, 0.3803952932357788, 0.0629580020904541, 0.40444695949554443, 0.929909884929657, 0.2950372099876404, 0.9592135548591614, 0.7788850665092468, 0.19765794277191162, 0.220294788479805, 0.15597397089004517, 0.9458245038986206, 0.5653760433197021, 0.9859554171562195, 0.5912695527076721, 0.58476322889328, 0.029043138027191162, 0.7348498106002808, 0.796540379524231]  ‚Üí  acq = 0.9420618451982918
X = [0.25103920698165894, 0.8755506873130798, 0.7550182938575745, 0.6520828008651733, 0.12126845121383667, 0.15181851387023926, 0.4944515824317932, 0.4904630780220032, 0.6590877175331116, 0.5368852019309998, 0.6762047410011292, 0.853022038936615, 0.7431577444076538, 0.25800275802612305, 0.6953723430633545, 0.9600623846054077, 0.9713211059570312, 0.23546575009822845, 0.23741686344146729]  ‚Üí  acq = 0.933593882928136
proposed candidate layer mask is:  tensor([1., 1., 0., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, 0, 0, 0, 0, 0, tensor(0.2842, dtype=torch.float64), tensor(0.7158, dtype=torch.float64), 32, 1, 1, 0, 0, 0, 128, 0.05492337861875808, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.5613e-16, dtype=torch.float64), tensor(1.6805e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2842, dtype=torch.float64), tensor(0.7158, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5492, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.284
  arc_challenge: 0.716

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.05492337861875808,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 0, 0]
lora rank:  128
lora dropout:  0.05492337861875808
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 54,525,952 || all params: 8,084,787,200 || trainable%: 0.6744
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:14,  6.63it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 40.76it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 51.63it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 54.89it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 58.15it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:00, 63.28it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 64.33it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 67.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 71.68it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 70.61it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 69.68it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 65.15it/s]
Evaluation performance at step 25: 0.61
{'loss': 2.9741, 'grad_norm': 0.6723637580871582, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.61}
{'eval_loss': 1.6457387208938599, 'eval_runtime': 7.527, 'eval_samples_per_second': 132.723, 'eval_steps_per_second': 8.37, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:04<06:57,  4.22s/it]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:04<00:15,  5.28it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:08,  8.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:04<00:05, 12.42it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:04<00:03, 17.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:04<00:02, 23.10it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:05<00:01, 28.72it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:05<00:01, 34.40it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:05<00:00, 46.79it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:05<00:00, 50.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 18.01it/s]
Evaluation performance at step 50: 0.59
{'loss': 1.3066, 'grad_norm': 0.2618768811225891, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.59}
{'eval_loss': 1.1864807605743408, 'eval_runtime': 7.503, 'eval_samples_per_second': 133.147, 'eval_steps_per_second': 8.397, 'epoch': 0.08}
{'loss': 1.1402, 'grad_norm': 0.3133115768432617, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0871903896331787, 'eval_runtime': 7.5842, 'eval_samples_per_second': 131.722, 'eval_steps_per_second': 8.307, 'epoch': 0.12}
{'loss': 1.0581, 'grad_norm': 0.27329254150390625, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0224086046218872, 'eval_runtime': 7.5727, 'eval_samples_per_second': 131.922, 'eval_steps_per_second': 8.319, 'epoch': 0.16}
{'loss': 0.9902, 'grad_norm': 0.32387179136276245, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9702812433242798, 'eval_runtime': 7.6056, 'eval_samples_per_second': 131.351, 'eval_steps_per_second': 8.283, 'epoch': 0.2}
{'loss': 0.92, 'grad_norm': 0.2804996967315674, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9338805079460144, 'eval_runtime': 7.6271, 'eval_samples_per_second': 130.981, 'eval_steps_per_second': 8.26, 'epoch': 0.24}
{'loss': 0.8921, 'grad_norm': 0.2717398405075073, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9140068888664246, 'eval_runtime': 7.6497, 'eval_samples_per_second': 130.594, 'eval_steps_per_second': 8.236, 'epoch': 0.28}
{'loss': 0.899, 'grad_norm': 0.2872076630592346, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8968456983566284, 'eval_runtime': 7.6562, 'eval_samples_per_second': 130.482, 'eval_steps_per_second': 8.229, 'epoch': 0.32}
{'loss': 0.892, 'grad_norm': 0.3077814280986786, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8820945620536804, 'eval_runtime': 7.6363, 'eval_samples_per_second': 130.822, 'eval_steps_per_second': 8.25, 'epoch': 0.36}
{'loss': 0.9366, 'grad_norm': 0.3578783869743347, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8671917915344238, 'eval_runtime': 7.6482, 'eval_samples_per_second': 130.619, 'eval_steps_per_second': 8.237, 'epoch': 0.4}
{'loss': 0.8939, 'grad_norm': 0.3152627944946289, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8465991616249084, 'eval_runtime': 7.6542, 'eval_samples_per_second': 130.517, 'eval_steps_per_second': 8.231, 'epoch': 0.44}
{'loss': 0.8406, 'grad_norm': 0.322113037109375, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.829772412776947, 'eval_runtime': 7.6411, 'eval_samples_per_second': 130.741, 'eval_steps_per_second': 8.245, 'epoch': 0.48}
{'loss': 0.8516, 'grad_norm': 0.3043459355831146, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8127804398536682, 'eval_runtime': 7.6974, 'eval_samples_per_second': 129.783, 'eval_steps_per_second': 8.185, 'epoch': 0.52}
{'loss': 0.8469, 'grad_norm': 0.36515259742736816, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.7922574877738953, 'eval_runtime': 7.6293, 'eval_samples_per_second': 130.943, 'eval_steps_per_second': 8.258, 'epoch': 0.56}
{'loss': 0.8194, 'grad_norm': 0.39667025208473206, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.7698094248771667, 'eval_runtime': 7.6503, 'eval_samples_per_second': 130.584, 'eval_steps_per_second': 8.235, 'epoch': 0.6}
{'loss': 0.8154, 'grad_norm': 0.4549412131309509, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.7499876618385315, 'eval_runtime': 7.6162, 'eval_samples_per_second': 131.168, 'eval_steps_per_second': 8.272, 'epoch': 0.64}
{'loss': 0.7872, 'grad_norm': 0.3841592073440552, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.7297365069389343, 'eval_runtime': 7.6083, 'eval_samples_per_second': 131.303, 'eval_steps_per_second': 8.28, 'epoch': 0.68}
{'loss': 0.8072, 'grad_norm': 0.41033703088760376, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.7126254439353943, 'eval_runtime': 7.6185, 'eval_samples_per_second': 131.129, 'eval_steps_per_second': 8.269, 'epoch': 0.72}
{'loss': 0.8069, 'grad_norm': 0.3900285065174103, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.6937033534049988, 'eval_runtime': 7.6325, 'eval_samples_per_second': 130.888, 'eval_steps_per_second': 8.254, 'epoch': 0.76}
{'loss': 0.7335, 'grad_norm': 0.4573357403278351, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.6772242784500122, 'eval_runtime': 7.5961, 'eval_samples_per_second': 131.515, 'eval_steps_per_second': 8.294, 'epoch': 0.8}
{'loss': 0.7345, 'grad_norm': 0.5535587668418884, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.6629552841186523, 'eval_runtime': 7.5828, 'eval_samples_per_second': 131.745, 'eval_steps_per_second': 8.308, 'epoch': 0.84}
{'loss': 0.7185, 'grad_norm': 0.5118581652641296, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.6481665968894958, 'eval_runtime': 7.5953, 'eval_samples_per_second': 131.528, 'eval_steps_per_second': 8.295, 'epoch': 0.88}
{'loss': 0.6736, 'grad_norm': 0.5157656669616699, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.6341777443885803, 'eval_runtime': 7.6331, 'eval_samples_per_second': 130.877, 'eval_steps_per_second': 8.254, 'epoch': 0.92}
{'loss': 0.7402, 'grad_norm': 0.6175400018692017, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.6261856555938721, 'eval_runtime': 7.6074, 'eval_samples_per_second': 131.319, 'eval_steps_per_second': 8.281, 'epoch': 0.96}
{'loss': 0.644, 'grad_norm': 0.6816443800926208, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.6232112050056458, 'eval_runtime': 7.5719, 'eval_samples_per_second': 131.934, 'eval_steps_per_second': 8.32, 'epoch': 1.0}
{'train_runtime': 448.0719, 'train_samples_per_second': 22.316, 'train_steps_per_second': 1.395, 'train_loss': 0.9488846008300781, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6457387208938599, 1.1864807605743408, 1.0871903896331787, 1.0224086046218872, 0.9702812433242798, 0.9338805079460144, 0.9140068888664246, 0.8968456983566284, 0.8820945620536804, 0.8671917915344238, 0.8465991616249084, 0.829772412776947, 0.8127804398536682, 0.7922574877738953, 0.7698094248771667, 0.7499876618385315, 0.7297365069389343, 0.7126254439353943, 0.6937033534049988, 0.6772242784500122, 0.6629552841186523, 0.6481665968894958, 0.6341777443885803, 0.6261856555938721, 0.6232112050056458], 'performance': [0.61, 0.59]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:25,  3.92it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.40it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 56.87it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 66.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 72.47it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 77.45it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 76.59it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.61, 0.59]
current iteration observed (possibly low-fid or predicted) performance:  1.2409820556640625
current iteration best possible performance (full train run):  0.6194999999999999
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7165 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.627888560295105, 0.347268283367157, 0.927651584148407, 0.12566155195236206, 0.6720914840698242, 0.24367386102676392, 0.7612348198890686, 0.0475926399230957, 0.6783119440078735, 0.15034209191799164, 0.9762507081031799, 0.5757505893707275, 0.4898245930671692, 0.108298659324646, 0.6219758987426758, 0.3066074252128601, 0.0372738242149353, 0.7824876308441162, 0.608344316482544]  ‚Üí  acq = 0.9439684491426092
X = [0.07865172624588013, 0.018745005130767822, 0.523985743522644, 0.674863338470459, 0.3089762330055237, 0.5539385080337524, 0.7370051145553589, 0.09216815233230591, 0.5046954154968262, 0.515200674533844, 0.5700511932373047, 0.7741730809211731, 0.8030701875686646, 0.6184405088424683, 0.9823189377784729, 0.9093483686447144, 0.25169283151626587, 0.10856461524963379, 0.9472829103469849]  ‚Üí  acq = 0.8205743437987572
X = [0.20579522848129272, 0.2745462656021118, 0.0467565655708313, 0.12268298864364624, 0.8995864987373352, 0.4294746518135071, 0.8099130988121033, 0.32034963369369507, 0.3956955671310425, 0.33181309700012207, 0.5975555181503296, 0.5622448325157166, 0.9312053918838501, 0.12464821338653564, 0.5944868326187134, 0.768297016620636, 0.8230517506599426, 0.8879033327102661, 0.5267534255981445]  ‚Üí  acq = 0.9439777406327557
X = [0.07899421453475952, 0.08295267820358276, 0.5324946641921997, 0.07091569900512695, 0.059478580951690674, 0.3839907646179199, 0.9971518516540527, 0.46307051181793213, 0.4799742102622986, 0.7556673288345337, 0.7385811805725098, 0.3194332718849182, 0.3628268837928772, 0.21030139923095703, 0.17926949262619019, 0.13405512273311615, 0.6794533133506775, 0.8636027574539185, 0.0024158358573913574]  ‚Üí  acq = 0.8442168165405478
X = [0.9268249273300171, 0.5992677807807922, 0.27979516983032227, 0.4184553623199463, 0.5482016205787659, 0.2852034568786621, 0.8431757092475891, 0.7084111571311951, 0.7899965047836304, 0.8779590725898743, 0.4447396993637085, 0.964455783367157, 0.5548134446144104, 0.9601840376853943, 0.6430163979530334, 0.027639517560601234, 0.82584148645401, 0.9994162321090698, 0.620703935623169]  ‚Üí  acq = 0.9437556637147761
proposed candidate layer mask is:  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
proposed candidate has all zero for layer mask, adjusting to have at least one layer to apply LoRA
proposed parameters for next round by BO: [tensor(0.1021, dtype=torch.float64), tensor(0.1748, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.7230, dtype=torch.float64), 32, 0, 0, 0, 0, 1, 128, 0.1, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1021, dtype=torch.float64), tensor(0.1748, dtype=torch.float64), tensor(3.8281e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(9.9659e-17, dtype=torch.float64), tensor(2.3486e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.7840e-17, dtype=torch.float64), tensor(0.7230, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.102
  gsm8k: 0.175
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.723

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 0, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 0, 1]
lora rank:  128
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.83it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 28.19it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.28it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 45.11it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 49.17it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.85it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.15it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.61it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.80it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.14it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 59.38it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 56.57it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.80it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.7113, 'grad_norm': 0.4140836298465729, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.4541569948196411, 'eval_runtime': 8.0758, 'eval_samples_per_second': 123.704, 'eval_steps_per_second': 7.801, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:16,  5.97it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.20it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.35it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.67it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 55.30it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.98it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.68it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 55.27it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.73it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.30it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 56.73it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 56.83it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.07it/s]
Evaluation performance at step 50: 0.61
{'loss': 1.2057, 'grad_norm': 0.1822042018175125, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.61}
{'eval_loss': 1.0556647777557373, 'eval_runtime': 8.0701, 'eval_samples_per_second': 123.791, 'eval_steps_per_second': 7.807, 'epoch': 0.08}
{'loss': 1.0191, 'grad_norm': 0.16079413890838623, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9566046595573425, 'eval_runtime': 8.1518, 'eval_samples_per_second': 122.549, 'eval_steps_per_second': 7.728, 'epoch': 0.12}
{'loss': 0.909, 'grad_norm': 0.1744469702243805, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8456294536590576, 'eval_runtime': 8.1707, 'eval_samples_per_second': 122.267, 'eval_steps_per_second': 7.711, 'epoch': 0.16}
{'loss': 0.814, 'grad_norm': 0.19283351302146912, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7797010540962219, 'eval_runtime': 8.2085, 'eval_samples_per_second': 121.704, 'eval_steps_per_second': 7.675, 'epoch': 0.2}
{'loss': 0.7969, 'grad_norm': 0.2237374484539032, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7421766519546509, 'eval_runtime': 8.1828, 'eval_samples_per_second': 122.086, 'eval_steps_per_second': 7.699, 'epoch': 0.24}
{'loss': 0.7386, 'grad_norm': 0.2363305687904358, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.6907309889793396, 'eval_runtime': 8.1819, 'eval_samples_per_second': 122.099, 'eval_steps_per_second': 7.7, 'epoch': 0.28}
{'loss': 0.7153, 'grad_norm': 0.2845245599746704, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6555398106575012, 'eval_runtime': 8.1717, 'eval_samples_per_second': 122.252, 'eval_steps_per_second': 7.71, 'epoch': 0.32}
{'loss': 0.6713, 'grad_norm': 0.32894933223724365, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.615644633769989, 'eval_runtime': 8.1585, 'eval_samples_per_second': 122.449, 'eval_steps_per_second': 7.722, 'epoch': 0.36}
{'loss': 0.6436, 'grad_norm': 0.3027571737766266, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.5881334543228149, 'eval_runtime': 8.1289, 'eval_samples_per_second': 122.896, 'eval_steps_per_second': 7.75, 'epoch': 0.4}
{'loss': 0.6088, 'grad_norm': 0.3107583820819855, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5523062348365784, 'eval_runtime': 8.1367, 'eval_samples_per_second': 122.777, 'eval_steps_per_second': 7.743, 'epoch': 0.44}
{'loss': 0.574, 'grad_norm': 0.28881046175956726, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5283711552619934, 'eval_runtime': 8.1287, 'eval_samples_per_second': 122.899, 'eval_steps_per_second': 7.75, 'epoch': 0.48}
{'loss': 0.6064, 'grad_norm': 0.2933543026447296, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5051620006561279, 'eval_runtime': 8.1211, 'eval_samples_per_second': 123.012, 'eval_steps_per_second': 7.758, 'epoch': 0.52}
{'loss': 0.5635, 'grad_norm': 0.2612004578113556, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.47384539246559143, 'eval_runtime': 8.1178, 'eval_samples_per_second': 123.063, 'eval_steps_per_second': 7.761, 'epoch': 0.56}
{'loss': 0.5148, 'grad_norm': 0.37307387590408325, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.4516027867794037, 'eval_runtime': 8.0751, 'eval_samples_per_second': 123.714, 'eval_steps_per_second': 7.802, 'epoch': 0.6}
{'loss': 0.521, 'grad_norm': 0.3479185104370117, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4369499981403351, 'eval_runtime': 8.0859, 'eval_samples_per_second': 123.548, 'eval_steps_per_second': 7.791, 'epoch': 0.64}
{'loss': 0.4845, 'grad_norm': 0.3739483058452606, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.4187704622745514, 'eval_runtime': 8.1147, 'eval_samples_per_second': 123.11, 'eval_steps_per_second': 7.764, 'epoch': 0.68}
{'loss': 0.5032, 'grad_norm': 0.38926324248313904, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.40444254875183105, 'eval_runtime': 8.1095, 'eval_samples_per_second': 123.188, 'eval_steps_per_second': 7.769, 'epoch': 0.72}
{'loss': 0.4887, 'grad_norm': 0.31847018003463745, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.386313796043396, 'eval_runtime': 8.1284, 'eval_samples_per_second': 122.902, 'eval_steps_per_second': 7.751, 'epoch': 0.76}
{'loss': 0.4462, 'grad_norm': 0.3797675371170044, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.37184134125709534, 'eval_runtime': 8.1381, 'eval_samples_per_second': 122.756, 'eval_steps_per_second': 7.741, 'epoch': 0.8}
{'loss': 0.4465, 'grad_norm': 0.22792093455791473, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.3617517948150635, 'eval_runtime': 8.1528, 'eval_samples_per_second': 122.535, 'eval_steps_per_second': 7.727, 'epoch': 0.84}
{'loss': 0.4146, 'grad_norm': 0.2600441575050354, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.35243234038352966, 'eval_runtime': 8.112, 'eval_samples_per_second': 123.151, 'eval_steps_per_second': 7.766, 'epoch': 0.88}
{'loss': 0.3967, 'grad_norm': 0.2570052742958069, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3474477231502533, 'eval_runtime': 8.1169, 'eval_samples_per_second': 123.077, 'eval_steps_per_second': 7.762, 'epoch': 0.92}
{'loss': 0.4107, 'grad_norm': 0.39954832196235657, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.3431902527809143, 'eval_runtime': 8.1112, 'eval_samples_per_second': 123.163, 'eval_steps_per_second': 7.767, 'epoch': 0.96}
{'loss': 0.3704, 'grad_norm': 0.31504571437835693, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3415431082248688, 'eval_runtime': 8.1214, 'eval_samples_per_second': 123.009, 'eval_steps_per_second': 7.757, 'epoch': 1.0}
{'train_runtime': 452.2191, 'train_samples_per_second': 22.111, 'train_steps_per_second': 1.382, 'train_loss': 0.7029869506835937, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4541569948196411, 1.0556647777557373, 0.9566046595573425, 0.8456294536590576, 0.7797010540962219, 0.7421766519546509, 0.6907309889793396, 0.6555398106575012, 0.615644633769989, 0.5881334543228149, 0.5523062348365784, 0.5283711552619934, 0.5051620006561279, 0.47384539246559143, 0.4516027867794037, 0.4369499981403351, 0.4187704622745514, 0.40444254875183105, 0.386313796043396, 0.37184134125709534, 0.3617517948150635, 0.35243234038352966, 0.3474477231502533, 0.3431902527809143, 0.3415431082248688], 'performance': [0.62, 0.61]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:29,  3.38it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 54.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:00<00:00, 58.46it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 66.30it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 72.69it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 70.44it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.61]
current iteration observed (possibly low-fid or predicted) performance:  1.2382837533950806
current iteration best possible performance (full train run):  0.651
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625, 1.2382837533950806]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8662 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4638015031814575, 0.5185000896453857, 0.8540962934494019, 0.50815749168396, 0.5551637411117554, 0.2149859070777893, 0.895283043384552, 0.3763464689254761, 0.16448825597763062, 0.7758210897445679, 0.9927905797958374, 0.6594863533973694, 0.35494714975357056, 0.07612484693527222, 0.9889960885047913, 0.3001246750354767, 0.5164159536361694, 0.6735315322875977, 0.9447562098503113]  ‚Üí  acq = 0.9438258830871822
X = [0.2754029631614685, 0.1917269229888916, 0.24771517515182495, 0.722519040107727, 0.5463160872459412, 0.26427507400512695, 0.4645794630050659, 0.14119797945022583, 0.8739979267120361, 0.7403943538665771, 0.829667866230011, 0.425983726978302, 0.08680939674377441, 0.7470961213111877, 0.355268657207489, 0.8218333721160889, 0.5673786401748657, 0.0677361786365509, 0.7489399313926697]  ‚Üí  acq = 0.9442179106133337
X = [0.9665655493736267, 0.22132408618927002, 0.03413969278335571, 0.9118659496307373, 0.9766972661018372, 0.8346678018569946, 0.3321903347969055, 0.8796674609184265, 0.9287291765213013, 0.5691953897476196, 0.49987637996673584, 0.21345609426498413, 0.2108299732208252, 0.5821679830551147, 0.06552600860595703, 0.6092031002044678, 0.11019653081893921, 0.8161331415176392, 0.17554330825805664]  ‚Üí  acq = 0.9444875613028835
X = [0.5539194345474243, 0.23614782094955444, 0.907264232635498, 0.1329517364501953, 0.8770277500152588, 0.32083964347839355, 0.002879023551940918, 0.8397672176361084, 0.45747995376586914, 0.9867486357688904, 0.09055590629577637, 0.14347541332244873, 0.07243746519088745, 0.6337834000587463, 0.9546571373939514, 0.3971007168292999, 0.8808532357215881, 0.9589905738830566, 0.10161328315734863]  ‚Üí  acq = 0.9444875585911615
X = [0.3319757580757141, 0.5938259959220886, 0.561281144618988, 0.4572746157646179, 0.6568410992622375, 0.3821471929550171, 0.8067867159843445, 0.042390286922454834, 0.3963356614112854, 0.5177018046379089, 0.6910930871963501, 0.3688967823982239, 0.29392629861831665, 0.32913219928741455, 0.6149353981018066, 0.4876957833766937, 0.9828105568885803, 0.6961022615432739, 0.6103644371032715]  ‚Üí  acq = 0.9444534867787953
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0875, dtype=torch.float64), 0, 0, tensor(0.2145, dtype=torch.float64), 0, 0, 0, 0, tensor(0.6979, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.040253934460248074, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0875, dtype=torch.float64), tensor(9.6947e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2145, dtype=torch.float64), tensor(8.6756e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6979, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4025, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.088
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.215
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.698

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.040253934460248074,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.040253934460248074
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.83it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.29it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.19it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 41.50it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.46it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.89it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 52.78it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.14it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.25it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.99it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 49.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 49.26it/s]
Evaluation performance at step 25: 0.64
{'loss': 2.3972, 'grad_norm': 0.5768190026283264, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.64}
{'eval_loss': 1.0747456550598145, 'eval_runtime': 7.5372, 'eval_samples_per_second': 132.542, 'eval_steps_per_second': 8.359, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.76it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.98it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.26it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.84it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 47.31it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 49.36it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 53.11it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.26it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 53.36it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.09it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 51.10it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.13it/s]
Evaluation performance at step 50: 0.64
{'loss': 0.8983, 'grad_norm': 0.48616793751716614, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.64}
{'eval_loss': 0.840303897857666, 'eval_runtime': 7.5327, 'eval_samples_per_second': 132.622, 'eval_steps_per_second': 8.364, 'epoch': 0.08}
{'loss': 0.8157, 'grad_norm': 0.27794355154037476, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7648104429244995, 'eval_runtime': 7.5543, 'eval_samples_per_second': 132.242, 'eval_steps_per_second': 8.34, 'epoch': 0.12}
{'loss': 0.7829, 'grad_norm': 0.30146947503089905, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.6954976320266724, 'eval_runtime': 7.5384, 'eval_samples_per_second': 132.521, 'eval_steps_per_second': 8.357, 'epoch': 0.16}
{'loss': 0.6597, 'grad_norm': 0.3355698883533478, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6432415246963501, 'eval_runtime': 7.5647, 'eval_samples_per_second': 132.061, 'eval_steps_per_second': 8.328, 'epoch': 0.2}
{'loss': 0.6333, 'grad_norm': 0.43860122561454773, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5826250910758972, 'eval_runtime': 7.5983, 'eval_samples_per_second': 131.476, 'eval_steps_per_second': 8.291, 'epoch': 0.24}
{'loss': 0.5626, 'grad_norm': 0.3417624235153198, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5288456082344055, 'eval_runtime': 7.586, 'eval_samples_per_second': 131.69, 'eval_steps_per_second': 8.305, 'epoch': 0.28}
{'loss': 0.5303, 'grad_norm': 0.3467523455619812, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.48243850469589233, 'eval_runtime': 7.5921, 'eval_samples_per_second': 131.584, 'eval_steps_per_second': 8.298, 'epoch': 0.32}
{'loss': 0.4867, 'grad_norm': 0.36351534724235535, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.43013373017311096, 'eval_runtime': 7.5695, 'eval_samples_per_second': 131.976, 'eval_steps_per_second': 8.323, 'epoch': 0.36}
{'loss': 0.454, 'grad_norm': 0.3265545964241028, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.38908836245536804, 'eval_runtime': 7.5641, 'eval_samples_per_second': 132.071, 'eval_steps_per_second': 8.329, 'epoch': 0.4}
{'loss': 0.4328, 'grad_norm': 0.40887320041656494, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.3624040484428406, 'eval_runtime': 7.5767, 'eval_samples_per_second': 131.852, 'eval_steps_per_second': 8.315, 'epoch': 0.44}
{'loss': 0.397, 'grad_norm': 0.3291262686252594, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.34043216705322266, 'eval_runtime': 7.5837, 'eval_samples_per_second': 131.73, 'eval_steps_per_second': 8.307, 'epoch': 0.48}
{'loss': 0.3853, 'grad_norm': 0.3192426264286041, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.31804147362709045, 'eval_runtime': 7.5892, 'eval_samples_per_second': 131.635, 'eval_steps_per_second': 8.301, 'epoch': 0.52}
{'loss': 0.3498, 'grad_norm': 0.345065712928772, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.2920374870300293, 'eval_runtime': 7.5934, 'eval_samples_per_second': 131.562, 'eval_steps_per_second': 8.297, 'epoch': 0.56}
{'loss': 0.327, 'grad_norm': 0.3980494737625122, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2745765745639801, 'eval_runtime': 7.583, 'eval_samples_per_second': 131.742, 'eval_steps_per_second': 8.308, 'epoch': 0.6}
{'loss': 0.3428, 'grad_norm': 0.23681995272636414, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.26613497734069824, 'eval_runtime': 7.5791, 'eval_samples_per_second': 131.81, 'eval_steps_per_second': 8.312, 'epoch': 0.64}
{'loss': 0.325, 'grad_norm': 0.3525722622871399, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.2515007555484772, 'eval_runtime': 7.5909, 'eval_samples_per_second': 131.604, 'eval_steps_per_second': 8.299, 'epoch': 0.68}
{'loss': 0.3138, 'grad_norm': 0.23334597051143646, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.23805460333824158, 'eval_runtime': 7.5886, 'eval_samples_per_second': 131.645, 'eval_steps_per_second': 8.302, 'epoch': 0.72}
{'loss': 0.2919, 'grad_norm': 0.33956727385520935, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.23086771368980408, 'eval_runtime': 7.5883, 'eval_samples_per_second': 131.65, 'eval_steps_per_second': 8.302, 'epoch': 0.76}
{'loss': 0.2752, 'grad_norm': 0.3047577142715454, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.2193225473165512, 'eval_runtime': 7.5856, 'eval_samples_per_second': 131.696, 'eval_steps_per_second': 8.305, 'epoch': 0.8}
{'loss': 0.3027, 'grad_norm': 0.1682325154542923, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.20997847616672516, 'eval_runtime': 7.5791, 'eval_samples_per_second': 131.811, 'eval_steps_per_second': 8.312, 'epoch': 0.84}
{'loss': 0.2781, 'grad_norm': 0.16655227541923523, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.20424337685108185, 'eval_runtime': 7.5773, 'eval_samples_per_second': 131.841, 'eval_steps_per_second': 8.314, 'epoch': 0.88}
{'loss': 0.2693, 'grad_norm': 0.12358547747135162, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.20066991448402405, 'eval_runtime': 7.5962, 'eval_samples_per_second': 131.513, 'eval_steps_per_second': 8.294, 'epoch': 0.92}
{'loss': 0.2598, 'grad_norm': 0.2062293142080307, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.1979215145111084, 'eval_runtime': 7.5707, 'eval_samples_per_second': 131.955, 'eval_steps_per_second': 8.322, 'epoch': 0.96}
{'loss': 0.2298, 'grad_norm': 0.12390296906232834, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.1971055567264557, 'eval_runtime': 7.5719, 'eval_samples_per_second': 131.936, 'eval_steps_per_second': 8.32, 'epoch': 1.0}
{'train_runtime': 438.1873, 'train_samples_per_second': 22.819, 'train_steps_per_second': 1.426, 'train_loss': 0.5200346771240234, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0747456550598145, 0.840303897857666, 0.7648104429244995, 0.6954976320266724, 0.6432415246963501, 0.5826250910758972, 0.5288456082344055, 0.48243850469589233, 0.43013373017311096, 0.38908836245536804, 0.3624040484428406, 0.34043216705322266, 0.31804147362709045, 0.2920374870300293, 0.2745765745639801, 0.26613497734069824, 0.2515007555484772, 0.23805460333824158, 0.23086771368980408, 0.2193225473165512, 0.20997847616672516, 0.20424337685108185, 0.20066991448402405, 0.1979215145111084, 0.1971055567264557], 'performance': [0.64, 0.64]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:35,  2.78it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.13it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.04it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.41it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.22it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 70.74it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.53it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.64, 0.64]
current iteration observed (possibly low-fid or predicted) performance:  1.2482209205627441
current iteration best possible performance (full train run):  0.63
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625, 1.2382837533950806, 1.2482209205627441]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 0.8323 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7073273658752441, 0.09663158655166626, 0.27794164419174194, 0.4941803812980652, 0.5840396285057068, 0.18096143007278442, 0.9301639795303345, 0.9306964874267578, 0.928162693977356, 0.9664798974990845, 0.541987955570221, 0.32591795921325684, 0.6547440886497498, 0.02298206090927124, 0.40784597396850586, 0.9110398292541504, 0.4732765555381775, 0.14317336678504944, 0.39339518547058105]  ‚Üí  acq = 0.9444338378728143
X = [0.20874351263046265, 0.805183470249176, 0.6072415113449097, 0.3002634048461914, 0.7828359007835388, 0.9287838339805603, 0.24894452095031738, 0.9706717729568481, 0.18094652891159058, 0.05699325352907181, 0.1791248917579651, 0.557305634021759, 0.7800944447517395, 0.2100543975830078, 0.1506522297859192, 0.8569538593292236, 0.6742131114006042, 0.1775025725364685, 0.4672756791114807]  ‚Üí  acq = 0.9445088771417952
X = [0.993894636631012, 0.18100738525390625, 0.3462757468223572, 0.830348789691925, 0.05861574411392212, 0.28312915563583374, 0.18509984016418457, 0.9236323833465576, 0.5869901776313782, 0.3186635971069336, 0.43648213148117065, 0.9086700677871704, 0.5526363849639893, 0.49218761920928955, 0.9322348237037659, 0.797860324382782, 0.5558359622955322, 0.2876761257648468, 0.1084679365158081]  ‚Üí  acq = 0.9701254455195257
X = [0.2068108320236206, 0.7440274357795715, 0.49366140365600586, 0.49079596996307373, 0.9038687348365784, 0.41010981798171997, 0.23211228847503662, 0.8897611498832703, 0.8686208724975586, 0.5826836228370667, 0.5033730268478394, 0.9515023231506348, 0.7423017024993896, 0.5275121927261353, 0.6228255033493042, 0.7893010377883911, 0.540503203868866, 0.532541036605835, 0.8318067789077759]  ‚Üí  acq = 0.9445094973442365
X = [0.45400530099868774, 0.9334540963172913, 0.7982248067855835, 0.20562613010406494, 0.2570183277130127, 0.8756731748580933, 0.1712471842765808, 0.6570968627929688, 0.45265841484069824, 0.48142698407173157, 0.14977627992630005, 0.3267480731010437, 0.022821128368377686, 0.7105581760406494, 0.7239904999732971, 0.7857414484024048, 0.8414496183395386, 0.9023213386535645, 0.8266160488128662]  ‚Üí  acq = 0.6936723597357446
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0178, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2736, dtype=torch.float64), 0, tensor(0.7086, dtype=torch.float64), 32, 1, 1, 1, 1, 1, 128, 0.03361875360744071, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.0178, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.0714e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.1564e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2736, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7086, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.3362, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.018
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.274
  mmlu: 0
  arc_challenge: 0.709

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.03361875360744071,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  128
lora dropout:  0.03361875360744071
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 281,018,368 || all params: 8,311,279,616 || trainable%: 3.3812
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:20,  4.87it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:03, 29.36it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 37.04it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 40.79it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 43.06it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:01, 46.44it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 46.85it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 51.27it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.33it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 52.68it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 51.77it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 49.13it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:02<00:00, 49.95it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.22it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.3798, 'grad_norm': 0.4162288308143616, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.291970133781433, 'eval_runtime': 9.0111, 'eval_samples_per_second': 110.863, 'eval_steps_per_second': 6.991, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<09:42,  5.88s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:06<00:44,  2.03it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:06<00:18,  4.38it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:06<00:10,  7.31it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:06<00:06, 10.80it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:03, 15.22it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:06<00:02, 19.41it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:12<00:11,  3.63it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:13<00:06,  5.08it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:13<00:03,  7.10it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:13<00:01,  9.55it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:13<00:00, 12.10it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:13<00:00, 15.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13<00:00,  7.20it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.196, 'grad_norm': 0.4006372094154358, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.0909241437911987, 'eval_runtime': 9.0824, 'eval_samples_per_second': 109.994, 'eval_steps_per_second': 6.937, 'epoch': 0.08}
{'loss': 1.0411, 'grad_norm': 0.30886662006378174, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9947285652160645, 'eval_runtime': 9.0444, 'eval_samples_per_second': 110.455, 'eval_steps_per_second': 6.966, 'epoch': 0.12}
{'loss': 1.0134, 'grad_norm': 0.32838040590286255, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9228686094284058, 'eval_runtime': 9.0486, 'eval_samples_per_second': 110.404, 'eval_steps_per_second': 6.962, 'epoch': 0.16}
{'loss': 0.9042, 'grad_norm': 0.33185628056526184, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8582620620727539, 'eval_runtime': 9.0593, 'eval_samples_per_second': 110.273, 'eval_steps_per_second': 6.954, 'epoch': 0.2}
{'loss': 0.8578, 'grad_norm': 0.3433767855167389, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.7882941365242004, 'eval_runtime': 9.0745, 'eval_samples_per_second': 110.088, 'eval_steps_per_second': 6.943, 'epoch': 0.24}
{'loss': 0.7893, 'grad_norm': 0.4964665472507477, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.7357208132743835, 'eval_runtime': 9.0824, 'eval_samples_per_second': 109.993, 'eval_steps_per_second': 6.937, 'epoch': 0.28}
{'loss': 0.8786, 'grad_norm': 0.37899693846702576, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.6957532167434692, 'eval_runtime': 9.084, 'eval_samples_per_second': 109.973, 'eval_steps_per_second': 6.935, 'epoch': 0.32}
{'loss': 0.7201, 'grad_norm': 0.38184845447540283, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.6494681239128113, 'eval_runtime': 9.0878, 'eval_samples_per_second': 109.927, 'eval_steps_per_second': 6.932, 'epoch': 0.36}
{'loss': 0.7006, 'grad_norm': 0.3938402533531189, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.6107624173164368, 'eval_runtime': 9.0846, 'eval_samples_per_second': 109.966, 'eval_steps_per_second': 6.935, 'epoch': 0.4}
{'loss': 0.6622, 'grad_norm': 0.42436110973358154, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.5734062194824219, 'eval_runtime': 9.0767, 'eval_samples_per_second': 110.062, 'eval_steps_per_second': 6.941, 'epoch': 0.44}
{'loss': 0.6702, 'grad_norm': 0.3541167080402374, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.5489257574081421, 'eval_runtime': 9.0618, 'eval_samples_per_second': 110.243, 'eval_steps_per_second': 6.952, 'epoch': 0.48}
{'loss': 0.6229, 'grad_norm': 0.42865100502967834, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.5206427574157715, 'eval_runtime': 9.1075, 'eval_samples_per_second': 109.69, 'eval_steps_per_second': 6.917, 'epoch': 0.52}
{'loss': 0.507, 'grad_norm': 0.2968463599681854, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.4992089867591858, 'eval_runtime': 9.0777, 'eval_samples_per_second': 110.05, 'eval_steps_per_second': 6.94, 'epoch': 0.56}
{'loss': 0.6116, 'grad_norm': 0.2963254451751709, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.48009106516838074, 'eval_runtime': 9.0716, 'eval_samples_per_second': 110.124, 'eval_steps_per_second': 6.945, 'epoch': 0.6}
{'loss': 0.5173, 'grad_norm': 0.3379511535167694, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.4650830626487732, 'eval_runtime': 9.0856, 'eval_samples_per_second': 109.954, 'eval_steps_per_second': 6.934, 'epoch': 0.64}
{'loss': 0.5104, 'grad_norm': 0.25208139419555664, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.45064282417297363, 'eval_runtime': 9.0883, 'eval_samples_per_second': 109.922, 'eval_steps_per_second': 6.932, 'epoch': 0.68}
{'loss': 0.4549, 'grad_norm': 0.38967519998550415, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.44049564003944397, 'eval_runtime': 9.082, 'eval_samples_per_second': 109.998, 'eval_steps_per_second': 6.937, 'epoch': 0.72}
{'loss': 0.456, 'grad_norm': 0.2613820433616638, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.42934730648994446, 'eval_runtime': 9.0517, 'eval_samples_per_second': 110.366, 'eval_steps_per_second': 6.96, 'epoch': 0.76}
{'loss': 0.5251, 'grad_norm': 0.3790283501148224, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.41869837045669556, 'eval_runtime': 9.1284, 'eval_samples_per_second': 109.438, 'eval_steps_per_second': 6.902, 'epoch': 0.8}
{'loss': 0.4706, 'grad_norm': 0.30187204480171204, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.41070011258125305, 'eval_runtime': 9.0906, 'eval_samples_per_second': 109.893, 'eval_steps_per_second': 6.93, 'epoch': 0.84}
{'loss': 0.4689, 'grad_norm': 0.22747018933296204, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.4023772180080414, 'eval_runtime': 9.1253, 'eval_samples_per_second': 109.476, 'eval_steps_per_second': 6.904, 'epoch': 0.88}
{'loss': 0.3991, 'grad_norm': 0.42183825373649597, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.3978898823261261, 'eval_runtime': 9.1388, 'eval_samples_per_second': 109.314, 'eval_steps_per_second': 6.894, 'epoch': 0.92}
{'loss': 0.445, 'grad_norm': 0.30329322814941406, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.39498311281204224, 'eval_runtime': 9.1142, 'eval_samples_per_second': 109.61, 'eval_steps_per_second': 6.912, 'epoch': 0.96}
{'loss': 0.4928, 'grad_norm': 0.2378649115562439, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.3939157724380493, 'eval_runtime': 9.0962, 'eval_samples_per_second': 109.826, 'eval_steps_per_second': 6.926, 'epoch': 1.0}
{'train_runtime': 517.0511, 'train_samples_per_second': 19.337, 'train_steps_per_second': 1.209, 'train_loss': 0.7317924942016601, 'epoch': 1.0}
train_results:  {'eval_loss': [1.291970133781433, 1.0909241437911987, 0.9947285652160645, 0.9228686094284058, 0.8582620620727539, 0.7882941365242004, 0.7357208132743835, 0.6957532167434692, 0.6494681239128113, 0.6107624173164368, 0.5734062194824219, 0.5489257574081421, 0.5206427574157715, 0.4992089867591858, 0.48009106516838074, 0.4650830626487732, 0.45064282417297363, 0.44049564003944397, 0.42934730648994446, 0.41869837045669556, 0.41070011258125305, 0.4023772180080414, 0.3978898823261261, 0.39498311281204224, 0.3939157724380493], 'performance': [0.63, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:37,  2.63it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 29.85it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.75it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 42.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 48.13it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.42it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 67.25it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.29it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.2482129335403442
current iteration best possible performance (full train run):  0.5880000000000001
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625, 1.2382837533950806, 1.2482209205627441, 1.2482129335403442]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1637 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2453942894935608, 0.5521987676620483, 0.9376865029335022, 0.6488873362541199, 0.5812278389930725, 0.8803036212921143, 0.4912112355232239, 0.3247528076171875, 0.9830014705657959, 0.5190694332122803, 0.8385055065155029, 0.34967100620269775, 0.7992465496063232, 0.6010072231292725, 0.21358972787857056, 0.9640829563140869, 0.43916982412338257, 0.717397928237915, 0.651369035243988]  ‚Üí  acq = 0.9262719729779721
X = [0.7622659206390381, 0.48969167470932007, 0.8040255904197693, 0.8540394306182861, 0.9792864322662354, 0.07990974187850952, 0.9462190866470337, 0.8024178743362427, 0.2915762662887573, 0.5766368508338928, 0.18841487169265747, 0.17352712154388428, 0.001062154769897461, 0.5064542889595032, 0.5487730503082275, 0.9796900749206543, 0.8438193202018738, 0.9829916954040527, 0.021804988384246826]  ‚Üí  acq = 0.9262993247792088
X = [0.329218327999115, 0.12537002563476562, 0.3958740234375, 0.5395618677139282, 0.37031126022338867, 0.1262500286102295, 0.2866043448448181, 0.34224021434783936, 0.29064154624938965, 0.293832927942276, 0.2867220640182495, 0.611409604549408, 0.5041229724884033, 0.20719236135482788, 0.7192603945732117, 0.4341471493244171, 0.7081349492073059, 0.5918272733688354, 0.4393441081047058]  ‚Üí  acq = 0.866055107665788
X = [0.12385231256484985, 0.30730265378952026, 0.9585211277008057, 0.4002786874771118, 0.5763075947761536, 0.7537026405334473, 0.15638738870620728, 0.0047035813331604, 0.8853250741958618, 0.36894020438194275, 0.7118407487869263, 0.6046555042266846, 0.7315640449523926, 0.22383207082748413, 0.0383492112159729, 0.964883029460907, 0.9546171426773071, 0.7669861316680908, 0.9712991118431091]  ‚Üí  acq = 0.9262394685495969
X = [0.9304882287979126, 0.6102762222290039, 0.6988106369972229, 0.51226806640625, 0.08356159925460815, 0.9000679850578308, 0.9574618339538574, 0.9216540455818176, 0.6973706483840942, 0.7503089904785156, 0.4817289113998413, 0.5024594068527222, 0.33512169122695923, 0.10719448328018188, 0.5954238772392273, 0.20982912182807922, 0.4613257646560669, 0.7915639877319336, 0.12225282192230225]  ‚Üí  acq = 0.9584921489179473
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [0, 0, tensor(0.1834, dtype=torch.float64), 0, 0, 0, tensor(0.0250, dtype=torch.float64), tensor(0.0857, dtype=torch.float64), tensor(0.7045, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.0471216238647985, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(3.2761e-16, dtype=torch.float64), tensor(0.0014, dtype=torch.float64), tensor(0.1834, dtype=torch.float64), tensor(1.6896e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0250, dtype=torch.float64), tensor(0.0857, dtype=torch.float64), tensor(0.7045, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0.4712, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0
  rowan_hellaswag: 0.183
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.025
  mmlu: 0.086
  arc_challenge: 0.704

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0471216238647985,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.0471216238647985
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9983
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  998
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.18it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.59it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.26it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.95it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.18it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.08it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.18it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.30it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.75it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.50it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.63it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.96it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.6157, 'grad_norm': 0.5787804126739502, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.4205979108810425, 'eval_runtime': 10.6188, 'eval_samples_per_second': 93.984, 'eval_steps_per_second': 5.933, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:21,  4.70it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 31.70it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 39.80it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.77it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 46.19it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 51.80it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.88it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 57.74it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.54it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.79it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.42it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.46it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.48it/s]
Evaluation performance at step 50: 0.56
{'loss': 1.266, 'grad_norm': 0.3850123882293701, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.56}
{'eval_loss': 1.1988725662231445, 'eval_runtime': 10.5629, 'eval_samples_per_second': 94.482, 'eval_steps_per_second': 5.964, 'epoch': 0.08}
{'loss': 1.1641, 'grad_norm': 0.2884903848171234, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.1264311075210571, 'eval_runtime': 10.5754, 'eval_samples_per_second': 94.37, 'eval_steps_per_second': 5.957, 'epoch': 0.12}
{'loss': 1.1344, 'grad_norm': 0.26076751947402954, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.0631108283996582, 'eval_runtime': 10.603, 'eval_samples_per_second': 94.124, 'eval_steps_per_second': 5.942, 'epoch': 0.16}
{'loss': 1.0521, 'grad_norm': 0.265027791261673, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.0117912292480469, 'eval_runtime': 10.6248, 'eval_samples_per_second': 93.932, 'eval_steps_per_second': 5.93, 'epoch': 0.2}
{'loss': 1.0403, 'grad_norm': 0.22443923354148865, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9632295370101929, 'eval_runtime': 10.6536, 'eval_samples_per_second': 93.677, 'eval_steps_per_second': 5.913, 'epoch': 0.24}
{'loss': 0.966, 'grad_norm': 0.25490328669548035, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.9237045645713806, 'eval_runtime': 10.6896, 'eval_samples_per_second': 93.362, 'eval_steps_per_second': 5.894, 'epoch': 0.28}
{'loss': 0.9539, 'grad_norm': 0.23797327280044556, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.8677712678909302, 'eval_runtime': 10.7424, 'eval_samples_per_second': 92.903, 'eval_steps_per_second': 5.865, 'epoch': 0.32}
{'loss': 0.9846, 'grad_norm': 0.3931928873062134, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.8323363661766052, 'eval_runtime': 10.769, 'eval_samples_per_second': 92.673, 'eval_steps_per_second': 5.85, 'epoch': 0.36}
{'loss': 0.8876, 'grad_norm': 0.32389360666275024, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.7943565249443054, 'eval_runtime': 10.7734, 'eval_samples_per_second': 92.636, 'eval_steps_per_second': 5.848, 'epoch': 0.4}
{'loss': 0.8776, 'grad_norm': 0.2971796989440918, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.7635939717292786, 'eval_runtime': 10.7272, 'eval_samples_per_second': 93.034, 'eval_steps_per_second': 5.873, 'epoch': 0.44}
{'loss': 0.8266, 'grad_norm': 0.29946646094322205, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.7305426597595215, 'eval_runtime': 10.7256, 'eval_samples_per_second': 93.049, 'eval_steps_per_second': 5.874, 'epoch': 0.48}
{'loss': 0.8391, 'grad_norm': 0.3238668143749237, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.7119765877723694, 'eval_runtime': 10.7183, 'eval_samples_per_second': 93.112, 'eval_steps_per_second': 5.878, 'epoch': 0.52}
{'loss': 0.8573, 'grad_norm': 0.2817586064338684, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.6908733248710632, 'eval_runtime': 10.7208, 'eval_samples_per_second': 93.09, 'eval_steps_per_second': 5.876, 'epoch': 0.56}
{'loss': 0.7665, 'grad_norm': 0.2971935570240021, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.6713652014732361, 'eval_runtime': 10.7204, 'eval_samples_per_second': 93.094, 'eval_steps_per_second': 5.877, 'epoch': 0.6}
{'loss': 0.6899, 'grad_norm': 0.25766605138778687, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.6528592109680176, 'eval_runtime': 10.7167, 'eval_samples_per_second': 93.126, 'eval_steps_per_second': 5.879, 'epoch': 0.64}
{'loss': 0.6755, 'grad_norm': 0.25928983092308044, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.6446395516395569, 'eval_runtime': 10.675, 'eval_samples_per_second': 93.49, 'eval_steps_per_second': 5.902, 'epoch': 0.68}
{'loss': 0.7293, 'grad_norm': 0.18827791512012482, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.6354168057441711, 'eval_runtime': 10.6626, 'eval_samples_per_second': 93.599, 'eval_steps_per_second': 5.909, 'epoch': 0.72}
{'loss': 0.7324, 'grad_norm': 0.20499618351459503, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.629802405834198, 'eval_runtime': 10.7013, 'eval_samples_per_second': 93.259, 'eval_steps_per_second': 5.887, 'epoch': 0.76}
{'loss': 0.7324, 'grad_norm': 0.24928109347820282, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.6236661672592163, 'eval_runtime': 10.7279, 'eval_samples_per_second': 93.029, 'eval_steps_per_second': 5.873, 'epoch': 0.8}
{'loss': 0.6508, 'grad_norm': 0.14768154919147491, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.6161253452301025, 'eval_runtime': 10.7461, 'eval_samples_per_second': 92.871, 'eval_steps_per_second': 5.863, 'epoch': 0.84}
{'loss': 0.6662, 'grad_norm': 0.15271775424480438, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.6125918626785278, 'eval_runtime': 10.6903, 'eval_samples_per_second': 93.355, 'eval_steps_per_second': 5.893, 'epoch': 0.88}
{'loss': 0.7111, 'grad_norm': 0.18881745636463165, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.6084592938423157, 'eval_runtime': 10.7112, 'eval_samples_per_second': 93.174, 'eval_steps_per_second': 5.882, 'epoch': 0.92}
{'loss': 0.6554, 'grad_norm': 0.21643762290477753, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.6060485243797302, 'eval_runtime': 10.7057, 'eval_samples_per_second': 93.221, 'eval_steps_per_second': 5.885, 'epoch': 0.96}
{'train_runtime': 556.7634, 'train_samples_per_second': 17.93, 'train_steps_per_second': 1.121, 'train_loss': 0.9254028476201571, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4205979108810425, 1.1988725662231445, 1.1264311075210571, 1.0631108283996582, 1.0117912292480469, 0.9632295370101929, 0.9237045645713806, 0.8677712678909302, 0.8323363661766052, 0.7943565249443054, 0.7635939717292786, 0.7305426597595215, 0.7119765877723694, 0.6908733248710632, 0.6713652014732361, 0.6528592109680176, 0.6446395516395569, 0.6354168057441711, 0.629802405834198, 0.6236661672592163, 0.6161253452301025, 0.6125918626785278, 0.6084592938423157, 0.6060485243797302], 'performance': [0.62, 0.56]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:39,  2.49it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 29.01it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.23it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 42.13it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 47.86it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 52.31it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 66.87it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.67it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.56]
current iteration observed (possibly low-fid or predicted) performance:  1.248682975769043
current iteration best possible performance (full train run):  0.6615000000000001
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625, 1.2382837533950806, 1.2482209205627441, 1.2482129335403442, 1.248682975769043]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2162 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9065292477607727, 0.6905171275138855, 0.2286663055419922, 0.5429636240005493, 0.35442054271698, 0.5503457188606262, 0.13326072692871094, 0.04208254814147949, 0.21428024768829346, 0.8380919694900513, 0.1712283492088318, 0.6945513486862183, 0.12751400470733643, 0.7887598872184753, 0.5074208974838257, 0.3228856027126312, 0.27404463291168213, 0.2284892499446869, 0.34479671716690063]  ‚Üí  acq = 0.8563188542803085
X = [0.5428206920623779, 0.542335569858551, 0.9006205201148987, 0.47442537546157837, 0.1363576054573059, 0.42921411991119385, 0.36274826526641846, 0.5200755596160889, 0.7777203321456909, 0.8333624601364136, 0.04449707269668579, 0.07040983438491821, 0.22011882066726685, 0.17211514711380005, 0.11167556047439575, 0.4851071834564209, 0.04607832431793213, 0.12579646706581116, 0.9442782998085022]  ‚Üí  acq = 0.9508742524925176
X = [0.4239559769630432, 0.4484034776687622, 0.6185203790664673, 0.12677007913589478, 0.12111145257949829, 0.7451815009117126, 0.0919198989868164, 0.9734746217727661, 0.27437329292297363, 0.589992880821228, 0.9344208836555481, 0.43477630615234375, 0.8103813529014587, 0.48312318325042725, 0.7626509666442871, 0.3780413568019867, 0.8526402115821838, 0.3300502896308899, 0.19652622938156128]  ‚Üí  acq = 0.6415257494123747
X = [0.2232549786567688, 0.9237229228019714, 0.7666183114051819, 0.2170935869216919, 0.30832600593566895, 0.21746474504470825, 0.10851836204528809, 0.9635545015335083, 0.1953245997428894, 0.7119964957237244, 0.833569347858429, 0.1173446774482727, 0.3110639452934265, 0.7628186345100403, 0.9602335095405579, 0.5299732089042664, 0.8821436166763306, 0.1912723332643509, 0.2651313543319702]  ‚Üí  acq = 0.7765842409443044
X = [0.5404798984527588, 0.9752495288848877, 0.6256819367408752, 0.8594304323196411, 0.6350014209747314, 0.5284474492073059, 0.013608753681182861, 0.1865140199661255, 0.47044074535369873, 0.9830683469772339, 0.9765622615814209, 0.28691399097442627, 0.28654128313064575, 0.9480607509613037, 0.22994285821914673, 0.7863863706588745, 0.4073483943939209, 0.3528243899345398, 0.11635196208953857]  ‚Üí  acq = 0.9355473022249805
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0904, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2144, dtype=torch.float64), 0, 0, tensor(0.6953, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.030469006777099267, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0.0904, dtype=torch.float64), tensor(7.3145e-17, dtype=torch.float64), tensor(6.9137e-17, dtype=torch.float64), tensor(3.1517e-17, dtype=torch.float64), tensor(5.8040e-17, dtype=torch.float64), tensor(0.2144, dtype=torch.float64), tensor(1.4724e-17, dtype=torch.float64), tensor(7.0953e-17, dtype=torch.float64), tensor(0.6953, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3047, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.09
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.214
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.695

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.030469006777099267,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  128
lora dropout:  0.030469006777099267
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9998
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:17,  5.64it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 34.20it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 43.32it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 47.74it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 50.50it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.35it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 54.95it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 60.19it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 56.81it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 61.88it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 60.77it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.56it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 50.15it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.83it/s]
Evaluation performance at step 25: 0.62
{'loss': 2.6483, 'grad_norm': 0.5547924041748047, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.62}
{'eval_loss': 1.245752215385437, 'eval_runtime': 6.6662, 'eval_samples_per_second': 149.86, 'eval_steps_per_second': 9.451, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:18,  5.48it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.24it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:01, 41.87it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 46.45it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 48.74it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 54.72it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 56.77it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 58.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.61it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 59.42it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 58.70it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 58.05it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 56.09it/s]
Evaluation performance at step 50: 0.62
{'loss': 1.1034, 'grad_norm': 0.20250453054904938, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 1.0147435665130615, 'eval_runtime': 6.6663, 'eval_samples_per_second': 149.859, 'eval_steps_per_second': 9.451, 'epoch': 0.08}
{'loss': 0.9331, 'grad_norm': 0.25305184721946716, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8585904240608215, 'eval_runtime': 6.7677, 'eval_samples_per_second': 147.612, 'eval_steps_per_second': 9.309, 'epoch': 0.12}
{'loss': 0.7608, 'grad_norm': 0.20654183626174927, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7361593842506409, 'eval_runtime': 6.772, 'eval_samples_per_second': 147.518, 'eval_steps_per_second': 9.303, 'epoch': 0.16}
{'loss': 0.7279, 'grad_norm': 0.26121634244918823, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.7038694620132446, 'eval_runtime': 6.7184, 'eval_samples_per_second': 148.697, 'eval_steps_per_second': 9.377, 'epoch': 0.2}
{'loss': 0.6671, 'grad_norm': 0.2759682536125183, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.634285032749176, 'eval_runtime': 6.7793, 'eval_samples_per_second': 147.361, 'eval_steps_per_second': 9.293, 'epoch': 0.24}
{'loss': 0.6347, 'grad_norm': 0.31678318977355957, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5732061266899109, 'eval_runtime': 6.7679, 'eval_samples_per_second': 147.608, 'eval_steps_per_second': 9.309, 'epoch': 0.28}
{'loss': 0.6195, 'grad_norm': 0.3440638482570648, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.521289050579071, 'eval_runtime': 6.7671, 'eval_samples_per_second': 147.626, 'eval_steps_per_second': 9.31, 'epoch': 0.32}
{'loss': 0.5749, 'grad_norm': 0.2910093665122986, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4748205840587616, 'eval_runtime': 6.7808, 'eval_samples_per_second': 147.328, 'eval_steps_per_second': 9.291, 'epoch': 0.36}
{'loss': 0.4672, 'grad_norm': 0.35121455788612366, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.4271160662174225, 'eval_runtime': 6.7198, 'eval_samples_per_second': 148.666, 'eval_steps_per_second': 9.375, 'epoch': 0.4}
{'loss': 0.4502, 'grad_norm': 0.3226276934146881, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.38687339425086975, 'eval_runtime': 6.7279, 'eval_samples_per_second': 148.486, 'eval_steps_per_second': 9.364, 'epoch': 0.44}
{'loss': 0.4101, 'grad_norm': 0.3503814935684204, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3532502055168152, 'eval_runtime': 6.7213, 'eval_samples_per_second': 148.631, 'eval_steps_per_second': 9.373, 'epoch': 0.48}
{'loss': 0.379, 'grad_norm': 0.3537207245826721, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.3229808807373047, 'eval_runtime': 6.7353, 'eval_samples_per_second': 148.323, 'eval_steps_per_second': 9.354, 'epoch': 0.52}
{'loss': 0.3698, 'grad_norm': 0.34824684262275696, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.2984413802623749, 'eval_runtime': 6.768, 'eval_samples_per_second': 147.607, 'eval_steps_per_second': 9.309, 'epoch': 0.56}
{'loss': 0.374, 'grad_norm': 0.292045533657074, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.2703336179256439, 'eval_runtime': 6.7439, 'eval_samples_per_second': 148.134, 'eval_steps_per_second': 9.342, 'epoch': 0.6}
{'loss': 0.3221, 'grad_norm': 0.20200315117835999, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.25426408648490906, 'eval_runtime': 6.7498, 'eval_samples_per_second': 148.004, 'eval_steps_per_second': 9.334, 'epoch': 0.64}
{'loss': 0.2988, 'grad_norm': 0.33073586225509644, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.24130463600158691, 'eval_runtime': 6.7344, 'eval_samples_per_second': 148.342, 'eval_steps_per_second': 9.355, 'epoch': 0.68}
{'loss': 0.2559, 'grad_norm': 0.2654658854007721, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.2255018651485443, 'eval_runtime': 6.7207, 'eval_samples_per_second': 148.646, 'eval_steps_per_second': 9.374, 'epoch': 0.72}
{'loss': 0.2739, 'grad_norm': 0.25175148248672485, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.21520085632801056, 'eval_runtime': 6.7285, 'eval_samples_per_second': 148.473, 'eval_steps_per_second': 9.363, 'epoch': 0.76}
{'loss': 0.2714, 'grad_norm': 0.38594043254852295, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.20428447425365448, 'eval_runtime': 6.7311, 'eval_samples_per_second': 148.415, 'eval_steps_per_second': 9.36, 'epoch': 0.8}
{'loss': 0.2365, 'grad_norm': 0.33205318450927734, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.19669993221759796, 'eval_runtime': 6.7223, 'eval_samples_per_second': 148.609, 'eval_steps_per_second': 9.372, 'epoch': 0.84}
{'loss': 0.2484, 'grad_norm': 0.2836250960826874, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.19079959392547607, 'eval_runtime': 6.7237, 'eval_samples_per_second': 148.578, 'eval_steps_per_second': 9.37, 'epoch': 0.88}
{'loss': 0.2293, 'grad_norm': 0.15028265118598938, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.18571263551712036, 'eval_runtime': 6.7182, 'eval_samples_per_second': 148.702, 'eval_steps_per_second': 9.378, 'epoch': 0.92}
{'loss': 0.2128, 'grad_norm': 0.22496911883354187, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.1830528974533081, 'eval_runtime': 6.7175, 'eval_samples_per_second': 148.716, 'eval_steps_per_second': 9.378, 'epoch': 0.96}
{'loss': 0.2262, 'grad_norm': 0.42745241522789, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.18184813857078552, 'eval_runtime': 6.713, 'eval_samples_per_second': 148.817, 'eval_steps_per_second': 9.385, 'epoch': 1.0}
{'train_runtime': 390.2097, 'train_samples_per_second': 25.622, 'train_steps_per_second': 1.602, 'train_loss': 0.5478159332275391, 'epoch': 1.0}
train_results:  {'eval_loss': [1.245752215385437, 1.0147435665130615, 0.8585904240608215, 0.7361593842506409, 0.7038694620132446, 0.634285032749176, 0.5732061266899109, 0.521289050579071, 0.4748205840587616, 0.4271160662174225, 0.38687339425086975, 0.3532502055168152, 0.3229808807373047, 0.2984413802623749, 0.2703336179256439, 0.25426408648490906, 0.24130463600158691, 0.2255018651485443, 0.21520085632801056, 0.20428447425365448, 0.19669993221759796, 0.19079959392547607, 0.18571263551712036, 0.1830528974533081, 0.18184813857078552], 'performance': [0.62, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:31,  3.16it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 34.91it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 44.57it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 53.98it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 60.55it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 64.06it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 62.12it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.62, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2498021125793457
current iteration best possible performance (full train run):  0.609
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625, 1.2382837533950806, 1.2482209205627441, 1.2482129335403442, 1.248682975769043, 1.2498021125793457]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.0160 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3885725140571594, 0.9679630994796753, 0.8397911190986633, 0.36329978704452515, 0.92229163646698, 0.1434715986251831, 0.17953276634216309, 0.5892705321311951, 0.2934316396713257, 0.19823451340198517, 0.28097856044769287, 0.8804008960723877, 0.07795566320419312, 0.5138989686965942, 0.3326285481452942, 0.8311651945114136, 0.5552680492401123, 0.8665866851806641, 0.312958300113678]  ‚Üí  acq = 0.9461072794767462
X = [0.7243848443031311, 0.5673260688781738, 0.17221713066101074, 0.4579539895057678, 0.4861464500427246, 0.9055273532867432, 0.20969432592391968, 0.4790216088294983, 0.10195028781890869, 0.7672865390777588, 0.7903406620025635, 0.40216881036758423, 0.4012981057167053, 0.08321833610534668, 0.5124111175537109, 0.6376338601112366, 0.4250326156616211, 0.6688123941421509, 0.5568657517433167]  ‚Üí  acq = 0.9384649320534
X = [0.5763764977455139, 0.05863410234451294, 0.34301215410232544, 0.9383164048194885, 0.5356301665306091, 0.445218026638031, 0.015187442302703857, 0.6969332695007324, 0.022352755069732666, 0.7871749401092529, 0.43641388416290283, 0.685420036315918, 0.7192437648773193, 0.9363342523574829, 0.5681769847869873, 0.3550992012023926, 0.2191227674484253, 0.9536852836608887, 0.9173991680145264]  ‚Üí  acq = 0.9446073644628734
X = [0.984084963798523, 0.19762492179870605, 0.12537074089050293, 0.1269587278366089, 0.792256236076355, 0.2060524821281433, 0.82547527551651, 0.043537437915802, 0.5995619297027588, 0.49003463983535767, 0.3509824872016907, 0.8041259050369263, 0.43569356203079224, 0.8498241305351257, 0.44921064376831055, 0.8645860552787781, 0.5376258492469788, 0.8953969478607178, 0.09309971332550049]  ‚Üí  acq = 0.9461071523700352
X = [0.9951540231704712, 0.6521599292755127, 0.3331634998321533, 0.48812413215637207, 0.7391839623451233, 0.6775466799736023, 0.45204871892929077, 0.5870893001556396, 0.41431349515914917, 0.5988803505897522, 0.17390727996826172, 0.3302229642868042, 0.8276078104972839, 0.8921228647232056, 0.6934785842895508, 0.42078936100006104, 0.24742817878723145, 0.7852686643600464, 0.4308863878250122]  ‚Üí  acq = 0.9461054211158151
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0460, dtype=torch.float64), 0, tensor(0.0472, dtype=torch.float64), tensor(0.0976, dtype=torch.float64), 0, 0, tensor(0.1025, dtype=torch.float64), 0, tensor(0.6979, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 128, 0.051224866616346815, 47.99999999999999, 0]
normalized proposed parameters for next round by BO: [tensor(0.0460, dtype=torch.float64), tensor(3.0188e-16, dtype=torch.float64), tensor(0.0472, dtype=torch.float64), tensor(0.0976, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1025, dtype=torch.float64), tensor(0.0088, dtype=torch.float64), tensor(0.6979, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5122, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.046
  gsm8k: 0
  rowan_hellaswag: 0.047
  sciq: 0.098
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0.102
  mmlu: 0
  arc_challenge: 0.698

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.051224866616346815,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (47.99999999999999,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  128
lora dropout:  0.051224866616346815
lora alpha:  47.99999999999999
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9908
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  990
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:05<08:22,  5.08s/it]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:05<00:38,  2.35it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:05<00:16,  5.13it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:05<00:08,  8.56it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:05<00:05, 12.68it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:05<00:03, 17.62it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:05<00:02, 22.65it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:06<00:01, 28.90it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:06<00:01, 32.84it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:06<00:00, 39.51it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:06<00:00, 43.11it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:06<00:00, 45.96it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:06<00:00, 48.75it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 14.64it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.5914, 'grad_norm': 0.38945236802101135, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.3475556373596191, 'eval_runtime': 8.5869, 'eval_samples_per_second': 115.291, 'eval_steps_per_second': 7.22, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.11it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 30.54it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 38.81it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 42.99it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.43it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 49.35it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 50.13it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 55.08it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.77it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.44it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 55.44it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 54.77it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 54.89it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.77it/s]
Evaluation performance at step 50: 0.6
{'loss': 1.268, 'grad_norm': 0.32303810119628906, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.6}
{'eval_loss': 1.1176797151565552, 'eval_runtime': 8.6031, 'eval_samples_per_second': 115.075, 'eval_steps_per_second': 7.207, 'epoch': 0.08}
{'loss': 1.0959, 'grad_norm': 0.27500325441360474, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 0.9828615784645081, 'eval_runtime': 8.664, 'eval_samples_per_second': 114.266, 'eval_steps_per_second': 7.156, 'epoch': 0.12}
{'loss': 1.0258, 'grad_norm': 0.2609730362892151, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.885492205619812, 'eval_runtime': 8.6284, 'eval_samples_per_second': 114.737, 'eval_steps_per_second': 7.186, 'epoch': 0.16}
{'loss': 0.9007, 'grad_norm': 0.29547151923179626, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.8223351836204529, 'eval_runtime': 8.6234, 'eval_samples_per_second': 114.804, 'eval_steps_per_second': 7.19, 'epoch': 0.2}
{'loss': 0.8485, 'grad_norm': 0.32085981965065, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.7693771123886108, 'eval_runtime': 8.6212, 'eval_samples_per_second': 114.833, 'eval_steps_per_second': 7.192, 'epoch': 0.24}
{'loss': 0.8439, 'grad_norm': 0.35552647709846497, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.7243753671646118, 'eval_runtime': 8.5892, 'eval_samples_per_second': 115.262, 'eval_steps_per_second': 7.218, 'epoch': 0.28}
{'loss': 0.7842, 'grad_norm': 0.34376582503318787, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.6749323606491089, 'eval_runtime': 8.6469, 'eval_samples_per_second': 114.492, 'eval_steps_per_second': 7.17, 'epoch': 0.32}
{'loss': 0.7561, 'grad_norm': 0.3110109269618988, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.629534125328064, 'eval_runtime': 8.6438, 'eval_samples_per_second': 114.533, 'eval_steps_per_second': 7.173, 'epoch': 0.36}
{'loss': 0.6678, 'grad_norm': 0.3435259461402893, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.5923105478286743, 'eval_runtime': 8.6488, 'eval_samples_per_second': 114.467, 'eval_steps_per_second': 7.169, 'epoch': 0.4}
{'loss': 0.7091, 'grad_norm': 0.2850200831890106, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.5485305786132812, 'eval_runtime': 8.6087, 'eval_samples_per_second': 115.0, 'eval_steps_per_second': 7.202, 'epoch': 0.44}
{'loss': 0.5537, 'grad_norm': 0.3935115337371826, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.5186205506324768, 'eval_runtime': 8.5976, 'eval_samples_per_second': 115.149, 'eval_steps_per_second': 7.211, 'epoch': 0.48}
{'loss': 0.5668, 'grad_norm': 0.32014894485473633, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.4814355969429016, 'eval_runtime': 8.6245, 'eval_samples_per_second': 114.789, 'eval_steps_per_second': 7.189, 'epoch': 0.52}
{'loss': 0.6008, 'grad_norm': 0.3270465135574341, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.4547125995159149, 'eval_runtime': 8.6499, 'eval_samples_per_second': 114.453, 'eval_steps_per_second': 7.168, 'epoch': 0.56}
{'loss': 0.5081, 'grad_norm': 0.24418774247169495, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.4361957013607025, 'eval_runtime': 8.6583, 'eval_samples_per_second': 114.341, 'eval_steps_per_second': 7.161, 'epoch': 0.6}
{'loss': 0.473, 'grad_norm': 0.3422984480857849, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.422821044921875, 'eval_runtime': 8.682, 'eval_samples_per_second': 114.029, 'eval_steps_per_second': 7.141, 'epoch': 0.65}
{'loss': 0.5711, 'grad_norm': 0.3458881974220276, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.40975865721702576, 'eval_runtime': 8.6573, 'eval_samples_per_second': 114.354, 'eval_steps_per_second': 7.162, 'epoch': 0.69}
{'loss': 0.4719, 'grad_norm': 0.23174098134040833, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.39792779088020325, 'eval_runtime': 8.6532, 'eval_samples_per_second': 114.409, 'eval_steps_per_second': 7.165, 'epoch': 0.73}
{'loss': 0.5045, 'grad_norm': 0.2515600621700287, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.3885347545146942, 'eval_runtime': 8.6503, 'eval_samples_per_second': 114.447, 'eval_steps_per_second': 7.167, 'epoch': 0.77}
{'loss': 0.5276, 'grad_norm': 0.17837664484977722, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.38365641236305237, 'eval_runtime': 8.6532, 'eval_samples_per_second': 114.409, 'eval_steps_per_second': 7.165, 'epoch': 0.81}
{'loss': 0.5113, 'grad_norm': 0.20489931106567383, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.37630584836006165, 'eval_runtime': 8.6432, 'eval_samples_per_second': 114.541, 'eval_steps_per_second': 7.173, 'epoch': 0.85}
{'loss': 0.4982, 'grad_norm': 0.17072030901908875, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.3707031309604645, 'eval_runtime': 8.6816, 'eval_samples_per_second': 114.034, 'eval_steps_per_second': 7.142, 'epoch': 0.89}
{'loss': 0.3949, 'grad_norm': 0.25990909337997437, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.36688336730003357, 'eval_runtime': 8.7378, 'eval_samples_per_second': 113.3, 'eval_steps_per_second': 7.096, 'epoch': 0.93}
{'loss': 0.4438, 'grad_norm': 0.14641568064689636, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.36453258991241455, 'eval_runtime': 8.7295, 'eval_samples_per_second': 113.408, 'eval_steps_per_second': 7.102, 'epoch': 0.97}
{'train_runtime': 470.9265, 'train_samples_per_second': 21.039, 'train_steps_per_second': 1.317, 'train_loss': 0.7447474618111888, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3475556373596191, 1.1176797151565552, 0.9828615784645081, 0.885492205619812, 0.8223351836204529, 0.7693771123886108, 0.7243753671646118, 0.6749323606491089, 0.629534125328064, 0.5923105478286743, 0.5485305786132812, 0.5186205506324768, 0.4814355969429016, 0.4547125995159149, 0.4361957013607025, 0.422821044921875, 0.40975865721702576, 0.39792779088020325, 0.3885347545146942, 0.38365641236305237, 0.37630584836006165, 0.3707031309604645, 0.36688336730003357, 0.36453258991241455], 'performance': [0.63, 0.6]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:39,  2.49it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.22it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 40.64it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 43.17it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 49.19it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 53.20it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 67.18it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 51.57it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.6]
current iteration observed (possibly low-fid or predicted) performance:  1.248852252960205
current iteration best possible performance (full train run):  0.672
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625, 1.2382837533950806, 1.2482209205627441, 1.2482129335403442, 1.248682975769043, 1.2498021125793457, 1.248852252960205]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1680 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9563958644866943, 0.2064303755760193, 0.9215262532234192, 0.25031691789627075, 0.3756294846534729, 0.5335946679115295, 0.4558410048484802, 0.7456666827201843, 0.10756498575210571, 0.38574671745300293, 0.06539911031723022, 0.5066487193107605, 0.5190140008926392, 0.6812496781349182, 0.3449122905731201, 0.4593932032585144, 0.23874396085739136, 0.3616427183151245, 0.664249062538147]  ‚Üí  acq = 0.8922199701519798
X = [0.7559679746627808, 0.9207594990730286, 0.4476115107536316, 0.9131696820259094, 0.886353075504303, 0.5307265520095825, 0.07725703716278076, 0.6558188796043396, 0.7438957691192627, 0.668861448764801, 0.3008582592010498, 0.697472870349884, 0.16915035247802734, 0.8690377473831177, 0.39414364099502563, 0.789122998714447, 0.6319531202316284, 0.7716639041900635, 0.5650159120559692]  ‚Üí  acq = 0.9488457218515932
X = [0.36505305767059326, 0.3095471262931824, 0.1368759274482727, 0.3289750814437866, 0.7039254903793335, 0.6800842881202698, 0.7628263831138611, 0.6555813550949097, 0.8407274484634399, 0.5354591012001038, 0.500869870185852, 0.7801300287246704, 0.5476385354995728, 0.5221925377845764, 0.04085111618041992, 0.34916937351226807, 0.8951978087425232, 0.9283794164657593, 0.7808082699775696]  ‚Üí  acq = 0.9488442391715209
X = [0.8291627168655396, 0.5358777642250061, 0.15468764305114746, 0.26509326696395874, 0.10710686445236206, 0.6012601256370544, 0.8979237675666809, 0.7757288813591003, 0.33256465196609497, 0.9805472493171692, 0.7419776320457458, 0.5683872103691101, 0.9310100674629211, 0.8808845281600952, 0.24417293071746826, 0.9200261831283569, 0.2543778419494629, 0.06822002679109573, 0.01114499568939209]  ‚Üí  acq = 0.7506846583554705
X = [0.29655390977859497, 0.33454740047454834, 0.6622326374053955, 0.4774198532104492, 0.4513353705406189, 0.33820128440856934, 0.800865650177002, 0.34824466705322266, 0.5349290370941162, 0.2061476856470108, 0.8499124646186829, 0.8195555210113525, 0.6093823909759521, 0.16061973571777344, 0.7091799378395081, 0.8643938302993774, 0.8188557028770447, 0.673103928565979, 0.5149895548820496]  ‚Üí  acq = 0.9358979901704816
proposed candidate layer mask is:  tensor([1., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1022, dtype=torch.float64), 0, 0, 0, 0, tensor(0.2015, dtype=torch.float64), 0, 0, tensor(0.6963, dtype=torch.float64), 32, 1, 0, 1, 1, 1, 128, 0.04603278053563914, 48.0, 1]
normalized proposed parameters for next round by BO: [tensor(0.1022, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2113e-16, dtype=torch.float64), tensor(7.9011e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2015, dtype=torch.float64), tensor(2.7204e-16, dtype=torch.float64), tensor(9.4426e-17, dtype=torch.float64), tensor(0.6963, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4603, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.102
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.201
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.696

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.04603278053563914,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 1, 1]
lora rank:  128
lora dropout:  0.04603278053563914
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 260,046,848 || all params: 8,290,308,096 || trainable%: 3.1368
Initialized PerformanceEvalCallback to evaluate only at steps {25, 50} for tasks: {'triviaqa': (1.0, 'exact_match,remove_whitespace')}
length of training data:  9999
evaluation dataset not given. This means we are not using evaluation loss. Will just use training data and evaluation loss
length of validation data:  999
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
Running evaluation for step 25...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.18it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 33.56it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 41.14it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 44.87it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 47.14it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.45it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.83it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.95it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 53.07it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 57.52it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.31it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 55.56it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 53.84it/s]
Evaluation performance at step 25: 0.63
{'loss': 2.4469, 'grad_norm': 0.4656500518321991, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04, 'performance_step_25': 0.63}
{'eval_loss': 1.0688914060592651, 'eval_runtime': 7.6631, 'eval_samples_per_second': 130.365, 'eval_steps_per_second': 8.221, 'epoch': 0.04}
Running evaluation for step 50...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 3
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:19,  5.08it/s]Running generate_until requests:   9%|‚ñâ         | 9/100 [00:00<00:02, 32.69it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 40.11it/s]Running generate_until requests:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:00<00:01, 43.43it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 45.53it/s]Running generate_until requests:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:00<00:01, 50.94it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:00, 52.74it/s]Running generate_until requests:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:01<00:00, 56.36it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 51.95it/s]Running generate_until requests:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:01<00:00, 56.18it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 54.75it/s]Running generate_until requests:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:01<00:00, 53.92it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 52.61it/s]
Evaluation performance at step 50: 0.62
{'loss': 0.9174, 'grad_norm': 0.5440605878829956, 'learning_rate': 0.000294, 'epoch': 0.08, 'performance_step_50': 0.62}
{'eval_loss': 0.8522309064865112, 'eval_runtime': 7.4385, 'eval_samples_per_second': 134.301, 'eval_steps_per_second': 8.469, 'epoch': 0.08}
{'loss': 0.8166, 'grad_norm': 0.24935586750507355, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.7742913961410522, 'eval_runtime': 7.5344, 'eval_samples_per_second': 132.591, 'eval_steps_per_second': 8.362, 'epoch': 0.12}
{'loss': 0.7701, 'grad_norm': 0.25264275074005127, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.7185779809951782, 'eval_runtime': 7.5316, 'eval_samples_per_second': 132.642, 'eval_steps_per_second': 8.365, 'epoch': 0.16}
{'loss': 0.7129, 'grad_norm': 0.33715197443962097, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.6456350088119507, 'eval_runtime': 7.5481, 'eval_samples_per_second': 132.352, 'eval_steps_per_second': 8.347, 'epoch': 0.2}
{'loss': 0.645, 'grad_norm': 0.3904194235801697, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.5801682472229004, 'eval_runtime': 7.5218, 'eval_samples_per_second': 132.814, 'eval_steps_per_second': 8.376, 'epoch': 0.24}
{'loss': 0.5866, 'grad_norm': 0.34630632400512695, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.5213245153427124, 'eval_runtime': 7.4977, 'eval_samples_per_second': 133.241, 'eval_steps_per_second': 8.403, 'epoch': 0.28}
{'loss': 0.5383, 'grad_norm': 0.3503061532974243, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.4675171971321106, 'eval_runtime': 7.4884, 'eval_samples_per_second': 133.406, 'eval_steps_per_second': 8.413, 'epoch': 0.32}
{'loss': 0.5043, 'grad_norm': 0.3019813895225525, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.4378931224346161, 'eval_runtime': 7.4834, 'eval_samples_per_second': 133.495, 'eval_steps_per_second': 8.419, 'epoch': 0.36}
{'loss': 0.4343, 'grad_norm': 0.29979419708251953, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.38100680708885193, 'eval_runtime': 7.4973, 'eval_samples_per_second': 133.249, 'eval_steps_per_second': 8.403, 'epoch': 0.4}
{'loss': 0.4085, 'grad_norm': 0.31908294558525085, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.35283640027046204, 'eval_runtime': 7.5253, 'eval_samples_per_second': 132.753, 'eval_steps_per_second': 8.372, 'epoch': 0.44}
{'loss': 0.3933, 'grad_norm': 0.2126510590314865, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.3172088861465454, 'eval_runtime': 7.4833, 'eval_samples_per_second': 133.497, 'eval_steps_per_second': 8.419, 'epoch': 0.48}
{'loss': 0.378, 'grad_norm': 0.3930296003818512, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.2904055714607239, 'eval_runtime': 7.484, 'eval_samples_per_second': 133.484, 'eval_steps_per_second': 8.418, 'epoch': 0.52}
{'loss': 0.3283, 'grad_norm': 0.28768688440322876, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.27206531167030334, 'eval_runtime': 7.4895, 'eval_samples_per_second': 133.386, 'eval_steps_per_second': 8.412, 'epoch': 0.56}
{'loss': 0.2958, 'grad_norm': 0.2091493010520935, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.24909397959709167, 'eval_runtime': 7.4893, 'eval_samples_per_second': 133.39, 'eval_steps_per_second': 8.412, 'epoch': 0.6}
{'loss': 0.3074, 'grad_norm': 0.3435685336589813, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.2336558848619461, 'eval_runtime': 7.4929, 'eval_samples_per_second': 133.326, 'eval_steps_per_second': 8.408, 'epoch': 0.64}
{'loss': 0.2742, 'grad_norm': 0.1758630871772766, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.22289115190505981, 'eval_runtime': 7.497, 'eval_samples_per_second': 133.254, 'eval_steps_per_second': 8.403, 'epoch': 0.68}
{'loss': 0.2756, 'grad_norm': 0.33619222044944763, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.21251219511032104, 'eval_runtime': 7.5028, 'eval_samples_per_second': 133.151, 'eval_steps_per_second': 8.397, 'epoch': 0.72}
{'loss': 0.2806, 'grad_norm': 0.21768012642860413, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.20115913450717926, 'eval_runtime': 7.5029, 'eval_samples_per_second': 133.149, 'eval_steps_per_second': 8.397, 'epoch': 0.76}
{'loss': 0.2505, 'grad_norm': 0.2415952831506729, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.19351671636104584, 'eval_runtime': 7.4883, 'eval_samples_per_second': 133.408, 'eval_steps_per_second': 8.413, 'epoch': 0.8}
{'loss': 0.2442, 'grad_norm': 0.2822246551513672, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.18663789331912994, 'eval_runtime': 7.4845, 'eval_samples_per_second': 133.476, 'eval_steps_per_second': 8.417, 'epoch': 0.84}
{'loss': 0.246, 'grad_norm': 0.21330907940864563, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.1807325780391693, 'eval_runtime': 7.4876, 'eval_samples_per_second': 133.421, 'eval_steps_per_second': 8.414, 'epoch': 0.88}
{'loss': 0.2305, 'grad_norm': 0.18728670477867126, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.17588920891284943, 'eval_runtime': 7.482, 'eval_samples_per_second': 133.52, 'eval_steps_per_second': 8.42, 'epoch': 0.92}
{'loss': 0.22, 'grad_norm': 0.19937019050121307, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.17312142252922058, 'eval_runtime': 7.479, 'eval_samples_per_second': 133.573, 'eval_steps_per_second': 8.424, 'epoch': 0.96}
{'loss': 0.1985, 'grad_norm': 0.19534751772880554, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.17194382846355438, 'eval_runtime': 7.4804, 'eval_samples_per_second': 133.55, 'eval_steps_per_second': 8.422, 'epoch': 1.0}
{'train_runtime': 418.7387, 'train_samples_per_second': 23.879, 'train_steps_per_second': 1.493, 'train_loss': 0.5081575447082519, 'epoch': 1.0}
train_results:  {'eval_loss': [1.0688914060592651, 0.8522309064865112, 0.7742913961410522, 0.7185779809951782, 0.6456350088119507, 0.5801682472229004, 0.5213245153427124, 0.4675171971321106, 0.4378931224346161, 0.38100680708885193, 0.35283640027046204, 0.3172088861465454, 0.2904055714607239, 0.27206531167030334, 0.24909397959709167, 0.2336558848619461, 0.22289115190505981, 0.21251219511032104, 0.20115913450717926, 0.19351671636104584, 0.18663789331912994, 0.1807325780391693, 0.17588920891284943, 0.17312142252922058, 0.17194382846355438], 'performance': [0.63, 0.62]}
evaluating with task performance...
creating HFLM wrapper for model_path
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
evaluating on tasks:  ['triviaqa']
Overwriting default num_fewshot of triviaqa from None to 5
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:914: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["input_ids"] = encoding["input_ids"][:, -left_truncate_len:]
/home/alfred/Data-Mixing/lm-evaluation-harness/lm_eval/models/huggingface.py:915: UserWarning: Truncating the start/stop/step of slice. This is likely because of saved old models when the start/stop/step were larger.
  encoding["attention_mask"] = encoding["attention_mask"][
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/alfred/miniconda3/envs/jobs/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   1%|          | 1/100 [00:00<00:36,  2.72it/s]Running generate_until requests:  17%|‚ñà‚ñã        | 17/100 [00:00<00:02, 30.98it/s]Running generate_until requests:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:00<00:01, 39.99it/s]Running generate_until requests:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:01<00:01, 48.64it/s]Running generate_until requests:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:01<00:00, 54.64it/s]Running generate_until requests:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:01<00:00, 56.47it/s]Running generate_until requests:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:01<00:00, 69.99it/s]Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 54.56it/s]
Applying JoBS: Extracted performance logs for performance prediction.
Length of performance logs:  2
performance logs:  [0.63, 0.62]
current iteration observed (possibly low-fid or predicted) performance:  1.2456339597702026
current iteration best possible performance (full train run):  0.5880000000000001
max performance so far:  0.6825000000000001
BO observations:  [1.1595536470413208, 1.1007390022277832, 0.9383885860443115, 1.0854151248931885, 1.2467622756958008, 1.1713917255401611, 1.2482727766036987, 1.24687922000885, 1.2465814352035522, 1.2491205930709839, 1.247354507446289, 1.2381198406219482, 1.2393970489501953, 1.245240569114685, 1.2445502281188965, 1.2452890872955322, 1.249295711517334, 1.0923447608947754, 1.2482903003692627, 1.2481999397277832, 1.2468851804733276, 1.2401230335235596, 1.2409820556640625, 1.2382837533950806, 1.2482209205627441, 1.2482129335403442, 1.248682975769043, 1.2498021125793457, 1.248852252960205, 1.2456339597702026]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5935 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.689376711845398, 0.8068364858627319, 0.40232396125793457, 0.524096667766571, 0.7960490584373474, 0.021674156188964844, 0.9051079154014587, 0.30671656131744385, 0.01976799964904785, 0.4357435405254364, 0.0507315993309021, 0.36988502740859985, 0.09059679508209229, 0.587839663028717, 0.2879335284233093, 0.6362209916114807, 0.25974810123443604, 0.338161438703537, 0.4360768795013428]  ‚Üí  acq = 0.9408335916871892
X = [0.1671653389930725, 0.8243348002433777, 0.24390113353729248, 0.48609602451324463, 0.21825015544891357, 0.22961974143981934, 0.8503690958023071, 0.7566047310829163, 0.26851314306259155, 0.5466057658195496, 0.36829400062561035, 0.28389930725097656, 0.8483900427818298, 0.013978004455566406, 0.8134440779685974, 0.4587240517139435, 0.5154920220375061, 0.21769246459007263, 0.5121077299118042]  ‚Üí  acq = 0.6635028280103583
X = [0.3521716594696045, 0.8249445557594299, 0.6134587526321411, 0.9726700186729431, 0.8058072328567505, 0.10300272703170776, 0.7388759851455688, 0.2983672022819519, 0.49528342485427856, 0.3969183564186096, 0.3575289249420166, 0.36265599727630615, 0.033598363399505615, 0.5630342364311218, 0.8969208598136902, 0.5042821764945984, 0.9185894131660461, 0.36380210518836975, 0.8705978393554688]  ‚Üí  acq = 0.9408377561691552
X = [0.36290156841278076, 0.18474721908569336, 0.18171274662017822, 0.015033304691314697, 0.7732937335968018, 0.6220834851264954, 0.8993080854415894, 0.3054540157318115, 0.7051181793212891, 0.9189110994338989, 0.8914339542388916, 0.9815457463264465, 0.2250869870185852, 0.8526580929756165, 0.20366638898849487, 0.34786948561668396, 0.47295230627059937, 0.20589981973171234, 0.527204155921936]  ‚Üí  acq = 0.9408220561128882
X = [0.8450332283973694, 0.04751211404800415, 0.15186965465545654, 0.12796109914779663, 0.417366087436676, 0.16371077299118042, 0.41620874404907227, 0.17068278789520264, 0.40559256076812744, 0.9195560812950134, 0.09945559501647949, 0.6197478175163269, 0.8085575103759766, 0.14114248752593994, 0.22963440418243408, 0.5870038270950317, 0.21952831745147705, 0.35161712765693665, 0.22909259796142578]  ‚Üí  acq = 0.9077610947974308
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1007, dtype=torch.float64), 0, 0, 0, 0, 0, tensor(0.2005, dtype=torch.float64), 0, tensor(0.6988, dtype=torch.float64), 32, 1, 0, 1, 0, 1, 128, 0.044599039726672646, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.1007, dtype=torch.float64), tensor(2.3098e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9562e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0559e-16, dtype=torch.float64), tensor(0.2005, dtype=torch.float64), tensor(6.2519e-15, dtype=torch.float64), tensor(0.6988, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4460, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [0.63, 0.63, 0.63, 0.6405, 0.651, 0.651, 0.651, 0.651, 0.651, 0.651, 0.651, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'triviaqa', 'eval_method': 'performance', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_triviaqa_performance_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['triviaqa'], 'weight': [1.0], 'random': [[0.6405, 0.6405, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001, 0.7140000000000001], [0.6405, 0.6405, 0.6405, 0.6405, 0.651, 0.651, 0.651, 0.651, 0.651, 0.6615000000000001, 0.6615000000000001, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672, 0.672], [0.63, 0.63, 0.63, 0.6405, 0.651, 0.651, 0.651, 0.651, 0.651, 0.651, 0.651, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6615000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001, 0.6825000000000001]], 'random_full_inputs': [[[0.715849048875918, 0, 0.04634683709496147, 0, 0.16216853651544186, 0, 0, 0, 0.07563557751367844, 4, 1, 1, 0, 0, 1, 110, 0.0, 16.38004380516881, 0], [0.8402810698071487, 0, 0, 0, 0.15928063870440823, 0, 0, 0, 0, 2, 1, 0, 1, 1, 1, 128, 0.06318159091447405, 36.63087834233318, 0], [0.9552572212633252, 0, 0, 0, 0.04458752275375931, 0, 0, 0, 0, 26, 1, 0, 1, 1, 1, 128, 5.4210108624275225e-21, 13.494683229879406, 0], [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 1, 0, 1, 1, 1, 69, 0.0, 48.0, 0], [0, 0, 0, 0, 1.0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 2, 2.776940493660833e-18, 1.4800000190734863, 0], [0.8431882824131218, 0, 0, 0.15681171758687842, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 29.813058415553638, 0], [0.8485073390998884, 0, 0, 0.1514926609001117, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 86, 0.1, 27.060918620034414, 0], [0.7557720566042215, 0, 0, 0.24422794339577844, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 48.0, 0], [0.7608210675330449, 0, 0.23917893246695493, 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 48.0, 0], [0.7703140292312792, 0, 0, 0, 0, 0, 0, 0, 0.22968597076872074, 32, 1, 1, 1, 1, 1, 128, 1.0691902393211288e-16, 48.0, 0], [0, 0, 0, 0, 0, 0, 1.0, 0, 0, 32, 1, 0, 1, 1, 1, 2, 0.0, 1.4800000190734863, 0], [0.7160302196701802, 0, 0, 0, 0, 0, 0, 0.28396978032981973, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 47.999999999999986, 0], [0.8292195832133656, 0, 0, 0, 0, 0, 0, 0.17078041678663441, 0, 32, 0, 1, 0, 0, 0, 128, 0.0, 48.0, 1], [0, 0.9999999999999812, 0, 0, 0, 0, 0, 0, 0, 20, 1, 0, 1, 1, 1, 2, 0.06393295880120368, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1.0, 32, 0, 0, 1, 1, 1, 2, 4.163336342344338e-18, 1.480000019073487, 1], [0.7879554568088224, 0, 0, 0, 0, 0, 0, 0.21204454319117763, 0, 32, 0, 1, 1, 0, 0, 128, 9.280208701726489e-19, 48.0, 1], [0.7826758532615501, 0, 0, 0, 0, 0, 0, 0.2173241467384501, 0, 32, 0, 0, 1, 1, 1, 128, 2.081668171172169e-18, 48.0, 0], [0, 0, 0, 0.9999999999999992, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 2, 1.2767564783189302e-16, 1.4800000190734883, 0], [0.7577902524210294, 0, 0, 0, 0, 0.24220974757897076, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.160699181775697e-18, 48.0, 0], [0.7361374035696022, 0.26386259643075727, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 1, 1, 1, 128, 2.553512956637859e-16, 48.0, 1], [0, 0, 1.0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 2, 0.0, 1.4800000190734863, 0], [0.776412954913802, 0, 0, 0, 0.22358704508619798, 0, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.0785207688568522e-33, 48.0, 0], [0.7752232132566358, 0, 0, 0, 0, 0, 0, 0.22477678674336413, 0, 32, 1, 0, 1, 1, 1, 128, 0.0, 48.0, 1], [0.7462227599181088, 0, 0, 0, 0, 0.25377724008189173, 0, 0, 0, 32, 1, 1, 1, 1, 0, 128, 0.0, 48.0, 0], [0.7510686836538834, 0, 0, 0, 0, 0, 0.2489313163461166, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.03427786955009829, 48.0, 0], [0.7423699135152386, 0, 0, 0.2576300864847615, 0, 0, 0, 0, 0, 32, 0, 1, 1, 1, 0, 128, 0.04191629066424122, 48.0, 1], [0.7608680612360497, 0, 0, 0, 0, 0, 0.23913193876395028, 0, 0, 32, 1, 0, 1, 0, 1, 128, 0.0, 48.0, 0], [0.7384642651643496, 0, 0, 0, 0, 0, 0.2615357348356503, 0, 0, 32, 1, 0, 1, 1, 1, 128, 1.1102230246251783e-17, 47.99999999999999, 0], [0.9511651973029128, 0, 0, 0, 0, 0, 0.04883480269708721, 0, 0, 32, 1, 0, 1, 1, 1, 128, 2.4905731076440046e-17, 48.0, 0], [0.9045784485557609, 0, 0, 0, 0, 0.09542155144423924, 0, 0, 0, 32, 1, 0, 1, 1, 1, 128, 0.1, 48.0, 0]], [[0.07778542189444039, 0.01722279512792426, 0.11445095441484274, 0, 0, 0.10996073085139087, 0, 0.07217960217929412, 0.6014973704773839, 32, 1, 0, 0, 0, 0, 119, 0.018879750832746307, 31.43773300116672, 1], [0.1485750981675555, 0, 0, 0, 0, 0, 0, 0, 0.8514249018324447, 32, 1, 0, 1, 0, 0, 128, 0.05176818064288932, 8.388502412888624, 0], [0, 0, 0.26025761411358667, 0, 0, 0, 0, 0, 0.7397423858864132, 32, 1, 1, 1, 0, 0, 3, 0.0, 47.99999999999995, 1], [0, 0, 0, 0, 0, 0, 0, 0.3334689504463143, 0.6665310495536858, 32, 0, 1, 1, 1, 1, 128, 2.551643258082968e-19, 1.4800000190734908, 0], [0, 0, 0, 0, 0, 0, 0.31849439704303234, 0, 0.6815056029569379, 32, 1, 0, 1, 0, 1, 128, 0.09999999999999984, 47.999999998847706, 1], [0, 0, 0, 0, 0, 0, 0.2919676878198013, 0, 0.7080323121801985, 10, 1, 0, 1, 0, 1, 128, 0.1, 48.0, 1], [0, 0, 0.3125980843241204, 0, 0, 0, 0, 0, 0.68740191567588, 32, 1, 0, 1, 1, 1, 128, 0.1, 47.99999999999995, 1], [0, 0, 0, 0.3242436691470165, 0, 0, 0, 0, 0.6757563308530328, 32, 1, 1, 1, 0, 1, 128, 0.1, 47.99999999999999, 1], [0, 0.31815834644911156, 0, 0, 0, 0, 0, 0, 0.6818416535508901, 32, 1, 1, 1, 1, 1, 128, 0.1, 48.0, 0], [0.2842144644413666, 0, 0, 0.011510320098573754, 0, 0, 0, 0, 0.6899306395721688, 32, 1, 1, 1, 1, 1, 128, 0.1, 48.0, 1], [0, 0, 0, 0, 0, 0.29302661618783077, 0, 0, 0.7069733838121697, 32, 0, 1, 1, 1, 0, 128, 0.09999999999999999, 48.0, 0], [0.07457093126937371, 0, 0, 0.19111301378104173, 0, 0, 0.019975970948767394, 0, 0.714340084000817, 32, 1, 1, 1, 1, 1, 128, 0.024617057686215985, 47.99999999999999, 1], [0, 0.29973943868507025, 0, 0, 0, 0, 0, 0, 0.7002605613149299, 32, 0, 0, 1, 0, 1, 128, 5.063001287087161e-17, 48.0, 1], [0, 0, 0, 0, 0, 0, 0.2844658979436652, 0, 0.7155341020563376, 32, 0, 1, 0, 1, 0, 128, 0.1, 48.0, 0], [0.08727950083531152, 0, 0, 0.22495205783459427, 0, 0, 0, 0, 0.6877071783790446, 32, 1, 0, 1, 1, 1, 128, 0.1, 12.28932446667006, 1], [0, 0, 0, 0, 0, 0, 0, 0.26390423961669535, 0.7360957603833044, 32, 1, 0, 1, 0, 1, 128, 0.09868658504349756, 47.99999999999999, 1], [0.23553727799166668, 0, 0, 0, 0, 0, 0, 0, 0.7644627220083334, 32, 1, 0, 0, 0, 0, 128, 0.08495139077558267, 47.99999999999999, 0], [0, 0, 0.26195836367853853, 0, 0, 0, 0, 0, 0.7380416363214616, 32, 1, 1, 0, 0, 1, 128, 0.06470992576311428, 48.0, 0], [0.09913064701704369, 0, 0, 0, 0, 0, 0, 0.19535846255664516, 0.7055108904263112, 32, 1, 1, 1, 0, 1, 128, 0.05497660376432817, 47.999999999999986, 0], [0.04375007075159362, 0.21996601021158108, 0, 0, 0, 0, 0, 0, 0.7362839190368256, 32, 1, 1, 1, 0, 1, 128, 0.1, 48.0, 1], [0.24024193782776376, 0, 0.041693994305861584, 0, 0, 0, 0, 0, 0.7180640678663746, 32, 1, 0, 1, 0, 1, 128, 0.04953754895750517, 48.0, 1], [0.09239226107030511, 0, 0, 0, 0, 0, 0, 0.201653026264186, 0.7059547126655087, 32, 0, 0, 0, 0, 1, 128, 0.07832334642505484, 48.0, 1], [0, 0, 0, 0.2656049260374091, 0, 0, 0, 0, 0.7343950739625945, 32, 1, 0, 1, 0, 1, 128, 0.08319820024213594, 48.0, 0], [0, 0.27220864386163296, 0, 0, 0, 0, 0, 0, 0.7277913561383677, 32, 1, 0, 1, 0, 0, 128, 0.0772460697458496, 48.0, 1], [0, 0, 0.27960715733057084, 0, 0, 0, 0, 0, 0.7203928426694403, 32, 1, 1, 1, 0, 1, 128, 0.06682537767061379, 48.0, 0], [0.08861576743699622, 0, 0, 0.18252663287612084, 0, 0, 0, 0, 0.7288575996868832, 32, 1, 1, 1, 0, 0, 128, 0.09827806550566766, 48.0, 0], [0.11250420173897249, 0, 0.1648366494024993, 0, 0, 0, 0, 0, 0.722659148858528, 32, 1, 0, 1, 0, 1, 128, 0.1, 48.0, 0], [0, 0, 0.03574995885352032, 0.23777083864615198, 0, 0, 0, 0, 0.7264792025003276, 32, 1, 0, 1, 0, 1, 128, 0.1, 48.0, 0], [0.09713019209313811, 0, 0, 0.18326146109079344, 0, 0, 0, 0, 0.7196083468160696, 32, 1, 0, 1, 0, 1, 128, 0.07447085693152322, 48.0, 0], [0.025053861407126708, 0, 0.12522229747353758, 0.1382535314035474, 0, 0, 0, 0, 0.7114703097157883, 32, 1, 0, 1, 1, 0, 128, 0.06890455699892178, 47.999999999999986, 0]], [[0.07778542189444039, 0.01722279512792426, 0.11445095441484274, 0, 0, 0.10996073085139087, 0, 0.07217960217929412, 0.6014973704773839, 32, 1, 0, 0, 0, 0, 119, 0.018879750832746307, 31.43773300116672, 1], [0.14901489018311037, 0, 0, 0, 0, 0, 0, 0, 0.8509851098168898, 32, 1, 0, 1, 0, 0, 128, 0.005714892469065631, 3.5508452430905937, 0], [0, 0, 0.27469933305579775, 0, 0, 0, 0, 0, 0.7253006669442023, 32, 1, 1, 1, 0, 0, 7, 0.1, 48.0, 1], [0, 0, 0, 0, 0, 0, 0, 0.34866808721147524, 0.6513319127885246, 32, 0, 1, 1, 1, 1, 128, 0.0, 1.4800000190734863, 0], [0, 0, 0, 0, 0, 0, 0.32671092103591043, 0, 0.6732890789640219, 32, 1, 0, 1, 0, 1, 128, 1.9047263766225327e-16, 47.99999999756032, 1], [0, 0, 0, 0, 0, 0, 0.2927021633626316, 0, 0.7072978366373681, 11, 1, 0, 1, 0, 1, 128, 1.214306433183765e-18, 48.0, 1], [0, 0, 0, 0, 0, 0, 0.3114905973318029, 0, 0.6885094026681969, 32, 1, 1, 1, 1, 1, 128, 0.1, 48.0, 0], [0, 0, 0, 0.3254515851204577, 0, 0, 0, 0, 0.6745484148795421, 32, 1, 0, 1, 0, 1, 128, 0.09999999999999978, 48.0, 1], [0, 0, 0.29944514697721675, 0, 0, 0, 0, 0, 0.700554853022783, 32, 1, 0, 1, 0, 1, 128, 0.09999999999999998, 48.0, 1], [0, 0.30027820987571496, 0, 0, 0, 0, 0, 0, 0.6997217901242851, 32, 1, 0, 1, 1, 1, 128, 0.07302511996015221, 47.99999999999999, 0], [0, 0, 0, 0, 0, 0.27548881220847404, 0, 0, 0.7245111877915259, 32, 1, 1, 1, 1, 1, 128, 0.0, 48.0, 0], [0, 0, 0, 0.2752949779839317, 0, 0, 0, 0, 0.7247050220160686, 32, 0, 1, 1, 1, 0, 128, 0.0, 48.0, 1], [0, 0, 0, 0, 0, 0.30513972311654614, 0, 0, 0.6948602768834539, 32, 0, 0, 1, 0, 0, 128, 0.09999999999999948, 48.0, 1], [0, 0.3121010346833111, 0, 0, 0, 0, 0, 0, 0.6878989653166888, 32, 1, 1, 1, 0, 1, 128, 2.4986263158116406e-16, 48.0, 1], [0, 0, 0, 0, 0, 0, 0.2868558078179589, 0, 0.7131441921820413, 32, 1, 0, 0, 0, 1, 128, 0.09999999999999999, 48.0, 1], [0, 0, 0.31169441893427385, 0, 0, 0, 0, 0, 0.6883055810657243, 32, 1, 1, 0, 1, 1, 128, 0.0, 48.0, 0], [0, 0, 0, 0, 0, 0, 0.3048501494026473, 0, 0.695149850597349, 32, 1, 0, 1, 1, 1, 128, 0.0, 47.99999999999999, 0], [0, 0.07358134048272323, 0, 0, 0, 0, 0.23759487920633637, 0, 0.6888237803109405, 32, 1, 1, 1, 1, 1, 128, 0.021164265239932825, 1.480000019073488, 0], [0.2815895894392321, 0, 0, 0, 0, 0, 0, 0, 0.7184104105607677, 32, 1, 1, 1, 1, 1, 128, 0.053139098946126756, 48.0, 0], [0.06461858901735185, 0, 0, 0, 0, 0, 0, 0.2247952582370386, 0.7105861527456094, 32, 1, 0, 1, 1, 1, 128, 0.05763182710689677, 47.99999999999999, 0], [0.1745347296587325, 0, 0, 0, 0, 0, 0.0921106412639042, 0, 0.7333546290773634, 32, 1, 0, 1, 0, 1, 128, 0.05260373633217113, 48.0, 1], [0.1840550813487276, 0, 0.08750473589565086, 0, 0, 0, 0.03269517633341517, 0, 0.6957450064222067, 32, 0, 0, 0, 1, 0, 128, 0.0, 48.0, 0], [0, 0, 0, 0, 0, 0, 0, 0.2841742876237169, 0.7158257123762835, 32, 1, 1, 0, 0, 0, 128, 0.05492337861875808, 48.0, 1], [0.10210761567386255, 0.17484681855521533, 0, 0, 0, 0, 0, 0, 0.7230455657709223, 32, 0, 0, 0, 0, 1, 128, 0.1, 48.0, 1], [0.08754592463809809, 0, 0, 0.2145193493765254, 0, 0, 0, 0, 0.6979347259853759, 32, 1, 1, 1, 1, 1, 128, 0.040253934460248074, 48.0, 0], [0.017780483622968194, 0, 0, 0, 0, 0, 0.2735894229719858, 0, 0.7086300934050458, 32, 1, 1, 1, 1, 1, 128, 0.03361875360744071, 48.0, 0], [0, 0, 0.18337501939550502, 0, 0, 0, 0.025014999161957395, 0.08568098030512573, 0.7044938601780436, 32, 1, 0, 1, 1, 1, 128, 0.0471216238647985, 47.99999999999999, 0], [0.09035202120075965, 0, 0, 0, 0, 0.21436449270506638, 0, 0, 0.6952834860941739, 32, 1, 0, 1, 0, 1, 128, 0.030469006777099267, 47.99999999999999, 0], [0.04599043348536082, 0, 0.04716554429276277, 0.0976456109436094, 0, 0, 0.10249524509477999, 0, 0.6978989824027383, 32, 1, 1, 1, 0, 1, 128, 0.051224866616346815, 47.99999999999999, 0], [0.1022241331968622, 0, 0, 0, 0, 0.2014672731728767, 0, 0, 0.696308593630261, 32, 1, 0, 1, 1, 1, 128, 0.04603278053563914, 48.0, 1]]], 'random_full_train_performance': [0.63, 0.63, 0.546, 0.6405, 0.651, 0.5565000000000001, 0.5984999999999999, 0.63, 0.6194999999999999, 0.5984999999999999, 0.5880000000000001, 0.6615000000000001, 0.5775000000000001, 0.546, 0.651, 0.63, 0.63, 0.5984999999999999, 0.63, 0.6825000000000001, 0.6615000000000001, 0.5775000000000001, 0.6194999999999999, 0.651, 0.63, 0.5880000000000001, 0.6615000000000001, 0.609, 0.672, 0.5880000000000001]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.054 MB of 0.054 MB uploadedwandb: \ 0.054 MB of 0.054 MB uploadedwandb: | 0.054 MB of 0.054 MB uploadedwandb: / 0.054 MB of 0.081 MB uploadedwandb: - 0.054 MB of 2.481 MB uploadedwandb: \ 0.893 MB of 2.481 MB uploadedwandb: | 2.481 MB of 2.481 MB uploadedwandb: / 2.481 MB of 2.481 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÅ‚ñÉ‚ñÅ
wandb:            eval/runtime ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ
wandb: eval/samples_per_second ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ
wandb:   eval/steps_per_second ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÉ
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÖ‚ñà‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÉ
wandb:         train/grad_norm ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÖ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñá‚ñÖ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÉ
wandb:              train/loss ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.17194
wandb:             eval/runtime 7.4804
wandb:  eval/samples_per_second 133.55
wandb:    eval/steps_per_second 8.422
wandb:               total_flos 7.570437241503744e+16
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 0.19535
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.1985
wandb:               train_loss 0.50816
wandb:            train_runtime 418.7387
wandb: train_samples_per_second 23.879
wandb:   train_steps_per_second 1.493
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/2pqalfpx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260101_073129-2pqalfpx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

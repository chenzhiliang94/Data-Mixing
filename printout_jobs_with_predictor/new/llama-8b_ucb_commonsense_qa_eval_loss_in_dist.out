2026-01-02 05:11:50.163191: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-02 05:11:50.191144: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-02 05:11:50.191190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-02 05:11:50.191908: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-02 05:11:50.196063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-02 05:11:51.070967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
command-line args:  {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'commonsense_qa', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_commonsense_qa_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}
current eval task:  ['commonsense_qa']
evaluation tasks and weights:  {'commonsense_qa': (1.0, 'acc,none')}
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
commonsense_qa
evaluation dataset:
data domain:  commonsense_qa  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/commonsense_qa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 17, 0, 1, 0, 1, 1, 74, 0.06944379333880846, 10, 0]
Checking history sample input_X_between_0_1:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 0.53125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.578125, 0.6944379333880846, 0.20833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -0.897139310836792
Checking history sample input_X:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 18, 0, 0, 1, 0, 0, 88, 0.07338847024538249, 48, 1]
Checking history sample input_X_between_0_1:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 0.5625, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6875, 0.7338847024538249, 1.0, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1220359802246094
Checking history sample input_X:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 27, 1, 1, 0, 1, 0, 35, 0.056205119903528195, 6, 0]
Checking history sample input_X_between_0_1:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 0.84375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2734375, 0.5620511990352819, 0.125, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9592704772949219
Checking history sample input_X:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 22, 0, 0, 0, 0, 1, 14, 0.05785794551060587, 9, 1]
Checking history sample input_X_between_0_1:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 0.6875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.109375, 0.5785794551060587, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0981301069259644
Checking history sample input_X:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 15, 1, 0, 0, 1, 1, 33, 0.046509797776235, 40, 0]
Checking history sample input_X_between_0_1:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 0.46875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2578125, 0.46509797776234996, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9429119229316711
Checking history sample input_X:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 11, 0, 0, 1, 1, 1, 70, 0.005359902393564798, 44, 1]
Checking history sample input_X_between_0_1:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 0.34375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.546875, 0.05359902393564797, 0.9166666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8976500034332275
Checking history sample input_X:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 3, 1, 1, 0, 0, 1, 89, 0.004230716614147934, 21, 1]
Checking history sample input_X_between_0_1:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 0.09375, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6953125, 0.042307166141479335, 0.4375, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0746725797653198
Checking history sample input_X:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 11, 0, 0, 1, 0, 1, 128, 0.02126981773222546, 43, 0]
Checking history sample input_X_between_0_1:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 0.34375, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2126981773222546, 0.8958333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0224553346633911
Checking history sample input_X:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 4, 0, 1, 0, 0, 1, 51, 0.048394033913014715, 30, 0]
Checking history sample input_X_between_0_1:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3984375, 0.4839403391301471, 0.625, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3750804662704468
Checking history sample input_X:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 25, 1, 0, 1, 0, 0, 103, 0.021006146654874183, 41, 0]
Checking history sample input_X_between_0_1:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 0.78125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8046875, 0.2100614665487418, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9797114729881287
Checking history sample input_X:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 30, 0, 0, 0, 1, 0, 30, 0.06473585772462145, 22, 0]
Checking history sample input_X_between_0_1:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.234375, 0.6473585772462145, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8777164220809937
Checking history sample input_X:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 10, 0, 0, 1, 0, 1, 33, 0.053839147652140054, 30, 1]
Checking history sample input_X_between_0_1:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 0.3125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2578125, 0.5383914765214005, 0.625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.424627661705017
Checking history sample input_X:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 27, 0, 1, 1, 1, 0, 91, 0.026257080995186557, 27, 0]
Checking history sample input_X_between_0_1:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 0.84375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7109375, 0.26257080995186555, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.6500163078308105
Checking history sample input_X:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 28, 1, 1, 1, 0, 0, 23, 0.048743516472132736, 35, 1]
Checking history sample input_X_between_0_1:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 0.875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1796875, 0.48743516472132736, 0.7291666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9704539775848389
Checking history sample input_X:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 14, 1, 0, 1, 0, 0, 32, 0.04806841934255027, 48, 0]
Checking history sample input_X_between_0_1:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 0.4375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.48068419342550267, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1657739877700806
Checking history sample input_X:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 19, 0, 0, 1, 1, 0, 22, 0.09470699670511286, 20, 1]
Checking history sample input_X_between_0_1:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 0.59375, 0.0, 0.0, 1.0, 1.0, 0.0, 0.171875, 0.9470699670511286, 0.4166666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1796187162399292
Checking history sample input_X:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 2, 0, 1, 1, 1, 0, 72, 0.07202092774167915, 33, 1]
Checking history sample input_X_between_0_1:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5625, 0.7202092774167914, 0.6875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2933062314987183
Checking history sample input_X:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 15, 1, 0, 1, 0, 1, 69, 0.09577839904714924, 28, 0]
Checking history sample input_X_between_0_1:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 0.46875, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5390625, 0.9577839904714924, 0.5833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0850565433502197
Checking history sample input_X:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 30, 1, 1, 1, 0, 1, 19, 0.07443761643578949, 27, 0]
Checking history sample input_X_between_0_1:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 0.9375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1484375, 0.7443761643578948, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.7435973286628723
Checking history sample input_X:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 12, 1, 0, 0, 1, 1, 19, 0.0774188690692264, 1, 1]
Checking history sample input_X_between_0_1:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1484375, 0.774188690692264, 0.020833333333333332, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1769657135009766
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.1635 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.597945511341095, 0.34575802087783813, 0.6510445475578308, 0.9679980874061584, 0.23734599351882935, 0.9120203852653503, 0.49711841344833374, 0.419142484664917, 0.599330484867096, 0.0567256398499012, 0.7360399961471558, 0.2220090627670288, 0.8421220779418945, 0.5935562252998352, 0.40004855394363403, 0.42206594347953796, 0.08245790004730225, 0.5577285289764404, 0.2625817656517029]  ‚Üí  acq = -0.22525213830031332
X = [0.008335649967193604, 0.1897701621055603, 0.09933590888977051, 0.12939268350601196, 0.9730328917503357, 0.709738552570343, 0.4399731755256653, 0.6977657079696655, 0.388555645942688, 0.6284391283988953, 0.02527099847793579, 0.3151426315307617, 0.255775511264801, 0.7400819659233093, 0.836790144443512, 0.46885883808135986, 0.11531579494476318, 0.5569977760314941, 0.8880566358566284]  ‚Üí  acq = -0.3905593937774726
X = [0.5602282881736755, 0.36697375774383545, 0.26274824142456055, 0.20650535821914673, 0.4102148413658142, 0.24290317296981812, 0.5474700927734375, 0.5423221588134766, 0.7374328374862671, 0.2795489728450775, 0.729978621006012, 0.03231692314147949, 0.6409918665885925, 0.553351879119873, 0.3332101106643677, 0.3740473687648773, 0.5966234803199768, 0.38538286089897156, 0.8659896850585938]  ‚Üí  acq = -0.19925052939911336
X = [0.6417070031166077, 0.6429912447929382, 0.561755895614624, 0.4652157425880432, 0.4850509762763977, 0.5256483554840088, 0.8300477862358093, 0.7100825905799866, 0.6360769867897034, 0.1203613132238388, 0.2775140404701233, 0.8931741714477539, 0.7270696759223938, 0.07220500707626343, 0.1920311450958252, 0.6416401267051697, 0.6263946294784546, 0.8261346817016602, 0.48437565565109253]  ‚Üí  acq = -0.22005379635500555
X = [0.5448372960090637, 0.3238586187362671, 0.43338948488235474, 0.482768177986145, 0.12664592266082764, 0.3881513476371765, 0.33303624391555786, 0.31841450929641724, 0.5839141607284546, 0.684761643409729, 0.022298038005828857, 0.6753737926483154, 0.18050718307495117, 0.456119179725647, 0.49149930477142334, 0.3433118164539337, 0.40275871753692627, 0.6916321516036987, 0.1015552282333374]  ‚Üí  acq = -0.22373850477710444
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.7400, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.2600, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 4.85722573273506e-18, 48.0, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.7400, dtype=torch.float64), tensor(5.8876e-17, dtype=torch.float64), tensor(3.1587e-17, dtype=torch.float64), tensor(1.2859e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.9066e-17, dtype=torch.float64), tensor(3.8470e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2600, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(4.8572e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.74
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.26

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (4.85722573273506e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  4.85722573273506e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: alfred-leong (alfred-leong-national-university-of-singapore). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.23.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /home/alfred/Data-Mixing/wandb/run-20260102_051345-917b72te
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trainer_output
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: üöÄ View run at https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/917b72te
{'loss': 2.9027, 'grad_norm': 0.924537718296051, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1955196857452393, 'eval_runtime': 4.9882, 'eval_samples_per_second': 200.473, 'eval_steps_per_second': 12.63, 'epoch': 0.04}
{'loss': 1.0087, 'grad_norm': 0.5370752215385437, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9343979358673096, 'eval_runtime': 4.9074, 'eval_samples_per_second': 203.775, 'eval_steps_per_second': 12.838, 'epoch': 0.08}
{'loss': 0.8957, 'grad_norm': 0.22736480832099915, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9027766585350037, 'eval_runtime': 4.9071, 'eval_samples_per_second': 203.788, 'eval_steps_per_second': 12.839, 'epoch': 0.12}
{'loss': 0.8859, 'grad_norm': 0.22303946316242218, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8852970004081726, 'eval_runtime': 4.9012, 'eval_samples_per_second': 204.03, 'eval_steps_per_second': 12.854, 'epoch': 0.16}
{'loss': 0.8664, 'grad_norm': 0.21903546154499054, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8776541352272034, 'eval_runtime': 4.9116, 'eval_samples_per_second': 203.598, 'eval_steps_per_second': 12.827, 'epoch': 0.2}
{'loss': 0.8241, 'grad_norm': 0.2133713662624359, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8700538277626038, 'eval_runtime': 4.9263, 'eval_samples_per_second': 202.993, 'eval_steps_per_second': 12.789, 'epoch': 0.24}
{'loss': 0.8408, 'grad_norm': 0.2462616115808487, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8645870089530945, 'eval_runtime': 4.9408, 'eval_samples_per_second': 202.396, 'eval_steps_per_second': 12.751, 'epoch': 0.28}
{'loss': 0.8291, 'grad_norm': 0.24049775302410126, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8629255294799805, 'eval_runtime': 4.9422, 'eval_samples_per_second': 202.339, 'eval_steps_per_second': 12.747, 'epoch': 0.32}
{'loss': 0.818, 'grad_norm': 0.1875712275505066, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8562204837799072, 'eval_runtime': 4.9484, 'eval_samples_per_second': 202.087, 'eval_steps_per_second': 12.731, 'epoch': 0.36}
{'loss': 0.8122, 'grad_norm': 0.21968954801559448, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8526540398597717, 'eval_runtime': 4.9526, 'eval_samples_per_second': 201.913, 'eval_steps_per_second': 12.721, 'epoch': 0.4}
{'loss': 0.8135, 'grad_norm': 0.20827147364616394, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8497072458267212, 'eval_runtime': 4.9529, 'eval_samples_per_second': 201.904, 'eval_steps_per_second': 12.72, 'epoch': 0.44}
{'loss': 0.8069, 'grad_norm': 0.2282596081495285, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8490513563156128, 'eval_runtime': 4.9845, 'eval_samples_per_second': 200.621, 'eval_steps_per_second': 12.639, 'epoch': 0.48}
{'loss': 0.809, 'grad_norm': 0.22611945867538452, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8413106799125671, 'eval_runtime': 5.0087, 'eval_samples_per_second': 199.651, 'eval_steps_per_second': 12.578, 'epoch': 0.52}
{'loss': 0.8119, 'grad_norm': 0.23326008021831512, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8403187394142151, 'eval_runtime': 4.9836, 'eval_samples_per_second': 200.656, 'eval_steps_per_second': 12.641, 'epoch': 0.56}
{'loss': 0.7999, 'grad_norm': 0.2912553548812866, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8382887244224548, 'eval_runtime': 4.9698, 'eval_samples_per_second': 201.215, 'eval_steps_per_second': 12.677, 'epoch': 0.6}
{'loss': 0.7828, 'grad_norm': 0.24470852315425873, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8343936204910278, 'eval_runtime': 4.9592, 'eval_samples_per_second': 201.644, 'eval_steps_per_second': 12.704, 'epoch': 0.64}
{'loss': 0.7723, 'grad_norm': 0.24675367772579193, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8315783739089966, 'eval_runtime': 4.9413, 'eval_samples_per_second': 202.378, 'eval_steps_per_second': 12.75, 'epoch': 0.68}
{'loss': 0.7733, 'grad_norm': 0.22087767720222473, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8296275734901428, 'eval_runtime': 4.9428, 'eval_samples_per_second': 202.313, 'eval_steps_per_second': 12.746, 'epoch': 0.72}
{'loss': 0.7693, 'grad_norm': 0.27651533484458923, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.831113874912262, 'eval_runtime': 4.9452, 'eval_samples_per_second': 202.215, 'eval_steps_per_second': 12.74, 'epoch': 0.76}
{'loss': 0.7518, 'grad_norm': 0.2605648934841156, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8264907598495483, 'eval_runtime': 4.9488, 'eval_samples_per_second': 202.067, 'eval_steps_per_second': 12.73, 'epoch': 0.8}
{'loss': 0.7849, 'grad_norm': 0.243662491440773, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.825380265712738, 'eval_runtime': 4.9473, 'eval_samples_per_second': 202.131, 'eval_steps_per_second': 12.734, 'epoch': 0.84}
{'loss': 0.7539, 'grad_norm': 0.2532280683517456, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8248560428619385, 'eval_runtime': 4.9412, 'eval_samples_per_second': 202.38, 'eval_steps_per_second': 12.75, 'epoch': 0.88}
{'loss': 0.7505, 'grad_norm': 0.2276444286108017, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8231193423271179, 'eval_runtime': 4.9475, 'eval_samples_per_second': 202.123, 'eval_steps_per_second': 12.734, 'epoch': 0.92}
{'loss': 0.746, 'grad_norm': 0.22025592625141144, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8224993944168091, 'eval_runtime': 4.944, 'eval_samples_per_second': 202.267, 'eval_steps_per_second': 12.743, 'epoch': 0.96}
{'loss': 0.7293, 'grad_norm': 0.2644668221473694, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8220756649971008, 'eval_runtime': 4.9452, 'eval_samples_per_second': 202.216, 'eval_steps_per_second': 12.74, 'epoch': 1.0}
{'train_runtime': 275.614, 'train_samples_per_second': 36.279, 'train_steps_per_second': 2.268, 'train_loss': 0.8935537109375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1955196857452393, 0.9343979358673096, 0.9027766585350037, 0.8852970004081726, 0.8776541352272034, 0.8700538277626038, 0.8645870089530945, 0.8629255294799805, 0.8562204837799072, 0.8526540398597717, 0.8497072458267212, 0.8490513563156128, 0.8413106799125671, 0.8403187394142151, 0.8382887244224548, 0.8343936204910278, 0.8315783739089966, 0.8296275734901428, 0.831113874912262, 0.8264907598495483, 0.825380265712738, 0.8248560428619385, 0.8231193423271179, 0.8224993944168091, 0.8220756649971008], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1955196857452393, 0.9343979358673096, 0.9027766585350037, 0.8852970004081726, 0.8776541352272034, 0.8700538277626038, 0.8645870089530945, 0.8629255294799805, 0.8562204837799072, 0.8526540398597717, 0.8497072458267212, 0.8490513563156128, 0.8413106799125671, 0.8403187394142151, 0.8382887244224548, 0.8343936204910278, 0.8315783739089966, 0.8296275734901428, 0.831113874912262, 0.8264907598495483, 0.825380265712738, 0.8248560428619385, 0.8231193423271179, 0.8224993944168091, 0.8220756649971008]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2131680250167847
current iteration best possible eval_loss (full train run):  -0.8220756649971008
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4091 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.5401323993772573
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.5242353867023014
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.5242355157083392
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.5748488510550787
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.5240231980075524
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2888, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.7112, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.09999999999999598, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2888, dtype=torch.float64), tensor(2.6234e-13, dtype=torch.float64), tensor(2.0016e-15, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0911e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.1509e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7112, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.289
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.711

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.09999999999999598,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.09999999999999598
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0803, 'grad_norm': 7.610352516174316, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.9062209129333496, 'eval_runtime': 4.4039, 'eval_samples_per_second': 227.07, 'eval_steps_per_second': 14.305, 'epoch': 0.04}
{'loss': 2.5051, 'grad_norm': 4.225182056427002, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.190112590789795, 'eval_runtime': 4.4292, 'eval_samples_per_second': 225.774, 'eval_steps_per_second': 14.224, 'epoch': 0.08}
{'loss': 1.5209, 'grad_norm': 2.5402755737304688, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5103362798690796, 'eval_runtime': 4.4448, 'eval_samples_per_second': 224.981, 'eval_steps_per_second': 14.174, 'epoch': 0.12}
{'loss': 1.2247, 'grad_norm': 2.516329050064087, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.2564895153045654, 'eval_runtime': 4.4507, 'eval_samples_per_second': 224.682, 'eval_steps_per_second': 14.155, 'epoch': 0.16}
{'loss': 1.0575, 'grad_norm': 2.1882972717285156, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.1627520322799683, 'eval_runtime': 4.4412, 'eval_samples_per_second': 225.166, 'eval_steps_per_second': 14.185, 'epoch': 0.2}
{'loss': 0.9947, 'grad_norm': 2.3835976123809814, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1296645402908325, 'eval_runtime': 4.4487, 'eval_samples_per_second': 224.783, 'eval_steps_per_second': 14.161, 'epoch': 0.24}
{'loss': 0.999, 'grad_norm': 1.9578224420547485, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1138147115707397, 'eval_runtime': 4.4517, 'eval_samples_per_second': 224.631, 'eval_steps_per_second': 14.152, 'epoch': 0.28}
{'loss': 0.9941, 'grad_norm': 2.0770795345306396, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.104116439819336, 'eval_runtime': 4.4707, 'eval_samples_per_second': 223.68, 'eval_steps_per_second': 14.092, 'epoch': 0.32}
{'loss': 0.9941, 'grad_norm': 2.2043700218200684, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0917054414749146, 'eval_runtime': 4.4617, 'eval_samples_per_second': 224.132, 'eval_steps_per_second': 14.12, 'epoch': 0.36}
{'loss': 0.986, 'grad_norm': 2.6734066009521484, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0880322456359863, 'eval_runtime': 4.4593, 'eval_samples_per_second': 224.249, 'eval_steps_per_second': 14.128, 'epoch': 0.4}
{'loss': 0.9666, 'grad_norm': 2.250009536743164, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0901230573654175, 'eval_runtime': 4.4693, 'eval_samples_per_second': 223.749, 'eval_steps_per_second': 14.096, 'epoch': 0.44}
{'loss': 0.9747, 'grad_norm': 2.062309980392456, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0799421072006226, 'eval_runtime': 4.4733, 'eval_samples_per_second': 223.546, 'eval_steps_per_second': 14.083, 'epoch': 0.48}
{'loss': 0.9911, 'grad_norm': 2.561279535293579, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.0733166933059692, 'eval_runtime': 4.4773, 'eval_samples_per_second': 223.348, 'eval_steps_per_second': 14.071, 'epoch': 0.52}
{'loss': 0.9384, 'grad_norm': 2.515504837036133, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0734853744506836, 'eval_runtime': 4.4786, 'eval_samples_per_second': 223.283, 'eval_steps_per_second': 14.067, 'epoch': 0.56}
{'loss': 0.9668, 'grad_norm': 1.9308663606643677, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.074383020401001, 'eval_runtime': 4.4744, 'eval_samples_per_second': 223.495, 'eval_steps_per_second': 14.08, 'epoch': 0.6}
{'loss': 0.9513, 'grad_norm': 2.384841203689575, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0697251558303833, 'eval_runtime': 4.4814, 'eval_samples_per_second': 223.142, 'eval_steps_per_second': 14.058, 'epoch': 0.64}
{'loss': 0.9735, 'grad_norm': 2.1738638877868652, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0634280443191528, 'eval_runtime': 4.4722, 'eval_samples_per_second': 223.603, 'eval_steps_per_second': 14.087, 'epoch': 0.68}
{'loss': 0.9698, 'grad_norm': 2.1558549404144287, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0626044273376465, 'eval_runtime': 4.4808, 'eval_samples_per_second': 223.173, 'eval_steps_per_second': 14.06, 'epoch': 0.72}
{'loss': 0.9827, 'grad_norm': 1.9496623277664185, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0613582134246826, 'eval_runtime': 4.4787, 'eval_samples_per_second': 223.278, 'eval_steps_per_second': 14.066, 'epoch': 0.76}
{'loss': 0.9384, 'grad_norm': 2.518080949783325, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0605043172836304, 'eval_runtime': 4.4814, 'eval_samples_per_second': 223.143, 'eval_steps_per_second': 14.058, 'epoch': 0.8}
{'loss': 0.9418, 'grad_norm': 2.215836524963379, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0591650009155273, 'eval_runtime': 4.4739, 'eval_samples_per_second': 223.52, 'eval_steps_per_second': 14.082, 'epoch': 0.84}
{'loss': 0.9439, 'grad_norm': 2.532196521759033, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0589348077774048, 'eval_runtime': 4.481, 'eval_samples_per_second': 223.162, 'eval_steps_per_second': 14.059, 'epoch': 0.88}
{'loss': 0.9397, 'grad_norm': 2.1228582859039307, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.062368392944336, 'eval_runtime': 4.4728, 'eval_samples_per_second': 223.573, 'eval_steps_per_second': 14.085, 'epoch': 0.92}
{'loss': 0.968, 'grad_norm': 2.2080814838409424, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.060231328010559, 'eval_runtime': 4.4842, 'eval_samples_per_second': 223.008, 'eval_steps_per_second': 14.049, 'epoch': 0.96}
{'loss': 0.9655, 'grad_norm': 2.184443235397339, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0593425035476685, 'eval_runtime': 4.4773, 'eval_samples_per_second': 223.349, 'eval_steps_per_second': 14.071, 'epoch': 1.0}
{'train_runtime': 179.6614, 'train_samples_per_second': 55.655, 'train_steps_per_second': 3.479, 'train_loss': 1.1907412719726562, 'epoch': 1.0}
train_results:  {'eval_loss': [3.9062209129333496, 2.190112590789795, 1.5103362798690796, 1.2564895153045654, 1.1627520322799683, 1.1296645402908325, 1.1138147115707397, 1.104116439819336, 1.0917054414749146, 1.0880322456359863, 1.0901230573654175, 1.0799421072006226, 1.0733166933059692, 1.0734853744506836, 1.074383020401001, 1.0697251558303833, 1.0634280443191528, 1.0626044273376465, 1.0613582134246826, 1.0605043172836304, 1.0591650009155273, 1.0589348077774048, 1.062368392944336, 1.060231328010559, 1.0593425035476685], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.9062209129333496, 2.190112590789795, 1.5103362798690796, 1.2564895153045654, 1.1627520322799683, 1.1296645402908325, 1.1138147115707397, 1.104116439819336, 1.0917054414749146, 1.0880322456359863, 1.0901230573654175, 1.0799421072006226, 1.0733166933059692, 1.0734853744506836, 1.074383020401001, 1.0697251558303833, 1.0634280443191528, 1.0626044273376465, 1.0613582134246826, 1.0605043172836304, 1.0591650009155273, 1.0589348077774048, 1.062368392944336, 1.060231328010559, 1.0593425035476685]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5157928466796875
current iteration best possible eval_loss (full train run):  -1.0593425035476685
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7275 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.7190361923967736
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.643956854439116
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.6442598940538785
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.6434834465596814
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.6439782864867565
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2129, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.4173, dtype=torch.float64), tensor(0.3698, dtype=torch.float64), 29, 0, 0, 1, 1, 1, 128, 1.0408340855860844e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2129, dtype=torch.float64), tensor(4.3790e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.6133e-18, dtype=torch.float64), tensor(6.7862e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4173, dtype=torch.float64), tensor(0.3698, dtype=torch.float64), tensor(0.9145, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0408e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.213
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.417
  arc_challenge: 0.37

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.0408340855860844e-18,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  1.0408340855860844e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,258,752 || all params: 8,235,520,000 || trainable%: 2.4924
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5153, 'grad_norm': 0.5094234943389893, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.221386432647705, 'eval_runtime': 5.297, 'eval_samples_per_second': 188.787, 'eval_steps_per_second': 11.894, 'epoch': 0.04}
{'loss': 1.1175, 'grad_norm': 0.2055455893278122, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9769501090049744, 'eval_runtime': 5.2897, 'eval_samples_per_second': 189.047, 'eval_steps_per_second': 11.91, 'epoch': 0.08}
{'loss': 1.0697, 'grad_norm': 0.3450262248516083, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9306057095527649, 'eval_runtime': 5.2994, 'eval_samples_per_second': 188.7, 'eval_steps_per_second': 11.888, 'epoch': 0.12}
{'loss': 1.033, 'grad_norm': 0.20420010387897491, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9312692880630493, 'eval_runtime': 5.3162, 'eval_samples_per_second': 188.106, 'eval_steps_per_second': 11.851, 'epoch': 0.16}
{'loss': 1.0349, 'grad_norm': 0.215250164270401, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9146891832351685, 'eval_runtime': 5.3209, 'eval_samples_per_second': 187.939, 'eval_steps_per_second': 11.84, 'epoch': 0.2}
{'loss': 1.0088, 'grad_norm': 0.21363921463489532, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9174768328666687, 'eval_runtime': 5.3247, 'eval_samples_per_second': 187.802, 'eval_steps_per_second': 11.832, 'epoch': 0.24}
{'loss': 0.9638, 'grad_norm': 0.19560211896896362, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9030808210372925, 'eval_runtime': 5.324, 'eval_samples_per_second': 187.83, 'eval_steps_per_second': 11.833, 'epoch': 0.28}
{'loss': 1.0341, 'grad_norm': 0.2280648648738861, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9061535000801086, 'eval_runtime': 5.3264, 'eval_samples_per_second': 187.745, 'eval_steps_per_second': 11.828, 'epoch': 0.32}
{'loss': 0.9328, 'grad_norm': 0.23652586340904236, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9007229804992676, 'eval_runtime': 5.3271, 'eval_samples_per_second': 187.719, 'eval_steps_per_second': 11.826, 'epoch': 0.36}
{'loss': 0.9608, 'grad_norm': 0.2495584636926651, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.892439067363739, 'eval_runtime': 5.3287, 'eval_samples_per_second': 187.665, 'eval_steps_per_second': 11.823, 'epoch': 0.4}
{'loss': 0.9019, 'grad_norm': 0.2218674123287201, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8974299430847168, 'eval_runtime': 5.334, 'eval_samples_per_second': 187.476, 'eval_steps_per_second': 11.811, 'epoch': 0.44}
{'loss': 0.9389, 'grad_norm': 0.23656344413757324, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8852420449256897, 'eval_runtime': 5.3368, 'eval_samples_per_second': 187.377, 'eval_steps_per_second': 11.805, 'epoch': 0.48}
{'loss': 0.9242, 'grad_norm': 0.2613232731819153, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8826578855514526, 'eval_runtime': 5.3327, 'eval_samples_per_second': 187.523, 'eval_steps_per_second': 11.814, 'epoch': 0.52}
{'loss': 0.8705, 'grad_norm': 0.2663384974002838, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8817458748817444, 'eval_runtime': 5.3314, 'eval_samples_per_second': 187.567, 'eval_steps_per_second': 11.817, 'epoch': 0.56}
{'loss': 0.8581, 'grad_norm': 0.2890455424785614, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.880305290222168, 'eval_runtime': 5.3356, 'eval_samples_per_second': 187.421, 'eval_steps_per_second': 11.808, 'epoch': 0.6}
{'loss': 0.8837, 'grad_norm': 0.36223337054252625, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.875572919845581, 'eval_runtime': 5.3376, 'eval_samples_per_second': 187.35, 'eval_steps_per_second': 11.803, 'epoch': 0.64}
{'loss': 0.8894, 'grad_norm': 0.2654518187046051, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8713098168373108, 'eval_runtime': 5.3388, 'eval_samples_per_second': 187.307, 'eval_steps_per_second': 11.8, 'epoch': 0.68}
{'loss': 0.8462, 'grad_norm': 0.2633306086063385, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8751286268234253, 'eval_runtime': 5.332, 'eval_samples_per_second': 187.547, 'eval_steps_per_second': 11.815, 'epoch': 0.72}
{'loss': 0.8817, 'grad_norm': 0.24379713833332062, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8667119145393372, 'eval_runtime': 5.349, 'eval_samples_per_second': 186.951, 'eval_steps_per_second': 11.778, 'epoch': 0.76}
{'loss': 0.8544, 'grad_norm': 0.3069746792316437, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.866818368434906, 'eval_runtime': 5.3427, 'eval_samples_per_second': 187.173, 'eval_steps_per_second': 11.792, 'epoch': 0.8}
{'loss': 0.7669, 'grad_norm': 0.2830893099308014, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8649489879608154, 'eval_runtime': 5.3542, 'eval_samples_per_second': 186.768, 'eval_steps_per_second': 11.766, 'epoch': 0.84}
{'loss': 0.8299, 'grad_norm': 0.2581326365470886, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8642183542251587, 'eval_runtime': 5.3359, 'eval_samples_per_second': 187.411, 'eval_steps_per_second': 11.807, 'epoch': 0.88}
{'loss': 0.7903, 'grad_norm': 0.276928186416626, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8635539412498474, 'eval_runtime': 5.3368, 'eval_samples_per_second': 187.38, 'eval_steps_per_second': 11.805, 'epoch': 0.92}
{'loss': 0.7645, 'grad_norm': 0.2634354829788208, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8625820875167847, 'eval_runtime': 5.33, 'eval_samples_per_second': 187.618, 'eval_steps_per_second': 11.82, 'epoch': 0.96}
{'loss': 0.8216, 'grad_norm': 0.3907586634159088, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.862255871295929, 'eval_runtime': 5.3375, 'eval_samples_per_second': 187.353, 'eval_steps_per_second': 11.803, 'epoch': 1.0}
{'train_runtime': 340.8012, 'train_samples_per_second': 29.337, 'train_steps_per_second': 1.834, 'train_loss': 0.9797119842529297, 'epoch': 1.0}
train_results:  {'eval_loss': [1.221386432647705, 0.9769501090049744, 0.9306057095527649, 0.9312692880630493, 0.9146891832351685, 0.9174768328666687, 0.9030808210372925, 0.9061535000801086, 0.9007229804992676, 0.892439067363739, 0.8974299430847168, 0.8852420449256897, 0.8826578855514526, 0.8817458748817444, 0.880305290222168, 0.875572919845581, 0.8713098168373108, 0.8751286268234253, 0.8667119145393372, 0.866818368434906, 0.8649489879608154, 0.8642183542251587, 0.8635539412498474, 0.8625820875167847, 0.862255871295929], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.221386432647705, 0.9769501090049744, 0.9306057095527649, 0.9312692880630493, 0.9146891832351685, 0.9174768328666687, 0.9030808210372925, 0.9061535000801086, 0.9007229804992676, 0.892439067363739, 0.8974299430847168, 0.8852420449256897, 0.8826578855514526, 0.8817458748817444, 0.880305290222168, 0.875572919845581, 0.8713098168373108, 0.8751286268234253, 0.8667119145393372, 0.866818368434906, 0.8649489879608154, 0.8642183542251587, 0.8635539412498474, 0.8625820875167847, 0.862255871295929]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2578315734863281
current iteration best possible eval_loss (full train run):  -0.862255871295929
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5281 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.7201682290818489
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.8330619002321711
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.8265712477741828
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.833006276035098
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.8329900788184036
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3120, dtype=torch.float64), 0, 0, 0, tensor(0.1604, dtype=torch.float64), tensor(0.2278, dtype=torch.float64), 0, 0, tensor(0.2998, dtype=torch.float64), 32, 0, 0, 0, 1, 1, 128, 0.0, 22.5376784297763, 0]
normalized proposed parameters for next round by BO: [tensor(0.3120, dtype=torch.float64), tensor(2.5003e-17, dtype=torch.float64), tensor(3.4344e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1604, dtype=torch.float64), tensor(0.2278, dtype=torch.float64), tensor(1.3348e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2998, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4695, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.312
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.16
  truthfulqa_gen: 0.228
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.3

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 1],)
  lora_alpha: (22.5376784297763,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  22.5376784297763
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 150,994,944 || all params: 8,181,256,192 || trainable%: 1.8456
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3604, 'grad_norm': 0.7914701104164124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.501569390296936, 'eval_runtime': 5.105, 'eval_samples_per_second': 195.888, 'eval_steps_per_second': 12.341, 'epoch': 0.04}
{'loss': 1.1395, 'grad_norm': 0.5815037488937378, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0115007162094116, 'eval_runtime': 5.0875, 'eval_samples_per_second': 196.561, 'eval_steps_per_second': 12.383, 'epoch': 0.08}
{'loss': 0.9387, 'grad_norm': 0.30618077516555786, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9321593642234802, 'eval_runtime': 5.0915, 'eval_samples_per_second': 196.405, 'eval_steps_per_second': 12.374, 'epoch': 0.12}
{'loss': 0.8836, 'grad_norm': 0.16390179097652435, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9251104593276978, 'eval_runtime': 5.1, 'eval_samples_per_second': 196.08, 'eval_steps_per_second': 12.353, 'epoch': 0.16}
{'loss': 0.8937, 'grad_norm': 0.1603938192129135, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9157029986381531, 'eval_runtime': 5.1007, 'eval_samples_per_second': 196.052, 'eval_steps_per_second': 12.351, 'epoch': 0.2}
{'loss': 0.8323, 'grad_norm': 0.1484316736459732, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9131193161010742, 'eval_runtime': 5.1243, 'eval_samples_per_second': 195.15, 'eval_steps_per_second': 12.294, 'epoch': 0.24}
{'loss': 0.8264, 'grad_norm': 0.1885586827993393, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8990267515182495, 'eval_runtime': 5.115, 'eval_samples_per_second': 195.502, 'eval_steps_per_second': 12.317, 'epoch': 0.28}
{'loss': 0.8366, 'grad_norm': 0.1653577834367752, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8970260620117188, 'eval_runtime': 5.1151, 'eval_samples_per_second': 195.498, 'eval_steps_per_second': 12.316, 'epoch': 0.32}
{'loss': 0.8179, 'grad_norm': 0.14306902885437012, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8926799297332764, 'eval_runtime': 5.1202, 'eval_samples_per_second': 195.307, 'eval_steps_per_second': 12.304, 'epoch': 0.36}
{'loss': 0.8373, 'grad_norm': 0.15790675580501556, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8822827935218811, 'eval_runtime': 5.1278, 'eval_samples_per_second': 195.017, 'eval_steps_per_second': 12.286, 'epoch': 0.4}
{'loss': 0.8046, 'grad_norm': 0.18203133344650269, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8818606734275818, 'eval_runtime': 5.1251, 'eval_samples_per_second': 195.119, 'eval_steps_per_second': 12.293, 'epoch': 0.44}
{'loss': 0.7925, 'grad_norm': 0.18959860503673553, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.880064845085144, 'eval_runtime': 5.121, 'eval_samples_per_second': 195.276, 'eval_steps_per_second': 12.302, 'epoch': 0.48}
{'loss': 0.7682, 'grad_norm': 0.20785687863826752, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8775200843811035, 'eval_runtime': 5.1229, 'eval_samples_per_second': 195.204, 'eval_steps_per_second': 12.298, 'epoch': 0.52}
{'loss': 0.7452, 'grad_norm': 0.23925666511058807, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8799472451210022, 'eval_runtime': 5.1246, 'eval_samples_per_second': 195.137, 'eval_steps_per_second': 12.294, 'epoch': 0.56}
{'loss': 0.7606, 'grad_norm': 0.23259751498699188, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8743515014648438, 'eval_runtime': 5.1217, 'eval_samples_per_second': 195.249, 'eval_steps_per_second': 12.301, 'epoch': 0.6}
{'loss': 0.748, 'grad_norm': 0.2463737428188324, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8691791892051697, 'eval_runtime': 5.1176, 'eval_samples_per_second': 195.404, 'eval_steps_per_second': 12.31, 'epoch': 0.64}
{'loss': 0.7154, 'grad_norm': 0.24050085246562958, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8696181178092957, 'eval_runtime': 5.126, 'eval_samples_per_second': 195.084, 'eval_steps_per_second': 12.29, 'epoch': 0.68}
{'loss': 0.6965, 'grad_norm': 0.2560162842273712, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8687072992324829, 'eval_runtime': 5.1576, 'eval_samples_per_second': 193.887, 'eval_steps_per_second': 12.215, 'epoch': 0.72}
{'loss': 0.7209, 'grad_norm': 0.26820218563079834, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8673462867736816, 'eval_runtime': 5.1347, 'eval_samples_per_second': 194.752, 'eval_steps_per_second': 12.269, 'epoch': 0.76}
{'loss': 0.6891, 'grad_norm': 0.2527685761451721, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.864468514919281, 'eval_runtime': 5.1718, 'eval_samples_per_second': 193.357, 'eval_steps_per_second': 12.181, 'epoch': 0.8}
{'loss': 0.6692, 'grad_norm': 0.23398666083812714, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8665139079093933, 'eval_runtime': 5.1331, 'eval_samples_per_second': 194.813, 'eval_steps_per_second': 12.273, 'epoch': 0.84}
{'loss': 0.6755, 'grad_norm': 0.2828570008277893, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8649111986160278, 'eval_runtime': 5.1252, 'eval_samples_per_second': 195.114, 'eval_steps_per_second': 12.292, 'epoch': 0.88}
{'loss': 0.6744, 'grad_norm': 0.23928815126419067, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8610495328903198, 'eval_runtime': 5.1181, 'eval_samples_per_second': 195.384, 'eval_steps_per_second': 12.309, 'epoch': 0.92}
{'loss': 0.6672, 'grad_norm': 0.2586629092693329, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8613828420639038, 'eval_runtime': 5.1274, 'eval_samples_per_second': 195.029, 'eval_steps_per_second': 12.287, 'epoch': 0.96}
{'loss': 0.656, 'grad_norm': 0.3200858235359192, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8608670234680176, 'eval_runtime': 5.1369, 'eval_samples_per_second': 194.671, 'eval_steps_per_second': 12.264, 'epoch': 1.0}
{'train_runtime': 272.7512, 'train_samples_per_second': 36.656, 'train_steps_per_second': 2.291, 'train_loss': 0.8859737518310546, 'epoch': 1.0}
train_results:  {'eval_loss': [1.501569390296936, 1.0115007162094116, 0.9321593642234802, 0.9251104593276978, 0.9157029986381531, 0.9131193161010742, 0.8990267515182495, 0.8970260620117188, 0.8926799297332764, 0.8822827935218811, 0.8818606734275818, 0.880064845085144, 0.8775200843811035, 0.8799472451210022, 0.8743515014648438, 0.8691791892051697, 0.8696181178092957, 0.8687072992324829, 0.8673462867736816, 0.864468514919281, 0.8665139079093933, 0.8649111986160278, 0.8610495328903198, 0.8613828420639038, 0.8608670234680176], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.501569390296936, 1.0115007162094116, 0.9321593642234802, 0.9251104593276978, 0.9157029986381531, 0.9131193161010742, 0.8990267515182495, 0.8970260620117188, 0.8926799297332764, 0.8822827935218811, 0.8818606734275818, 0.880064845085144, 0.8775200843811035, 0.8799472451210022, 0.8743515014648438, 0.8691791892051697, 0.8696181178092957, 0.8687072992324829, 0.8673462867736816, 0.864468514919281, 0.8665139079093933, 0.8649111986160278, 0.8610495328903198, 0.8613828420639038, 0.8608670234680176]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.109604835510254
current iteration best possible eval_loss (full train run):  -0.8608670234680176
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5272 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.8380356777941864
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.8342694282525638
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.8337567059427798
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.8338209461142778
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.8337960190417611
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3277, dtype=torch.float64), tensor(0.0903, dtype=torch.float64), 0, 0, 0, tensor(0.0111, dtype=torch.float64), tensor(0.0584, dtype=torch.float64), tensor(0.0562, dtype=torch.float64), tensor(0.4563, dtype=torch.float64), 31, 0, 1, 1, 1, 0, 79, 0.05456455613909951, 27.34771376568814, 0]
normalized proposed parameters for next round by BO: [tensor(0.3277, dtype=torch.float64), tensor(0.0903, dtype=torch.float64), tensor(4.5359e-18, dtype=torch.float64), tensor(1.2761e-17, dtype=torch.float64), tensor(8.8572e-18, dtype=torch.float64), tensor(0.0111, dtype=torch.float64), tensor(0.0584, dtype=torch.float64), tensor(0.0562, dtype=torch.float64), tensor(0.4563, dtype=torch.float64), tensor(0.9685, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6139, dtype=torch.float64), tensor(0.5456, dtype=torch.float64), tensor(0.5697, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.328
  gsm8k: 0.09
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.011
  wikitext: 0.058
  mmlu: 0.056
  arc_challenge: 0.456

LoRA Parameters:
  lora_r: (79,)
  lora_dropout: (0.05456455613909951,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (27.34771376568814,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  79
lora dropout:  0.05456455613909951
lora alpha:  27.34771376568814
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 102,818,816 || all params: 8,133,080,064 || trainable%: 1.2642
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6654, 'grad_norm': 0.7612850666046143, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2733542919158936, 'eval_runtime': 5.555, 'eval_samples_per_second': 180.019, 'eval_steps_per_second': 11.341, 'epoch': 0.04}
{'loss': 1.1446, 'grad_norm': 0.5247290730476379, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9800015687942505, 'eval_runtime': 5.5537, 'eval_samples_per_second': 180.059, 'eval_steps_per_second': 11.344, 'epoch': 0.08}
{'loss': 1.0107, 'grad_norm': 0.30666059255599976, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.939439594745636, 'eval_runtime': 5.5497, 'eval_samples_per_second': 180.189, 'eval_steps_per_second': 11.352, 'epoch': 0.12}
{'loss': 0.9557, 'grad_norm': 0.2036377191543579, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.917137861251831, 'eval_runtime': 5.563, 'eval_samples_per_second': 179.758, 'eval_steps_per_second': 11.325, 'epoch': 0.16}
{'loss': 0.9409, 'grad_norm': 0.24249939620494843, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9043334722518921, 'eval_runtime': 5.5964, 'eval_samples_per_second': 178.686, 'eval_steps_per_second': 11.257, 'epoch': 0.2}
{'loss': 0.9266, 'grad_norm': 0.2693365216255188, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9066495299339294, 'eval_runtime': 5.5847, 'eval_samples_per_second': 179.062, 'eval_steps_per_second': 11.281, 'epoch': 0.24}
{'loss': 0.921, 'grad_norm': 0.24556554853916168, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8957403302192688, 'eval_runtime': 5.6164, 'eval_samples_per_second': 178.051, 'eval_steps_per_second': 11.217, 'epoch': 0.28}
{'loss': 0.8653, 'grad_norm': 0.294810950756073, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8941192626953125, 'eval_runtime': 5.5961, 'eval_samples_per_second': 178.696, 'eval_steps_per_second': 11.258, 'epoch': 0.32}
{'loss': 0.8893, 'grad_norm': 0.2744963467121124, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8893401026725769, 'eval_runtime': 5.5973, 'eval_samples_per_second': 178.658, 'eval_steps_per_second': 11.255, 'epoch': 0.36}
{'loss': 0.8686, 'grad_norm': 0.3563325107097626, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.885208249092102, 'eval_runtime': 5.5709, 'eval_samples_per_second': 179.504, 'eval_steps_per_second': 11.309, 'epoch': 0.4}
{'loss': 0.8415, 'grad_norm': 0.26672786474227905, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8761582970619202, 'eval_runtime': 5.5798, 'eval_samples_per_second': 179.218, 'eval_steps_per_second': 11.291, 'epoch': 0.44}
{'loss': 0.8332, 'grad_norm': 0.2779165208339691, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8732466697692871, 'eval_runtime': 5.5786, 'eval_samples_per_second': 179.256, 'eval_steps_per_second': 11.293, 'epoch': 0.48}
{'loss': 0.8014, 'grad_norm': 0.33075764775276184, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8708570003509521, 'eval_runtime': 5.5876, 'eval_samples_per_second': 178.967, 'eval_steps_per_second': 11.275, 'epoch': 0.52}
{'loss': 0.7264, 'grad_norm': 0.2533072233200073, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8697960376739502, 'eval_runtime': 5.5735, 'eval_samples_per_second': 179.421, 'eval_steps_per_second': 11.304, 'epoch': 0.56}
{'loss': 0.798, 'grad_norm': 0.3623632788658142, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.866309404373169, 'eval_runtime': 5.5716, 'eval_samples_per_second': 179.48, 'eval_steps_per_second': 11.307, 'epoch': 0.6}
{'loss': 0.7936, 'grad_norm': 0.3369373381137848, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8631764054298401, 'eval_runtime': 5.5764, 'eval_samples_per_second': 179.327, 'eval_steps_per_second': 11.298, 'epoch': 0.64}
{'loss': 0.7416, 'grad_norm': 0.3382062017917633, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8650627732276917, 'eval_runtime': 5.5724, 'eval_samples_per_second': 179.455, 'eval_steps_per_second': 11.306, 'epoch': 0.68}
{'loss': 0.7175, 'grad_norm': 0.3215630352497101, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8624497652053833, 'eval_runtime': 5.5718, 'eval_samples_per_second': 179.475, 'eval_steps_per_second': 11.307, 'epoch': 0.72}
{'loss': 0.7145, 'grad_norm': 0.37748661637306213, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8606939911842346, 'eval_runtime': 5.5701, 'eval_samples_per_second': 179.529, 'eval_steps_per_second': 11.31, 'epoch': 0.76}
{'loss': 0.738, 'grad_norm': 0.34912246465682983, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8574911952018738, 'eval_runtime': 5.563, 'eval_samples_per_second': 179.76, 'eval_steps_per_second': 11.325, 'epoch': 0.8}
{'loss': 0.7154, 'grad_norm': 0.3588150143623352, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8568830490112305, 'eval_runtime': 5.5637, 'eval_samples_per_second': 179.737, 'eval_steps_per_second': 11.323, 'epoch': 0.84}
{'loss': 0.6556, 'grad_norm': 0.562203586101532, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8544091582298279, 'eval_runtime': 5.5681, 'eval_samples_per_second': 179.594, 'eval_steps_per_second': 11.314, 'epoch': 0.88}
{'loss': 0.6904, 'grad_norm': 0.3664301037788391, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.853507399559021, 'eval_runtime': 5.5644, 'eval_samples_per_second': 179.714, 'eval_steps_per_second': 11.322, 'epoch': 0.92}
{'loss': 0.6673, 'grad_norm': 0.2927998900413513, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8535158634185791, 'eval_runtime': 5.5663, 'eval_samples_per_second': 179.653, 'eval_steps_per_second': 11.318, 'epoch': 0.96}
{'loss': 0.6983, 'grad_norm': 0.4092886447906494, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8525269031524658, 'eval_runtime': 5.582, 'eval_samples_per_second': 179.147, 'eval_steps_per_second': 11.286, 'epoch': 1.0}
{'train_runtime': 358.2711, 'train_samples_per_second': 27.906, 'train_steps_per_second': 1.744, 'train_loss': 0.8928380004882812, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2733542919158936, 0.9800015687942505, 0.939439594745636, 0.917137861251831, 0.9043334722518921, 0.9066495299339294, 0.8957403302192688, 0.8941192626953125, 0.8893401026725769, 0.885208249092102, 0.8761582970619202, 0.8732466697692871, 0.8708570003509521, 0.8697960376739502, 0.866309404373169, 0.8631764054298401, 0.8650627732276917, 0.8624497652053833, 0.8606939911842346, 0.8574911952018738, 0.8568830490112305, 0.8544091582298279, 0.853507399559021, 0.8535158634185791, 0.8525269031524658], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2733542919158936, 0.9800015687942505, 0.939439594745636, 0.917137861251831, 0.9043334722518921, 0.9066495299339294, 0.8957403302192688, 0.8941192626953125, 0.8893401026725769, 0.885208249092102, 0.8761582970619202, 0.8732466697692871, 0.8708570003509521, 0.8697960376739502, 0.866309404373169, 0.8631764054298401, 0.8650627732276917, 0.8624497652053833, 0.8606939911842346, 0.8574911952018738, 0.8568830490112305, 0.8544091582298279, 0.853507399559021, 0.8535158634185791, 0.8525269031524658]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.045748233795166
current iteration best possible eval_loss (full train run):  -0.8525269031524658
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6920 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.8280148075793682
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.827722030689602
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.827722030689602
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.8277825050125402
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.827722030689602
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3840, dtype=torch.float64), 0, tensor(0.0614, dtype=torch.float64), 0, tensor(0.0316, dtype=torch.float64), 0, 0, tensor(0.0731, dtype=torch.float64), tensor(0.4499, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 128, 2.6266350919432143e-18, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.3840, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0614, dtype=torch.float64), tensor(1.8414e-17, dtype=torch.float64), tensor(0.0316, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0731, dtype=torch.float64), tensor(0.4499, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.6266e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.384
  gsm8k: 0
  rowan_hellaswag: 0.061
  sciq: 0
  triviaqa: 0.032
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.073
  arc_challenge: 0.45

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.6266350919432143e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  2.6266350919432143e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6286, 'grad_norm': 0.4682599604129791, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1789613962173462, 'eval_runtime': 5.4147, 'eval_samples_per_second': 184.682, 'eval_steps_per_second': 11.635, 'epoch': 0.04}
{'loss': 1.2142, 'grad_norm': 0.454978883266449, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9641925692558289, 'eval_runtime': 5.4293, 'eval_samples_per_second': 184.186, 'eval_steps_per_second': 11.604, 'epoch': 0.08}
{'loss': 1.0292, 'grad_norm': 0.2682907283306122, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9174554944038391, 'eval_runtime': 5.4147, 'eval_samples_per_second': 184.683, 'eval_steps_per_second': 11.635, 'epoch': 0.12}
{'loss': 0.9839, 'grad_norm': 0.21925798058509827, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9005774855613708, 'eval_runtime': 5.4238, 'eval_samples_per_second': 184.372, 'eval_steps_per_second': 11.615, 'epoch': 0.16}
{'loss': 0.9941, 'grad_norm': 0.24598760902881622, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8978416919708252, 'eval_runtime': 5.4267, 'eval_samples_per_second': 184.273, 'eval_steps_per_second': 11.609, 'epoch': 0.2}
{'loss': 0.9822, 'grad_norm': 0.5246871709823608, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8845729827880859, 'eval_runtime': 5.4312, 'eval_samples_per_second': 184.122, 'eval_steps_per_second': 11.6, 'epoch': 0.24}
{'loss': 0.939, 'grad_norm': 0.23295645415782928, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8795309066772461, 'eval_runtime': 5.4331, 'eval_samples_per_second': 184.058, 'eval_steps_per_second': 11.596, 'epoch': 0.28}
{'loss': 0.8812, 'grad_norm': 0.2606801688671112, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8744562864303589, 'eval_runtime': 5.4302, 'eval_samples_per_second': 184.154, 'eval_steps_per_second': 11.602, 'epoch': 0.32}
{'loss': 0.8848, 'grad_norm': 0.2857592701911926, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.878296971321106, 'eval_runtime': 5.4351, 'eval_samples_per_second': 183.988, 'eval_steps_per_second': 11.591, 'epoch': 0.36}
{'loss': 0.8494, 'grad_norm': 0.24587172269821167, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8669509887695312, 'eval_runtime': 5.4463, 'eval_samples_per_second': 183.61, 'eval_steps_per_second': 11.567, 'epoch': 0.4}
{'loss': 0.8727, 'grad_norm': 0.28165116906166077, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8713984489440918, 'eval_runtime': 5.4463, 'eval_samples_per_second': 183.61, 'eval_steps_per_second': 11.567, 'epoch': 0.44}
{'loss': 0.8363, 'grad_norm': 0.27606359124183655, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8623567223548889, 'eval_runtime': 5.4558, 'eval_samples_per_second': 183.291, 'eval_steps_per_second': 11.547, 'epoch': 0.48}
{'loss': 0.8391, 'grad_norm': 0.2769779562950134, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8539584875106812, 'eval_runtime': 5.4458, 'eval_samples_per_second': 183.627, 'eval_steps_per_second': 11.568, 'epoch': 0.52}
{'loss': 0.83, 'grad_norm': 0.24253346025943756, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8557048439979553, 'eval_runtime': 5.453, 'eval_samples_per_second': 183.387, 'eval_steps_per_second': 11.553, 'epoch': 0.56}
{'loss': 0.7435, 'grad_norm': 0.3123692572116852, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.852156400680542, 'eval_runtime': 5.4713, 'eval_samples_per_second': 182.773, 'eval_steps_per_second': 11.515, 'epoch': 0.6}
{'loss': 0.7797, 'grad_norm': 0.33120763301849365, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8517641425132751, 'eval_runtime': 5.464, 'eval_samples_per_second': 183.016, 'eval_steps_per_second': 11.53, 'epoch': 0.64}
{'loss': 0.7575, 'grad_norm': 0.2651824355125427, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8514686226844788, 'eval_runtime': 5.4683, 'eval_samples_per_second': 182.871, 'eval_steps_per_second': 11.521, 'epoch': 0.68}
{'loss': 0.6679, 'grad_norm': 0.2568400502204895, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8476310968399048, 'eval_runtime': 5.4553, 'eval_samples_per_second': 183.309, 'eval_steps_per_second': 11.548, 'epoch': 0.72}
{'loss': 0.7333, 'grad_norm': 0.25577643513679504, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8419543504714966, 'eval_runtime': 5.4538, 'eval_samples_per_second': 183.359, 'eval_steps_per_second': 11.552, 'epoch': 0.76}
{'loss': 0.686, 'grad_norm': 0.25441253185272217, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8424994349479675, 'eval_runtime': 5.4537, 'eval_samples_per_second': 183.362, 'eval_steps_per_second': 11.552, 'epoch': 0.8}
{'loss': 0.6728, 'grad_norm': 0.2582966089248657, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8396166563034058, 'eval_runtime': 5.4411, 'eval_samples_per_second': 183.786, 'eval_steps_per_second': 11.579, 'epoch': 0.84}
{'loss': 0.6744, 'grad_norm': 0.2635166347026825, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8409352898597717, 'eval_runtime': 5.4478, 'eval_samples_per_second': 183.56, 'eval_steps_per_second': 11.564, 'epoch': 0.88}
{'loss': 0.7224, 'grad_norm': 0.2766720652580261, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8386512398719788, 'eval_runtime': 5.4492, 'eval_samples_per_second': 183.513, 'eval_steps_per_second': 11.561, 'epoch': 0.92}
{'loss': 0.6656, 'grad_norm': 0.3195335268974304, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8387681841850281, 'eval_runtime': 5.4646, 'eval_samples_per_second': 182.996, 'eval_steps_per_second': 11.529, 'epoch': 0.96}
{'loss': 0.7708, 'grad_norm': 0.3569410741329193, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8377062082290649, 'eval_runtime': 5.467, 'eval_samples_per_second': 182.916, 'eval_steps_per_second': 11.524, 'epoch': 1.0}
{'train_runtime': 351.0904, 'train_samples_per_second': 28.474, 'train_steps_per_second': 1.78, 'train_loss': 0.9055330841064453, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1789613962173462, 0.9641925692558289, 0.9174554944038391, 0.9005774855613708, 0.8978416919708252, 0.8845729827880859, 0.8795309066772461, 0.8744562864303589, 0.878296971321106, 0.8669509887695312, 0.8713984489440918, 0.8623567223548889, 0.8539584875106812, 0.8557048439979553, 0.852156400680542, 0.8517641425132751, 0.8514686226844788, 0.8476310968399048, 0.8419543504714966, 0.8424994349479675, 0.8396166563034058, 0.8409352898597717, 0.8386512398719788, 0.8387681841850281, 0.8377062082290649], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1789613962173462, 0.9641925692558289, 0.9174554944038391, 0.9005774855613708, 0.8978416919708252, 0.8845729827880859, 0.8795309066772461, 0.8744562864303589, 0.878296971321106, 0.8669509887695312, 0.8713984489440918, 0.8623567223548889, 0.8539584875106812, 0.8557048439979553, 0.852156400680542, 0.8517641425132751, 0.8514686226844788, 0.8476310968399048, 0.8419543504714966, 0.8424994349479675, 0.8396166563034058, 0.8409352898597717, 0.8386512398719788, 0.8387681841850281, 0.8377062082290649]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2228338718414307
current iteration best possible eval_loss (full train run):  -0.8377062082290649
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1984 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.8633207057086618
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.8629770635416334
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.905482855021404
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.8636129448465502
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.863717092255242
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4419, dtype=torch.float64), tensor(0.1255, dtype=torch.float64), 0, tensor(0.0287, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.0712, dtype=torch.float64), 0, 0, tensor(0.2365, dtype=torch.float64), 22, 0, 1, 1, 0, 0, 63, 0.004942536284200288, 23.81322528526384, 0]
normalized proposed parameters for next round by BO: [tensor(0.4419, dtype=torch.float64), tensor(0.1255, dtype=torch.float64), tensor(7.6232e-17, dtype=torch.float64), tensor(0.0287, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.0712, dtype=torch.float64), tensor(2.4410e-17, dtype=torch.float64), tensor(4.1381e-17, dtype=torch.float64), tensor(0.2365, dtype=torch.float64), tensor(0.6759, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4913, dtype=torch.float64), tensor(0.0494, dtype=torch.float64), tensor(0.4961, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.442
  gsm8k: 0.126
  rowan_hellaswag: 0
  sciq: 0.029
  triviaqa: 0.096
  truthfulqa_gen: 0.071
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.236

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (0.004942536284200288,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (23.81322528526384,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  63
lora dropout:  0.004942536284200288
lora alpha:  23.81322528526384
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 32,643,072 || all params: 8,062,904,320 || trainable%: 0.4049
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2743, 'grad_norm': 0.9849443435668945, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8577566146850586, 'eval_runtime': 4.8083, 'eval_samples_per_second': 207.973, 'eval_steps_per_second': 13.102, 'epoch': 0.04}
{'loss': 1.3794, 'grad_norm': 0.25512126088142395, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.187338948249817, 'eval_runtime': 4.8286, 'eval_samples_per_second': 207.101, 'eval_steps_per_second': 13.047, 'epoch': 0.08}
{'loss': 1.1388, 'grad_norm': 0.2401386946439743, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1085132360458374, 'eval_runtime': 4.8223, 'eval_samples_per_second': 207.369, 'eval_steps_per_second': 13.064, 'epoch': 0.12}
{'loss': 1.0742, 'grad_norm': 0.22700220346450806, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0371156930923462, 'eval_runtime': 4.8258, 'eval_samples_per_second': 207.221, 'eval_steps_per_second': 13.055, 'epoch': 0.16}
{'loss': 0.9776, 'grad_norm': 0.21532222628593445, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9970148801803589, 'eval_runtime': 4.8453, 'eval_samples_per_second': 206.387, 'eval_steps_per_second': 13.002, 'epoch': 0.2}
{'loss': 0.9858, 'grad_norm': 0.24066273868083954, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9699015021324158, 'eval_runtime': 4.8505, 'eval_samples_per_second': 206.166, 'eval_steps_per_second': 12.988, 'epoch': 0.24}
{'loss': 0.9496, 'grad_norm': 0.27123820781707764, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9336073398590088, 'eval_runtime': 4.8511, 'eval_samples_per_second': 206.138, 'eval_steps_per_second': 12.987, 'epoch': 0.28}
{'loss': 0.9094, 'grad_norm': 0.23348914086818695, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9054418802261353, 'eval_runtime': 4.8514, 'eval_samples_per_second': 206.126, 'eval_steps_per_second': 12.986, 'epoch': 0.32}
{'loss': 0.8992, 'grad_norm': 0.20563434064388275, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8998380303382874, 'eval_runtime': 4.8523, 'eval_samples_per_second': 206.086, 'eval_steps_per_second': 12.983, 'epoch': 0.36}
{'loss': 0.8872, 'grad_norm': 0.21817348897457123, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8948387503623962, 'eval_runtime': 4.8594, 'eval_samples_per_second': 205.786, 'eval_steps_per_second': 12.965, 'epoch': 0.4}
{'loss': 0.8817, 'grad_norm': 0.24693964421749115, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8895785808563232, 'eval_runtime': 4.8551, 'eval_samples_per_second': 205.97, 'eval_steps_per_second': 12.976, 'epoch': 0.44}
{'loss': 0.8638, 'grad_norm': 0.2143988311290741, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.887886643409729, 'eval_runtime': 4.8508, 'eval_samples_per_second': 206.152, 'eval_steps_per_second': 12.988, 'epoch': 0.48}
{'loss': 0.8666, 'grad_norm': 0.22087307274341583, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.880862295627594, 'eval_runtime': 4.9307, 'eval_samples_per_second': 202.812, 'eval_steps_per_second': 12.777, 'epoch': 0.52}
{'loss': 0.8709, 'grad_norm': 0.23423375189304352, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8772764205932617, 'eval_runtime': 4.8915, 'eval_samples_per_second': 204.436, 'eval_steps_per_second': 12.879, 'epoch': 0.56}
{'loss': 0.8314, 'grad_norm': 0.22959671914577484, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8804237246513367, 'eval_runtime': 4.8848, 'eval_samples_per_second': 204.719, 'eval_steps_per_second': 12.897, 'epoch': 0.6}
{'loss': 0.8558, 'grad_norm': 0.22742106020450592, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8769286870956421, 'eval_runtime': 4.9039, 'eval_samples_per_second': 203.921, 'eval_steps_per_second': 12.847, 'epoch': 0.64}
{'loss': 0.8589, 'grad_norm': 0.2666217088699341, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8744562864303589, 'eval_runtime': 4.8754, 'eval_samples_per_second': 205.113, 'eval_steps_per_second': 12.922, 'epoch': 0.68}
{'loss': 0.819, 'grad_norm': 0.2536538243293762, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.87054443359375, 'eval_runtime': 4.8588, 'eval_samples_per_second': 205.811, 'eval_steps_per_second': 12.966, 'epoch': 0.72}
{'loss': 0.8451, 'grad_norm': 0.2938430607318878, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8714057803153992, 'eval_runtime': 4.8546, 'eval_samples_per_second': 205.991, 'eval_steps_per_second': 12.977, 'epoch': 0.76}
{'loss': 0.8518, 'grad_norm': 0.2691986858844757, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8693174123764038, 'eval_runtime': 4.8608, 'eval_samples_per_second': 205.728, 'eval_steps_per_second': 12.961, 'epoch': 0.8}
{'loss': 0.8443, 'grad_norm': 0.22991067171096802, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8669178485870361, 'eval_runtime': 4.8635, 'eval_samples_per_second': 205.613, 'eval_steps_per_second': 12.954, 'epoch': 0.84}
{'loss': 0.8233, 'grad_norm': 0.25251829624176025, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8645834922790527, 'eval_runtime': 4.86, 'eval_samples_per_second': 205.76, 'eval_steps_per_second': 12.963, 'epoch': 0.88}
{'loss': 0.8621, 'grad_norm': 0.2979166805744171, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8653212785720825, 'eval_runtime': 4.856, 'eval_samples_per_second': 205.933, 'eval_steps_per_second': 12.974, 'epoch': 0.92}
{'loss': 0.816, 'grad_norm': 0.27148935198783875, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8637406229972839, 'eval_runtime': 4.859, 'eval_samples_per_second': 205.806, 'eval_steps_per_second': 12.966, 'epoch': 0.96}
{'loss': 0.8261, 'grad_norm': 0.36855533719062805, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8629176020622253, 'eval_runtime': 4.8589, 'eval_samples_per_second': 205.809, 'eval_steps_per_second': 12.966, 'epoch': 1.0}
{'train_runtime': 277.6299, 'train_samples_per_second': 36.005, 'train_steps_per_second': 2.251, 'train_loss': 1.0076903015136718, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8577566146850586, 1.187338948249817, 1.1085132360458374, 1.0371156930923462, 0.9970148801803589, 0.9699015021324158, 0.9336073398590088, 0.9054418802261353, 0.8998380303382874, 0.8948387503623962, 0.8895785808563232, 0.887886643409729, 0.880862295627594, 0.8772764205932617, 0.8804237246513367, 0.8769286870956421, 0.8744562864303589, 0.87054443359375, 0.8714057803153992, 0.8693174123764038, 0.8669178485870361, 0.8645834922790527, 0.8653212785720825, 0.8637406229972839, 0.8629176020622253], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8577566146850586, 1.187338948249817, 1.1085132360458374, 1.0371156930923462, 0.9970148801803589, 0.9699015021324158, 0.9336073398590088, 0.9054418802261353, 0.8998380303382874, 0.8948387503623962, 0.8895785808563232, 0.887886643409729, 0.880862295627594, 0.8772764205932617, 0.8804237246513367, 0.8769286870956421, 0.8744562864303589, 0.87054443359375, 0.8714057803153992, 0.8693174123764038, 0.8669178485870361, 0.8645834922790527, 0.8653212785720825, 0.8637406229972839, 0.8629176020622253]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0637390613555908
current iteration best possible eval_loss (full train run):  -0.8629176020622253
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.1128 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.8549378681049198
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.8549378681049213
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.8549378681049213
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.8549378681049247
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.8549378654369391
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1772, dtype=torch.float64), tensor(0.2011, dtype=torch.float64), tensor(0.0620, dtype=torch.float64), tensor(0.0240, dtype=torch.float64), tensor(0.1213, dtype=torch.float64), 0, tensor(0.0166, dtype=torch.float64), 0, tensor(0.3978, dtype=torch.float64), 25, 0, 1, 1, 1, 0, 87, 0.059330546894060844, 30.622254077718864, 1]
normalized proposed parameters for next round by BO: [tensor(0.1772, dtype=torch.float64), tensor(0.2011, dtype=torch.float64), tensor(0.0620, dtype=torch.float64), tensor(0.0240, dtype=torch.float64), tensor(0.1213, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0166, dtype=torch.float64), tensor(9.5542e-18, dtype=torch.float64), tensor(0.3978, dtype=torch.float64), tensor(0.7764, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6765, dtype=torch.float64), tensor(0.5933, dtype=torch.float64), tensor(0.6380, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.177
  gsm8k: 0.201
  rowan_hellaswag: 0.062
  sciq: 0.024
  triviaqa: 0.121
  truthfulqa_gen: 0
  wikitext: 0.017
  mmlu: 0
  arc_challenge: 0.398

LoRA Parameters:
  lora_r: (87,)
  lora_dropout: (0.059330546894060844,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (30.622254077718864,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  87
lora dropout:  0.059330546894060844
lora alpha:  30.622254077718864
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 91,315,200 || all params: 8,121,576,448 || trainable%: 1.1244
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7495, 'grad_norm': 0.9366053938865662, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.399533987045288, 'eval_runtime': 5.3584, 'eval_samples_per_second': 186.622, 'eval_steps_per_second': 11.757, 'epoch': 0.04}
{'loss': 1.2731, 'grad_norm': 0.3273223042488098, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0607402324676514, 'eval_runtime': 5.345, 'eval_samples_per_second': 187.091, 'eval_steps_per_second': 11.787, 'epoch': 0.08}
{'loss': 1.1367, 'grad_norm': 0.34220075607299805, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.021484613418579, 'eval_runtime': 5.3387, 'eval_samples_per_second': 187.31, 'eval_steps_per_second': 11.801, 'epoch': 0.12}
{'loss': 1.046, 'grad_norm': 0.3463062047958374, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9698808193206787, 'eval_runtime': 5.3505, 'eval_samples_per_second': 186.898, 'eval_steps_per_second': 11.775, 'epoch': 0.16}
{'loss': 1.0092, 'grad_norm': 0.2614603042602539, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9352386593818665, 'eval_runtime': 5.364, 'eval_samples_per_second': 186.429, 'eval_steps_per_second': 11.745, 'epoch': 0.2}
{'loss': 0.9989, 'grad_norm': 0.2255573868751526, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9289491772651672, 'eval_runtime': 5.3692, 'eval_samples_per_second': 186.247, 'eval_steps_per_second': 11.734, 'epoch': 0.24}
{'loss': 0.9265, 'grad_norm': 0.23271946609020233, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9167963266372681, 'eval_runtime': 5.3647, 'eval_samples_per_second': 186.405, 'eval_steps_per_second': 11.744, 'epoch': 0.28}
{'loss': 0.9743, 'grad_norm': 0.23320525884628296, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9129443168640137, 'eval_runtime': 5.381, 'eval_samples_per_second': 185.838, 'eval_steps_per_second': 11.708, 'epoch': 0.32}
{'loss': 0.9776, 'grad_norm': 0.2414928525686264, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.913724422454834, 'eval_runtime': 5.3773, 'eval_samples_per_second': 185.968, 'eval_steps_per_second': 11.716, 'epoch': 0.36}
{'loss': 0.9566, 'grad_norm': 0.2462492436170578, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9076040983200073, 'eval_runtime': 5.3726, 'eval_samples_per_second': 186.13, 'eval_steps_per_second': 11.726, 'epoch': 0.4}
{'loss': 0.9188, 'grad_norm': 0.3344625234603882, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9074251055717468, 'eval_runtime': 5.3753, 'eval_samples_per_second': 186.037, 'eval_steps_per_second': 11.72, 'epoch': 0.44}
{'loss': 0.9171, 'grad_norm': 0.27151918411254883, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8996651768684387, 'eval_runtime': 5.3705, 'eval_samples_per_second': 186.202, 'eval_steps_per_second': 11.731, 'epoch': 0.48}
{'loss': 0.8656, 'grad_norm': 0.2294476479291916, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8990770578384399, 'eval_runtime': 5.3792, 'eval_samples_per_second': 185.901, 'eval_steps_per_second': 11.712, 'epoch': 0.52}
{'loss': 0.9057, 'grad_norm': 0.27771317958831787, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8940985202789307, 'eval_runtime': 5.3734, 'eval_samples_per_second': 186.102, 'eval_steps_per_second': 11.724, 'epoch': 0.56}
{'loss': 0.8494, 'grad_norm': 0.31889477372169495, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8923441767692566, 'eval_runtime': 5.3731, 'eval_samples_per_second': 186.113, 'eval_steps_per_second': 11.725, 'epoch': 0.6}
{'loss': 0.8422, 'grad_norm': 0.2765418291091919, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.890762209892273, 'eval_runtime': 5.3731, 'eval_samples_per_second': 186.112, 'eval_steps_per_second': 11.725, 'epoch': 0.64}
{'loss': 0.8651, 'grad_norm': 0.2645706534385681, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8903989791870117, 'eval_runtime': 5.3625, 'eval_samples_per_second': 186.48, 'eval_steps_per_second': 11.748, 'epoch': 0.68}
{'loss': 0.7885, 'grad_norm': 0.2091391384601593, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8852714896202087, 'eval_runtime': 5.3738, 'eval_samples_per_second': 186.088, 'eval_steps_per_second': 11.724, 'epoch': 0.72}
{'loss': 0.8496, 'grad_norm': 0.39786767959594727, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8859703540802002, 'eval_runtime': 5.3787, 'eval_samples_per_second': 185.92, 'eval_steps_per_second': 11.713, 'epoch': 0.76}
{'loss': 0.8373, 'grad_norm': 0.423059344291687, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.885116696357727, 'eval_runtime': 5.3484, 'eval_samples_per_second': 186.973, 'eval_steps_per_second': 11.779, 'epoch': 0.8}
{'loss': 0.8006, 'grad_norm': 0.32624587416648865, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8836110830307007, 'eval_runtime': 5.3504, 'eval_samples_per_second': 186.903, 'eval_steps_per_second': 11.775, 'epoch': 0.84}
{'loss': 0.8504, 'grad_norm': 0.30787691473960876, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8839432597160339, 'eval_runtime': 5.3428, 'eval_samples_per_second': 187.167, 'eval_steps_per_second': 11.792, 'epoch': 0.88}
{'loss': 0.791, 'grad_norm': 0.31245705485343933, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8810702562332153, 'eval_runtime': 5.3632, 'eval_samples_per_second': 186.457, 'eval_steps_per_second': 11.747, 'epoch': 0.92}
{'loss': 0.7374, 'grad_norm': 0.3473663330078125, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8801844120025635, 'eval_runtime': 5.3443, 'eval_samples_per_second': 187.114, 'eval_steps_per_second': 11.788, 'epoch': 0.96}
{'loss': 0.7996, 'grad_norm': 0.39246469736099243, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8799464106559753, 'eval_runtime': 5.3536, 'eval_samples_per_second': 186.789, 'eval_steps_per_second': 11.768, 'epoch': 1.0}
{'train_runtime': 369.0847, 'train_samples_per_second': 27.083, 'train_steps_per_second': 1.693, 'train_loss': 0.9866721649169922, 'epoch': 1.0}
train_results:  {'eval_loss': [1.399533987045288, 1.0607402324676514, 1.021484613418579, 0.9698808193206787, 0.9352386593818665, 0.9289491772651672, 0.9167963266372681, 0.9129443168640137, 0.913724422454834, 0.9076040983200073, 0.9074251055717468, 0.8996651768684387, 0.8990770578384399, 0.8940985202789307, 0.8923441767692566, 0.890762209892273, 0.8903989791870117, 0.8852714896202087, 0.8859703540802002, 0.885116696357727, 0.8836110830307007, 0.8839432597160339, 0.8810702562332153, 0.8801844120025635, 0.8799464106559753], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.399533987045288, 1.0607402324676514, 1.021484613418579, 0.9698808193206787, 0.9352386593818665, 0.9289491772651672, 0.9167963266372681, 0.9129443168640137, 0.913724422454834, 0.9076040983200073, 0.9074251055717468, 0.8996651768684387, 0.8990770578384399, 0.8940985202789307, 0.8923441767692566, 0.890762209892273, 0.8903989791870117, 0.8852714896202087, 0.8859703540802002, 0.885116696357727, 0.8836110830307007, 0.8839432597160339, 0.8810702562332153, 0.8801844120025635, 0.8799464106559753]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1067668199539185
current iteration best possible eval_loss (full train run):  -0.8799464106559753
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.7778 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.8392985381075195
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.8392048546388524
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.8389347170643399
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.838856231153089
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.8389437727728966
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0778, dtype=torch.float64), 0, 0, 0, tensor(0.2460, dtype=torch.float64), tensor(0.1487, dtype=torch.float64), tensor(0.0270, dtype=torch.float64), 0, tensor(0.5005, dtype=torch.float64), 26, 0, 1, 1, 1, 0, 5, 0.031094385890600353, 31.63428654780393, 0]
normalized proposed parameters for next round by BO: [tensor(0.0778, dtype=torch.float64), tensor(4.3426e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.0622e-18, dtype=torch.float64), tensor(0.2460, dtype=torch.float64), tensor(0.1487, dtype=torch.float64), tensor(0.0270, dtype=torch.float64), tensor(1.2411e-17, dtype=torch.float64), tensor(0.5005, dtype=torch.float64), tensor(0.8257, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0359, dtype=torch.float64), tensor(0.3109, dtype=torch.float64), tensor(0.6590, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.078
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.246
  truthfulqa_gen: 0.149
  wikitext: 0.027
  mmlu: 0
  arc_challenge: 0.501

LoRA Parameters:
  lora_r: (5,)
  lora_dropout: (0.031094385890600353,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (31.63428654780393,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  5
lora dropout:  0.031094385890600353
lora alpha:  31.63428654780393
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 5,457,920 || all params: 8,035,719,168 || trainable%: 0.0679
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9315, 'grad_norm': 4.868316650390625, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.395845651626587, 'eval_runtime': 5.2238, 'eval_samples_per_second': 191.431, 'eval_steps_per_second': 12.06, 'epoch': 0.04}
{'loss': 1.0288, 'grad_norm': 1.6284019947052002, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0357632637023926, 'eval_runtime': 5.2427, 'eval_samples_per_second': 190.742, 'eval_steps_per_second': 12.017, 'epoch': 0.08}
{'loss': 0.8973, 'grad_norm': 0.944539487361908, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9767568707466125, 'eval_runtime': 5.278, 'eval_samples_per_second': 189.467, 'eval_steps_per_second': 11.936, 'epoch': 0.12}
{'loss': 0.904, 'grad_norm': 0.9812794923782349, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9797728061676025, 'eval_runtime': 5.2814, 'eval_samples_per_second': 189.344, 'eval_steps_per_second': 11.929, 'epoch': 0.16}
{'loss': 0.8394, 'grad_norm': 0.9196357131004333, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9625330567359924, 'eval_runtime': 5.2749, 'eval_samples_per_second': 189.577, 'eval_steps_per_second': 11.943, 'epoch': 0.2}
{'loss': 0.823, 'grad_norm': 1.2702913284301758, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9492529034614563, 'eval_runtime': 5.3075, 'eval_samples_per_second': 188.413, 'eval_steps_per_second': 11.87, 'epoch': 0.24}
{'loss': 0.8251, 'grad_norm': 1.270674228668213, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9414327144622803, 'eval_runtime': 5.2726, 'eval_samples_per_second': 189.658, 'eval_steps_per_second': 11.948, 'epoch': 0.28}
{'loss': 0.8163, 'grad_norm': 1.172013282775879, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9476944804191589, 'eval_runtime': 5.2609, 'eval_samples_per_second': 190.083, 'eval_steps_per_second': 11.975, 'epoch': 0.32}
{'loss': 0.7072, 'grad_norm': 1.0893539190292358, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9463388919830322, 'eval_runtime': 5.2685, 'eval_samples_per_second': 189.806, 'eval_steps_per_second': 11.958, 'epoch': 0.36}
{'loss': 0.7094, 'grad_norm': 1.2926955223083496, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9383849501609802, 'eval_runtime': 5.2681, 'eval_samples_per_second': 189.822, 'eval_steps_per_second': 11.959, 'epoch': 0.4}
{'loss': 0.7215, 'grad_norm': 1.3350539207458496, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9297927021980286, 'eval_runtime': 5.2597, 'eval_samples_per_second': 190.126, 'eval_steps_per_second': 11.978, 'epoch': 0.44}
{'loss': 0.6729, 'grad_norm': 1.153903841972351, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9332453608512878, 'eval_runtime': 5.2497, 'eval_samples_per_second': 190.487, 'eval_steps_per_second': 12.001, 'epoch': 0.48}
{'loss': 0.6392, 'grad_norm': 1.2111458778381348, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9209374785423279, 'eval_runtime': 5.2439, 'eval_samples_per_second': 190.697, 'eval_steps_per_second': 12.014, 'epoch': 0.52}
{'loss': 0.6318, 'grad_norm': 1.4385448694229126, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.928515613079071, 'eval_runtime': 5.2514, 'eval_samples_per_second': 190.425, 'eval_steps_per_second': 11.997, 'epoch': 0.56}
{'loss': 0.6464, 'grad_norm': 1.3672720193862915, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9178617596626282, 'eval_runtime': 5.2515, 'eval_samples_per_second': 190.421, 'eval_steps_per_second': 11.996, 'epoch': 0.6}
{'loss': 0.5824, 'grad_norm': 1.4841837882995605, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.91558837890625, 'eval_runtime': 5.255, 'eval_samples_per_second': 190.295, 'eval_steps_per_second': 11.989, 'epoch': 0.64}
{'loss': 0.5777, 'grad_norm': 1.781758189201355, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9198740124702454, 'eval_runtime': 5.2696, 'eval_samples_per_second': 189.767, 'eval_steps_per_second': 11.955, 'epoch': 0.68}
{'loss': 0.5458, 'grad_norm': 1.7851589918136597, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9162523150444031, 'eval_runtime': 5.2846, 'eval_samples_per_second': 189.229, 'eval_steps_per_second': 11.921, 'epoch': 0.72}
{'loss': 0.549, 'grad_norm': 1.7098569869995117, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9164103865623474, 'eval_runtime': 5.3111, 'eval_samples_per_second': 188.284, 'eval_steps_per_second': 11.862, 'epoch': 0.76}
{'loss': 0.5139, 'grad_norm': 1.545791506767273, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9148502349853516, 'eval_runtime': 5.3049, 'eval_samples_per_second': 188.506, 'eval_steps_per_second': 11.876, 'epoch': 0.8}
{'loss': 0.5345, 'grad_norm': 1.1210442781448364, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9141826629638672, 'eval_runtime': 5.3163, 'eval_samples_per_second': 188.1, 'eval_steps_per_second': 11.85, 'epoch': 0.84}
{'loss': 0.5048, 'grad_norm': 1.70486319065094, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9097625017166138, 'eval_runtime': 5.3099, 'eval_samples_per_second': 188.328, 'eval_steps_per_second': 11.865, 'epoch': 0.88}
{'loss': 0.5051, 'grad_norm': 1.6153216361999512, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9086569547653198, 'eval_runtime': 5.3286, 'eval_samples_per_second': 187.668, 'eval_steps_per_second': 11.823, 'epoch': 0.92}
{'loss': 0.4277, 'grad_norm': 1.353122353553772, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9104218482971191, 'eval_runtime': 5.3186, 'eval_samples_per_second': 188.019, 'eval_steps_per_second': 11.845, 'epoch': 0.96}
{'loss': 0.4722, 'grad_norm': 1.0809447765350342, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9106418490409851, 'eval_runtime': 5.2821, 'eval_samples_per_second': 189.32, 'eval_steps_per_second': 11.927, 'epoch': 1.0}
{'train_runtime': 288.487, 'train_samples_per_second': 34.653, 'train_steps_per_second': 2.166, 'train_loss': 0.7602756698608398, 'epoch': 1.0}
train_results:  {'eval_loss': [1.395845651626587, 1.0357632637023926, 0.9767568707466125, 0.9797728061676025, 0.9625330567359924, 0.9492529034614563, 0.9414327144622803, 0.9476944804191589, 0.9463388919830322, 0.9383849501609802, 0.9297927021980286, 0.9332453608512878, 0.9209374785423279, 0.928515613079071, 0.9178617596626282, 0.91558837890625, 0.9198740124702454, 0.9162523150444031, 0.9164103865623474, 0.9148502349853516, 0.9141826629638672, 0.9097625017166138, 0.9086569547653198, 0.9104218482971191, 0.9106418490409851], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.395845651626587, 1.0357632637023926, 0.9767568707466125, 0.9797728061676025, 0.9625330567359924, 0.9492529034614563, 0.9414327144622803, 0.9476944804191589, 0.9463388919830322, 0.9383849501609802, 0.9297927021980286, 0.9332453608512878, 0.9209374785423279, 0.928515613079071, 0.9178617596626282, 0.91558837890625, 0.9198740124702454, 0.9162523150444031, 0.9164103865623474, 0.9148502349853516, 0.9141826629638672, 0.9097625017166138, 0.9086569547653198, 0.9104218482971191, 0.9106418490409851]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1692951917648315
current iteration best possible eval_loss (full train run):  -0.9106418490409851
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.8758 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.8955911555666856
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.8955748729031159
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.8955963162568719
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.8955995027905808
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.8955894592218934
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1920, dtype=torch.float64), 0, tensor(0.0353, dtype=torch.float64), 0, tensor(0.1171, dtype=torch.float64), tensor(0.1542, dtype=torch.float64), 0, tensor(0.0966, dtype=torch.float64), tensor(0.4049, dtype=torch.float64), 28, 0, 1, 1, 1, 0, 67, 0.02712092141236019, 15.91471119582505, 0]
normalized proposed parameters for next round by BO: [tensor(0.1920, dtype=torch.float64), tensor(1.3857e-18, dtype=torch.float64), tensor(0.0353, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1171, dtype=torch.float64), tensor(0.1542, dtype=torch.float64), tensor(5.1831e-19, dtype=torch.float64), tensor(0.0966, dtype=torch.float64), tensor(0.4049, dtype=torch.float64), tensor(0.8644, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5243, dtype=torch.float64), tensor(0.2712, dtype=torch.float64), tensor(0.3316, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.192
  gsm8k: 0
  rowan_hellaswag: 0.035
  sciq: 0
  triviaqa: 0.117
  truthfulqa_gen: 0.154
  wikitext: 0
  mmlu: 0.097
  arc_challenge: 0.405

LoRA Parameters:
  lora_r: (67,)
  lora_dropout: (0.02712092141236019,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (15.91471119582505,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  67
lora dropout:  0.02712092141236019
lora alpha:  15.91471119582505
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 78,761,984 || all params: 8,109,023,232 || trainable%: 0.9713
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2764, 'grad_norm': 0.8518728017807007, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8185604810714722, 'eval_runtime': 5.383, 'eval_samples_per_second': 185.771, 'eval_steps_per_second': 11.704, 'epoch': 0.04}
{'loss': 1.3693, 'grad_norm': 0.3699357211589813, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0569570064544678, 'eval_runtime': 5.3814, 'eval_samples_per_second': 185.824, 'eval_steps_per_second': 11.707, 'epoch': 0.08}
{'loss': 1.0473, 'grad_norm': 0.2743340730667114, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9779556393623352, 'eval_runtime': 5.3836, 'eval_samples_per_second': 185.748, 'eval_steps_per_second': 11.702, 'epoch': 0.12}
{'loss': 1.0046, 'grad_norm': 0.21605060994625092, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9663050770759583, 'eval_runtime': 5.4031, 'eval_samples_per_second': 185.079, 'eval_steps_per_second': 11.66, 'epoch': 0.16}
{'loss': 1.0266, 'grad_norm': 0.21850328147411346, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9377170205116272, 'eval_runtime': 5.4021, 'eval_samples_per_second': 185.114, 'eval_steps_per_second': 11.662, 'epoch': 0.2}
{'loss': 1.0263, 'grad_norm': 0.21489596366882324, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.94100421667099, 'eval_runtime': 5.3931, 'eval_samples_per_second': 185.42, 'eval_steps_per_second': 11.681, 'epoch': 0.24}
{'loss': 1.0175, 'grad_norm': 0.22101959586143494, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9297043681144714, 'eval_runtime': 5.3872, 'eval_samples_per_second': 185.624, 'eval_steps_per_second': 11.694, 'epoch': 0.28}
{'loss': 0.9208, 'grad_norm': 0.21742554008960724, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9213867783546448, 'eval_runtime': 5.3925, 'eval_samples_per_second': 185.441, 'eval_steps_per_second': 11.683, 'epoch': 0.32}
{'loss': 0.9895, 'grad_norm': 0.2128608375787735, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9180265069007874, 'eval_runtime': 5.3978, 'eval_samples_per_second': 185.26, 'eval_steps_per_second': 11.671, 'epoch': 0.36}
{'loss': 0.8885, 'grad_norm': 0.24082627892494202, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.916831910610199, 'eval_runtime': 5.3962, 'eval_samples_per_second': 185.316, 'eval_steps_per_second': 11.675, 'epoch': 0.4}
{'loss': 0.8989, 'grad_norm': 0.22732654213905334, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9127033948898315, 'eval_runtime': 5.3896, 'eval_samples_per_second': 185.543, 'eval_steps_per_second': 11.689, 'epoch': 0.44}
{'loss': 0.8909, 'grad_norm': 0.20824439823627472, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9110140204429626, 'eval_runtime': 5.3957, 'eval_samples_per_second': 185.334, 'eval_steps_per_second': 11.676, 'epoch': 0.48}
{'loss': 0.9199, 'grad_norm': 0.25055888295173645, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9011103510856628, 'eval_runtime': 5.3995, 'eval_samples_per_second': 185.204, 'eval_steps_per_second': 11.668, 'epoch': 0.52}
{'loss': 0.9201, 'grad_norm': 0.23458106815814972, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8992286324501038, 'eval_runtime': 5.3941, 'eval_samples_per_second': 185.387, 'eval_steps_per_second': 11.679, 'epoch': 0.56}
{'loss': 0.8916, 'grad_norm': 0.2597101032733917, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9015281796455383, 'eval_runtime': 5.3969, 'eval_samples_per_second': 185.292, 'eval_steps_per_second': 11.673, 'epoch': 0.6}
{'loss': 0.8654, 'grad_norm': 0.3121608793735504, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9003146886825562, 'eval_runtime': 5.407, 'eval_samples_per_second': 184.945, 'eval_steps_per_second': 11.652, 'epoch': 0.64}
{'loss': 0.8105, 'grad_norm': 0.35094332695007324, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9002325534820557, 'eval_runtime': 5.4247, 'eval_samples_per_second': 184.344, 'eval_steps_per_second': 11.614, 'epoch': 0.68}
{'loss': 0.8327, 'grad_norm': 0.29014718532562256, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8914390802383423, 'eval_runtime': 5.4216, 'eval_samples_per_second': 184.449, 'eval_steps_per_second': 11.62, 'epoch': 0.72}
{'loss': 0.8403, 'grad_norm': 0.3193051517009735, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8920828700065613, 'eval_runtime': 5.4199, 'eval_samples_per_second': 184.505, 'eval_steps_per_second': 11.624, 'epoch': 0.76}
{'loss': 0.841, 'grad_norm': 0.27395519614219666, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8928024768829346, 'eval_runtime': 5.4322, 'eval_samples_per_second': 184.088, 'eval_steps_per_second': 11.598, 'epoch': 0.8}
{'loss': 0.8408, 'grad_norm': 0.31663042306900024, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8919730186462402, 'eval_runtime': 5.4186, 'eval_samples_per_second': 184.549, 'eval_steps_per_second': 11.627, 'epoch': 0.84}
{'loss': 0.8069, 'grad_norm': 0.3561805188655853, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8890202641487122, 'eval_runtime': 5.4245, 'eval_samples_per_second': 184.35, 'eval_steps_per_second': 11.614, 'epoch': 0.88}
{'loss': 0.8091, 'grad_norm': 0.42195743322372437, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8884969353675842, 'eval_runtime': 5.4128, 'eval_samples_per_second': 184.746, 'eval_steps_per_second': 11.639, 'epoch': 0.92}
{'loss': 0.8188, 'grad_norm': 0.3599399924278259, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8869729042053223, 'eval_runtime': 5.4358, 'eval_samples_per_second': 183.965, 'eval_steps_per_second': 11.59, 'epoch': 0.96}
{'loss': 0.7923, 'grad_norm': 0.5791540145874023, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8862984776496887, 'eval_runtime': 5.4413, 'eval_samples_per_second': 183.78, 'eval_steps_per_second': 11.578, 'epoch': 1.0}
{'train_runtime': 329.8448, 'train_samples_per_second': 30.305, 'train_steps_per_second': 1.895, 'train_loss': 1.0138500244140625, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8185604810714722, 1.0569570064544678, 0.9779556393623352, 0.9663050770759583, 0.9377170205116272, 0.94100421667099, 0.9297043681144714, 0.9213867783546448, 0.9180265069007874, 0.916831910610199, 0.9127033948898315, 0.9110140204429626, 0.9011103510856628, 0.8992286324501038, 0.9015281796455383, 0.9003146886825562, 0.9002325534820557, 0.8914390802383423, 0.8920828700065613, 0.8928024768829346, 0.8919730186462402, 0.8890202641487122, 0.8884969353675842, 0.8869729042053223, 0.8862984776496887], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8185604810714722, 1.0569570064544678, 0.9779556393623352, 0.9663050770759583, 0.9377170205116272, 0.94100421667099, 0.9297043681144714, 0.9213867783546448, 0.9180265069007874, 0.916831910610199, 0.9127033948898315, 0.9110140204429626, 0.9011103510856628, 0.8992286324501038, 0.9015281796455383, 0.9003146886825562, 0.9002325534820557, 0.8914390802383423, 0.8920828700065613, 0.8928024768829346, 0.8919730186462402, 0.8890202641487122, 0.8884969353675842, 0.8869729042053223, 0.8862984776496887]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0037460327148438
current iteration best possible eval_loss (full train run):  -0.8862984776496887
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 18.7192 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.858176981256309
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.8602240055906896
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.8596093995220251
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.8580404806977602
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.8580272990011235
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0741, dtype=torch.float64), 0, 0, tensor(0.0230, dtype=torch.float64), tensor(0.2573, dtype=torch.float64), tensor(0.2276, dtype=torch.float64), tensor(0.0520, dtype=torch.float64), 0, tensor(0.3595, dtype=torch.float64), 18, 0, 1, 1, 1, 0, 91, 0.01201202743568238, 27.728622443450995, 0]
normalized proposed parameters for next round by BO: [tensor(0.0741, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.5702e-18, dtype=torch.float64), tensor(0.0230, dtype=torch.float64), tensor(0.2573, dtype=torch.float64), tensor(0.2276, dtype=torch.float64), tensor(0.0520, dtype=torch.float64), tensor(0.0064, dtype=torch.float64), tensor(0.3595, dtype=torch.float64), tensor(0.5589, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7142, dtype=torch.float64), tensor(0.1201, dtype=torch.float64), tensor(0.5777, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.074
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0.023
  triviaqa: 0.257
  truthfulqa_gen: 0.228
  wikitext: 0.052
  mmlu: 0
  arc_challenge: 0.359

LoRA Parameters:
  lora_r: (91,)
  lora_dropout: (0.01201202743568238,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (27.728622443450995,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  91
lora dropout:  0.01201202743568238
lora alpha:  27.728622443450995
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 68,769,792 || all params: 8,099,031,040 || trainable%: 0.8491
length of training data:  9932
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3456, 'grad_norm': 1.0181026458740234, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8634122610092163, 'eval_runtime': 5.0592, 'eval_samples_per_second': 197.658, 'eval_steps_per_second': 12.452, 'epoch': 0.04}
{'loss': 1.2452, 'grad_norm': 0.3603716492652893, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1072317361831665, 'eval_runtime': 5.0759, 'eval_samples_per_second': 197.009, 'eval_steps_per_second': 12.412, 'epoch': 0.08}
{'loss': 1.0284, 'grad_norm': 0.2912028431892395, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.0630249977111816, 'eval_runtime': 5.0688, 'eval_samples_per_second': 197.285, 'eval_steps_per_second': 12.429, 'epoch': 0.12}
{'loss': 0.9988, 'grad_norm': 0.26389145851135254, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.0367567539215088, 'eval_runtime': 5.0691, 'eval_samples_per_second': 197.275, 'eval_steps_per_second': 12.428, 'epoch': 0.16}
{'loss': 0.9681, 'grad_norm': 0.21738679707050323, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 1.0161511898040771, 'eval_runtime': 5.0814, 'eval_samples_per_second': 196.795, 'eval_steps_per_second': 12.398, 'epoch': 0.2}
{'loss': 0.9362, 'grad_norm': 0.22045499086380005, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 1.0114296674728394, 'eval_runtime': 5.1011, 'eval_samples_per_second': 196.036, 'eval_steps_per_second': 12.35, 'epoch': 0.24}
{'loss': 0.9353, 'grad_norm': 0.28073641657829285, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 1.005912184715271, 'eval_runtime': 5.1028, 'eval_samples_per_second': 195.972, 'eval_steps_per_second': 12.346, 'epoch': 0.28}
{'loss': 0.9287, 'grad_norm': 0.23073147237300873, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.9979067444801331, 'eval_runtime': 5.1006, 'eval_samples_per_second': 196.056, 'eval_steps_per_second': 12.352, 'epoch': 0.32}
{'loss': 0.9416, 'grad_norm': 0.2485261857509613, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.983566164970398, 'eval_runtime': 5.106, 'eval_samples_per_second': 195.848, 'eval_steps_per_second': 12.338, 'epoch': 0.36}
{'loss': 0.9067, 'grad_norm': 0.2505333423614502, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.9791722297668457, 'eval_runtime': 5.1169, 'eval_samples_per_second': 195.432, 'eval_steps_per_second': 12.312, 'epoch': 0.4}
{'loss': 0.8957, 'grad_norm': 0.2732250690460205, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.9927111864089966, 'eval_runtime': 5.1234, 'eval_samples_per_second': 195.183, 'eval_steps_per_second': 12.297, 'epoch': 0.44}
{'loss': 0.8684, 'grad_norm': 0.32561424374580383, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.9846418499946594, 'eval_runtime': 5.1188, 'eval_samples_per_second': 195.36, 'eval_steps_per_second': 12.308, 'epoch': 0.48}
{'loss': 0.8221, 'grad_norm': 0.329588919878006, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.9831071496009827, 'eval_runtime': 5.1191, 'eval_samples_per_second': 195.346, 'eval_steps_per_second': 12.307, 'epoch': 0.52}
{'loss': 0.8712, 'grad_norm': 0.2898816168308258, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.9719899892807007, 'eval_runtime': 5.1161, 'eval_samples_per_second': 195.46, 'eval_steps_per_second': 12.314, 'epoch': 0.56}
{'loss': 0.7728, 'grad_norm': 0.3581750988960266, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.9664852023124695, 'eval_runtime': 5.1111, 'eval_samples_per_second': 195.651, 'eval_steps_per_second': 12.326, 'epoch': 0.6}
{'loss': 0.8023, 'grad_norm': 0.3469829857349396, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.9702529907226562, 'eval_runtime': 5.1169, 'eval_samples_per_second': 195.433, 'eval_steps_per_second': 12.312, 'epoch': 0.64}
{'loss': 0.8148, 'grad_norm': 0.36674565076828003, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.9576402306556702, 'eval_runtime': 5.1117, 'eval_samples_per_second': 195.63, 'eval_steps_per_second': 12.325, 'epoch': 0.68}
{'loss': 0.7643, 'grad_norm': 0.34932073950767517, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.9516493678092957, 'eval_runtime': 5.1129, 'eval_samples_per_second': 195.582, 'eval_steps_per_second': 12.322, 'epoch': 0.72}
{'loss': 0.7689, 'grad_norm': 0.41084033250808716, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.961967408657074, 'eval_runtime': 5.112, 'eval_samples_per_second': 195.618, 'eval_steps_per_second': 12.324, 'epoch': 0.76}
{'loss': 0.7689, 'grad_norm': 0.3555614948272705, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.9501184225082397, 'eval_runtime': 5.11, 'eval_samples_per_second': 195.696, 'eval_steps_per_second': 12.329, 'epoch': 0.81}
{'loss': 0.7841, 'grad_norm': 0.373028427362442, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.9481860399246216, 'eval_runtime': 5.1158, 'eval_samples_per_second': 195.472, 'eval_steps_per_second': 12.315, 'epoch': 0.85}
{'loss': 0.7367, 'grad_norm': 0.36752307415008545, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.9489566683769226, 'eval_runtime': 5.1122, 'eval_samples_per_second': 195.612, 'eval_steps_per_second': 12.324, 'epoch': 0.89}
{'loss': 0.7609, 'grad_norm': 0.3796937167644501, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.946182131767273, 'eval_runtime': 5.1064, 'eval_samples_per_second': 195.833, 'eval_steps_per_second': 12.337, 'epoch': 0.93}
{'loss': 0.7202, 'grad_norm': 0.4437919855117798, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.946328341960907, 'eval_runtime': 5.1049, 'eval_samples_per_second': 195.89, 'eval_steps_per_second': 12.341, 'epoch': 0.97}
{'train_runtime': 249.2263, 'train_samples_per_second': 39.851, 'train_steps_per_second': 2.492, 'train_loss': 0.966566901276077, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8634122610092163, 1.1072317361831665, 1.0630249977111816, 1.0367567539215088, 1.0161511898040771, 1.0114296674728394, 1.005912184715271, 0.9979067444801331, 0.983566164970398, 0.9791722297668457, 0.9927111864089966, 0.9846418499946594, 0.9831071496009827, 0.9719899892807007, 0.9664852023124695, 0.9702529907226562, 0.9576402306556702, 0.9516493678092957, 0.961967408657074, 0.9501184225082397, 0.9481860399246216, 0.9489566683769226, 0.946182131767273, 0.946328341960907], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.8634122610092163, 1.1072317361831665, 1.0630249977111816, 1.0367567539215088, 1.0161511898040771, 1.0114296674728394, 1.005912184715271, 0.9979067444801331, 0.983566164970398, 0.9791722297668457, 0.9927111864089966, 0.9846418499946594, 0.9831071496009827, 0.9719899892807007, 0.9664852023124695, 0.9702529907226562, 0.9576402306556702, 0.9516493678092957, 0.961967408657074, 0.9501184225082397, 0.9481860399246216, 0.9489566683769226, 0.946182131767273, 0.946328341960907]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.148607611656189
current iteration best possible eval_loss (full train run):  -0.946328341960907
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.2193 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3405855894088745, 0.2697674036026001, 0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2527220845222473, 0.2958891987800598, 0.798973798751831, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.42427369952201843, 0.12062281370162964, 0.7909995317459106, 0.5773974061012268]  ‚Üí  acq = -0.9008375286205172
X = [0.33804601430892944, 0.07152366638183594, 0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.505533754825592, 0.6193363070487976, 0.5416111350059509, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9287145733833313, 0.6309018731117249, 0.6364489793777466, 0.2499348521232605]  ‚Üí  acq = -0.9206310697732483
X = [0.6847147345542908, 0.7650787830352783, 0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5654566884040833, 0.984917938709259, 0.907849907875061, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4765317142009735, 0.10925418138504028, 0.4907895624637604, 0.022005975246429443]  ‚Üí  acq = -0.9005348405018738
X = [0.15384602546691895, 0.49969446659088135, 0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7945950031280518, 0.5358787775039673, 0.48621666431427, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9266265034675598, 0.8326297998428345, 0.6380935907363892, 0.630294680595398]  ‚Üí  acq = -0.901272539564576
X = [0.4769304394721985, 0.06750988960266113, 0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6422049403190613, 0.6458637714385986, 0.9798121452331543, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.285779744386673, 0.47408175468444824, 0.7827727794647217, 0.2598608732223511]  ‚Üí  acq = -0.8950142609165632
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4779, dtype=torch.float64), tensor(0.0977, dtype=torch.float64), tensor(0.0332, dtype=torch.float64), tensor(0.0779, dtype=torch.float64), tensor(0.1531, dtype=torch.float64), tensor(0.0559, dtype=torch.float64), 0, tensor(0.1027, dtype=torch.float64), 0, 32, 1, 1, 1, 1, 1, 76, 0.07291596506131386, 28.9231859549192, 0]
normalized proposed parameters for next round by BO: [tensor(0.4779, dtype=torch.float64), tensor(0.0977, dtype=torch.float64), tensor(0.0332, dtype=torch.float64), tensor(0.0779, dtype=torch.float64), tensor(0.1531, dtype=torch.float64), tensor(0.0559, dtype=torch.float64), tensor(2.4568e-18, dtype=torch.float64), tensor(0.1027, dtype=torch.float64), tensor(0.0016, dtype=torch.float64), tensor(0.9969, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5899, dtype=torch.float64), tensor(0.7292, dtype=torch.float64), tensor(0.6026, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.478
  gsm8k: 0.098
  rowan_hellaswag: 0.033
  sciq: 0.078
  triviaqa: 0.153
  truthfulqa_gen: 0.056
  wikitext: 0
  mmlu: 0.103
  arc_challenge: 0

LoRA Parameters:
  lora_r: (76,)
  lora_dropout: (0.07291596506131386,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 1],)
  lora_alpha: (28.9231859549192,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 1]
lora rank:  76
lora dropout:  0.07291596506131386
lora alpha:  28.9231859549192
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 166,854,656 || all params: 8,197,115,904 || trainable%: 2.0355
length of training data:  9981
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9386, 'grad_norm': 0.5751332640647888, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1885979175567627, 'eval_runtime': 5.98, 'eval_samples_per_second': 167.223, 'eval_steps_per_second': 10.535, 'epoch': 0.04}
{'loss': 1.2639, 'grad_norm': 0.45540332794189453, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9225369095802307, 'eval_runtime': 5.9849, 'eval_samples_per_second': 167.087, 'eval_steps_per_second': 10.526, 'epoch': 0.08}
{'loss': 1.0507, 'grad_norm': 0.28115853667259216, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.903357744216919, 'eval_runtime': 5.975, 'eval_samples_per_second': 167.363, 'eval_steps_per_second': 10.544, 'epoch': 0.12}
{'loss': 1.0257, 'grad_norm': 0.27722033858299255, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.8906041383743286, 'eval_runtime': 5.9789, 'eval_samples_per_second': 167.255, 'eval_steps_per_second': 10.537, 'epoch': 0.16}
{'loss': 1.0314, 'grad_norm': 0.19714510440826416, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.8854331374168396, 'eval_runtime': 5.9919, 'eval_samples_per_second': 166.891, 'eval_steps_per_second': 10.514, 'epoch': 0.2}
{'loss': 1.04, 'grad_norm': 0.21052910387516022, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.8825613856315613, 'eval_runtime': 5.9987, 'eval_samples_per_second': 166.702, 'eval_steps_per_second': 10.502, 'epoch': 0.24}
{'loss': 0.972, 'grad_norm': 0.2354620397090912, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.8748980760574341, 'eval_runtime': 6.0019, 'eval_samples_per_second': 166.613, 'eval_steps_per_second': 10.497, 'epoch': 0.28}
{'loss': 0.9793, 'grad_norm': 0.20859180390834808, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.8667717576026917, 'eval_runtime': 6.0073, 'eval_samples_per_second': 166.465, 'eval_steps_per_second': 10.487, 'epoch': 0.32}
{'loss': 0.9669, 'grad_norm': 0.2276732176542282, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.8604246973991394, 'eval_runtime': 6.0023, 'eval_samples_per_second': 166.604, 'eval_steps_per_second': 10.496, 'epoch': 0.36}
{'loss': 0.9997, 'grad_norm': 0.21887920796871185, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.8584150075912476, 'eval_runtime': 6.0241, 'eval_samples_per_second': 166.0, 'eval_steps_per_second': 10.458, 'epoch': 0.4}
{'loss': 0.9984, 'grad_norm': 0.24721243977546692, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.854632556438446, 'eval_runtime': 6.0268, 'eval_samples_per_second': 165.925, 'eval_steps_per_second': 10.453, 'epoch': 0.44}
{'loss': 0.9581, 'grad_norm': 0.2020529806613922, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.8515276908874512, 'eval_runtime': 6.0208, 'eval_samples_per_second': 166.09, 'eval_steps_per_second': 10.464, 'epoch': 0.48}
{'loss': 0.9179, 'grad_norm': 0.20429712533950806, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.8436111807823181, 'eval_runtime': 6.0172, 'eval_samples_per_second': 166.19, 'eval_steps_per_second': 10.47, 'epoch': 0.52}
{'loss': 0.9631, 'grad_norm': 0.21032388508319855, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.8447443246841431, 'eval_runtime': 6.0092, 'eval_samples_per_second': 166.41, 'eval_steps_per_second': 10.484, 'epoch': 0.56}
{'loss': 0.9654, 'grad_norm': 0.22311830520629883, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.8403840065002441, 'eval_runtime': 6.0099, 'eval_samples_per_second': 166.393, 'eval_steps_per_second': 10.483, 'epoch': 0.6}
{'loss': 0.9826, 'grad_norm': 0.21513605117797852, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.8370059728622437, 'eval_runtime': 6.0076, 'eval_samples_per_second': 166.457, 'eval_steps_per_second': 10.487, 'epoch': 0.64}
{'loss': 0.9666, 'grad_norm': 0.20747044682502747, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.8371984958648682, 'eval_runtime': 6.0026, 'eval_samples_per_second': 166.595, 'eval_steps_per_second': 10.496, 'epoch': 0.68}
{'loss': 0.9391, 'grad_norm': 0.21711716055870056, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.836024284362793, 'eval_runtime': 6.0182, 'eval_samples_per_second': 166.163, 'eval_steps_per_second': 10.468, 'epoch': 0.72}
{'loss': 0.9675, 'grad_norm': 0.20549748837947845, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.8328774571418762, 'eval_runtime': 6.0527, 'eval_samples_per_second': 165.217, 'eval_steps_per_second': 10.409, 'epoch': 0.76}
{'loss': 0.9349, 'grad_norm': 0.24882812798023224, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.8307642936706543, 'eval_runtime': 6.0559, 'eval_samples_per_second': 165.128, 'eval_steps_per_second': 10.403, 'epoch': 0.8}
{'loss': 0.9027, 'grad_norm': 0.25063619017601013, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.8302055597305298, 'eval_runtime': 6.0818, 'eval_samples_per_second': 164.426, 'eval_steps_per_second': 10.359, 'epoch': 0.84}
{'loss': 0.9619, 'grad_norm': 0.2302875816822052, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.8285510540008545, 'eval_runtime': 6.0691, 'eval_samples_per_second': 164.768, 'eval_steps_per_second': 10.38, 'epoch': 0.88}
{'loss': 0.9516, 'grad_norm': 0.20131416618824005, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.8266152143478394, 'eval_runtime': 6.053, 'eval_samples_per_second': 165.208, 'eval_steps_per_second': 10.408, 'epoch': 0.92}
{'loss': 0.9262, 'grad_norm': 0.2269536852836609, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.8263602256774902, 'eval_runtime': 6.0366, 'eval_samples_per_second': 165.657, 'eval_steps_per_second': 10.436, 'epoch': 0.96}
{'train_runtime': 394.5002, 'train_samples_per_second': 25.3, 'train_steps_per_second': 1.582, 'train_loss': 1.0628341436386108, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1885979175567627, 0.9225369095802307, 0.903357744216919, 0.8906041383743286, 0.8854331374168396, 0.8825613856315613, 0.8748980760574341, 0.8667717576026917, 0.8604246973991394, 0.8584150075912476, 0.854632556438446, 0.8515276908874512, 0.8436111807823181, 0.8447443246841431, 0.8403840065002441, 0.8370059728622437, 0.8371984958648682, 0.836024284362793, 0.8328774571418762, 0.8307642936706543, 0.8302055597305298, 0.8285510540008545, 0.8266152143478394, 0.8263602256774902], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.1885979175567627, 0.9225369095802307, 0.903357744216919, 0.8906041383743286, 0.8854331374168396, 0.8825613856315613, 0.8748980760574341, 0.8667717576026917, 0.8604246973991394, 0.8584150075912476, 0.854632556438446, 0.8515276908874512, 0.8436111807823181, 0.8447443246841431, 0.8403840065002441, 0.8370059728622437, 0.8371984958648682, 0.836024284362793, 0.8328774571418762, 0.8307642936706543, 0.8302055597305298, 0.8285510540008545, 0.8266152143478394, 0.8263602256774902]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0456465482711792
current iteration best possible eval_loss (full train run):  -0.8263602256774902
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.8810 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1795889139175415, 0.5639668703079224, 0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.6092051267623901, 0.8282220363616943, 0.8343875408172607, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.22647850215435028, 0.034817516803741455, 0.2369377166032791, 0.8301550149917603]  ‚Üí  acq = -0.8744420999294273
X = [0.3627225160598755, 0.4275026321411133, 0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.20957553386688232, 0.29544466733932495, 0.45958811044692993, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7996509075164795, 0.16252559423446655, 0.23583632707595825, 0.46233898401260376]  ‚Üí  acq = -0.8741879175601557
X = [0.3497363328933716, 0.9076562523841858, 0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.451727032661438, 0.6221595406532288, 0.021270394325256348, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.990592360496521, 0.6705264449119568, 0.5292245149612427, 0.4145408272743225]  ‚Üí  acq = -0.8741005013507872
X = [0.8545095920562744, 0.7290428876876831, 0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.1896294802427292, 0.13708847761154175, 0.2295888066291809, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.42133286595344543, 0.9889391660690308, 0.27955883741378784, 0.647262454032898]  ‚Üí  acq = -0.8741920035938402
X = [0.25802141427993774, 0.15090978145599365, 0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.16085723042488098, 0.13615381717681885, 0.3176853060722351, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7902160882949829, 0.012897372245788574, 0.5332701206207275, 0.154333233833313]  ‚Üí  acq = -0.8741926314433212
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2487, dtype=torch.float64), tensor(0.1108, dtype=torch.float64), tensor(0.0748, dtype=torch.float64), 0, tensor(0.1581, dtype=torch.float64), tensor(0.0498, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.0670, dtype=torch.float64), tensor(0.2649, dtype=torch.float64), 32, 0, 0, 0, 1, 0, 20, 0.06112524177819106, 42.07337631880069, 0]
normalized proposed parameters for next round by BO: [tensor(0.2487, dtype=torch.float64), tensor(0.1108, dtype=torch.float64), tensor(0.0748, dtype=torch.float64), tensor(3.8920e-17, dtype=torch.float64), tensor(0.1581, dtype=torch.float64), tensor(0.0498, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.0670, dtype=torch.float64), tensor(0.2649, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1577, dtype=torch.float64), tensor(0.6113, dtype=torch.float64), tensor(0.8765, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.249
  gsm8k: 0.111
  rowan_hellaswag: 0.075
  sciq: 0
  triviaqa: 0.158
  truthfulqa_gen: 0.05
  wikitext: 0.026
  mmlu: 0.067
  arc_challenge: 0.265

LoRA Parameters:
  lora_r: (20,)
  lora_dropout: (0.06112524177819106,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (42.07337631880069,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  20
lora dropout:  0.06112524177819106
lora alpha:  42.07337631880069
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 11,796,480 || all params: 8,042,057,728 || trainable%: 0.1467
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1185, 'grad_norm': 2.1130058765411377, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5327303409576416, 'eval_runtime': 4.7812, 'eval_samples_per_second': 209.154, 'eval_steps_per_second': 13.177, 'epoch': 0.04}
{'loss': 1.3491, 'grad_norm': 1.4079549312591553, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0037189722061157, 'eval_runtime': 4.8024, 'eval_samples_per_second': 208.23, 'eval_steps_per_second': 13.118, 'epoch': 0.08}
{'loss': 1.1613, 'grad_norm': 0.5262854099273682, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9513895511627197, 'eval_runtime': 4.797, 'eval_samples_per_second': 208.462, 'eval_steps_per_second': 13.133, 'epoch': 0.12}
{'loss': 1.1557, 'grad_norm': 0.5441533923149109, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.943385124206543, 'eval_runtime': 4.8116, 'eval_samples_per_second': 207.833, 'eval_steps_per_second': 13.093, 'epoch': 0.16}
{'loss': 1.1172, 'grad_norm': 0.6223089098930359, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9168856143951416, 'eval_runtime': 4.8169, 'eval_samples_per_second': 207.601, 'eval_steps_per_second': 13.079, 'epoch': 0.2}
{'loss': 1.1288, 'grad_norm': 0.39917683601379395, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9167435169219971, 'eval_runtime': 4.8289, 'eval_samples_per_second': 207.085, 'eval_steps_per_second': 13.046, 'epoch': 0.24}
{'loss': 1.0356, 'grad_norm': 0.39899730682373047, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9080713391304016, 'eval_runtime': 4.8389, 'eval_samples_per_second': 206.66, 'eval_steps_per_second': 13.02, 'epoch': 0.28}
{'loss': 1.0418, 'grad_norm': 0.4476013779640198, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9075381755828857, 'eval_runtime': 4.8472, 'eval_samples_per_second': 206.305, 'eval_steps_per_second': 12.997, 'epoch': 0.32}
{'loss': 1.0707, 'grad_norm': 0.5195786356925964, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8932028412818909, 'eval_runtime': 4.8261, 'eval_samples_per_second': 207.205, 'eval_steps_per_second': 13.054, 'epoch': 0.36}
{'loss': 1.1019, 'grad_norm': 0.42465484142303467, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9020940065383911, 'eval_runtime': 4.8939, 'eval_samples_per_second': 204.335, 'eval_steps_per_second': 12.873, 'epoch': 0.4}
{'loss': 1.0445, 'grad_norm': 0.5354945659637451, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8969469666481018, 'eval_runtime': 4.857, 'eval_samples_per_second': 205.889, 'eval_steps_per_second': 12.971, 'epoch': 0.44}
{'loss': 1.0034, 'grad_norm': 0.41476842761039734, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8936612606048584, 'eval_runtime': 4.8668, 'eval_samples_per_second': 205.474, 'eval_steps_per_second': 12.945, 'epoch': 0.48}
{'loss': 1.0596, 'grad_norm': 0.46713560819625854, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8829560279846191, 'eval_runtime': 4.8613, 'eval_samples_per_second': 205.706, 'eval_steps_per_second': 12.959, 'epoch': 0.52}
{'loss': 1.0451, 'grad_norm': 1.0067929029464722, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8836367130279541, 'eval_runtime': 4.8429, 'eval_samples_per_second': 206.487, 'eval_steps_per_second': 13.009, 'epoch': 0.56}
{'loss': 0.9964, 'grad_norm': 0.49861958622932434, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8819021582603455, 'eval_runtime': 4.8347, 'eval_samples_per_second': 206.839, 'eval_steps_per_second': 13.031, 'epoch': 0.6}
{'loss': 1.0037, 'grad_norm': 0.44147127866744995, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.879243791103363, 'eval_runtime': 4.835, 'eval_samples_per_second': 206.827, 'eval_steps_per_second': 13.03, 'epoch': 0.64}
{'loss': 1.0093, 'grad_norm': 0.4947753846645355, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8775814175605774, 'eval_runtime': 4.8263, 'eval_samples_per_second': 207.197, 'eval_steps_per_second': 13.053, 'epoch': 0.68}
{'loss': 0.9253, 'grad_norm': 0.5082232356071472, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.873720645904541, 'eval_runtime': 4.8153, 'eval_samples_per_second': 207.671, 'eval_steps_per_second': 13.083, 'epoch': 0.72}
{'loss': 0.9891, 'grad_norm': 0.47984492778778076, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8771788477897644, 'eval_runtime': 4.8082, 'eval_samples_per_second': 207.979, 'eval_steps_per_second': 13.103, 'epoch': 0.76}
{'loss': 0.955, 'grad_norm': 0.5841543674468994, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8745802640914917, 'eval_runtime': 4.8135, 'eval_samples_per_second': 207.747, 'eval_steps_per_second': 13.088, 'epoch': 0.8}
{'loss': 0.9893, 'grad_norm': 0.4969956874847412, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8741405010223389, 'eval_runtime': 4.8397, 'eval_samples_per_second': 206.625, 'eval_steps_per_second': 13.017, 'epoch': 0.84}
{'loss': 1.0195, 'grad_norm': 0.4561362564563751, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8722706437110901, 'eval_runtime': 4.8311, 'eval_samples_per_second': 206.994, 'eval_steps_per_second': 13.041, 'epoch': 0.88}
{'loss': 1.0216, 'grad_norm': 0.5196521878242493, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8701393008232117, 'eval_runtime': 4.8372, 'eval_samples_per_second': 206.732, 'eval_steps_per_second': 13.024, 'epoch': 0.92}
{'loss': 0.9651, 'grad_norm': 0.550742506980896, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8696930408477783, 'eval_runtime': 4.8484, 'eval_samples_per_second': 206.255, 'eval_steps_per_second': 12.994, 'epoch': 0.96}
{'loss': 0.9913, 'grad_norm': 0.555783748626709, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8696079254150391, 'eval_runtime': 4.8573, 'eval_samples_per_second': 205.874, 'eval_steps_per_second': 12.97, 'epoch': 1.0}
{'train_runtime': 322.0884, 'train_samples_per_second': 31.035, 'train_steps_per_second': 1.94, 'train_loss': 1.131953384399414, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5327303409576416, 1.0037189722061157, 0.9513895511627197, 0.943385124206543, 0.9168856143951416, 0.9167435169219971, 0.9080713391304016, 0.9075381755828857, 0.8932028412818909, 0.9020940065383911, 0.8969469666481018, 0.8936612606048584, 0.8829560279846191, 0.8836367130279541, 0.8819021582603455, 0.879243791103363, 0.8775814175605774, 0.873720645904541, 0.8771788477897644, 0.8745802640914917, 0.8741405010223389, 0.8722706437110901, 0.8701393008232117, 0.8696930408477783, 0.8696079254150391], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5327303409576416, 1.0037189722061157, 0.9513895511627197, 0.943385124206543, 0.9168856143951416, 0.9167435169219971, 0.9080713391304016, 0.9075381755828857, 0.8932028412818909, 0.9020940065383911, 0.8969469666481018, 0.8936612606048584, 0.8829560279846191, 0.8836367130279541, 0.8819021582603455, 0.879243791103363, 0.8775814175605774, 0.873720645904541, 0.8771788477897644, 0.8745802640914917, 0.8741405010223389, 0.8722706437110901, 0.8701393008232117, 0.8696930408477783, 0.8696079254150391]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.168910264968872
current iteration best possible eval_loss (full train run):  -0.8696079254150391
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.8698 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.9169863773833624
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.9169864845467017
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.916987388977532
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.9151636475834979
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.9171212983413026
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2604, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.0581, dtype=torch.float64), tensor(0.1685, dtype=torch.float64), tensor(0.0568, dtype=torch.float64), tensor(0.0221, dtype=torch.float64), tensor(0.1218, dtype=torch.float64), tensor(0.2297, dtype=torch.float64), 26, 0, 1, 1, 1, 0, 72, 0.02565048671884278, 22.941524366408963, 0]
normalized proposed parameters for next round by BO: [tensor(0.2604, dtype=torch.float64), tensor(0.0705, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.0581, dtype=torch.float64), tensor(0.1685, dtype=torch.float64), tensor(0.0568, dtype=torch.float64), tensor(0.0221, dtype=torch.float64), tensor(0.1218, dtype=torch.float64), tensor(0.2297, dtype=torch.float64), tensor(0.8023, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5601, dtype=torch.float64), tensor(0.2565, dtype=torch.float64), tensor(0.4779, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.26
  gsm8k: 0.071
  rowan_hellaswag: 0.012
  sciq: 0.058
  triviaqa: 0.168
  truthfulqa_gen: 0.057
  wikitext: 0.022
  mmlu: 0.122
  arc_challenge: 0.23

LoRA Parameters:
  lora_r: (72,)
  lora_dropout: (0.02565048671884278,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (22.941524366408963,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  72
lora dropout:  0.02565048671884278
lora alpha:  22.941524366408963
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 78,594,048 || all params: 8,108,855,296 || trainable%: 0.9692
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0219, 'grad_norm': 1.0018408298492432, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.636863350868225, 'eval_runtime': 5.0841, 'eval_samples_per_second': 196.691, 'eval_steps_per_second': 12.392, 'epoch': 0.04}
{'loss': 1.2741, 'grad_norm': 0.35688456892967224, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9985827803611755, 'eval_runtime': 5.0752, 'eval_samples_per_second': 197.037, 'eval_steps_per_second': 12.413, 'epoch': 0.08}
{'loss': 1.0554, 'grad_norm': 0.23066166043281555, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9520435929298401, 'eval_runtime': 5.0982, 'eval_samples_per_second': 196.148, 'eval_steps_per_second': 12.357, 'epoch': 0.12}
{'loss': 1.0589, 'grad_norm': 0.2381514459848404, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9351842999458313, 'eval_runtime': 5.1023, 'eval_samples_per_second': 195.991, 'eval_steps_per_second': 12.347, 'epoch': 0.16}
{'loss': 1.0509, 'grad_norm': 0.23901253938674927, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9262614250183105, 'eval_runtime': 5.1046, 'eval_samples_per_second': 195.901, 'eval_steps_per_second': 12.342, 'epoch': 0.2}
{'loss': 1.0016, 'grad_norm': 0.22277067601680756, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9133889675140381, 'eval_runtime': 5.0855, 'eval_samples_per_second': 196.638, 'eval_steps_per_second': 12.388, 'epoch': 0.24}
{'loss': 1.0173, 'grad_norm': 0.22630542516708374, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9176137447357178, 'eval_runtime': 5.0853, 'eval_samples_per_second': 196.646, 'eval_steps_per_second': 12.389, 'epoch': 0.28}
{'loss': 1.0066, 'grad_norm': 0.2515834867954254, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9017468094825745, 'eval_runtime': 5.0932, 'eval_samples_per_second': 196.342, 'eval_steps_per_second': 12.37, 'epoch': 0.32}
{'loss': 0.9922, 'grad_norm': 0.21812598407268524, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8972758650779724, 'eval_runtime': 5.0903, 'eval_samples_per_second': 196.453, 'eval_steps_per_second': 12.377, 'epoch': 0.36}
{'loss': 1.0411, 'grad_norm': 0.19967465102672577, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8945804238319397, 'eval_runtime': 5.0957, 'eval_samples_per_second': 196.244, 'eval_steps_per_second': 12.363, 'epoch': 0.4}
{'loss': 1.0239, 'grad_norm': 0.1940084546804428, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8933648467063904, 'eval_runtime': 5.0956, 'eval_samples_per_second': 196.248, 'eval_steps_per_second': 12.364, 'epoch': 0.44}
{'loss': 0.9448, 'grad_norm': 0.1967761069536209, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8896840810775757, 'eval_runtime': 5.1275, 'eval_samples_per_second': 195.027, 'eval_steps_per_second': 12.287, 'epoch': 0.48}
{'loss': 0.9613, 'grad_norm': 0.26747187972068787, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8918136954307556, 'eval_runtime': 5.1439, 'eval_samples_per_second': 194.404, 'eval_steps_per_second': 12.247, 'epoch': 0.52}
{'loss': 0.9428, 'grad_norm': 0.21706753969192505, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8884591460227966, 'eval_runtime': 5.1329, 'eval_samples_per_second': 194.82, 'eval_steps_per_second': 12.274, 'epoch': 0.56}
{'loss': 0.9683, 'grad_norm': 0.2551477253437042, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8834015727043152, 'eval_runtime': 5.1448, 'eval_samples_per_second': 194.372, 'eval_steps_per_second': 12.245, 'epoch': 0.6}
{'loss': 0.9817, 'grad_norm': 0.23820742964744568, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8807912468910217, 'eval_runtime': 5.1257, 'eval_samples_per_second': 195.096, 'eval_steps_per_second': 12.291, 'epoch': 0.64}
{'loss': 0.9384, 'grad_norm': 0.23865200579166412, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8791101574897766, 'eval_runtime': 5.1248, 'eval_samples_per_second': 195.129, 'eval_steps_per_second': 12.293, 'epoch': 0.68}
{'loss': 0.9616, 'grad_norm': 0.2393130511045456, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.873595118522644, 'eval_runtime': 5.1133, 'eval_samples_per_second': 195.57, 'eval_steps_per_second': 12.321, 'epoch': 0.72}
{'loss': 0.9562, 'grad_norm': 0.2456587702035904, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8781110048294067, 'eval_runtime': 5.0956, 'eval_samples_per_second': 196.247, 'eval_steps_per_second': 12.364, 'epoch': 0.76}
{'loss': 0.947, 'grad_norm': 0.23651669919490814, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8728339076042175, 'eval_runtime': 5.0876, 'eval_samples_per_second': 196.556, 'eval_steps_per_second': 12.383, 'epoch': 0.8}
{'loss': 0.9185, 'grad_norm': 0.2302197366952896, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8729991316795349, 'eval_runtime': 5.0957, 'eval_samples_per_second': 196.244, 'eval_steps_per_second': 12.363, 'epoch': 0.84}
{'loss': 0.96, 'grad_norm': 0.2732733488082886, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8706594109535217, 'eval_runtime': 5.0819, 'eval_samples_per_second': 196.778, 'eval_steps_per_second': 12.397, 'epoch': 0.88}
{'loss': 0.9825, 'grad_norm': 0.27706480026245117, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8694665431976318, 'eval_runtime': 5.089, 'eval_samples_per_second': 196.502, 'eval_steps_per_second': 12.38, 'epoch': 0.92}
{'loss': 0.9427, 'grad_norm': 0.2690258026123047, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8689206838607788, 'eval_runtime': 5.0812, 'eval_samples_per_second': 196.805, 'eval_steps_per_second': 12.399, 'epoch': 0.96}
{'loss': 0.9308, 'grad_norm': 0.3039338290691376, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8687499761581421, 'eval_runtime': 5.0725, 'eval_samples_per_second': 197.143, 'eval_steps_per_second': 12.42, 'epoch': 1.0}
{'train_runtime': 313.0807, 'train_samples_per_second': 31.925, 'train_steps_per_second': 1.996, 'train_loss': 1.0752188171386718, 'epoch': 1.0}
train_results:  {'eval_loss': [1.636863350868225, 0.9985827803611755, 0.9520435929298401, 0.9351842999458313, 0.9262614250183105, 0.9133889675140381, 0.9176137447357178, 0.9017468094825745, 0.8972758650779724, 0.8945804238319397, 0.8933648467063904, 0.8896840810775757, 0.8918136954307556, 0.8884591460227966, 0.8834015727043152, 0.8807912468910217, 0.8791101574897766, 0.873595118522644, 0.8781110048294067, 0.8728339076042175, 0.8729991316795349, 0.8706594109535217, 0.8694665431976318, 0.8689206838607788, 0.8687499761581421], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.636863350868225, 0.9985827803611755, 0.9520435929298401, 0.9351842999458313, 0.9262614250183105, 0.9133889675140381, 0.9176137447357178, 0.9017468094825745, 0.8972758650779724, 0.8945804238319397, 0.8933648467063904, 0.8896840810775757, 0.8918136954307556, 0.8884591460227966, 0.8834015727043152, 0.8807912468910217, 0.8791101574897766, 0.873595118522644, 0.8781110048294067, 0.8728339076042175, 0.8729991316795349, 0.8706594109535217, 0.8694665431976318, 0.8689206838607788, 0.8687499761581421]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0472297668457031
current iteration best possible eval_loss (full train run):  -0.8687499761581421
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.8817 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.9250561196473321
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.9293489250039634
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.9221345624449806
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.9265105811151892
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.9248841274372427
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1120, dtype=torch.float64), tensor(0.0372, dtype=torch.float64), tensor(0.0797, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(0.1332, dtype=torch.float64), tensor(0.1073, dtype=torch.float64), tensor(0.0223, dtype=torch.float64), 0, tensor(0.4769, dtype=torch.float64), 25, 0, 1, 1, 1, 0, 45, 0.028049643287062793, 16.548879105974425, 0]
normalized proposed parameters for next round by BO: [tensor(0.1120, dtype=torch.float64), tensor(0.0372, dtype=torch.float64), tensor(0.0797, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(0.1332, dtype=torch.float64), tensor(0.1073, dtype=torch.float64), tensor(0.0223, dtype=torch.float64), tensor(1.7630e-23, dtype=torch.float64), tensor(0.4769, dtype=torch.float64), tensor(0.7879, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3523, dtype=torch.float64), tensor(0.2805, dtype=torch.float64), tensor(0.3448, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.112
  gsm8k: 0.037
  rowan_hellaswag: 0.08
  sciq: 0.031
  triviaqa: 0.133
  truthfulqa_gen: 0.107
  wikitext: 0.022
  mmlu: 0
  arc_challenge: 0.477

LoRA Parameters:
  lora_r: (45,)
  lora_dropout: (0.028049643287062793,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (16.548879105974425,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  45
lora dropout:  0.028049643287062793
lora alpha:  16.548879105974425
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 47,232,000 || all params: 8,077,493,248 || trainable%: 0.5847
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.27, 'grad_norm': 1.0477001667022705, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9058221578598022, 'eval_runtime': 5.2497, 'eval_samples_per_second': 190.486, 'eval_steps_per_second': 12.001, 'epoch': 0.04}
{'loss': 1.3918, 'grad_norm': 0.5829578042030334, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.083267092704773, 'eval_runtime': 5.2569, 'eval_samples_per_second': 190.225, 'eval_steps_per_second': 11.984, 'epoch': 0.08}
{'loss': 1.116, 'grad_norm': 0.24920937418937683, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0043275356292725, 'eval_runtime': 5.2424, 'eval_samples_per_second': 190.752, 'eval_steps_per_second': 12.017, 'epoch': 0.12}
{'loss': 1.1503, 'grad_norm': 0.28516367077827454, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9843852519989014, 'eval_runtime': 5.2516, 'eval_samples_per_second': 190.417, 'eval_steps_per_second': 11.996, 'epoch': 0.16}
{'loss': 1.0854, 'grad_norm': 0.28354331851005554, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9803749918937683, 'eval_runtime': 5.2408, 'eval_samples_per_second': 190.812, 'eval_steps_per_second': 12.021, 'epoch': 0.2}
{'loss': 1.0408, 'grad_norm': 0.2454504817724228, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9566378593444824, 'eval_runtime': 5.2535, 'eval_samples_per_second': 190.348, 'eval_steps_per_second': 11.992, 'epoch': 0.24}
{'loss': 1.0146, 'grad_norm': 0.2741318345069885, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9512718319892883, 'eval_runtime': 5.2582, 'eval_samples_per_second': 190.179, 'eval_steps_per_second': 11.981, 'epoch': 0.28}
{'loss': 1.0236, 'grad_norm': 0.22119109332561493, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9475162625312805, 'eval_runtime': 5.2586, 'eval_samples_per_second': 190.166, 'eval_steps_per_second': 11.98, 'epoch': 0.32}
{'loss': 1.0087, 'grad_norm': 0.2691561281681061, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9491708874702454, 'eval_runtime': 5.2665, 'eval_samples_per_second': 189.879, 'eval_steps_per_second': 11.962, 'epoch': 0.36}
{'loss': 0.9545, 'grad_norm': 0.328621506690979, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9389604330062866, 'eval_runtime': 5.2577, 'eval_samples_per_second': 190.196, 'eval_steps_per_second': 11.982, 'epoch': 0.4}
{'loss': 0.9822, 'grad_norm': 0.286278635263443, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9401636123657227, 'eval_runtime': 5.2574, 'eval_samples_per_second': 190.208, 'eval_steps_per_second': 11.983, 'epoch': 0.44}
{'loss': 0.9432, 'grad_norm': 0.26281020045280457, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9276586174964905, 'eval_runtime': 5.2588, 'eval_samples_per_second': 190.157, 'eval_steps_per_second': 11.98, 'epoch': 0.48}
{'loss': 0.9995, 'grad_norm': 0.30454957485198975, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9243559837341309, 'eval_runtime': 5.2663, 'eval_samples_per_second': 189.886, 'eval_steps_per_second': 11.963, 'epoch': 0.52}
{'loss': 0.9383, 'grad_norm': 0.2907991409301758, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.928031325340271, 'eval_runtime': 5.2781, 'eval_samples_per_second': 189.463, 'eval_steps_per_second': 11.936, 'epoch': 0.56}
{'loss': 0.9146, 'grad_norm': 0.42729973793029785, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9207233786582947, 'eval_runtime': 5.2646, 'eval_samples_per_second': 189.948, 'eval_steps_per_second': 11.967, 'epoch': 0.6}
{'loss': 0.8726, 'grad_norm': 0.3838095963001251, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9140381217002869, 'eval_runtime': 5.2785, 'eval_samples_per_second': 189.449, 'eval_steps_per_second': 11.935, 'epoch': 0.64}
{'loss': 0.8948, 'grad_norm': 0.393327921628952, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9197167754173279, 'eval_runtime': 5.2882, 'eval_samples_per_second': 189.1, 'eval_steps_per_second': 11.913, 'epoch': 0.68}
{'loss': 0.8282, 'grad_norm': 0.4134449362754822, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9159260392189026, 'eval_runtime': 5.2885, 'eval_samples_per_second': 189.09, 'eval_steps_per_second': 11.913, 'epoch': 0.72}
{'loss': 0.8415, 'grad_norm': 0.4554610848426819, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9136367440223694, 'eval_runtime': 5.2937, 'eval_samples_per_second': 188.905, 'eval_steps_per_second': 11.901, 'epoch': 0.76}
{'loss': 0.7848, 'grad_norm': 0.47853657603263855, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9155610203742981, 'eval_runtime': 5.3149, 'eval_samples_per_second': 188.15, 'eval_steps_per_second': 11.853, 'epoch': 0.8}
{'loss': 0.8054, 'grad_norm': 0.4475133717060089, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9135844707489014, 'eval_runtime': 5.3329, 'eval_samples_per_second': 187.515, 'eval_steps_per_second': 11.813, 'epoch': 0.84}
{'loss': 0.7425, 'grad_norm': 0.5807343125343323, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9088969230651855, 'eval_runtime': 5.3413, 'eval_samples_per_second': 187.221, 'eval_steps_per_second': 11.795, 'epoch': 0.88}
{'loss': 0.8185, 'grad_norm': 0.5589722394943237, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9095454812049866, 'eval_runtime': 5.3222, 'eval_samples_per_second': 187.891, 'eval_steps_per_second': 11.837, 'epoch': 0.92}
{'loss': 0.7905, 'grad_norm': 0.6815511584281921, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9111673831939697, 'eval_runtime': 5.3484, 'eval_samples_per_second': 186.972, 'eval_steps_per_second': 11.779, 'epoch': 0.96}
{'loss': 0.8061, 'grad_norm': 0.5558890104293823, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.910797119140625, 'eval_runtime': 5.3389, 'eval_samples_per_second': 187.305, 'eval_steps_per_second': 11.8, 'epoch': 1.0}
{'train_runtime': 330.5354, 'train_samples_per_second': 30.245, 'train_steps_per_second': 1.891, 'train_loss': 1.0407301818847656, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9058221578598022, 1.083267092704773, 1.0043275356292725, 0.9843852519989014, 0.9803749918937683, 0.9566378593444824, 0.9512718319892883, 0.9475162625312805, 0.9491708874702454, 0.9389604330062866, 0.9401636123657227, 0.9276586174964905, 0.9243559837341309, 0.928031325340271, 0.9207233786582947, 0.9140381217002869, 0.9197167754173279, 0.9159260392189026, 0.9136367440223694, 0.9155610203742981, 0.9135844707489014, 0.9088969230651855, 0.9095454812049866, 0.9111673831939697, 0.910797119140625], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9058221578598022, 1.083267092704773, 1.0043275356292725, 0.9843852519989014, 0.9803749918937683, 0.9566378593444824, 0.9512718319892883, 0.9475162625312805, 0.9491708874702454, 0.9389604330062866, 0.9401636123657227, 0.9276586174964905, 0.9243559837341309, 0.928031325340271, 0.9207233786582947, 0.9140381217002869, 0.9197167754173279, 0.9159260392189026, 0.9136367440223694, 0.9155610203742981, 0.9135844707489014, 0.9088969230651855, 0.9095454812049866, 0.9111673831939697, 0.910797119140625]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9853423833847046
current iteration best possible eval_loss (full train run):  -0.910797119140625
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.6301 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.8482108803283341
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.8482108835128543
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.8482108835128481
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.848209185956673
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.8482108835128543
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1518, dtype=torch.float64), 0, tensor(0.0461, dtype=torch.float64), tensor(0.0396, dtype=torch.float64), tensor(0.0871, dtype=torch.float64), 0, 0, tensor(0.0467, dtype=torch.float64), tensor(0.6262, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 30, 0.06963112249929612, 27.147932249241055, 0]
normalized proposed parameters for next round by BO: [tensor(0.1518, dtype=torch.float64), tensor(4.6414e-17, dtype=torch.float64), tensor(0.0461, dtype=torch.float64), tensor(0.0396, dtype=torch.float64), tensor(0.0871, dtype=torch.float64), tensor(0.0026, dtype=torch.float64), tensor(9.7655e-18, dtype=torch.float64), tensor(0.0467, dtype=torch.float64), tensor(0.6262, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2307, dtype=torch.float64), tensor(0.6963, dtype=torch.float64), tensor(0.5656, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.152
  gsm8k: 0
  rowan_hellaswag: 0.046
  sciq: 0.04
  triviaqa: 0.087
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.047
  arc_challenge: 0.626

LoRA Parameters:
  lora_r: (30,)
  lora_dropout: (0.06963112249929612,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (27.147932249241055,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  30
lora dropout:  0.06963112249929612
lora alpha:  27.147932249241055
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 48,168,960 || all params: 8,078,430,208 || trainable%: 0.5963
length of training data:  9971
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7591, 'grad_norm': 1.0586069822311401, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2787023782730103, 'eval_runtime': 5.4876, 'eval_samples_per_second': 182.228, 'eval_steps_per_second': 11.48, 'epoch': 0.04}
{'loss': 1.1545, 'grad_norm': 0.424491822719574, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0193593502044678, 'eval_runtime': 5.5107, 'eval_samples_per_second': 181.465, 'eval_steps_per_second': 11.432, 'epoch': 0.08}
{'loss': 1.0096, 'grad_norm': 0.34481555223464966, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.9666323065757751, 'eval_runtime': 5.4848, 'eval_samples_per_second': 182.323, 'eval_steps_per_second': 11.486, 'epoch': 0.12}
{'loss': 0.9196, 'grad_norm': 0.4493509531021118, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.9425950050354004, 'eval_runtime': 5.4968, 'eval_samples_per_second': 181.923, 'eval_steps_per_second': 11.461, 'epoch': 0.16}
{'loss': 0.9032, 'grad_norm': 0.39467743039131165, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.9423269629478455, 'eval_runtime': 5.4993, 'eval_samples_per_second': 181.84, 'eval_steps_per_second': 11.456, 'epoch': 0.2}
{'loss': 0.8453, 'grad_norm': 0.44819149374961853, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.92951500415802, 'eval_runtime': 5.5054, 'eval_samples_per_second': 181.641, 'eval_steps_per_second': 11.443, 'epoch': 0.24}
{'loss': 0.843, 'grad_norm': 0.6215689182281494, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.9207260608673096, 'eval_runtime': 5.5097, 'eval_samples_per_second': 181.498, 'eval_steps_per_second': 11.434, 'epoch': 0.28}
{'loss': 0.8062, 'grad_norm': 0.5250077247619629, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.9125483632087708, 'eval_runtime': 5.5129, 'eval_samples_per_second': 181.393, 'eval_steps_per_second': 11.428, 'epoch': 0.32}
{'loss': 0.7639, 'grad_norm': 0.5558914542198181, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.9167478084564209, 'eval_runtime': 5.5043, 'eval_samples_per_second': 181.676, 'eval_steps_per_second': 11.446, 'epoch': 0.36}
{'loss': 0.719, 'grad_norm': 0.5455178022384644, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.908250629901886, 'eval_runtime': 5.5031, 'eval_samples_per_second': 181.716, 'eval_steps_per_second': 11.448, 'epoch': 0.4}
{'loss': 0.74, 'grad_norm': 0.5367652773857117, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.9050212502479553, 'eval_runtime': 5.5135, 'eval_samples_per_second': 181.372, 'eval_steps_per_second': 11.426, 'epoch': 0.44}
{'loss': 0.6521, 'grad_norm': 0.6904229521751404, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.9022757411003113, 'eval_runtime': 5.499, 'eval_samples_per_second': 181.851, 'eval_steps_per_second': 11.457, 'epoch': 0.48}
{'loss': 0.6619, 'grad_norm': 0.5641193389892578, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.9051754474639893, 'eval_runtime': 5.4918, 'eval_samples_per_second': 182.091, 'eval_steps_per_second': 11.472, 'epoch': 0.52}
{'loss': 0.6726, 'grad_norm': 0.4374675154685974, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.8951576948165894, 'eval_runtime': 5.4924, 'eval_samples_per_second': 182.071, 'eval_steps_per_second': 11.47, 'epoch': 0.56}
{'loss': 0.6413, 'grad_norm': 0.5755566954612732, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.8939013481140137, 'eval_runtime': 5.4793, 'eval_samples_per_second': 182.506, 'eval_steps_per_second': 11.498, 'epoch': 0.6}
{'loss': 0.5962, 'grad_norm': 0.8261063694953918, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.8894460201263428, 'eval_runtime': 5.4762, 'eval_samples_per_second': 182.609, 'eval_steps_per_second': 11.504, 'epoch': 0.64}
{'loss': 0.5909, 'grad_norm': 0.4740648567676544, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.8903601765632629, 'eval_runtime': 5.4808, 'eval_samples_per_second': 182.454, 'eval_steps_per_second': 11.495, 'epoch': 0.68}
{'loss': 0.6356, 'grad_norm': 0.45291560888290405, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.8898707032203674, 'eval_runtime': 5.4746, 'eval_samples_per_second': 182.663, 'eval_steps_per_second': 11.508, 'epoch': 0.72}
{'loss': 0.5651, 'grad_norm': 0.6308320760726929, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.8820820450782776, 'eval_runtime': 5.4796, 'eval_samples_per_second': 182.496, 'eval_steps_per_second': 11.497, 'epoch': 0.76}
{'loss': 0.5062, 'grad_norm': 0.5952642560005188, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.8821200728416443, 'eval_runtime': 5.481, 'eval_samples_per_second': 182.448, 'eval_steps_per_second': 11.494, 'epoch': 0.8}
{'loss': 0.5765, 'grad_norm': 0.674979031085968, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.880652666091919, 'eval_runtime': 5.4873, 'eval_samples_per_second': 182.238, 'eval_steps_per_second': 11.481, 'epoch': 0.84}
{'loss': 0.4899, 'grad_norm': 0.2963750660419464, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.8805267214775085, 'eval_runtime': 5.4861, 'eval_samples_per_second': 182.279, 'eval_steps_per_second': 11.484, 'epoch': 0.88}
{'loss': 0.5119, 'grad_norm': 0.5758669376373291, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.8792029023170471, 'eval_runtime': 5.4783, 'eval_samples_per_second': 182.537, 'eval_steps_per_second': 11.5, 'epoch': 0.92}
{'loss': 0.4609, 'grad_norm': 0.6024735569953918, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.8799960613250732, 'eval_runtime': 5.4852, 'eval_samples_per_second': 182.31, 'eval_steps_per_second': 11.486, 'epoch': 0.96}
{'train_runtime': 345.1218, 'train_samples_per_second': 28.891, 'train_steps_per_second': 1.808, 'train_loss': 0.7823427548775306, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2787023782730103, 1.0193593502044678, 0.9666323065757751, 0.9425950050354004, 0.9423269629478455, 0.92951500415802, 0.9207260608673096, 0.9125483632087708, 0.9167478084564209, 0.908250629901886, 0.9050212502479553, 0.9022757411003113, 0.9051754474639893, 0.8951576948165894, 0.8939013481140137, 0.8894460201263428, 0.8903601765632629, 0.8898707032203674, 0.8820820450782776, 0.8821200728416443, 0.880652666091919, 0.8805267214775085, 0.8792029023170471, 0.8799960613250732], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.2787023782730103, 1.0193593502044678, 0.9666323065757751, 0.9425950050354004, 0.9423269629478455, 0.92951500415802, 0.9207260608673096, 0.9125483632087708, 0.9167478084564209, 0.908250629901886, 0.9050212502479553, 0.9022757411003113, 0.9051754474639893, 0.8951576948165894, 0.8939013481140137, 0.8894460201263428, 0.8903601765632629, 0.8898707032203674, 0.8820820450782776, 0.8821200728416443, 0.880652666091919, 0.8805267214775085, 0.8792029023170471, 0.8799960613250732]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0026899576187134
current iteration best possible eval_loss (full train run):  -0.8799960613250732
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4218 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.007365286350250244, 0.05544036626815796, 0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9870830178260803, 0.754863977432251, 0.9104870557785034, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8787160515785217, 0.16884422302246094, 0.48048388957977295, 0.9369502067565918]  ‚Üí  acq = -0.9344461799487754
X = [0.06936377286911011, 0.632781982421875, 0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7510114908218384, 0.5261915326118469, 0.9568371772766113, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.13144083321094513, 0.18786925077438354, 0.9205564260482788, 0.21238505840301514]  ‚Üí  acq = -0.9344606802007643
X = [0.1816890835762024, 0.5671297311782837, 0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.3449631333351135, 0.8766421675682068, 0.5072073936462402, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.30366525053977966, 0.8914388418197632, 0.9394388198852539, 0.7902622818946838]  ‚Üí  acq = -0.9345860953078123
X = [0.4756810665130615, 0.8949254751205444, 0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.4715011417865753, 0.27662307024002075, 0.1057593822479248, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.19461123645305634, 0.8357580900192261, 0.8589680194854736, 0.6040682792663574]  ‚Üí  acq = -0.9344451527656892
X = [0.14415353536605835, 0.1752747893333435, 0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.6041354537010193, 0.18114352226257324, 0.5152944922447205, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.49458131194114685, 0.7254683971405029, 0.4729866683483124, 0.9238991737365723]  ‚Üí  acq = -0.9344447752200014
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1512, dtype=torch.float64), tensor(0.0466, dtype=torch.float64), 0, tensor(0.1535, dtype=torch.float64), tensor(0.0799, dtype=torch.float64), tensor(0.0904, dtype=torch.float64), 0, tensor(0.1263, dtype=torch.float64), tensor(0.3446, dtype=torch.float64), 25, 1, 1, 1, 0, 1, 43, 0.06951548211272905, 23.20359977342312, 0]
normalized proposed parameters for next round by BO: [tensor(0.1512, dtype=torch.float64), tensor(0.0466, dtype=torch.float64), tensor(0.0048, dtype=torch.float64), tensor(0.1535, dtype=torch.float64), tensor(0.0799, dtype=torch.float64), tensor(0.0904, dtype=torch.float64), tensor(0.0028, dtype=torch.float64), tensor(0.1263, dtype=torch.float64), tensor(0.3446, dtype=torch.float64), tensor(0.7961, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3325, dtype=torch.float64), tensor(0.6952, dtype=torch.float64), tensor(0.4834, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.151
  gsm8k: 0.047
  rowan_hellaswag: 0
  sciq: 0.154
  triviaqa: 0.08
  truthfulqa_gen: 0.09
  wikitext: 0
  mmlu: 0.126
  arc_challenge: 0.345

LoRA Parameters:
  lora_r: (43,)
  lora_dropout: (0.06951548211272905,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (23.20359977342312,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  43
lora dropout:  0.06951548211272905
lora alpha:  23.20359977342312
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,939,200 || all params: 8,084,200,448 || trainable%: 0.6672
length of training data:  9921
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0974, 'grad_norm': 0.7542099952697754, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6531306505203247, 'eval_runtime': 5.3432, 'eval_samples_per_second': 187.152, 'eval_steps_per_second': 11.791, 'epoch': 0.04}
{'loss': 1.345, 'grad_norm': 0.4083957374095917, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1922374963760376, 'eval_runtime': 5.3637, 'eval_samples_per_second': 186.437, 'eval_steps_per_second': 11.746, 'epoch': 0.08}
{'loss': 1.1792, 'grad_norm': 0.32138949632644653, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.0803054571151733, 'eval_runtime': 5.3417, 'eval_samples_per_second': 187.207, 'eval_steps_per_second': 11.794, 'epoch': 0.12}
{'loss': 1.0586, 'grad_norm': 0.28945261240005493, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.0318130254745483, 'eval_runtime': 5.4049, 'eval_samples_per_second': 185.018, 'eval_steps_per_second': 11.656, 'epoch': 0.16}
{'loss': 0.9881, 'grad_norm': 0.2797926068305969, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.9486152529716492, 'eval_runtime': 5.4001, 'eval_samples_per_second': 185.183, 'eval_steps_per_second': 11.667, 'epoch': 0.2}
{'loss': 0.9101, 'grad_norm': 0.2813533842563629, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.9360191226005554, 'eval_runtime': 5.4095, 'eval_samples_per_second': 184.862, 'eval_steps_per_second': 11.646, 'epoch': 0.24}
{'loss': 0.9276, 'grad_norm': 0.2593460977077484, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.9288167357444763, 'eval_runtime': 5.4026, 'eval_samples_per_second': 185.096, 'eval_steps_per_second': 11.661, 'epoch': 0.28}
{'loss': 0.8923, 'grad_norm': 0.2663611173629761, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.9221392273902893, 'eval_runtime': 5.4163, 'eval_samples_per_second': 184.628, 'eval_steps_per_second': 11.632, 'epoch': 0.32}
{'loss': 0.8984, 'grad_norm': 0.2608315050601959, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.9129676222801208, 'eval_runtime': 5.4208, 'eval_samples_per_second': 184.474, 'eval_steps_per_second': 11.622, 'epoch': 0.36}
{'loss': 0.8767, 'grad_norm': 0.25368860363960266, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.917727530002594, 'eval_runtime': 5.4342, 'eval_samples_per_second': 184.021, 'eval_steps_per_second': 11.593, 'epoch': 0.4}
{'loss': 0.8367, 'grad_norm': 0.29157134890556335, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.9148848652839661, 'eval_runtime': 5.4278, 'eval_samples_per_second': 184.237, 'eval_steps_per_second': 11.607, 'epoch': 0.44}
{'loss': 0.899, 'grad_norm': 0.31226810812950134, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.9105631113052368, 'eval_runtime': 5.4202, 'eval_samples_per_second': 184.495, 'eval_steps_per_second': 11.623, 'epoch': 0.48}
{'loss': 0.8491, 'grad_norm': 0.2994154095649719, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.9064388871192932, 'eval_runtime': 5.4481, 'eval_samples_per_second': 183.55, 'eval_steps_per_second': 11.564, 'epoch': 0.52}
{'loss': 0.8707, 'grad_norm': 0.2955256402492523, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.9097954630851746, 'eval_runtime': 5.437, 'eval_samples_per_second': 183.924, 'eval_steps_per_second': 11.587, 'epoch': 0.56}
{'loss': 0.8849, 'grad_norm': 0.339906245470047, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.9030274152755737, 'eval_runtime': 5.4486, 'eval_samples_per_second': 183.532, 'eval_steps_per_second': 11.563, 'epoch': 0.6}
{'loss': 0.8303, 'grad_norm': 0.3459327816963196, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.9014807343482971, 'eval_runtime': 5.4471, 'eval_samples_per_second': 183.585, 'eval_steps_per_second': 11.566, 'epoch': 0.64}
{'loss': 0.8194, 'grad_norm': 0.40748462080955505, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.8982832431793213, 'eval_runtime': 5.4426, 'eval_samples_per_second': 183.736, 'eval_steps_per_second': 11.575, 'epoch': 0.68}
{'loss': 0.8204, 'grad_norm': 0.3934386372566223, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.8986908197402954, 'eval_runtime': 5.4493, 'eval_samples_per_second': 183.51, 'eval_steps_per_second': 11.561, 'epoch': 0.72}
{'loss': 0.7619, 'grad_norm': 0.3294525146484375, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.8941307663917542, 'eval_runtime': 5.4376, 'eval_samples_per_second': 183.904, 'eval_steps_per_second': 11.586, 'epoch': 0.76}
{'loss': 0.7862, 'grad_norm': 0.4311566948890686, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.8968547582626343, 'eval_runtime': 5.4284, 'eval_samples_per_second': 184.215, 'eval_steps_per_second': 11.606, 'epoch': 0.81}
{'loss': 0.7867, 'grad_norm': 0.30290457606315613, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.8926070332527161, 'eval_runtime': 5.4716, 'eval_samples_per_second': 182.763, 'eval_steps_per_second': 11.514, 'epoch': 0.85}
{'loss': 0.7501, 'grad_norm': 0.44346874952316284, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.8918411731719971, 'eval_runtime': 5.4454, 'eval_samples_per_second': 183.641, 'eval_steps_per_second': 11.569, 'epoch': 0.89}
{'loss': 0.7607, 'grad_norm': 0.4919394850730896, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.8919546008110046, 'eval_runtime': 5.4072, 'eval_samples_per_second': 184.938, 'eval_steps_per_second': 11.651, 'epoch': 0.93}
{'loss': 0.7474, 'grad_norm': 0.5149019956588745, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.8902040719985962, 'eval_runtime': 5.4007, 'eval_samples_per_second': 185.163, 'eval_steps_per_second': 11.665, 'epoch': 0.97}
{'train_runtime': 315.4695, 'train_samples_per_second': 31.448, 'train_steps_per_second': 1.968, 'train_loss': 0.9735876012731481, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6531306505203247, 1.1922374963760376, 1.0803054571151733, 1.0318130254745483, 0.9486152529716492, 0.9360191226005554, 0.9288167357444763, 0.9221392273902893, 0.9129676222801208, 0.917727530002594, 0.9148848652839661, 0.9105631113052368, 0.9064388871192932, 0.9097954630851746, 0.9030274152755737, 0.9014807343482971, 0.8982832431793213, 0.8986908197402954, 0.8941307663917542, 0.8968547582626343, 0.8926070332527161, 0.8918411731719971, 0.8919546008110046, 0.8902040719985962], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6531306505203247, 1.1922374963760376, 1.0803054571151733, 1.0318130254745483, 0.9486152529716492, 0.9360191226005554, 0.9288167357444763, 0.9221392273902893, 0.9129676222801208, 0.917727530002594, 0.9148848652839661, 0.9105631113052368, 0.9064388871192932, 0.9097954630851746, 0.9030274152755737, 0.9014807343482971, 0.8982832431793213, 0.8986908197402954, 0.8941307663917542, 0.8968547582626343, 0.8926070332527161, 0.8918411731719971, 0.8919546008110046, 0.8902040719985962]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0178601741790771
current iteration best possible eval_loss (full train run):  -0.8902040719985962
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.8311 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -0.9083080472283936
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -0.9075971284142148
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -0.9075404807369363
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -0.9075408946645518
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -0.9075483595497282
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3110, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.0455, dtype=torch.float64), 0, tensor(0.0931, dtype=torch.float64), 0, 0, tensor(0.4910, dtype=torch.float64), 23, 0, 1, 1, 1, 0, 94, 0.02726666721070673, 16.139421552750363, 0]
normalized proposed parameters for next round by BO: [tensor(0.3110, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.0455, dtype=torch.float64), tensor(0.0035, dtype=torch.float64), tensor(0.0931, dtype=torch.float64), tensor(0.0062, dtype=torch.float64), tensor(0.0052, dtype=torch.float64), tensor(0.4910, dtype=torch.float64), tensor(0.7089, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7316, dtype=torch.float64), tensor(0.2727, dtype=torch.float64), tensor(0.3362, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.311
  gsm8k: 0.026
  rowan_hellaswag: 0.019
  sciq: 0.046
  triviaqa: 0
  truthfulqa_gen: 0.093
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.491

LoRA Parameters:
  lora_r: (94,)
  lora_dropout: (0.02726666721070673,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (16.139421552750363,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  94
lora dropout:  0.02726666721070673
lora alpha:  16.139421552750363
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 90,769,408 || all params: 8,121,030,656 || trainable%: 1.1177
length of training data:  9848
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2736, 'grad_norm': 0.6933653950691223, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7435520887374878, 'eval_runtime': 5.1134, 'eval_samples_per_second': 195.566, 'eval_steps_per_second': 12.321, 'epoch': 0.04}
{'loss': 1.2365, 'grad_norm': 0.18904000520706177, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0528093576431274, 'eval_runtime': 5.118, 'eval_samples_per_second': 195.387, 'eval_steps_per_second': 12.309, 'epoch': 0.08}
{'loss': 1.008, 'grad_norm': 0.15194441378116608, 'learning_rate': 0.0002872791519434629, 'epoch': 0.12}
{'eval_loss': 0.9702100157737732, 'eval_runtime': 5.1036, 'eval_samples_per_second': 195.94, 'eval_steps_per_second': 12.344, 'epoch': 0.12}
{'loss': 0.9077, 'grad_norm': 0.15327224135398865, 'learning_rate': 0.00027402826855123675, 'epoch': 0.16}
{'eval_loss': 0.9442391395568848, 'eval_runtime': 5.1159, 'eval_samples_per_second': 195.471, 'eval_steps_per_second': 12.315, 'epoch': 0.16}
{'loss': 0.9261, 'grad_norm': 0.15276563167572021, 'learning_rate': 0.00026077738515901055, 'epoch': 0.2}
{'eval_loss': 0.9368689656257629, 'eval_runtime': 5.1217, 'eval_samples_per_second': 195.247, 'eval_steps_per_second': 12.301, 'epoch': 0.2}
{'loss': 0.8864, 'grad_norm': 0.15865448117256165, 'learning_rate': 0.0002475265017667844, 'epoch': 0.24}
{'eval_loss': 0.9198713898658752, 'eval_runtime': 5.1241, 'eval_samples_per_second': 195.158, 'eval_steps_per_second': 12.295, 'epoch': 0.24}
{'loss': 0.8573, 'grad_norm': 0.16095463931560516, 'learning_rate': 0.00023427561837455828, 'epoch': 0.28}
{'eval_loss': 0.9128720760345459, 'eval_runtime': 5.1233, 'eval_samples_per_second': 195.187, 'eval_steps_per_second': 12.297, 'epoch': 0.28}
{'loss': 0.8512, 'grad_norm': 0.17284242808818817, 'learning_rate': 0.00022102473498233213, 'epoch': 0.32}
{'eval_loss': 0.910531222820282, 'eval_runtime': 5.1261, 'eval_samples_per_second': 195.079, 'eval_steps_per_second': 12.29, 'epoch': 0.32}
{'loss': 0.8307, 'grad_norm': 0.18798018991947174, 'learning_rate': 0.00020777385159010599, 'epoch': 0.37}
{'eval_loss': 0.9113612174987793, 'eval_runtime': 5.1428, 'eval_samples_per_second': 194.447, 'eval_steps_per_second': 12.25, 'epoch': 0.37}
{'loss': 0.8742, 'grad_norm': 0.17552651464939117, 'learning_rate': 0.00019452296819787987, 'epoch': 0.41}
{'eval_loss': 0.9046216011047363, 'eval_runtime': 5.1439, 'eval_samples_per_second': 194.405, 'eval_steps_per_second': 12.247, 'epoch': 0.41}
{'loss': 0.8387, 'grad_norm': 0.22108907997608185, 'learning_rate': 0.0001812720848056537, 'epoch': 0.45}
{'eval_loss': 0.8981450200080872, 'eval_runtime': 5.1473, 'eval_samples_per_second': 194.278, 'eval_steps_per_second': 12.24, 'epoch': 0.45}
{'loss': 0.8509, 'grad_norm': 0.1905684471130371, 'learning_rate': 0.00016802120141342754, 'epoch': 0.49}
{'eval_loss': 0.8930488228797913, 'eval_runtime': 5.1369, 'eval_samples_per_second': 194.672, 'eval_steps_per_second': 12.264, 'epoch': 0.49}
{'loss': 0.7872, 'grad_norm': 0.23075664043426514, 'learning_rate': 0.0001547703180212014, 'epoch': 0.53}
{'eval_loss': 0.8910804390907288, 'eval_runtime': 5.1323, 'eval_samples_per_second': 194.843, 'eval_steps_per_second': 12.275, 'epoch': 0.53}
{'loss': 0.787, 'grad_norm': 0.24675610661506653, 'learning_rate': 0.00014151943462897525, 'epoch': 0.57}
{'eval_loss': 0.8920226693153381, 'eval_runtime': 5.1392, 'eval_samples_per_second': 194.583, 'eval_steps_per_second': 12.259, 'epoch': 0.57}
{'loss': 0.7595, 'grad_norm': 0.2368192821741104, 'learning_rate': 0.0001282685512367491, 'epoch': 0.61}
{'eval_loss': 0.8848440051078796, 'eval_runtime': 5.127, 'eval_samples_per_second': 195.044, 'eval_steps_per_second': 12.288, 'epoch': 0.61}
{'loss': 0.7739, 'grad_norm': 0.2933538556098938, 'learning_rate': 0.00011501766784452296, 'epoch': 0.65}
{'eval_loss': 0.8861900568008423, 'eval_runtime': 5.1246, 'eval_samples_per_second': 195.137, 'eval_steps_per_second': 12.294, 'epoch': 0.65}
{'loss': 0.7222, 'grad_norm': 0.3245849609375, 'learning_rate': 0.00010176678445229682, 'epoch': 0.69}
{'eval_loss': 0.8866622447967529, 'eval_runtime': 5.1131, 'eval_samples_per_second': 195.574, 'eval_steps_per_second': 12.321, 'epoch': 0.69}
{'loss': 0.801, 'grad_norm': 0.28326383233070374, 'learning_rate': 8.851590106007066e-05, 'epoch': 0.73}
{'eval_loss': 0.8851909637451172, 'eval_runtime': 5.1192, 'eval_samples_per_second': 195.344, 'eval_steps_per_second': 12.307, 'epoch': 0.73}
{'loss': 0.7483, 'grad_norm': 0.3153701722621918, 'learning_rate': 7.526501766784451e-05, 'epoch': 0.77}
{'eval_loss': 0.8827189803123474, 'eval_runtime': 5.1276, 'eval_samples_per_second': 195.025, 'eval_steps_per_second': 12.287, 'epoch': 0.77}
{'loss': 0.7238, 'grad_norm': 0.29379820823669434, 'learning_rate': 6.201413427561837e-05, 'epoch': 0.81}
{'eval_loss': 0.8817315101623535, 'eval_runtime': 5.1193, 'eval_samples_per_second': 195.339, 'eval_steps_per_second': 12.306, 'epoch': 0.81}
{'loss': 0.6864, 'grad_norm': 0.36370500922203064, 'learning_rate': 4.876325088339222e-05, 'epoch': 0.85}
{'eval_loss': 0.8764566779136658, 'eval_runtime': 5.1225, 'eval_samples_per_second': 195.216, 'eval_steps_per_second': 12.299, 'epoch': 0.85}
{'loss': 0.6758, 'grad_norm': 0.40163087844848633, 'learning_rate': 3.551236749116607e-05, 'epoch': 0.89}
{'eval_loss': 0.8794741034507751, 'eval_runtime': 5.1193, 'eval_samples_per_second': 195.339, 'eval_steps_per_second': 12.306, 'epoch': 0.89}
{'loss': 0.6611, 'grad_norm': 0.33107540011405945, 'learning_rate': 2.2261484098939926e-05, 'epoch': 0.93}
{'eval_loss': 0.8799855709075928, 'eval_runtime': 5.1303, 'eval_samples_per_second': 194.92, 'eval_steps_per_second': 12.28, 'epoch': 0.93}
{'loss': 0.6792, 'grad_norm': 0.3019550144672394, 'learning_rate': 9.010600706713779e-06, 'epoch': 0.97}
{'eval_loss': 0.8769344687461853, 'eval_runtime': 5.1129, 'eval_samples_per_second': 195.583, 'eval_steps_per_second': 12.322, 'epoch': 0.97}
{'train_runtime': 281.5049, 'train_samples_per_second': 34.983, 'train_steps_per_second': 2.188, 'train_loss': 0.9156204911021443, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7435520887374878, 1.0528093576431274, 0.9702100157737732, 0.9442391395568848, 0.9368689656257629, 0.9198713898658752, 0.9128720760345459, 0.910531222820282, 0.9113612174987793, 0.9046216011047363, 0.8981450200080872, 0.8930488228797913, 0.8910804390907288, 0.8920226693153381, 0.8848440051078796, 0.8861900568008423, 0.8866622447967529, 0.8851909637451172, 0.8827189803123474, 0.8817315101623535, 0.8764566779136658, 0.8794741034507751, 0.8799855709075928, 0.8769344687461853], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7435520887374878, 1.0528093576431274, 0.9702100157737732, 0.9442391395568848, 0.9368689656257629, 0.9198713898658752, 0.9128720760345459, 0.910531222820282, 0.9113612174987793, 0.9046216011047363, 0.8981450200080872, 0.8930488228797913, 0.8910804390907288, 0.8920226693153381, 0.8848440051078796, 0.8861900568008423, 0.8866622447967529, 0.8851909637451172, 0.8827189803123474, 0.8817315101623535, 0.8764566779136658, 0.8794741034507751, 0.8799855709075928, 0.8769344687461853]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0726120471954346
current iteration best possible eval_loss (full train run):  -0.8769344687461853
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.1295 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9994975924491882, 0.07707124948501587, 0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5594547390937805, 0.1537771224975586, 0.817682683467865, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.14512693881988525, 0.883480966091156, 0.9173967838287354, 0.8848122358322144]  ‚Üí  acq = -0.9618712652753307
X = [0.9514877796173096, 0.9132158160209656, 0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.41754159331321716, 0.28531861305236816, 0.5411667227745056, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.23015734553337097, 0.9975385665893555, 0.2174660712480545, 0.13808739185333252]  ‚Üí  acq = -0.9616443677475401
X = [0.39850908517837524, 0.8170029520988464, 0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5720815062522888, 0.12135124206542969, 0.050965309143066406, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8528783321380615, 0.15156877040863037, 0.1581156700849533, 0.8321003913879395]  ‚Üí  acq = -0.9616563113221177
X = [0.28970110416412354, 0.6408820152282715, 0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7433760762214661, 0.9123662114143372, 0.8424274921417236, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5417225956916809, 0.7911179065704346, 0.9902287721633911, 0.020802080631256104]  ‚Üí  acq = -0.9616435211693894
X = [0.5538846254348755, 0.17085957527160645, 0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6919615864753723, 0.1586219072341919, 0.2204926609992981, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.45917418599128723, 0.6860421299934387, 0.8713036775588989, 0.5007997155189514]  ‚Üí  acq = -0.9616435342487725
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2691, dtype=torch.float64), tensor(0.1391, dtype=torch.float64), tensor(0.0723, dtype=torch.float64), tensor(0.0257, dtype=torch.float64), tensor(0.1434, dtype=torch.float64), tensor(0.0939, dtype=torch.float64), tensor(0.0140, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(0.2027, dtype=torch.float64), 25, 0, 1, 1, 1, 0, 95, 0.028864645837079178, 19.910828630969426, 0]
normalized proposed parameters for next round by BO: [tensor(0.2691, dtype=torch.float64), tensor(0.1391, dtype=torch.float64), tensor(0.0723, dtype=torch.float64), tensor(0.0257, dtype=torch.float64), tensor(0.1434, dtype=torch.float64), tensor(0.0939, dtype=torch.float64), tensor(0.0140, dtype=torch.float64), tensor(0.0399, dtype=torch.float64), tensor(0.2027, dtype=torch.float64), tensor(0.7861, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7394, dtype=torch.float64), tensor(0.2886, dtype=torch.float64), tensor(0.4148, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.269
  gsm8k: 0.139
  rowan_hellaswag: 0.072
  sciq: 0.026
  triviaqa: 0.143
  truthfulqa_gen: 0.094
  wikitext: 0.014
  mmlu: 0.04
  arc_challenge: 0.203

LoRA Parameters:
  lora_r: (95,)
  lora_dropout: (0.028864645837079178,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (19.910828630969426,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  95
lora dropout:  0.028864645837079178
lora alpha:  19.910828630969426
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 99,712,000 || all params: 8,129,973,248 || trainable%: 1.2265
length of training data:  9993
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1734, 'grad_norm': 0.6946541666984558, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7667434215545654, 'eval_runtime': 5.3766, 'eval_samples_per_second': 185.992, 'eval_steps_per_second': 11.717, 'epoch': 0.04}
{'loss': 1.497, 'grad_norm': 0.3497235178947449, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0000817775726318, 'eval_runtime': 5.3854, 'eval_samples_per_second': 185.687, 'eval_steps_per_second': 11.698, 'epoch': 0.08}
{'loss': 1.1489, 'grad_norm': 0.20488019287586212, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9467530846595764, 'eval_runtime': 5.396, 'eval_samples_per_second': 185.323, 'eval_steps_per_second': 11.675, 'epoch': 0.12}
{'loss': 1.1089, 'grad_norm': 0.16738490760326385, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9391278028488159, 'eval_runtime': 5.4139, 'eval_samples_per_second': 184.71, 'eval_steps_per_second': 11.637, 'epoch': 0.16}
{'loss': 1.0702, 'grad_norm': 0.1673571765422821, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9193139672279358, 'eval_runtime': 5.4129, 'eval_samples_per_second': 184.745, 'eval_steps_per_second': 11.639, 'epoch': 0.2}
{'loss': 1.0622, 'grad_norm': 0.17811834812164307, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.918229877948761, 'eval_runtime': 5.4148, 'eval_samples_per_second': 184.678, 'eval_steps_per_second': 11.635, 'epoch': 0.24}
{'loss': 1.018, 'grad_norm': 0.16225023567676544, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.924186646938324, 'eval_runtime': 5.4162, 'eval_samples_per_second': 184.63, 'eval_steps_per_second': 11.632, 'epoch': 0.28}
{'loss': 1.0245, 'grad_norm': 0.15366224944591522, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9093697667121887, 'eval_runtime': 5.4087, 'eval_samples_per_second': 184.888, 'eval_steps_per_second': 11.648, 'epoch': 0.32}
{'loss': 1.0167, 'grad_norm': 0.2010892629623413, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9095219373703003, 'eval_runtime': 5.4405, 'eval_samples_per_second': 183.805, 'eval_steps_per_second': 11.58, 'epoch': 0.36}
{'loss': 1.0454, 'grad_norm': 0.18870843946933746, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8987727761268616, 'eval_runtime': 5.3876, 'eval_samples_per_second': 185.613, 'eval_steps_per_second': 11.694, 'epoch': 0.4}
{'loss': 1.039, 'grad_norm': 0.17492321133613586, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8994325995445251, 'eval_runtime': 5.3673, 'eval_samples_per_second': 186.314, 'eval_steps_per_second': 11.738, 'epoch': 0.44}
{'loss': 1.0013, 'grad_norm': 0.16332708299160004, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8934853076934814, 'eval_runtime': 5.3644, 'eval_samples_per_second': 186.414, 'eval_steps_per_second': 11.744, 'epoch': 0.48}
{'loss': 1.0415, 'grad_norm': 0.16460144519805908, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8905821442604065, 'eval_runtime': 5.3635, 'eval_samples_per_second': 186.447, 'eval_steps_per_second': 11.746, 'epoch': 0.52}
{'loss': 1.0687, 'grad_norm': 0.2059948891401291, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.889817476272583, 'eval_runtime': 5.3606, 'eval_samples_per_second': 186.546, 'eval_steps_per_second': 11.752, 'epoch': 0.56}
{'loss': 0.9495, 'grad_norm': 0.17480690777301788, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8888189196586609, 'eval_runtime': 5.3539, 'eval_samples_per_second': 186.778, 'eval_steps_per_second': 11.767, 'epoch': 0.6}
{'loss': 1.0384, 'grad_norm': 0.17777900397777557, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8858128786087036, 'eval_runtime': 5.3691, 'eval_samples_per_second': 186.249, 'eval_steps_per_second': 11.734, 'epoch': 0.64}
{'loss': 1.0031, 'grad_norm': 0.24103078246116638, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8807870149612427, 'eval_runtime': 5.3732, 'eval_samples_per_second': 186.107, 'eval_steps_per_second': 11.725, 'epoch': 0.68}
{'loss': 0.9947, 'grad_norm': 0.1578013300895691, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8779453635215759, 'eval_runtime': 5.3571, 'eval_samples_per_second': 186.668, 'eval_steps_per_second': 11.76, 'epoch': 0.72}
{'loss': 0.9655, 'grad_norm': 0.17071717977523804, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8780372142791748, 'eval_runtime': 5.3549, 'eval_samples_per_second': 186.745, 'eval_steps_per_second': 11.765, 'epoch': 0.76}
{'loss': 1.0186, 'grad_norm': 0.17059914767742157, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8776191473007202, 'eval_runtime': 5.3516, 'eval_samples_per_second': 186.859, 'eval_steps_per_second': 11.772, 'epoch': 0.8}
{'loss': 1.0093, 'grad_norm': 0.17730459570884705, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8756147623062134, 'eval_runtime': 5.3572, 'eval_samples_per_second': 186.664, 'eval_steps_per_second': 11.76, 'epoch': 0.84}
{'loss': 0.9963, 'grad_norm': 0.21376438438892365, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8726122379302979, 'eval_runtime': 5.3606, 'eval_samples_per_second': 186.545, 'eval_steps_per_second': 11.752, 'epoch': 0.88}
{'loss': 0.9664, 'grad_norm': 0.21682927012443542, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8730587363243103, 'eval_runtime': 5.3537, 'eval_samples_per_second': 186.786, 'eval_steps_per_second': 11.767, 'epoch': 0.92}
{'loss': 0.9481, 'grad_norm': 0.18180254101753235, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8713237643241882, 'eval_runtime': 5.3548, 'eval_samples_per_second': 186.75, 'eval_steps_per_second': 11.765, 'epoch': 0.96}
{'loss': 0.9772, 'grad_norm': 0.23490066826343536, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8712455034255981, 'eval_runtime': 5.35, 'eval_samples_per_second': 186.916, 'eval_steps_per_second': 11.776, 'epoch': 1.0}
{'train_runtime': 345.764, 'train_samples_per_second': 28.901, 'train_steps_per_second': 1.808, 'train_loss': 1.1273173767089844, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7667434215545654, 1.0000817775726318, 0.9467530846595764, 0.9391278028488159, 0.9193139672279358, 0.918229877948761, 0.924186646938324, 0.9093697667121887, 0.9095219373703003, 0.8987727761268616, 0.8994325995445251, 0.8934853076934814, 0.8905821442604065, 0.889817476272583, 0.8888189196586609, 0.8858128786087036, 0.8807870149612427, 0.8779453635215759, 0.8780372142791748, 0.8776191473007202, 0.8756147623062134, 0.8726122379302979, 0.8730587363243103, 0.8713237643241882, 0.8712455034255981], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7667434215545654, 1.0000817775726318, 0.9467530846595764, 0.9391278028488159, 0.9193139672279358, 0.918229877948761, 0.924186646938324, 0.9093697667121887, 0.9095219373703003, 0.8987727761268616, 0.8994325995445251, 0.8934853076934814, 0.8905821442604065, 0.889817476272583, 0.8888189196586609, 0.8858128786087036, 0.8807870149612427, 0.8779453635215759, 0.8780372142791748, 0.8776191473007202, 0.8756147623062134, 0.8726122379302979, 0.8730587363243103, 0.8713237643241882, 0.8712455034255981]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0783615112304688
current iteration best possible eval_loss (full train run):  -0.8712455034255981
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.8688 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.9471147519536198
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9472267868685049
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.9499002460259872
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9471199273690798
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.9471198223897703
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1373, dtype=torch.float64), 0, tensor(0.0883, dtype=torch.float64), tensor(0.0904, dtype=torch.float64), tensor(0.0940, dtype=torch.float64), tensor(0.1581, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.0317, dtype=torch.float64), tensor(0.3744, dtype=torch.float64), 28, 0, 1, 1, 1, 1, 75, 0.018234822411539616, 26.872567487706792, 0]
normalized proposed parameters for next round by BO: [tensor(0.1373, dtype=torch.float64), tensor(1.7264e-18, dtype=torch.float64), tensor(0.0883, dtype=torch.float64), tensor(0.0904, dtype=torch.float64), tensor(0.0940, dtype=torch.float64), tensor(0.1581, dtype=torch.float64), tensor(0.0259, dtype=torch.float64), tensor(0.0317, dtype=torch.float64), tensor(0.3744, dtype=torch.float64), tensor(0.8811, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5874, dtype=torch.float64), tensor(0.1823, dtype=torch.float64), tensor(0.5598, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.137
  gsm8k: 0
  rowan_hellaswag: 0.088
  sciq: 0.09
  triviaqa: 0.094
  truthfulqa_gen: 0.158
  wikitext: 0.026
  mmlu: 0.032
  arc_challenge: 0.374

LoRA Parameters:
  lora_r: (75,)
  lora_dropout: (0.018234822411539616,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (26.872567487706792,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  75
lora dropout:  0.018234822411539616
lora alpha:  26.872567487706792
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 126,873,600 || all params: 8,157,134,848 || trainable%: 1.5554
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.049, 'grad_norm': 0.8866368532180786, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4237775802612305, 'eval_runtime': 5.7996, 'eval_samples_per_second': 172.427, 'eval_steps_per_second': 10.863, 'epoch': 0.04}
{'loss': 1.3976, 'grad_norm': 0.29243505001068115, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0169432163238525, 'eval_runtime': 5.8109, 'eval_samples_per_second': 172.09, 'eval_steps_per_second': 10.842, 'epoch': 0.08}
{'loss': 1.1084, 'grad_norm': 0.27366212010383606, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9719802737236023, 'eval_runtime': 5.8095, 'eval_samples_per_second': 172.131, 'eval_steps_per_second': 10.844, 'epoch': 0.12}
{'loss': 1.0238, 'grad_norm': 0.2393963485956192, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9596564769744873, 'eval_runtime': 5.8078, 'eval_samples_per_second': 172.181, 'eval_steps_per_second': 10.847, 'epoch': 0.16}
{'loss': 1.1047, 'grad_norm': 0.24388350546360016, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9313698410987854, 'eval_runtime': 5.8089, 'eval_samples_per_second': 172.149, 'eval_steps_per_second': 10.845, 'epoch': 0.2}
{'loss': 1.051, 'grad_norm': 0.23498599231243134, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9411830902099609, 'eval_runtime': 5.8205, 'eval_samples_per_second': 171.805, 'eval_steps_per_second': 10.824, 'epoch': 0.24}
{'loss': 1.0423, 'grad_norm': 0.2221754789352417, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9239606857299805, 'eval_runtime': 5.8342, 'eval_samples_per_second': 171.402, 'eval_steps_per_second': 10.798, 'epoch': 0.28}
{'loss': 1.0378, 'grad_norm': 0.26607194542884827, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9203491806983948, 'eval_runtime': 5.8366, 'eval_samples_per_second': 171.332, 'eval_steps_per_second': 10.794, 'epoch': 0.32}
{'loss': 0.986, 'grad_norm': 0.2858482301235199, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9226610064506531, 'eval_runtime': 5.842, 'eval_samples_per_second': 171.174, 'eval_steps_per_second': 10.784, 'epoch': 0.36}
{'loss': 0.9865, 'grad_norm': 0.32446354627609253, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9131976962089539, 'eval_runtime': 5.8381, 'eval_samples_per_second': 171.289, 'eval_steps_per_second': 10.791, 'epoch': 0.4}
{'loss': 0.9478, 'grad_norm': 0.31529054045677185, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9150658249855042, 'eval_runtime': 5.8259, 'eval_samples_per_second': 171.647, 'eval_steps_per_second': 10.814, 'epoch': 0.44}
{'loss': 0.9411, 'grad_norm': 0.29733601212501526, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9106428027153015, 'eval_runtime': 5.8278, 'eval_samples_per_second': 171.591, 'eval_steps_per_second': 10.81, 'epoch': 0.48}
{'loss': 0.9521, 'grad_norm': 0.3027225732803345, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9026162028312683, 'eval_runtime': 5.8314, 'eval_samples_per_second': 171.484, 'eval_steps_per_second': 10.803, 'epoch': 0.52}
{'loss': 0.9364, 'grad_norm': 0.27540725469589233, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8969330787658691, 'eval_runtime': 5.834, 'eval_samples_per_second': 171.409, 'eval_steps_per_second': 10.799, 'epoch': 0.56}
{'loss': 0.9267, 'grad_norm': 0.4443252384662628, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8955559730529785, 'eval_runtime': 5.8285, 'eval_samples_per_second': 171.571, 'eval_steps_per_second': 10.809, 'epoch': 0.6}
{'loss': 0.8929, 'grad_norm': 0.3932260572910309, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8952732086181641, 'eval_runtime': 5.8317, 'eval_samples_per_second': 171.476, 'eval_steps_per_second': 10.803, 'epoch': 0.64}
{'loss': 0.8667, 'grad_norm': 0.37341585755348206, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9010749459266663, 'eval_runtime': 5.8269, 'eval_samples_per_second': 171.619, 'eval_steps_per_second': 10.812, 'epoch': 0.68}
{'loss': 0.8413, 'grad_norm': 0.32281604409217834, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8919323682785034, 'eval_runtime': 5.8297, 'eval_samples_per_second': 171.534, 'eval_steps_per_second': 10.807, 'epoch': 0.72}
{'loss': 0.7686, 'grad_norm': 0.3498934507369995, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8890832662582397, 'eval_runtime': 5.8264, 'eval_samples_per_second': 171.632, 'eval_steps_per_second': 10.813, 'epoch': 0.76}
{'loss': 0.8403, 'grad_norm': 0.3130345940589905, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8922947645187378, 'eval_runtime': 5.8192, 'eval_samples_per_second': 171.845, 'eval_steps_per_second': 10.826, 'epoch': 0.8}
{'loss': 0.8698, 'grad_norm': 0.45026451349258423, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8894021511077881, 'eval_runtime': 5.8226, 'eval_samples_per_second': 171.744, 'eval_steps_per_second': 10.82, 'epoch': 0.84}
{'loss': 0.8292, 'grad_norm': 0.31801703572273254, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8891959190368652, 'eval_runtime': 5.8533, 'eval_samples_per_second': 170.845, 'eval_steps_per_second': 10.763, 'epoch': 0.88}
{'loss': 0.7852, 'grad_norm': 0.4852009117603302, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.885577380657196, 'eval_runtime': 5.8571, 'eval_samples_per_second': 170.732, 'eval_steps_per_second': 10.756, 'epoch': 0.92}
{'loss': 0.8004, 'grad_norm': 0.3805547058582306, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8858448266983032, 'eval_runtime': 5.8279, 'eval_samples_per_second': 171.59, 'eval_steps_per_second': 10.81, 'epoch': 0.96}
{'loss': 0.7658, 'grad_norm': 0.4371958374977112, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8846457004547119, 'eval_runtime': 5.8196, 'eval_samples_per_second': 171.833, 'eval_steps_per_second': 10.825, 'epoch': 1.0}
{'train_runtime': 369.6747, 'train_samples_per_second': 27.04, 'train_steps_per_second': 1.691, 'train_loss': 1.0300583740234375, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4237775802612305, 1.0169432163238525, 0.9719802737236023, 0.9596564769744873, 0.9313698410987854, 0.9411830902099609, 0.9239606857299805, 0.9203491806983948, 0.9226610064506531, 0.9131976962089539, 0.9150658249855042, 0.9106428027153015, 0.9026162028312683, 0.8969330787658691, 0.8955559730529785, 0.8952732086181641, 0.9010749459266663, 0.8919323682785034, 0.8890832662582397, 0.8922947645187378, 0.8894021511077881, 0.8891959190368652, 0.885577380657196, 0.8858448266983032, 0.8846457004547119], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4237775802612305, 1.0169432163238525, 0.9719802737236023, 0.9596564769744873, 0.9313698410987854, 0.9411830902099609, 0.9239606857299805, 0.9203491806983948, 0.9226610064506531, 0.9131976962089539, 0.9150658249855042, 0.9106428027153015, 0.9026162028312683, 0.8969330787658691, 0.8955559730529785, 0.8952732086181641, 0.9010749459266663, 0.8919323682785034, 0.8890832662582397, 0.8922947645187378, 0.8894021511077881, 0.8891959190368652, 0.885577380657196, 0.8858448266983032, 0.8846457004547119]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0563428401947021
current iteration best possible eval_loss (full train run):  -0.8846457004547119
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4197 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.2952781915664673, 0.1894705891609192, 0.750534176826477, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.2380649447441101, 0.6103376746177673, 0.05392260104417801, 0.1532604694366455, 0.929862380027771, 0.01835566759109497]  ‚Üí  acq = -0.9789968172596883
X = [0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.46430331468582153, 0.852687656879425, 0.4268176257610321, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7904440760612488, 0.03410828113555908, 0.9552485346794128, 0.6570152640342712, 0.40869808197021484, 0.1672157645225525]  ‚Üí  acq = -0.9646265484645752
X = [0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8143539428710938, 0.09181463718414307, 0.8008258938789368, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8474487662315369, 0.8251820206642151, 0.8353192210197449, 0.5925764441490173, 0.7678316831588745, 0.9365105628967285]  ‚Üí  acq = -0.9815593457217401
X = [0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.11066454648971558, 0.7609574198722839, 0.3981233835220337, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9708411693572998, 0.19831812381744385, 0.37542372941970825, 0.6161256432533264, 0.05335615947842598, 0.5331325531005859]  ‚Üí  acq = -0.9646265484642192
X = [0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8019734621047974, 0.707656979560852, 0.6432923674583435, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.4109269380569458, 0.38733386993408203, 0.7676031589508057, 0.287418007850647, 0.42260944843292236, 0.8850849866867065]  ‚Üí  acq = -0.9644795846835875
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [0, tensor(0.4145, dtype=torch.float64), 0, tensor(0.2025, dtype=torch.float64), tensor(0.0934, dtype=torch.float64), tensor(0.0240, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.1209, dtype=torch.float64), tensor(0.1269, dtype=torch.float64), 26, 1, 0, 1, 0, 0, 47, 0.05687618171755198, 40.856770248123354, 0]
normalized proposed parameters for next round by BO: [tensor(1.3502e-18, dtype=torch.float64), tensor(0.4145, dtype=torch.float64), tensor(0.0058, dtype=torch.float64), tensor(0.2025, dtype=torch.float64), tensor(0.0934, dtype=torch.float64), tensor(0.0240, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.1209, dtype=torch.float64), tensor(0.1269, dtype=torch.float64), tensor(0.8078, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3693, dtype=torch.float64), tensor(0.5688, dtype=torch.float64), tensor(0.8512, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0
  gsm8k: 0.414
  rowan_hellaswag: 0
  sciq: 0.202
  triviaqa: 0.093
  truthfulqa_gen: 0.024
  wikitext: 0.012
  mmlu: 0.121
  arc_challenge: 0.127

LoRA Parameters:
  lora_r: (47,)
  lora_dropout: (0.05687618171755198,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (40.856770248123354,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  47
lora dropout:  0.05687618171755198
lora alpha:  40.856770248123354
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 32,534,528 || all params: 8,062,795,776 || trainable%: 0.4035
length of training data:  9938
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.4114, 'grad_norm': 0.8096429109573364, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.404867172241211, 'eval_runtime': 4.9058, 'eval_samples_per_second': 203.842, 'eval_steps_per_second': 12.842, 'epoch': 0.04}
{'loss': 1.265, 'grad_norm': 0.33439701795578003, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.815148949623108, 'eval_runtime': 4.9043, 'eval_samples_per_second': 203.903, 'eval_steps_per_second': 12.846, 'epoch': 0.08}
{'loss': 1.1205, 'grad_norm': 0.28884202241897583, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 1.7442166805267334, 'eval_runtime': 4.9107, 'eval_samples_per_second': 203.636, 'eval_steps_per_second': 12.829, 'epoch': 0.12}
{'loss': 1.0775, 'grad_norm': 0.2847684323787689, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 1.702623724937439, 'eval_runtime': 4.923, 'eval_samples_per_second': 203.126, 'eval_steps_per_second': 12.797, 'epoch': 0.16}
{'loss': 1.0025, 'grad_norm': 0.26404067873954773, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 1.6711276769638062, 'eval_runtime': 4.9131, 'eval_samples_per_second': 203.539, 'eval_steps_per_second': 12.823, 'epoch': 0.2}
{'loss': 0.96, 'grad_norm': 0.25557655096054077, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 1.6817140579223633, 'eval_runtime': 4.9163, 'eval_samples_per_second': 203.403, 'eval_steps_per_second': 12.814, 'epoch': 0.24}
{'loss': 0.9371, 'grad_norm': 0.21932514011859894, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 1.6563562154769897, 'eval_runtime': 4.9225, 'eval_samples_per_second': 203.15, 'eval_steps_per_second': 12.798, 'epoch': 0.28}
{'loss': 0.9132, 'grad_norm': 0.2704908549785614, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 1.668424367904663, 'eval_runtime': 4.9183, 'eval_samples_per_second': 203.323, 'eval_steps_per_second': 12.809, 'epoch': 0.32}
{'loss': 0.9504, 'grad_norm': 0.20401746034622192, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 1.6495051383972168, 'eval_runtime': 4.9258, 'eval_samples_per_second': 203.013, 'eval_steps_per_second': 12.79, 'epoch': 0.36}
{'loss': 0.8917, 'grad_norm': 0.27950170636177063, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 1.655375599861145, 'eval_runtime': 4.9396, 'eval_samples_per_second': 202.444, 'eval_steps_per_second': 12.754, 'epoch': 0.4}
{'loss': 0.9361, 'grad_norm': 0.2265809178352356, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 1.6684008836746216, 'eval_runtime': 4.9707, 'eval_samples_per_second': 201.181, 'eval_steps_per_second': 12.674, 'epoch': 0.44}
{'loss': 0.8636, 'grad_norm': 0.25385865569114685, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 1.6800329685211182, 'eval_runtime': 4.9593, 'eval_samples_per_second': 201.643, 'eval_steps_per_second': 12.704, 'epoch': 0.48}
{'loss': 0.8954, 'grad_norm': 0.603602409362793, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 1.6971279382705688, 'eval_runtime': 4.9571, 'eval_samples_per_second': 201.731, 'eval_steps_per_second': 12.709, 'epoch': 0.52}
{'loss': 0.9105, 'grad_norm': 0.27340662479400635, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 1.692562222480774, 'eval_runtime': 4.9492, 'eval_samples_per_second': 202.054, 'eval_steps_per_second': 12.729, 'epoch': 0.56}
{'loss': 0.891, 'grad_norm': 0.25963568687438965, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 1.7090084552764893, 'eval_runtime': 4.9422, 'eval_samples_per_second': 202.339, 'eval_steps_per_second': 12.747, 'epoch': 0.6}
{'loss': 0.8553, 'grad_norm': 0.20936773717403412, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 1.7442402839660645, 'eval_runtime': 4.9391, 'eval_samples_per_second': 202.466, 'eval_steps_per_second': 12.755, 'epoch': 0.64}
{'loss': 0.8765, 'grad_norm': 0.24161964654922485, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 1.7167246341705322, 'eval_runtime': 4.9448, 'eval_samples_per_second': 202.234, 'eval_steps_per_second': 12.741, 'epoch': 0.68}
{'loss': 0.8738, 'grad_norm': 0.26825568079948425, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 1.7410995960235596, 'eval_runtime': 4.9294, 'eval_samples_per_second': 202.866, 'eval_steps_per_second': 12.781, 'epoch': 0.72}
{'loss': 0.8715, 'grad_norm': 0.26051339507102966, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 1.7628806829452515, 'eval_runtime': 4.9281, 'eval_samples_per_second': 202.917, 'eval_steps_per_second': 12.784, 'epoch': 0.76}
{'loss': 0.8877, 'grad_norm': 0.2574068605899811, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 1.7847275733947754, 'eval_runtime': 4.9302, 'eval_samples_per_second': 202.832, 'eval_steps_per_second': 12.778, 'epoch': 0.8}
{'loss': 0.8987, 'grad_norm': 0.24412007629871368, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 1.768653154373169, 'eval_runtime': 4.9449, 'eval_samples_per_second': 202.227, 'eval_steps_per_second': 12.74, 'epoch': 0.84}
{'loss': 0.9129, 'grad_norm': 0.25609704852104187, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 1.7682774066925049, 'eval_runtime': 4.9544, 'eval_samples_per_second': 201.839, 'eval_steps_per_second': 12.716, 'epoch': 0.88}
{'loss': 0.8905, 'grad_norm': 0.2648937404155731, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 1.7735264301300049, 'eval_runtime': 4.9513, 'eval_samples_per_second': 201.967, 'eval_steps_per_second': 12.724, 'epoch': 0.92}
{'loss': 0.869, 'grad_norm': 0.2540130913257599, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 1.7723358869552612, 'eval_runtime': 4.9575, 'eval_samples_per_second': 201.713, 'eval_steps_per_second': 12.708, 'epoch': 0.96}
{'train_runtime': 313.5827, 'train_samples_per_second': 31.692, 'train_steps_per_second': 1.984, 'train_loss': 0.994476318359375, 'epoch': 1.0}
train_results:  {'eval_loss': [2.404867172241211, 1.815148949623108, 1.7442166805267334, 1.702623724937439, 1.6711276769638062, 1.6817140579223633, 1.6563562154769897, 1.668424367904663, 1.6495051383972168, 1.655375599861145, 1.6684008836746216, 1.6800329685211182, 1.6971279382705688, 1.692562222480774, 1.7090084552764893, 1.7442402839660645, 1.7167246341705322, 1.7410995960235596, 1.7628806829452515, 1.7847275733947754, 1.768653154373169, 1.7682774066925049, 1.7735264301300049, 1.7723358869552612], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.404867172241211, 1.815148949623108, 1.7442166805267334, 1.702623724937439, 1.6711276769638062, 1.6817140579223633, 1.6563562154769897, 1.668424367904663, 1.6495051383972168, 1.655375599861145, 1.6684008836746216, 1.6800329685211182, 1.6971279382705688, 1.692562222480774, 1.7090084552764893, 1.7442402839660645, 1.7167246341705322, 1.7410995960235596, 1.7628806829452515, 1.7847275733947754, 1.768653154373169, 1.7682774066925049, 1.7735264301300049, 1.7723358869552612]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2032301425933838
current iteration best possible eval_loss (full train run):  -1.7723358869552612
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.9681 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6823287010192871, 0.1862812042236328, 0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8836188316345215, 0.9964625239372253, 0.7934750914573669, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.33229848742485046, 0.9432199001312256, 0.3315914273262024, 0.7924636006355286]  ‚Üí  acq = -0.937733765695443
X = [0.6971794366836548, 0.0029845833778381348, 0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9298456907272339, 0.9158058762550354, 0.6235672831535339, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5776915550231934, 0.40536046028137207, 0.8601958751678467, 0.07649117708206177]  ‚Üí  acq = -0.9350523627437184
X = [0.7746965289115906, 0.45509761571884155, 0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.08178042620420456, 0.07738137245178223, 0.8753153681755066, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7854319214820862, 0.9972329139709473, 0.8323233127593994, 0.5278875827789307]  ‚Üí  acq = -0.937737139972314
X = [0.6284062266349792, 0.25792115926742554, 0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.653922438621521, 0.4825098514556885, 0.12023776769638062, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.750758707523346, 0.8871928453445435, 0.8711596727371216, 0.8900622725486755]  ‚Üí  acq = -0.9378995458952201
X = [0.7428956627845764, 0.7939770817756653, 0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.10134205967187881, 0.695570707321167, 0.8957255482673645, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2861731946468353, 0.8315107822418213, 0.7387340068817139, 0.4935872554779053]  ‚Üí  acq = -0.9382982399928019
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0349, dtype=torch.float64), tensor(0.5716, dtype=torch.float64), tensor(0.0354, dtype=torch.float64), 0, tensor(0.1575, dtype=torch.float64), tensor(0.0317, dtype=torch.float64), tensor(0.0393, dtype=torch.float64), 0, tensor(0.1272, dtype=torch.float64), 21, 0, 0, 0, 1, 0, 76, 0.025213715012841528, 21.982305938196237, 0]
normalized proposed parameters for next round by BO: [tensor(0.0349, dtype=torch.float64), tensor(0.5716, dtype=torch.float64), tensor(0.0354, dtype=torch.float64), tensor(0.0024, dtype=torch.float64), tensor(0.1575, dtype=torch.float64), tensor(0.0317, dtype=torch.float64), tensor(0.0393, dtype=torch.float64), tensor(3.1636e-19, dtype=torch.float64), tensor(0.1272, dtype=torch.float64), tensor(0.6551, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5961, dtype=torch.float64), tensor(0.2521, dtype=torch.float64), tensor(0.4580, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.035
  gsm8k: 0.572
  rowan_hellaswag: 0.035
  sciq: 0
  triviaqa: 0.158
  truthfulqa_gen: 0.032
  wikitext: 0.039
  mmlu: 0
  arc_challenge: 0.127

LoRA Parameters:
  lora_r: (76,)
  lora_dropout: (0.025213715012841528,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (21.982305938196237,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  76
lora dropout:  0.025213715012841528
lora alpha:  21.982305938196237
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 29,417,472 || all params: 8,059,678,720 || trainable%: 0.3650
length of training data:  9971
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.648, 'grad_norm': 0.6637316942214966, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.572312355041504, 'eval_runtime': 4.684, 'eval_samples_per_second': 213.492, 'eval_steps_per_second': 13.45, 'epoch': 0.04}
{'loss': 1.3895, 'grad_norm': 0.2994508147239685, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3151546716690063, 'eval_runtime': 4.6935, 'eval_samples_per_second': 213.061, 'eval_steps_per_second': 13.423, 'epoch': 0.08}
{'loss': 1.09, 'grad_norm': 0.24330240488052368, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.2393715381622314, 'eval_runtime': 4.6925, 'eval_samples_per_second': 213.108, 'eval_steps_per_second': 13.426, 'epoch': 0.12}
{'loss': 0.986, 'grad_norm': 0.15288087725639343, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.0949831008911133, 'eval_runtime': 4.7104, 'eval_samples_per_second': 212.295, 'eval_steps_per_second': 13.375, 'epoch': 0.16}
{'loss': 0.9636, 'grad_norm': 0.1696290373802185, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.099328637123108, 'eval_runtime': 4.7388, 'eval_samples_per_second': 211.025, 'eval_steps_per_second': 13.295, 'epoch': 0.2}
{'loss': 0.978, 'grad_norm': 0.16448086500167847, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 1.0646958351135254, 'eval_runtime': 4.7155, 'eval_samples_per_second': 212.068, 'eval_steps_per_second': 13.36, 'epoch': 0.24}
{'loss': 0.9998, 'grad_norm': 0.15952077507972717, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 1.0783495903015137, 'eval_runtime': 4.7212, 'eval_samples_per_second': 211.809, 'eval_steps_per_second': 13.344, 'epoch': 0.28}
{'loss': 0.9955, 'grad_norm': 0.1785186529159546, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 1.0759040117263794, 'eval_runtime': 4.7195, 'eval_samples_per_second': 211.886, 'eval_steps_per_second': 13.349, 'epoch': 0.32}
{'loss': 0.939, 'grad_norm': 0.163532093167305, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 1.0496186017990112, 'eval_runtime': 4.7208, 'eval_samples_per_second': 211.826, 'eval_steps_per_second': 13.345, 'epoch': 0.36}
{'loss': 0.9404, 'grad_norm': 0.19166199862957, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 1.0247384309768677, 'eval_runtime': 4.7246, 'eval_samples_per_second': 211.658, 'eval_steps_per_second': 13.334, 'epoch': 0.4}
{'loss': 0.9586, 'grad_norm': 0.16971959173679352, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 1.0382028818130493, 'eval_runtime': 4.7311, 'eval_samples_per_second': 211.367, 'eval_steps_per_second': 13.316, 'epoch': 0.44}
{'loss': 0.9584, 'grad_norm': 0.1542515605688095, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 1.0338200330734253, 'eval_runtime': 4.7327, 'eval_samples_per_second': 211.295, 'eval_steps_per_second': 13.312, 'epoch': 0.48}
{'loss': 0.9875, 'grad_norm': 0.15540668368339539, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 1.034027099609375, 'eval_runtime': 4.7342, 'eval_samples_per_second': 211.229, 'eval_steps_per_second': 13.307, 'epoch': 0.52}
{'loss': 0.942, 'grad_norm': 0.16446419060230255, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 1.031563639640808, 'eval_runtime': 4.7399, 'eval_samples_per_second': 210.977, 'eval_steps_per_second': 13.292, 'epoch': 0.56}
{'loss': 0.9418, 'grad_norm': 0.16808170080184937, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 1.0267690420150757, 'eval_runtime': 4.7395, 'eval_samples_per_second': 210.993, 'eval_steps_per_second': 13.293, 'epoch': 0.6}
{'loss': 0.9326, 'grad_norm': 0.16873577237129211, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 1.0251213312149048, 'eval_runtime': 4.7387, 'eval_samples_per_second': 211.027, 'eval_steps_per_second': 13.295, 'epoch': 0.64}
{'loss': 0.9699, 'grad_norm': 0.1617719680070877, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 1.0151853561401367, 'eval_runtime': 4.7531, 'eval_samples_per_second': 210.389, 'eval_steps_per_second': 13.254, 'epoch': 0.68}
{'loss': 0.919, 'grad_norm': 0.17689508199691772, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 1.0122730731964111, 'eval_runtime': 4.7404, 'eval_samples_per_second': 210.953, 'eval_steps_per_second': 13.29, 'epoch': 0.72}
{'loss': 0.9563, 'grad_norm': 0.19200730323791504, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 1.0036067962646484, 'eval_runtime': 4.7363, 'eval_samples_per_second': 211.134, 'eval_steps_per_second': 13.301, 'epoch': 0.76}
{'loss': 0.9097, 'grad_norm': 0.16832678020000458, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 1.0050514936447144, 'eval_runtime': 4.7367, 'eval_samples_per_second': 211.118, 'eval_steps_per_second': 13.3, 'epoch': 0.8}
{'loss': 0.9698, 'grad_norm': 0.22437463700771332, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.9968675971031189, 'eval_runtime': 4.7332, 'eval_samples_per_second': 211.274, 'eval_steps_per_second': 13.31, 'epoch': 0.84}
{'loss': 0.9673, 'grad_norm': 0.16419491171836853, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 1.002570390701294, 'eval_runtime': 4.7402, 'eval_samples_per_second': 210.962, 'eval_steps_per_second': 13.291, 'epoch': 0.88}
{'loss': 0.9511, 'grad_norm': 0.17090913653373718, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.9948515892028809, 'eval_runtime': 4.7363, 'eval_samples_per_second': 211.137, 'eval_steps_per_second': 13.302, 'epoch': 0.92}
{'loss': 0.9269, 'grad_norm': 0.18455801904201508, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.9959595799446106, 'eval_runtime': 4.723, 'eval_samples_per_second': 211.728, 'eval_steps_per_second': 13.339, 'epoch': 0.96}
{'train_runtime': 287.2058, 'train_samples_per_second': 34.717, 'train_steps_per_second': 2.173, 'train_loss': 1.0457522991376045, 'epoch': 1.0}
train_results:  {'eval_loss': [2.572312355041504, 1.3151546716690063, 1.2393715381622314, 1.0949831008911133, 1.099328637123108, 1.0646958351135254, 1.0783495903015137, 1.0759040117263794, 1.0496186017990112, 1.0247384309768677, 1.0382028818130493, 1.0338200330734253, 1.034027099609375, 1.031563639640808, 1.0267690420150757, 1.0251213312149048, 1.0151853561401367, 1.0122730731964111, 1.0036067962646484, 1.0050514936447144, 0.9968675971031189, 1.002570390701294, 0.9948515892028809, 0.9959595799446106], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.572312355041504, 1.3151546716690063, 1.2393715381622314, 1.0949831008911133, 1.099328637123108, 1.0646958351135254, 1.0783495903015137, 1.0759040117263794, 1.0496186017990112, 1.0247384309768677, 1.0382028818130493, 1.0338200330734253, 1.034027099609375, 1.031563639640808, 1.0267690420150757, 1.0251213312149048, 1.0151853561401367, 1.0122730731964111, 1.0036067962646484, 1.0050514936447144, 0.9968675971031189, 1.002570390701294, 0.9948515892028809, 0.9959595799446106]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0832126140594482
current iteration best possible eval_loss (full train run):  -0.9959595799446106
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.2057 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1793595552444458, 0.3586342930793762, 0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.34960928559303284, 0.057854533195495605, 0.7710180282592773, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6546881198883057, 0.3577408194541931, 0.8840495347976685, 0.5400984883308411]  ‚Üí  acq = -0.8921418015046916
X = [0.7988576889038086, 0.1653779149055481, 0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9513463973999023, 0.8898367881774902, 0.2988312840461731, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.24033895134925842, 0.2050917148590088, 0.051226988434791565, 0.8115118741989136]  ‚Üí  acq = -0.8921539070539883
X = [0.22445803880691528, 0.07812052965164185, 0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9895481467247009, 0.6373879313468933, 0.9490465521812439, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6339337825775146, 0.44919973611831665, 0.9787859916687012, 0.45290279388427734]  ‚Üí  acq = -0.8921418015052693
X = [0.3654218316078186, 0.6524207592010498, 0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.8971459269523621, 0.9170647859573364, 0.6597838997840881, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.8601893186569214, 0.164650559425354, 0.14350587129592896, 0.7966952323913574]  ‚Üí  acq = -0.8921418015053331
X = [0.4475772976875305, 0.6951091885566711, 0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.06307335197925568, 0.7526708841323853, 0.20946532487869263, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.10499551892280579, 0.07692551612854004, 0.719855546951294, 0.04362046718597412]  ‚Üí  acq = -0.8921477394280164
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0608, dtype=torch.float64), tensor(0.1208, dtype=torch.float64), tensor(0.0429, dtype=torch.float64), tensor(0.0666, dtype=torch.float64), tensor(0.1307, dtype=torch.float64), tensor(0.2829, dtype=torch.float64), tensor(0.0197, dtype=torch.float64), tensor(0.1948, dtype=torch.float64), tensor(0.0809, dtype=torch.float64), 20, 0, 1, 0, 1, 1, 111, 0.047805984082680796, 10.03877457847671, 0]
normalized proposed parameters for next round by BO: [tensor(0.0608, dtype=torch.float64), tensor(0.1208, dtype=torch.float64), tensor(0.0429, dtype=torch.float64), tensor(0.0666, dtype=torch.float64), tensor(0.1307, dtype=torch.float64), tensor(0.2829, dtype=torch.float64), tensor(0.0197, dtype=torch.float64), tensor(0.1948, dtype=torch.float64), tensor(0.0809, dtype=torch.float64), tensor(0.6095, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.8690, dtype=torch.float64), tensor(0.4781, dtype=torch.float64), tensor(0.2091, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.061
  gsm8k: 0.121
  rowan_hellaswag: 0.043
  sciq: 0.067
  triviaqa: 0.131
  truthfulqa_gen: 0.283
  wikitext: 0.02
  mmlu: 0.195
  arc_challenge: 0.081

LoRA Parameters:
  lora_r: (111,)
  lora_dropout: (0.047805984082680796,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (10.03877457847671,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  111
lora dropout:  0.047805984082680796
lora alpha:  10.03877457847671
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 93,204,480 || all params: 8,123,465,728 || trainable%: 1.1473
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4884, 'grad_norm': 0.4975619912147522, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.614227056503296, 'eval_runtime': 5.2464, 'eval_samples_per_second': 190.608, 'eval_steps_per_second': 12.008, 'epoch': 0.04}
{'loss': 1.6564, 'grad_norm': 0.16244356334209442, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2762656211853027, 'eval_runtime': 5.2607, 'eval_samples_per_second': 190.09, 'eval_steps_per_second': 11.976, 'epoch': 0.08}
{'loss': 1.2566, 'grad_norm': 0.15113358199596405, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0915637016296387, 'eval_runtime': 5.249, 'eval_samples_per_second': 190.514, 'eval_steps_per_second': 12.002, 'epoch': 0.12}
{'loss': 1.1533, 'grad_norm': 0.11082746833562851, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.051425576210022, 'eval_runtime': 5.2396, 'eval_samples_per_second': 190.855, 'eval_steps_per_second': 12.024, 'epoch': 0.16}
{'loss': 1.1379, 'grad_norm': 0.12683017551898956, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0320111513137817, 'eval_runtime': 5.216, 'eval_samples_per_second': 191.717, 'eval_steps_per_second': 12.078, 'epoch': 0.2}
{'loss': 1.1477, 'grad_norm': 0.1351480334997177, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0199029445648193, 'eval_runtime': 5.2485, 'eval_samples_per_second': 190.53, 'eval_steps_per_second': 12.003, 'epoch': 0.24}
{'loss': 1.1412, 'grad_norm': 0.17793554067611694, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.0140047073364258, 'eval_runtime': 5.2133, 'eval_samples_per_second': 191.818, 'eval_steps_per_second': 12.085, 'epoch': 0.28}
{'loss': 1.1155, 'grad_norm': 0.11011618375778198, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9996129274368286, 'eval_runtime': 5.2097, 'eval_samples_per_second': 191.95, 'eval_steps_per_second': 12.093, 'epoch': 0.32}
{'loss': 1.1224, 'grad_norm': 0.11807001382112503, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.004611611366272, 'eval_runtime': 5.2187, 'eval_samples_per_second': 191.619, 'eval_steps_per_second': 12.072, 'epoch': 0.36}
{'loss': 1.0337, 'grad_norm': 0.13673323392868042, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.0285011529922485, 'eval_runtime': 5.2239, 'eval_samples_per_second': 191.43, 'eval_steps_per_second': 12.06, 'epoch': 0.4}
{'loss': 1.0949, 'grad_norm': 0.12956732511520386, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9822917580604553, 'eval_runtime': 5.2235, 'eval_samples_per_second': 191.444, 'eval_steps_per_second': 12.061, 'epoch': 0.44}
{'loss': 1.0545, 'grad_norm': 0.11765050888061523, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.978180468082428, 'eval_runtime': 5.2251, 'eval_samples_per_second': 191.384, 'eval_steps_per_second': 12.057, 'epoch': 0.48}
{'loss': 1.1037, 'grad_norm': 0.115202397108078, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.974118173122406, 'eval_runtime': 5.226, 'eval_samples_per_second': 191.352, 'eval_steps_per_second': 12.055, 'epoch': 0.52}
{'loss': 1.0764, 'grad_norm': 0.1385042816400528, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9829766750335693, 'eval_runtime': 5.2258, 'eval_samples_per_second': 191.359, 'eval_steps_per_second': 12.056, 'epoch': 0.56}
{'loss': 1.087, 'grad_norm': 0.11578670144081116, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.971721887588501, 'eval_runtime': 5.2316, 'eval_samples_per_second': 191.145, 'eval_steps_per_second': 12.042, 'epoch': 0.6}
{'loss': 1.0835, 'grad_norm': 0.12149649858474731, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9779912233352661, 'eval_runtime': 5.2299, 'eval_samples_per_second': 191.209, 'eval_steps_per_second': 12.046, 'epoch': 0.64}
{'loss': 1.0615, 'grad_norm': 0.1408037394285202, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9747492671012878, 'eval_runtime': 5.2274, 'eval_samples_per_second': 191.3, 'eval_steps_per_second': 12.052, 'epoch': 0.68}
{'loss': 1.079, 'grad_norm': 0.136831596493721, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9627886414527893, 'eval_runtime': 5.2272, 'eval_samples_per_second': 191.306, 'eval_steps_per_second': 12.052, 'epoch': 0.72}
{'loss': 1.0571, 'grad_norm': 0.16392914950847626, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9776959419250488, 'eval_runtime': 5.2212, 'eval_samples_per_second': 191.528, 'eval_steps_per_second': 12.066, 'epoch': 0.76}
{'loss': 1.0276, 'grad_norm': 0.15016262233257294, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9578413367271423, 'eval_runtime': 5.2219, 'eval_samples_per_second': 191.499, 'eval_steps_per_second': 12.064, 'epoch': 0.8}
{'loss': 1.0372, 'grad_norm': 0.16084343194961548, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.960166871547699, 'eval_runtime': 5.2232, 'eval_samples_per_second': 191.455, 'eval_steps_per_second': 12.062, 'epoch': 0.84}
{'loss': 1.0245, 'grad_norm': 0.2047499418258667, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9592136144638062, 'eval_runtime': 5.2248, 'eval_samples_per_second': 191.394, 'eval_steps_per_second': 12.058, 'epoch': 0.88}
{'loss': 1.0578, 'grad_norm': 0.1398758590221405, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9552594423294067, 'eval_runtime': 5.2242, 'eval_samples_per_second': 191.415, 'eval_steps_per_second': 12.059, 'epoch': 0.92}
{'loss': 1.0297, 'grad_norm': 0.15869833528995514, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9558297395706177, 'eval_runtime': 5.229, 'eval_samples_per_second': 191.24, 'eval_steps_per_second': 12.048, 'epoch': 0.96}
{'loss': 1.0358, 'grad_norm': 0.2445983588695526, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9561683535575867, 'eval_runtime': 5.2362, 'eval_samples_per_second': 190.978, 'eval_steps_per_second': 12.032, 'epoch': 1.0}
{'train_runtime': 318.7687, 'train_samples_per_second': 31.352, 'train_steps_per_second': 1.961, 'train_loss': 1.2065279388427734, 'epoch': 1.0}
train_results:  {'eval_loss': [2.614227056503296, 1.2762656211853027, 1.0915637016296387, 1.051425576210022, 1.0320111513137817, 1.0199029445648193, 1.0140047073364258, 0.9996129274368286, 1.004611611366272, 1.0285011529922485, 0.9822917580604553, 0.978180468082428, 0.974118173122406, 0.9829766750335693, 0.971721887588501, 0.9779912233352661, 0.9747492671012878, 0.9627886414527893, 0.9776959419250488, 0.9578413367271423, 0.960166871547699, 0.9592136144638062, 0.9552594423294067, 0.9558297395706177, 0.9561683535575867], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.614227056503296, 1.2762656211853027, 1.0915637016296387, 1.051425576210022, 1.0320111513137817, 1.0199029445648193, 1.0140047073364258, 0.9996129274368286, 1.004611611366272, 1.0285011529922485, 0.9822917580604553, 0.978180468082428, 0.974118173122406, 0.9829766750335693, 0.971721887588501, 0.9779912233352661, 0.9747492671012878, 0.9627886414527893, 0.9776959419250488, 0.9578413367271423, 0.960166871547699, 0.9592136144638062, 0.9552594423294067, 0.9558297395706177, 0.9561683535575867]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1008516550064087
current iteration best possible eval_loss (full train run):  -0.9561683535575867
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.9735 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.9199804837843998
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.9199781052845513
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.9199780481181327
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.922195510606526
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.9199859369898926
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2418, dtype=torch.float64), tensor(0.2588, dtype=torch.float64), tensor(0.0148, dtype=torch.float64), 0, tensor(0.1583, dtype=torch.float64), tensor(0.0469, dtype=torch.float64), tensor(0.0278, dtype=torch.float64), 0, tensor(0.2516, dtype=torch.float64), 26, 0, 1, 1, 1, 0, 91, 0.028014435217873997, 27.14035356818026, 0]
normalized proposed parameters for next round by BO: [tensor(0.2418, dtype=torch.float64), tensor(0.2588, dtype=torch.float64), tensor(0.0148, dtype=torch.float64), tensor(3.1761e-18, dtype=torch.float64), tensor(0.1583, dtype=torch.float64), tensor(0.0469, dtype=torch.float64), tensor(0.0278, dtype=torch.float64), tensor(9.5685e-20, dtype=torch.float64), tensor(0.2516, dtype=torch.float64), tensor(0.8216, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7121, dtype=torch.float64), tensor(0.2801, dtype=torch.float64), tensor(0.5654, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.242
  gsm8k: 0.259
  rowan_hellaswag: 0.015
  sciq: 0
  triviaqa: 0.158
  truthfulqa_gen: 0.047
  wikitext: 0.028
  mmlu: 0
  arc_challenge: 0.252

LoRA Parameters:
  lora_r: (91,)
  lora_dropout: (0.028014435217873997,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (27.14035356818026,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  91
lora dropout:  0.028014435217873997
lora alpha:  27.14035356818026
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 99,334,144 || all params: 8,129,595,392 || trainable%: 1.2219
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8042, 'grad_norm': 0.7901118993759155, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.491407871246338, 'eval_runtime': 5.4128, 'eval_samples_per_second': 184.747, 'eval_steps_per_second': 11.639, 'epoch': 0.04}
{'loss': 1.2158, 'grad_norm': 0.27974772453308105, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9927292466163635, 'eval_runtime': 5.4392, 'eval_samples_per_second': 183.849, 'eval_steps_per_second': 11.582, 'epoch': 0.08}
{'loss': 1.0214, 'grad_norm': 0.19688835740089417, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9604758024215698, 'eval_runtime': 5.3831, 'eval_samples_per_second': 185.768, 'eval_steps_per_second': 11.703, 'epoch': 0.12}
{'loss': 0.9898, 'grad_norm': 0.2101801186800003, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9454031586647034, 'eval_runtime': 5.3879, 'eval_samples_per_second': 185.6, 'eval_steps_per_second': 11.693, 'epoch': 0.16}
{'loss': 0.9289, 'grad_norm': 0.20579615235328674, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9196408987045288, 'eval_runtime': 5.3829, 'eval_samples_per_second': 185.774, 'eval_steps_per_second': 11.704, 'epoch': 0.2}
{'loss': 0.9262, 'grad_norm': 0.18801641464233398, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9176439642906189, 'eval_runtime': 5.3934, 'eval_samples_per_second': 185.411, 'eval_steps_per_second': 11.681, 'epoch': 0.24}
{'loss': 0.9539, 'grad_norm': 0.1852392554283142, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9123986959457397, 'eval_runtime': 5.4, 'eval_samples_per_second': 185.185, 'eval_steps_per_second': 11.667, 'epoch': 0.28}
{'loss': 0.9175, 'grad_norm': 0.197585329413414, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9075856804847717, 'eval_runtime': 5.4029, 'eval_samples_per_second': 185.086, 'eval_steps_per_second': 11.66, 'epoch': 0.32}
{'loss': 0.9125, 'grad_norm': 0.19391381740570068, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9038733839988708, 'eval_runtime': 5.402, 'eval_samples_per_second': 185.116, 'eval_steps_per_second': 11.662, 'epoch': 0.36}
{'loss': 0.9123, 'grad_norm': 0.17986291646957397, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8999956250190735, 'eval_runtime': 5.4051, 'eval_samples_per_second': 185.012, 'eval_steps_per_second': 11.656, 'epoch': 0.4}
{'loss': 0.9052, 'grad_norm': 0.19736400246620178, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8973995447158813, 'eval_runtime': 5.4009, 'eval_samples_per_second': 185.155, 'eval_steps_per_second': 11.665, 'epoch': 0.44}
{'loss': 0.9069, 'grad_norm': 0.16324301064014435, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8926546573638916, 'eval_runtime': 5.4003, 'eval_samples_per_second': 185.176, 'eval_steps_per_second': 11.666, 'epoch': 0.48}
{'loss': 0.8689, 'grad_norm': 0.19853007793426514, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8900516629219055, 'eval_runtime': 5.4031, 'eval_samples_per_second': 185.078, 'eval_steps_per_second': 11.66, 'epoch': 0.52}
{'loss': 0.8835, 'grad_norm': 0.20115041732788086, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8876499533653259, 'eval_runtime': 5.3871, 'eval_samples_per_second': 185.627, 'eval_steps_per_second': 11.695, 'epoch': 0.56}
{'loss': 0.9038, 'grad_norm': 0.21114569902420044, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8870163559913635, 'eval_runtime': 5.3979, 'eval_samples_per_second': 185.256, 'eval_steps_per_second': 11.671, 'epoch': 0.6}
{'loss': 0.8866, 'grad_norm': 0.1906576156616211, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8849254846572876, 'eval_runtime': 5.4001, 'eval_samples_per_second': 185.182, 'eval_steps_per_second': 11.666, 'epoch': 0.64}
{'loss': 0.8444, 'grad_norm': 0.2295546978712082, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8858917951583862, 'eval_runtime': 5.3979, 'eval_samples_per_second': 185.256, 'eval_steps_per_second': 11.671, 'epoch': 0.68}
{'loss': 0.8755, 'grad_norm': 0.204971581697464, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8790871500968933, 'eval_runtime': 5.3981, 'eval_samples_per_second': 185.25, 'eval_steps_per_second': 11.671, 'epoch': 0.72}
{'loss': 0.8583, 'grad_norm': 0.2509872615337372, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8779959082603455, 'eval_runtime': 5.4143, 'eval_samples_per_second': 184.696, 'eval_steps_per_second': 11.636, 'epoch': 0.76}
{'loss': 0.8779, 'grad_norm': 0.2123154252767563, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.876793384552002, 'eval_runtime': 5.4096, 'eval_samples_per_second': 184.856, 'eval_steps_per_second': 11.646, 'epoch': 0.8}
{'loss': 0.8551, 'grad_norm': 0.2029973566532135, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8727517127990723, 'eval_runtime': 5.4052, 'eval_samples_per_second': 185.007, 'eval_steps_per_second': 11.655, 'epoch': 0.84}
{'loss': 0.8783, 'grad_norm': 0.19684848189353943, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.87485271692276, 'eval_runtime': 5.4123, 'eval_samples_per_second': 184.763, 'eval_steps_per_second': 11.64, 'epoch': 0.88}
{'loss': 0.8194, 'grad_norm': 0.2266358882188797, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8730065226554871, 'eval_runtime': 5.41, 'eval_samples_per_second': 184.841, 'eval_steps_per_second': 11.645, 'epoch': 0.92}
{'loss': 0.8508, 'grad_norm': 0.23808026313781738, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8719209432601929, 'eval_runtime': 5.4046, 'eval_samples_per_second': 185.028, 'eval_steps_per_second': 11.657, 'epoch': 0.96}
{'loss': 0.8312, 'grad_norm': 0.26520004868507385, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8714426755905151, 'eval_runtime': 5.406, 'eval_samples_per_second': 184.979, 'eval_steps_per_second': 11.654, 'epoch': 1.0}
{'train_runtime': 352.3755, 'train_samples_per_second': 28.367, 'train_steps_per_second': 1.774, 'train_loss': 0.9851283172607422, 'epoch': 1.0}
train_results:  {'eval_loss': [1.491407871246338, 0.9927292466163635, 0.9604758024215698, 0.9454031586647034, 0.9196408987045288, 0.9176439642906189, 0.9123986959457397, 0.9075856804847717, 0.9038733839988708, 0.8999956250190735, 0.8973995447158813, 0.8926546573638916, 0.8900516629219055, 0.8876499533653259, 0.8870163559913635, 0.8849254846572876, 0.8858917951583862, 0.8790871500968933, 0.8779959082603455, 0.876793384552002, 0.8727517127990723, 0.87485271692276, 0.8730065226554871, 0.8719209432601929, 0.8714426755905151], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.491407871246338, 0.9927292466163635, 0.9604758024215698, 0.9454031586647034, 0.9196408987045288, 0.9176439642906189, 0.9123986959457397, 0.9075856804847717, 0.9038733839988708, 0.8999956250190735, 0.8973995447158813, 0.8926546573638916, 0.8900516629219055, 0.8876499533653259, 0.8870163559913635, 0.8849254846572876, 0.8858917951583862, 0.8790871500968933, 0.8779959082603455, 0.876793384552002, 0.8727517127990723, 0.87485271692276, 0.8730065226554871, 0.8719209432601929, 0.8714426755905151]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0897812843322754
current iteration best possible eval_loss (full train run):  -0.8714426755905151
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087, -1.0897812843322754]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.3561 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -0.9037635489215485
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -0.8976789310595066
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -0.9032293575590548
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -0.9037635555964681
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -0.9037635489931498
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1888, dtype=torch.float64), tensor(0.2141, dtype=torch.float64), 0, tensor(0.0303, dtype=torch.float64), tensor(0.1400, dtype=torch.float64), tensor(0.2485, dtype=torch.float64), tensor(0.0185, dtype=torch.float64), 0, tensor(0.1502, dtype=torch.float64), 20, 0, 1, 1, 1, 0, 19, 0.07976915102905811, 7.509609085313165, 0]
normalized proposed parameters for next round by BO: [tensor(0.1888, dtype=torch.float64), tensor(0.2141, dtype=torch.float64), tensor(0.0097, dtype=torch.float64), tensor(0.0303, dtype=torch.float64), tensor(0.1400, dtype=torch.float64), tensor(0.2485, dtype=torch.float64), tensor(0.0185, dtype=torch.float64), tensor(5.3444e-19, dtype=torch.float64), tensor(0.1502, dtype=torch.float64), tensor(0.6169, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1474, dtype=torch.float64), tensor(0.7977, dtype=torch.float64), tensor(0.1565, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.189
  gsm8k: 0.214
  rowan_hellaswag: 0
  sciq: 0.03
  triviaqa: 0.14
  truthfulqa_gen: 0.248
  wikitext: 0.018
  mmlu: 0
  arc_challenge: 0.15

LoRA Parameters:
  lora_r: (19,)
  lora_dropout: (0.07976915102905811,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (7.509609085313165,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  19
lora dropout:  0.07976915102905811
lora alpha:  7.509609085313165
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 15,953,920 || all params: 8,046,215,168 || trainable%: 0.1983
length of training data:  9899
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4175, 'grad_norm': 0.7600146532058716, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.5493810176849365, 'eval_runtime': 5.06, 'eval_samples_per_second': 197.629, 'eval_steps_per_second': 12.451, 'epoch': 0.04}
{'loss': 1.5239, 'grad_norm': 0.4797981381416321, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1681143045425415, 'eval_runtime': 5.0697, 'eval_samples_per_second': 197.249, 'eval_steps_per_second': 12.427, 'epoch': 0.08}
{'loss': 1.0104, 'grad_norm': 0.3150310218334198, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.0252612829208374, 'eval_runtime': 5.0496, 'eval_samples_per_second': 198.037, 'eval_steps_per_second': 12.476, 'epoch': 0.12}
{'loss': 0.9556, 'grad_norm': 0.23737318813800812, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 0.9852490425109863, 'eval_runtime': 5.056, 'eval_samples_per_second': 197.785, 'eval_steps_per_second': 12.46, 'epoch': 0.16}
{'loss': 0.9501, 'grad_norm': 0.2733898460865021, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 0.9764776825904846, 'eval_runtime': 5.0509, 'eval_samples_per_second': 197.985, 'eval_steps_per_second': 12.473, 'epoch': 0.2}
{'loss': 0.8956, 'grad_norm': 0.22754839062690735, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 0.9649766087532043, 'eval_runtime': 5.0544, 'eval_samples_per_second': 197.847, 'eval_steps_per_second': 12.464, 'epoch': 0.24}
{'loss': 0.9094, 'grad_norm': 0.24407213926315308, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 0.9461297988891602, 'eval_runtime': 5.0749, 'eval_samples_per_second': 197.048, 'eval_steps_per_second': 12.414, 'epoch': 0.28}
{'loss': 0.9559, 'grad_norm': 0.19370044767856598, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 0.9544854164123535, 'eval_runtime': 5.0698, 'eval_samples_per_second': 197.246, 'eval_steps_per_second': 12.427, 'epoch': 0.32}
{'loss': 0.914, 'grad_norm': 0.22804942727088928, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 0.9463551640510559, 'eval_runtime': 5.0687, 'eval_samples_per_second': 197.289, 'eval_steps_per_second': 12.429, 'epoch': 0.36}
{'loss': 0.8996, 'grad_norm': 0.23980340361595154, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.9316055774688721, 'eval_runtime': 5.0696, 'eval_samples_per_second': 197.253, 'eval_steps_per_second': 12.427, 'epoch': 0.4}
{'loss': 0.9266, 'grad_norm': 0.2607503831386566, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.9385931491851807, 'eval_runtime': 5.069, 'eval_samples_per_second': 197.279, 'eval_steps_per_second': 12.429, 'epoch': 0.44}
{'loss': 0.8953, 'grad_norm': 0.24974748492240906, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.9303953647613525, 'eval_runtime': 5.0744, 'eval_samples_per_second': 197.066, 'eval_steps_per_second': 12.415, 'epoch': 0.48}
{'loss': 0.9202, 'grad_norm': 0.25984981656074524, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.9283122420310974, 'eval_runtime': 5.0695, 'eval_samples_per_second': 197.258, 'eval_steps_per_second': 12.427, 'epoch': 0.53}
{'loss': 0.891, 'grad_norm': 0.25512415170669556, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.9255802035331726, 'eval_runtime': 5.0751, 'eval_samples_per_second': 197.041, 'eval_steps_per_second': 12.414, 'epoch': 0.57}
{'loss': 0.9396, 'grad_norm': 0.27659347653388977, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.9215807318687439, 'eval_runtime': 5.0757, 'eval_samples_per_second': 197.019, 'eval_steps_per_second': 12.412, 'epoch': 0.61}
{'loss': 0.8629, 'grad_norm': 0.34404197335243225, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.9181516170501709, 'eval_runtime': 5.0603, 'eval_samples_per_second': 197.617, 'eval_steps_per_second': 12.45, 'epoch': 0.65}
{'loss': 0.8736, 'grad_norm': 0.22355367243289948, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.9218514561653137, 'eval_runtime': 5.049, 'eval_samples_per_second': 198.061, 'eval_steps_per_second': 12.478, 'epoch': 0.69}
{'loss': 0.8674, 'grad_norm': 0.25799259543418884, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.913373589515686, 'eval_runtime': 5.0512, 'eval_samples_per_second': 197.971, 'eval_steps_per_second': 12.472, 'epoch': 0.73}
{'loss': 0.8683, 'grad_norm': 0.2823483645915985, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.9159854650497437, 'eval_runtime': 5.0459, 'eval_samples_per_second': 198.182, 'eval_steps_per_second': 12.485, 'epoch': 0.77}
{'loss': 0.8709, 'grad_norm': 0.28330495953559875, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.9139039516448975, 'eval_runtime': 5.0433, 'eval_samples_per_second': 198.281, 'eval_steps_per_second': 12.492, 'epoch': 0.81}
{'loss': 0.8675, 'grad_norm': 0.24430863559246063, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.915142834186554, 'eval_runtime': 5.0464, 'eval_samples_per_second': 198.161, 'eval_steps_per_second': 12.484, 'epoch': 0.85}
{'loss': 0.8736, 'grad_norm': 0.26489734649658203, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.906907856464386, 'eval_runtime': 5.0434, 'eval_samples_per_second': 198.278, 'eval_steps_per_second': 12.491, 'epoch': 0.89}
{'loss': 0.8493, 'grad_norm': 0.2879289984703064, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.914004921913147, 'eval_runtime': 5.0519, 'eval_samples_per_second': 197.946, 'eval_steps_per_second': 12.471, 'epoch': 0.93}
{'loss': 0.8657, 'grad_norm': 0.2912525534629822, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.9086922407150269, 'eval_runtime': 5.0409, 'eval_samples_per_second': 198.376, 'eval_steps_per_second': 12.498, 'epoch': 0.97}
{'train_runtime': 296.2796, 'train_samples_per_second': 33.411, 'train_steps_per_second': 2.089, 'train_loss': 1.0281701095655007, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5493810176849365, 1.1681143045425415, 1.0252612829208374, 0.9852490425109863, 0.9764776825904846, 0.9649766087532043, 0.9461297988891602, 0.9544854164123535, 0.9463551640510559, 0.9316055774688721, 0.9385931491851807, 0.9303953647613525, 0.9283122420310974, 0.9255802035331726, 0.9215807318687439, 0.9181516170501709, 0.9218514561653137, 0.913373589515686, 0.9159854650497437, 0.9139039516448975, 0.915142834186554, 0.906907856464386, 0.914004921913147, 0.9086922407150269], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.5493810176849365, 1.1681143045425415, 1.0252612829208374, 0.9852490425109863, 0.9764776825904846, 0.9649766087532043, 0.9461297988891602, 0.9544854164123535, 0.9463551640510559, 0.9316055774688721, 0.9385931491851807, 0.9303953647613525, 0.9283122420310974, 0.9255802035331726, 0.9215807318687439, 0.9181516170501709, 0.9218514561653137, 0.913373589515686, 0.9159854650497437, 0.9139039516448975, 0.915142834186554, 0.906907856464386, 0.914004921913147, 0.9086922407150269]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9427335262298584
current iteration best possible eval_loss (full train run):  -0.9086922407150269
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087, -1.0897812843322754, -0.9427335262298584]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.6059 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.08295202255249023, 0.8953140377998352, 0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.36218807101249695, 0.6549691557884216, 0.8173236846923828, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3347404897212982, 0.355441689491272, 0.4367692470550537, 0.45111382007598877]  ‚Üí  acq = -0.9117560184790398
X = [0.35591208934783936, 0.098782479763031, 0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.36877870559692383, 0.8672244548797607, 0.5090312361717224, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9806126356124878, 0.5262150168418884, 0.617688775062561, 0.313107967376709]  ‚Üí  acq = -0.9117560184790398
X = [0.2322162389755249, 0.03715479373931885, 0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.40351834893226624, 0.9447525143623352, 0.40425533056259155, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9491793513298035, 0.21524584293365479, 0.22934073209762573, 0.15074169635772705]  ‚Üí  acq = -0.9117560184790398
X = [0.5550482869148254, 0.7731989026069641, 0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6225687265396118, 0.3806919455528259, 0.9421022534370422, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7150893807411194, 0.5437468886375427, 0.6237008571624756, 0.9093855619430542]  ‚Üí  acq = -0.9117560184790398
X = [0.7380771636962891, 0.9890440702438354, 0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.4263235926628113, 0.9630946516990662, 0.6111319065093994, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.834472119808197, 0.3034810423851013, 0.033137477934360504, 0.012760698795318604]  ‚Üí  acq = -0.9117560184790398
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2921, dtype=torch.float64), tensor(0.1504, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), 0, tensor(0.0783, dtype=torch.float64), tensor(0.1021, dtype=torch.float64), tensor(0.0406, dtype=torch.float64), tensor(0.1821, dtype=torch.float64), tensor(0.1410, dtype=torch.float64), 31, 1, 1, 0, 1, 1, 6, 0.08478921916083872, 24.263302530112142, 0]
normalized proposed parameters for next round by BO: [tensor(0.2921, dtype=torch.float64), tensor(0.1504, dtype=torch.float64), tensor(0.0134, dtype=torch.float64), tensor(3.2167e-18, dtype=torch.float64), tensor(0.0783, dtype=torch.float64), tensor(0.1021, dtype=torch.float64), tensor(0.0406, dtype=torch.float64), tensor(0.1821, dtype=torch.float64), tensor(0.1410, dtype=torch.float64), tensor(0.9565, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0459, dtype=torch.float64), tensor(0.8479, dtype=torch.float64), tensor(0.5055, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.292
  gsm8k: 0.15
  rowan_hellaswag: 0.013
  sciq: 0
  triviaqa: 0.078
  truthfulqa_gen: 0.102
  wikitext: 0.041
  mmlu: 0.182
  arc_challenge: 0.141

LoRA Parameters:
  lora_r: (6,)
  lora_dropout: (0.08478921916083872,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (24.263302530112142,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  6
lora dropout:  0.08478921916083872
lora alpha:  24.263302530112142
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 9,332,736 || all params: 8,039,593,984 || trainable%: 0.1161
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.852, 'grad_norm': 1.7273708581924438, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4114880561828613, 'eval_runtime': 5.4531, 'eval_samples_per_second': 183.383, 'eval_steps_per_second': 11.553, 'epoch': 0.04}
{'loss': 1.2714, 'grad_norm': 1.2410777807235718, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9636724591255188, 'eval_runtime': 5.446, 'eval_samples_per_second': 183.622, 'eval_steps_per_second': 11.568, 'epoch': 0.08}
{'loss': 1.1221, 'grad_norm': 0.7486435770988464, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9289240837097168, 'eval_runtime': 5.4372, 'eval_samples_per_second': 183.917, 'eval_steps_per_second': 11.587, 'epoch': 0.12}
{'loss': 1.0778, 'grad_norm': 0.693991482257843, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9184225797653198, 'eval_runtime': 5.4704, 'eval_samples_per_second': 182.802, 'eval_steps_per_second': 11.517, 'epoch': 0.16}
{'loss': 1.0563, 'grad_norm': 0.7461742758750916, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9056370854377747, 'eval_runtime': 5.4557, 'eval_samples_per_second': 183.294, 'eval_steps_per_second': 11.548, 'epoch': 0.2}
{'loss': 1.059, 'grad_norm': 0.7346014976501465, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8960466980934143, 'eval_runtime': 5.4726, 'eval_samples_per_second': 182.728, 'eval_steps_per_second': 11.512, 'epoch': 0.24}
{'loss': 1.032, 'grad_norm': 1.1373709440231323, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8913145661354065, 'eval_runtime': 5.4735, 'eval_samples_per_second': 182.699, 'eval_steps_per_second': 11.51, 'epoch': 0.28}
{'loss': 1.0194, 'grad_norm': 0.5845053195953369, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8855524659156799, 'eval_runtime': 5.4767, 'eval_samples_per_second': 182.591, 'eval_steps_per_second': 11.503, 'epoch': 0.32}
{'loss': 1.0322, 'grad_norm': 0.6322925686836243, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8832784295082092, 'eval_runtime': 5.4819, 'eval_samples_per_second': 182.419, 'eval_steps_per_second': 11.492, 'epoch': 0.36}
{'loss': 0.9898, 'grad_norm': 0.6323109269142151, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8822070956230164, 'eval_runtime': 5.4973, 'eval_samples_per_second': 181.909, 'eval_steps_per_second': 11.46, 'epoch': 0.4}
{'loss': 0.9951, 'grad_norm': 0.7318881750106812, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8802358508110046, 'eval_runtime': 5.4963, 'eval_samples_per_second': 181.942, 'eval_steps_per_second': 11.462, 'epoch': 0.44}
{'loss': 0.9932, 'grad_norm': 0.7493948936462402, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8728658556938171, 'eval_runtime': 5.51, 'eval_samples_per_second': 181.487, 'eval_steps_per_second': 11.434, 'epoch': 0.48}
{'loss': 0.9448, 'grad_norm': 0.7026081681251526, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8674500584602356, 'eval_runtime': 5.494, 'eval_samples_per_second': 182.017, 'eval_steps_per_second': 11.467, 'epoch': 0.52}
{'loss': 1.0028, 'grad_norm': 0.6918560862541199, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8675559759140015, 'eval_runtime': 5.4942, 'eval_samples_per_second': 182.011, 'eval_steps_per_second': 11.467, 'epoch': 0.56}
{'loss': 0.9444, 'grad_norm': 1.1026781797409058, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8644229173660278, 'eval_runtime': 5.5087, 'eval_samples_per_second': 181.532, 'eval_steps_per_second': 11.437, 'epoch': 0.6}
{'loss': 0.9885, 'grad_norm': 0.6271186470985413, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8621528148651123, 'eval_runtime': 5.4498, 'eval_samples_per_second': 183.495, 'eval_steps_per_second': 11.56, 'epoch': 0.64}
{'loss': 1.0043, 'grad_norm': 0.659366250038147, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8587503433227539, 'eval_runtime': 5.4536, 'eval_samples_per_second': 183.366, 'eval_steps_per_second': 11.552, 'epoch': 0.68}
{'loss': 0.9503, 'grad_norm': 0.875532865524292, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8599494695663452, 'eval_runtime': 5.4599, 'eval_samples_per_second': 183.154, 'eval_steps_per_second': 11.539, 'epoch': 0.72}
{'loss': 0.9469, 'grad_norm': 0.6338870525360107, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8584197759628296, 'eval_runtime': 5.4577, 'eval_samples_per_second': 183.227, 'eval_steps_per_second': 11.543, 'epoch': 0.76}
{'loss': 0.9634, 'grad_norm': 0.7107768058776855, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8591627478599548, 'eval_runtime': 5.4546, 'eval_samples_per_second': 183.331, 'eval_steps_per_second': 11.55, 'epoch': 0.8}
{'loss': 0.9151, 'grad_norm': 0.7085963487625122, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8556424379348755, 'eval_runtime': 5.4606, 'eval_samples_per_second': 183.131, 'eval_steps_per_second': 11.537, 'epoch': 0.84}
{'loss': 0.9493, 'grad_norm': 0.6276357769966125, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8542154431343079, 'eval_runtime': 5.4717, 'eval_samples_per_second': 182.759, 'eval_steps_per_second': 11.514, 'epoch': 0.88}
{'loss': 0.9469, 'grad_norm': 0.7195579409599304, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8530662059783936, 'eval_runtime': 5.4525, 'eval_samples_per_second': 183.403, 'eval_steps_per_second': 11.554, 'epoch': 0.92}
{'loss': 0.9575, 'grad_norm': 0.6810424327850342, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8534414172172546, 'eval_runtime': 5.4491, 'eval_samples_per_second': 183.517, 'eval_steps_per_second': 11.562, 'epoch': 0.96}
{'loss': 0.9165, 'grad_norm': 0.9364878535270691, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.853015124797821, 'eval_runtime': 5.4499, 'eval_samples_per_second': 183.488, 'eval_steps_per_second': 11.56, 'epoch': 1.0}
{'train_runtime': 372.949, 'train_samples_per_second': 26.805, 'train_steps_per_second': 1.676, 'train_loss': 1.0772397674560548, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4114880561828613, 0.9636724591255188, 0.9289240837097168, 0.9184225797653198, 0.9056370854377747, 0.8960466980934143, 0.8913145661354065, 0.8855524659156799, 0.8832784295082092, 0.8822070956230164, 0.8802358508110046, 0.8728658556938171, 0.8674500584602356, 0.8675559759140015, 0.8644229173660278, 0.8621528148651123, 0.8587503433227539, 0.8599494695663452, 0.8584197759628296, 0.8591627478599548, 0.8556424379348755, 0.8542154431343079, 0.8530662059783936, 0.8534414172172546, 0.853015124797821], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4114880561828613, 0.9636724591255188, 0.9289240837097168, 0.9184225797653198, 0.9056370854377747, 0.8960466980934143, 0.8913145661354065, 0.8855524659156799, 0.8832784295082092, 0.8822070956230164, 0.8802358508110046, 0.8728658556938171, 0.8674500584602356, 0.8675559759140015, 0.8644229173660278, 0.8621528148651123, 0.8587503433227539, 0.8599494695663452, 0.8584197759628296, 0.8591627478599548, 0.8556424379348755, 0.8542154431343079, 0.8530662059783936, 0.8534414172172546, 0.853015124797821]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0629347562789917
current iteration best possible eval_loss (full train run):  -0.853015124797821
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087, -1.0897812843322754, -0.9427335262298584, -1.0629347562789917]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.8821 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -0.9790782691996207
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.9805407937943305
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.9783380790270638
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.9783441952288234
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.977941563231074
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4783, dtype=torch.float64), 0, tensor(0.0172, dtype=torch.float64), tensor(0.0162, dtype=torch.float64), tensor(0.0924, dtype=torch.float64), tensor(0.0711, dtype=torch.float64), tensor(0.0164, dtype=torch.float64), tensor(0.0803, dtype=torch.float64), tensor(0.2282, dtype=torch.float64), 27, 0, 1, 1, 1, 0, 78, 0.056331287752765136, 26.644967083465193, 0]
normalized proposed parameters for next round by BO: [tensor(0.4783, dtype=torch.float64), tensor(3.2929e-05, dtype=torch.float64), tensor(0.0172, dtype=torch.float64), tensor(0.0162, dtype=torch.float64), tensor(0.0924, dtype=torch.float64), tensor(0.0711, dtype=torch.float64), tensor(0.0164, dtype=torch.float64), tensor(0.0803, dtype=torch.float64), tensor(0.2282, dtype=torch.float64), tensor(0.8400, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6058, dtype=torch.float64), tensor(0.5633, dtype=torch.float64), tensor(0.5551, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.478
  gsm8k: 0
  rowan_hellaswag: 0.017
  sciq: 0.016
  triviaqa: 0.092
  truthfulqa_gen: 0.071
  wikitext: 0.016
  mmlu: 0.08
  arc_challenge: 0.228

LoRA Parameters:
  lora_r: (78,)
  lora_dropout: (0.056331287752765136,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (26.644967083465193,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  78
lora dropout:  0.056331287752765136
lora alpha:  26.644967083465193
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 88,418,304 || all params: 8,118,679,552 || trainable%: 1.0891
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.148, 'grad_norm': 1.2199358940124512, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3923275470733643, 'eval_runtime': 5.2086, 'eval_samples_per_second': 191.989, 'eval_steps_per_second': 12.095, 'epoch': 0.04}
{'loss': 1.2181, 'grad_norm': 0.3122812509536743, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9574931263923645, 'eval_runtime': 5.2297, 'eval_samples_per_second': 191.215, 'eval_steps_per_second': 12.047, 'epoch': 0.08}
{'loss': 1.0258, 'grad_norm': 0.2388637512922287, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9186960458755493, 'eval_runtime': 5.2389, 'eval_samples_per_second': 190.878, 'eval_steps_per_second': 12.025, 'epoch': 0.12}
{'loss': 1.0062, 'grad_norm': 0.24859890341758728, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.90669846534729, 'eval_runtime': 5.2176, 'eval_samples_per_second': 191.661, 'eval_steps_per_second': 12.075, 'epoch': 0.16}
{'loss': 1.0406, 'grad_norm': 0.22524073719978333, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8969578742980957, 'eval_runtime': 5.2334, 'eval_samples_per_second': 191.081, 'eval_steps_per_second': 12.038, 'epoch': 0.2}
{'loss': 0.9467, 'grad_norm': 0.23460674285888672, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8852311968803406, 'eval_runtime': 5.2384, 'eval_samples_per_second': 190.897, 'eval_steps_per_second': 12.027, 'epoch': 0.24}
{'loss': 0.9543, 'grad_norm': 0.21069170534610748, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8852081298828125, 'eval_runtime': 5.2552, 'eval_samples_per_second': 190.289, 'eval_steps_per_second': 11.988, 'epoch': 0.28}
{'loss': 0.9615, 'grad_norm': 0.20645761489868164, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8767199516296387, 'eval_runtime': 5.2315, 'eval_samples_per_second': 191.15, 'eval_steps_per_second': 12.042, 'epoch': 0.32}
{'loss': 0.9157, 'grad_norm': 0.23144648969173431, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8763953447341919, 'eval_runtime': 5.236, 'eval_samples_per_second': 190.984, 'eval_steps_per_second': 12.032, 'epoch': 0.36}
{'loss': 0.987, 'grad_norm': 0.21370498836040497, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8715566396713257, 'eval_runtime': 5.2155, 'eval_samples_per_second': 191.737, 'eval_steps_per_second': 12.079, 'epoch': 0.4}
{'loss': 0.9253, 'grad_norm': 0.228751540184021, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8730806708335876, 'eval_runtime': 5.2098, 'eval_samples_per_second': 191.947, 'eval_steps_per_second': 12.093, 'epoch': 0.44}
{'loss': 0.938, 'grad_norm': 0.22521841526031494, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.865717887878418, 'eval_runtime': 5.2143, 'eval_samples_per_second': 191.78, 'eval_steps_per_second': 12.082, 'epoch': 0.48}
{'loss': 0.9245, 'grad_norm': 0.2124980390071869, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8632328510284424, 'eval_runtime': 5.2156, 'eval_samples_per_second': 191.733, 'eval_steps_per_second': 12.079, 'epoch': 0.52}
{'loss': 0.9369, 'grad_norm': 0.23277632892131805, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8585566282272339, 'eval_runtime': 5.2225, 'eval_samples_per_second': 191.48, 'eval_steps_per_second': 12.063, 'epoch': 0.56}
{'loss': 0.9559, 'grad_norm': 0.22114215791225433, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8543360829353333, 'eval_runtime': 5.2254, 'eval_samples_per_second': 191.374, 'eval_steps_per_second': 12.057, 'epoch': 0.6}
{'loss': 0.9049, 'grad_norm': 0.2219184786081314, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8560471534729004, 'eval_runtime': 5.2212, 'eval_samples_per_second': 191.525, 'eval_steps_per_second': 12.066, 'epoch': 0.64}
{'loss': 0.9415, 'grad_norm': 0.2311452478170395, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8500417470932007, 'eval_runtime': 5.2137, 'eval_samples_per_second': 191.802, 'eval_steps_per_second': 12.084, 'epoch': 0.68}
{'loss': 0.935, 'grad_norm': 0.22473758459091187, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8475453853607178, 'eval_runtime': 5.2072, 'eval_samples_per_second': 192.044, 'eval_steps_per_second': 12.099, 'epoch': 0.72}
{'loss': 0.8849, 'grad_norm': 0.2640470564365387, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8482145071029663, 'eval_runtime': 5.2093, 'eval_samples_per_second': 191.963, 'eval_steps_per_second': 12.094, 'epoch': 0.76}
{'loss': 0.8945, 'grad_norm': 0.23867450654506683, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8458503484725952, 'eval_runtime': 5.2037, 'eval_samples_per_second': 192.172, 'eval_steps_per_second': 12.107, 'epoch': 0.8}
{'loss': 0.8812, 'grad_norm': 0.32221075892448425, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8440036177635193, 'eval_runtime': 5.2106, 'eval_samples_per_second': 191.918, 'eval_steps_per_second': 12.091, 'epoch': 0.84}
{'loss': 0.9302, 'grad_norm': 0.2906521260738373, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.84262615442276, 'eval_runtime': 5.2064, 'eval_samples_per_second': 192.073, 'eval_steps_per_second': 12.101, 'epoch': 0.88}
{'loss': 0.9261, 'grad_norm': 0.31835687160491943, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8428863286972046, 'eval_runtime': 5.2208, 'eval_samples_per_second': 191.542, 'eval_steps_per_second': 12.067, 'epoch': 0.92}
{'loss': 0.8482, 'grad_norm': 0.2848413288593292, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8406420946121216, 'eval_runtime': 5.2101, 'eval_samples_per_second': 191.936, 'eval_steps_per_second': 12.092, 'epoch': 0.96}
{'loss': 0.8341, 'grad_norm': 0.3795352578163147, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8407555222511292, 'eval_runtime': 5.2083, 'eval_samples_per_second': 192.003, 'eval_steps_per_second': 12.096, 'epoch': 1.0}
{'train_runtime': 296.9981, 'train_samples_per_second': 33.653, 'train_steps_per_second': 2.104, 'train_loss': 1.034605126953125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3923275470733643, 0.9574931263923645, 0.9186960458755493, 0.90669846534729, 0.8969578742980957, 0.8852311968803406, 0.8852081298828125, 0.8767199516296387, 0.8763953447341919, 0.8715566396713257, 0.8730806708335876, 0.865717887878418, 0.8632328510284424, 0.8585566282272339, 0.8543360829353333, 0.8560471534729004, 0.8500417470932007, 0.8475453853607178, 0.8482145071029663, 0.8458503484725952, 0.8440036177635193, 0.84262615442276, 0.8428863286972046, 0.8406420946121216, 0.8407555222511292], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3923275470733643, 0.9574931263923645, 0.9186960458755493, 0.90669846534729, 0.8969578742980957, 0.8852311968803406, 0.8852081298828125, 0.8767199516296387, 0.8763953447341919, 0.8715566396713257, 0.8730806708335876, 0.865717887878418, 0.8632328510284424, 0.8585566282272339, 0.8543360829353333, 0.8560471534729004, 0.8500417470932007, 0.8475453853607178, 0.8482145071029663, 0.8458503484725952, 0.8440036177635193, 0.84262615442276, 0.8428863286972046, 0.8406420946121216, 0.8407555222511292]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0640900135040283
current iteration best possible eval_loss (full train run):  -0.8407555222511292
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087, -1.0897812843322754, -0.9427335262298584, -1.0629347562789917, -1.0640900135040283]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.7513 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -0.9631934572871691
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -0.9632042051114807
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -0.9632055259189901
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -0.963204197535146
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -0.9632067817284113
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2732, dtype=torch.float64), tensor(0.0503, dtype=torch.float64), tensor(0.0251, dtype=torch.float64), tensor(0.0389, dtype=torch.float64), tensor(0.1408, dtype=torch.float64), tensor(0.1590, dtype=torch.float64), tensor(0.0115, dtype=torch.float64), tensor(0.0401, dtype=torch.float64), tensor(0.2611, dtype=torch.float64), 28, 1, 1, 1, 0, 1, 68, 0.05632547729148834, 23.21037368538042, 0]
normalized proposed parameters for next round by BO: [tensor(0.2732, dtype=torch.float64), tensor(0.0503, dtype=torch.float64), tensor(0.0251, dtype=torch.float64), tensor(0.0389, dtype=torch.float64), tensor(0.1408, dtype=torch.float64), tensor(0.1590, dtype=torch.float64), tensor(0.0115, dtype=torch.float64), tensor(0.0401, dtype=torch.float64), tensor(0.2611, dtype=torch.float64), tensor(0.8902, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5330, dtype=torch.float64), tensor(0.5633, dtype=torch.float64), tensor(0.4835, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.273
  gsm8k: 0.05
  rowan_hellaswag: 0.025
  sciq: 0.039
  triviaqa: 0.141
  truthfulqa_gen: 0.159
  wikitext: 0.012
  mmlu: 0.04
  arc_challenge: 0.261

LoRA Parameters:
  lora_r: (68,)
  lora_dropout: (0.05632547729148834,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (23.21037368538042,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  68
lora dropout:  0.05632547729148834
lora alpha:  23.21037368538042
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 95,535,104 || all params: 8,125,796,352 || trainable%: 1.1757
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2622, 'grad_norm': 0.5562227964401245, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6082777976989746, 'eval_runtime': 5.3985, 'eval_samples_per_second': 185.235, 'eval_steps_per_second': 11.67, 'epoch': 0.04}
{'loss': 1.3682, 'grad_norm': 0.24081817269325256, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1541897058486938, 'eval_runtime': 5.4353, 'eval_samples_per_second': 183.983, 'eval_steps_per_second': 11.591, 'epoch': 0.08}
{'loss': 1.18, 'grad_norm': 0.23522590100765228, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0592551231384277, 'eval_runtime': 5.4387, 'eval_samples_per_second': 183.867, 'eval_steps_per_second': 11.584, 'epoch': 0.12}
{'loss': 1.0896, 'grad_norm': 0.2357548326253891, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.985770046710968, 'eval_runtime': 5.4267, 'eval_samples_per_second': 184.274, 'eval_steps_per_second': 11.609, 'epoch': 0.16}
{'loss': 1.036, 'grad_norm': 0.25232982635498047, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9228180646896362, 'eval_runtime': 5.413, 'eval_samples_per_second': 184.74, 'eval_steps_per_second': 11.639, 'epoch': 0.2}
{'loss': 0.9283, 'grad_norm': 0.2213839292526245, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.909561276435852, 'eval_runtime': 5.4413, 'eval_samples_per_second': 183.778, 'eval_steps_per_second': 11.578, 'epoch': 0.24}
{'loss': 0.9897, 'grad_norm': 0.21962422132492065, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8964608311653137, 'eval_runtime': 5.429, 'eval_samples_per_second': 184.194, 'eval_steps_per_second': 11.604, 'epoch': 0.28}
{'loss': 0.983, 'grad_norm': 0.20970745384693146, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8932715058326721, 'eval_runtime': 5.4632, 'eval_samples_per_second': 183.042, 'eval_steps_per_second': 11.532, 'epoch': 0.32}
{'loss': 0.9129, 'grad_norm': 0.2192574441432953, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.898001492023468, 'eval_runtime': 5.4669, 'eval_samples_per_second': 182.918, 'eval_steps_per_second': 11.524, 'epoch': 0.36}
{'loss': 0.909, 'grad_norm': 0.24809420108795166, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8950742483139038, 'eval_runtime': 5.4575, 'eval_samples_per_second': 183.235, 'eval_steps_per_second': 11.544, 'epoch': 0.4}
{'loss': 0.905, 'grad_norm': 0.27197298407554626, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8961146473884583, 'eval_runtime': 5.4509, 'eval_samples_per_second': 183.455, 'eval_steps_per_second': 11.558, 'epoch': 0.44}
{'loss': 0.8841, 'grad_norm': 0.26987186074256897, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8824276924133301, 'eval_runtime': 5.4606, 'eval_samples_per_second': 183.13, 'eval_steps_per_second': 11.537, 'epoch': 0.48}
{'loss': 0.8687, 'grad_norm': 0.23906373977661133, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8779940605163574, 'eval_runtime': 5.4574, 'eval_samples_per_second': 183.238, 'eval_steps_per_second': 11.544, 'epoch': 0.52}
{'loss': 0.9013, 'grad_norm': 0.26078885793685913, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8780704736709595, 'eval_runtime': 5.4783, 'eval_samples_per_second': 182.539, 'eval_steps_per_second': 11.5, 'epoch': 0.56}
{'loss': 0.8677, 'grad_norm': 0.258230596780777, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8755053877830505, 'eval_runtime': 5.4524, 'eval_samples_per_second': 183.405, 'eval_steps_per_second': 11.554, 'epoch': 0.6}
{'loss': 0.8725, 'grad_norm': 0.24143224954605103, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8721215724945068, 'eval_runtime': 5.4504, 'eval_samples_per_second': 183.474, 'eval_steps_per_second': 11.559, 'epoch': 0.64}
{'loss': 0.9118, 'grad_norm': 0.26845645904541016, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8745412826538086, 'eval_runtime': 5.4421, 'eval_samples_per_second': 183.754, 'eval_steps_per_second': 11.576, 'epoch': 0.68}
{'loss': 0.9107, 'grad_norm': 0.28836575150489807, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8712090849876404, 'eval_runtime': 5.4559, 'eval_samples_per_second': 183.287, 'eval_steps_per_second': 11.547, 'epoch': 0.72}
{'loss': 0.9152, 'grad_norm': 0.3029190003871918, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8670904636383057, 'eval_runtime': 5.4533, 'eval_samples_per_second': 183.377, 'eval_steps_per_second': 11.553, 'epoch': 0.76}
{'loss': 0.8339, 'grad_norm': 0.3460816442966461, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8677468299865723, 'eval_runtime': 5.4417, 'eval_samples_per_second': 183.766, 'eval_steps_per_second': 11.577, 'epoch': 0.8}
{'loss': 0.8355, 'grad_norm': 0.2518073618412018, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8662095665931702, 'eval_runtime': 5.4467, 'eval_samples_per_second': 183.599, 'eval_steps_per_second': 11.567, 'epoch': 0.84}
{'loss': 0.8085, 'grad_norm': 0.3155660927295685, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8643648624420166, 'eval_runtime': 5.4509, 'eval_samples_per_second': 183.455, 'eval_steps_per_second': 11.558, 'epoch': 0.88}
{'loss': 0.8671, 'grad_norm': 0.29997140169143677, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8638181090354919, 'eval_runtime': 5.4442, 'eval_samples_per_second': 183.682, 'eval_steps_per_second': 11.572, 'epoch': 0.92}
{'loss': 0.8085, 'grad_norm': 0.29819250106811523, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8654946684837341, 'eval_runtime': 5.4249, 'eval_samples_per_second': 184.336, 'eval_steps_per_second': 11.613, 'epoch': 0.96}
{'loss': 0.8793, 'grad_norm': 0.3473426103591919, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.863493800163269, 'eval_runtime': 5.4242, 'eval_samples_per_second': 184.36, 'eval_steps_per_second': 11.615, 'epoch': 1.0}
{'train_runtime': 329.5956, 'train_samples_per_second': 30.331, 'train_steps_per_second': 1.896, 'train_loss': 1.0291510528564454, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6082777976989746, 1.1541897058486938, 1.0592551231384277, 0.985770046710968, 0.9228180646896362, 0.909561276435852, 0.8964608311653137, 0.8932715058326721, 0.898001492023468, 0.8950742483139038, 0.8961146473884583, 0.8824276924133301, 0.8779940605163574, 0.8780704736709595, 0.8755053877830505, 0.8721215724945068, 0.8745412826538086, 0.8712090849876404, 0.8670904636383057, 0.8677468299865723, 0.8662095665931702, 0.8643648624420166, 0.8638181090354919, 0.8654946684837341, 0.863493800163269], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6082777976989746, 1.1541897058486938, 1.0592551231384277, 0.985770046710968, 0.9228180646896362, 0.909561276435852, 0.8964608311653137, 0.8932715058326721, 0.898001492023468, 0.8950742483139038, 0.8961146473884583, 0.8824276924133301, 0.8779940605163574, 0.8780704736709595, 0.8755053877830505, 0.8721215724945068, 0.8745412826538086, 0.8712090849876404, 0.8670904636383057, 0.8677468299865723, 0.8662095665931702, 0.8643648624420166, 0.8638181090354919, 0.8654946684837341, 0.863493800163269]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0356372594833374
current iteration best possible eval_loss (full train run):  -0.863493800163269
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087, -1.0897812843322754, -0.9427335262298584, -1.0629347562789917, -1.0640900135040283, -1.0356372594833374]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.5534 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -0.9731908911598568
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -0.973190919234778
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -0.9731908911513754
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -0.9731908911504856
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -0.9731959493172679
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1845, dtype=torch.float64), tensor(0.0820, dtype=torch.float64), tensor(0.0506, dtype=torch.float64), tensor(0.0206, dtype=torch.float64), tensor(0.1402, dtype=torch.float64), tensor(0.1374, dtype=torch.float64), tensor(0.0217, dtype=torch.float64), tensor(0.0771, dtype=torch.float64), tensor(0.2858, dtype=torch.float64), 29, 1, 1, 1, 0, 1, 10, 0.026697194238399877, 26.741969854191524, 0]
normalized proposed parameters for next round by BO: [tensor(0.1845, dtype=torch.float64), tensor(0.0820, dtype=torch.float64), tensor(0.0506, dtype=torch.float64), tensor(0.0206, dtype=torch.float64), tensor(0.1402, dtype=torch.float64), tensor(0.1374, dtype=torch.float64), tensor(0.0217, dtype=torch.float64), tensor(0.0771, dtype=torch.float64), tensor(0.2858, dtype=torch.float64), tensor(0.9098, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0790, dtype=torch.float64), tensor(0.2670, dtype=torch.float64), tensor(0.5571, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.185
  gsm8k: 0.082
  rowan_hellaswag: 0.051
  sciq: 0.021
  triviaqa: 0.14
  truthfulqa_gen: 0.137
  wikitext: 0.022
  mmlu: 0.077
  arc_challenge: 0.286

LoRA Parameters:
  lora_r: (10,)
  lora_dropout: (0.026697194238399877,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (26.741969854191524,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  10
lora dropout:  0.026697194238399877
lora alpha:  26.741969854191524
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 14,551,040 || all params: 8,044,812,288 || trainable%: 0.1809
length of training data:  9994
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.025, 'grad_norm': 2.8703787326812744, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.524908423423767, 'eval_runtime': 5.2945, 'eval_samples_per_second': 188.875, 'eval_steps_per_second': 11.899, 'epoch': 0.04}
{'loss': 1.3962, 'grad_norm': 0.6457450985908508, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1319022178649902, 'eval_runtime': 5.3034, 'eval_samples_per_second': 188.558, 'eval_steps_per_second': 11.879, 'epoch': 0.08}
{'loss': 1.2121, 'grad_norm': 0.6324051022529602, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0238280296325684, 'eval_runtime': 5.2915, 'eval_samples_per_second': 188.982, 'eval_steps_per_second': 11.906, 'epoch': 0.12}
{'loss': 1.0877, 'grad_norm': 0.6081620454788208, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9655560255050659, 'eval_runtime': 5.3065, 'eval_samples_per_second': 188.449, 'eval_steps_per_second': 11.872, 'epoch': 0.16}
{'loss': 1.0457, 'grad_norm': 0.6154152750968933, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9307551383972168, 'eval_runtime': 5.3054, 'eval_samples_per_second': 188.488, 'eval_steps_per_second': 11.875, 'epoch': 0.2}
{'loss': 1.007, 'grad_norm': 0.5412490367889404, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9244846701622009, 'eval_runtime': 5.3088, 'eval_samples_per_second': 188.368, 'eval_steps_per_second': 11.867, 'epoch': 0.24}
{'loss': 1.019, 'grad_norm': 0.4837433397769928, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9091602563858032, 'eval_runtime': 5.3102, 'eval_samples_per_second': 188.316, 'eval_steps_per_second': 11.864, 'epoch': 0.28}
{'loss': 0.9817, 'grad_norm': 0.5182129144668579, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9107983112335205, 'eval_runtime': 5.3151, 'eval_samples_per_second': 188.144, 'eval_steps_per_second': 11.853, 'epoch': 0.32}
{'loss': 1.0728, 'grad_norm': 0.5558561682701111, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9042809009552002, 'eval_runtime': 5.3135, 'eval_samples_per_second': 188.199, 'eval_steps_per_second': 11.857, 'epoch': 0.36}
{'loss': 0.9597, 'grad_norm': 0.6434485912322998, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9186034798622131, 'eval_runtime': 5.3237, 'eval_samples_per_second': 187.839, 'eval_steps_per_second': 11.834, 'epoch': 0.4}
{'loss': 0.9477, 'grad_norm': 0.5975710153579712, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8921884894371033, 'eval_runtime': 5.3178, 'eval_samples_per_second': 188.048, 'eval_steps_per_second': 11.847, 'epoch': 0.44}
{'loss': 0.958, 'grad_norm': 0.63001948595047, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8912631869316101, 'eval_runtime': 5.3227, 'eval_samples_per_second': 187.874, 'eval_steps_per_second': 11.836, 'epoch': 0.48}
{'loss': 0.9705, 'grad_norm': 0.6020914316177368, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8917927145957947, 'eval_runtime': 5.3166, 'eval_samples_per_second': 188.088, 'eval_steps_per_second': 11.85, 'epoch': 0.52}
{'loss': 0.958, 'grad_norm': 0.5768929123878479, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8891646265983582, 'eval_runtime': 5.3091, 'eval_samples_per_second': 188.357, 'eval_steps_per_second': 11.866, 'epoch': 0.56}
{'loss': 0.9502, 'grad_norm': 0.650396466255188, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8847916126251221, 'eval_runtime': 5.309, 'eval_samples_per_second': 188.36, 'eval_steps_per_second': 11.867, 'epoch': 0.6}
{'loss': 0.9167, 'grad_norm': 0.6745296716690063, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8858832716941833, 'eval_runtime': 5.2943, 'eval_samples_per_second': 188.882, 'eval_steps_per_second': 11.9, 'epoch': 0.64}
{'loss': 0.9049, 'grad_norm': 0.6464133858680725, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8856807947158813, 'eval_runtime': 5.2909, 'eval_samples_per_second': 189.003, 'eval_steps_per_second': 11.907, 'epoch': 0.68}
{'loss': 0.9321, 'grad_norm': 0.6155691742897034, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8785179853439331, 'eval_runtime': 5.2926, 'eval_samples_per_second': 188.944, 'eval_steps_per_second': 11.903, 'epoch': 0.72}
{'loss': 0.9398, 'grad_norm': 0.8031368255615234, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8810693025588989, 'eval_runtime': 5.2941, 'eval_samples_per_second': 188.891, 'eval_steps_per_second': 11.9, 'epoch': 0.76}
{'loss': 0.8844, 'grad_norm': 0.6671046614646912, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.878673791885376, 'eval_runtime': 5.2962, 'eval_samples_per_second': 188.816, 'eval_steps_per_second': 11.895, 'epoch': 0.8}
{'loss': 0.8604, 'grad_norm': 0.8324722647666931, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8797844648361206, 'eval_runtime': 5.2926, 'eval_samples_per_second': 188.942, 'eval_steps_per_second': 11.903, 'epoch': 0.84}
{'loss': 0.8611, 'grad_norm': 0.8239010572433472, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.875948965549469, 'eval_runtime': 5.3099, 'eval_samples_per_second': 188.327, 'eval_steps_per_second': 11.865, 'epoch': 0.88}
{'loss': 0.8667, 'grad_norm': 0.8000597953796387, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8748401403427124, 'eval_runtime': 5.3286, 'eval_samples_per_second': 187.666, 'eval_steps_per_second': 11.823, 'epoch': 0.92}
{'loss': 0.8291, 'grad_norm': 0.6895326375961304, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8724868297576904, 'eval_runtime': 5.326, 'eval_samples_per_second': 187.757, 'eval_steps_per_second': 11.829, 'epoch': 0.96}
{'loss': 0.8097, 'grad_norm': 0.8855628371238708, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8731890916824341, 'eval_runtime': 5.3457, 'eval_samples_per_second': 187.065, 'eval_steps_per_second': 11.785, 'epoch': 1.0}
{'train_runtime': 341.061, 'train_samples_per_second': 29.303, 'train_steps_per_second': 1.833, 'train_loss': 1.055847378540039, 'epoch': 1.0}
train_results:  {'eval_loss': [1.524908423423767, 1.1319022178649902, 1.0238280296325684, 0.9655560255050659, 0.9307551383972168, 0.9244846701622009, 0.9091602563858032, 0.9107983112335205, 0.9042809009552002, 0.9186034798622131, 0.8921884894371033, 0.8912631869316101, 0.8917927145957947, 0.8891646265983582, 0.8847916126251221, 0.8858832716941833, 0.8856807947158813, 0.8785179853439331, 0.8810693025588989, 0.878673791885376, 0.8797844648361206, 0.875948965549469, 0.8748401403427124, 0.8724868297576904, 0.8731890916824341], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.524908423423767, 1.1319022178649902, 1.0238280296325684, 0.9655560255050659, 0.9307551383972168, 0.9244846701622009, 0.9091602563858032, 0.9107983112335205, 0.9042809009552002, 0.9186034798622131, 0.8921884894371033, 0.8912631869316101, 0.8917927145957947, 0.8891646265983582, 0.8847916126251221, 0.8858832716941833, 0.8856807947158813, 0.8785179853439331, 0.8810693025588989, 0.878673791885376, 0.8797844648361206, 0.875948965549469, 0.8748401403427124, 0.8724868297576904, 0.8731890916824341]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1162586212158203
current iteration best possible eval_loss (full train run):  -0.8731890916824341
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087, -1.0897812843322754, -0.9427335262298584, -1.0629347562789917, -1.0640900135040283, -1.0356372594833374, -1.1162586212158203]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.8961 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -0.957467303905755
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -0.9573752361729571
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -0.9579752381116201
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -0.9573819745148975
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -0.9573883368299225
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2689, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), tensor(0.0191, dtype=torch.float64), 0, tensor(0.1405, dtype=torch.float64), tensor(0.2521, dtype=torch.float64), 0, 0, tensor(0.2994, dtype=torch.float64), 28, 1, 1, 1, 0, 1, 12, 0.0753065894720415, 28.313414505967327, 0]
normalized proposed parameters for next round by BO: [tensor(0.2689, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), tensor(0.0191, dtype=torch.float64), tensor(8.8891e-18, dtype=torch.float64), tensor(0.1405, dtype=torch.float64), tensor(0.2521, dtype=torch.float64), tensor(2.6497e-20, dtype=torch.float64), tensor(4.5012e-20, dtype=torch.float64), tensor(0.2994, dtype=torch.float64), tensor(0.8699, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0906, dtype=torch.float64), tensor(0.7531, dtype=torch.float64), tensor(0.5899, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.269
  gsm8k: 0.02
  rowan_hellaswag: 0.019
  sciq: 0
  triviaqa: 0.14
  truthfulqa_gen: 0.252
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.299

LoRA Parameters:
  lora_r: (12,)
  lora_dropout: (0.0753065894720415,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (28.313414505967327,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  12
lora dropout:  0.0753065894720415
lora alpha:  28.313414505967327
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 16,859,136 || all params: 8,047,120,384 || trainable%: 0.2095
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1574, 'grad_norm': 1.8662384748458862, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4410427808761597, 'eval_runtime': 5.3103, 'eval_samples_per_second': 188.315, 'eval_steps_per_second': 11.864, 'epoch': 0.04}
{'loss': 1.267, 'grad_norm': 0.7258475422859192, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.12302827835083, 'eval_runtime': 5.329, 'eval_samples_per_second': 187.653, 'eval_steps_per_second': 11.822, 'epoch': 0.08}
{'loss': 1.0688, 'grad_norm': 0.578863263130188, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0082252025604248, 'eval_runtime': 5.2924, 'eval_samples_per_second': 188.949, 'eval_steps_per_second': 11.904, 'epoch': 0.12}
{'loss': 0.9514, 'grad_norm': 0.6512382626533508, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9213113784790039, 'eval_runtime': 5.2892, 'eval_samples_per_second': 189.066, 'eval_steps_per_second': 11.911, 'epoch': 0.16}
{'loss': 0.8905, 'grad_norm': 0.5478470325469971, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9069719314575195, 'eval_runtime': 5.3031, 'eval_samples_per_second': 188.568, 'eval_steps_per_second': 11.88, 'epoch': 0.2}
{'loss': 0.9046, 'grad_norm': 0.53764808177948, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8973442316055298, 'eval_runtime': 5.2982, 'eval_samples_per_second': 188.742, 'eval_steps_per_second': 11.891, 'epoch': 0.24}
{'loss': 0.8822, 'grad_norm': 0.5477430820465088, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8956875801086426, 'eval_runtime': 5.2991, 'eval_samples_per_second': 188.712, 'eval_steps_per_second': 11.889, 'epoch': 0.28}
{'loss': 0.827, 'grad_norm': 0.5490974187850952, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8930451273918152, 'eval_runtime': 5.3107, 'eval_samples_per_second': 188.3, 'eval_steps_per_second': 11.863, 'epoch': 0.32}
{'loss': 0.8175, 'grad_norm': 0.6099934577941895, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8937497735023499, 'eval_runtime': 5.3069, 'eval_samples_per_second': 188.435, 'eval_steps_per_second': 11.871, 'epoch': 0.36}
{'loss': 0.8152, 'grad_norm': 0.6426708102226257, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8908095955848694, 'eval_runtime': 5.3008, 'eval_samples_per_second': 188.65, 'eval_steps_per_second': 11.885, 'epoch': 0.4}
{'loss': 0.8082, 'grad_norm': 0.6886253952980042, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8939549326896667, 'eval_runtime': 5.2776, 'eval_samples_per_second': 189.48, 'eval_steps_per_second': 11.937, 'epoch': 0.44}
{'loss': 0.7846, 'grad_norm': 0.6329470872879028, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8792926073074341, 'eval_runtime': 5.2876, 'eval_samples_per_second': 189.123, 'eval_steps_per_second': 11.915, 'epoch': 0.48}
{'loss': 0.7796, 'grad_norm': 0.7043452858924866, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8702329397201538, 'eval_runtime': 5.2756, 'eval_samples_per_second': 189.553, 'eval_steps_per_second': 11.942, 'epoch': 0.52}
{'loss': 0.7328, 'grad_norm': 0.7254627346992493, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8737860321998596, 'eval_runtime': 5.291, 'eval_samples_per_second': 188.999, 'eval_steps_per_second': 11.907, 'epoch': 0.56}
{'loss': 0.7474, 'grad_norm': 0.6175286173820496, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8666034936904907, 'eval_runtime': 5.2921, 'eval_samples_per_second': 188.961, 'eval_steps_per_second': 11.905, 'epoch': 0.6}
{'loss': 0.7072, 'grad_norm': 0.6932457089424133, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8632053732872009, 'eval_runtime': 5.2934, 'eval_samples_per_second': 188.914, 'eval_steps_per_second': 11.902, 'epoch': 0.64}
{'loss': 0.6958, 'grad_norm': 0.7228651642799377, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8677277565002441, 'eval_runtime': 5.2956, 'eval_samples_per_second': 188.835, 'eval_steps_per_second': 11.897, 'epoch': 0.68}
{'loss': 0.6976, 'grad_norm': 0.6528400182723999, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8662132620811462, 'eval_runtime': 5.281, 'eval_samples_per_second': 189.359, 'eval_steps_per_second': 11.93, 'epoch': 0.72}
{'loss': 0.7219, 'grad_norm': 0.8572351932525635, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8625987768173218, 'eval_runtime': 5.291, 'eval_samples_per_second': 189.001, 'eval_steps_per_second': 11.907, 'epoch': 0.76}
{'loss': 0.683, 'grad_norm': 0.902021050453186, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8616122007369995, 'eval_runtime': 5.2913, 'eval_samples_per_second': 188.991, 'eval_steps_per_second': 11.906, 'epoch': 0.8}
{'loss': 0.7028, 'grad_norm': 0.6821050047874451, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8613947033882141, 'eval_runtime': 5.2939, 'eval_samples_per_second': 188.895, 'eval_steps_per_second': 11.9, 'epoch': 0.84}
{'loss': 0.6657, 'grad_norm': 0.8702883124351501, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8589328527450562, 'eval_runtime': 5.2928, 'eval_samples_per_second': 188.936, 'eval_steps_per_second': 11.903, 'epoch': 0.88}
{'loss': 0.6876, 'grad_norm': 0.741087794303894, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8571349382400513, 'eval_runtime': 5.2972, 'eval_samples_per_second': 188.78, 'eval_steps_per_second': 11.893, 'epoch': 0.92}
{'loss': 0.6411, 'grad_norm': 0.725549042224884, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8591417074203491, 'eval_runtime': 5.2948, 'eval_samples_per_second': 188.864, 'eval_steps_per_second': 11.898, 'epoch': 0.96}
{'loss': 0.64, 'grad_norm': 0.6646121740341187, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8582038283348083, 'eval_runtime': 5.2935, 'eval_samples_per_second': 188.91, 'eval_steps_per_second': 11.901, 'epoch': 1.0}
{'train_runtime': 305.8648, 'train_samples_per_second': 32.684, 'train_steps_per_second': 2.043, 'train_loss': 0.8910810729980468, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4410427808761597, 1.12302827835083, 1.0082252025604248, 0.9213113784790039, 0.9069719314575195, 0.8973442316055298, 0.8956875801086426, 0.8930451273918152, 0.8937497735023499, 0.8908095955848694, 0.8939549326896667, 0.8792926073074341, 0.8702329397201538, 0.8737860321998596, 0.8666034936904907, 0.8632053732872009, 0.8677277565002441, 0.8662132620811462, 0.8625987768173218, 0.8616122007369995, 0.8613947033882141, 0.8589328527450562, 0.8571349382400513, 0.8591417074203491, 0.8582038283348083], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4410427808761597, 1.12302827835083, 1.0082252025604248, 0.9213113784790039, 0.9069719314575195, 0.8973442316055298, 0.8956875801086426, 0.8930451273918152, 0.8937497735023499, 0.8908095955848694, 0.8939549326896667, 0.8792926073074341, 0.8702329397201538, 0.8737860321998596, 0.8666034936904907, 0.8632053732872009, 0.8677277565002441, 0.8662132620811462, 0.8625987768173218, 0.8616122007369995, 0.8613947033882141, 0.8589328527450562, 0.8571349382400513, 0.8591417074203491, 0.8582038283348083]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1284140348434448
current iteration best possible eval_loss (full train run):  -0.8582038283348083
max eval_loss so far:  -0.8220756649971008
BO observations:  [-1.2131680250167847, -1.5157928466796875, -1.2578315734863281, -1.109604835510254, -1.045748233795166, -1.2228338718414307, -1.0637390613555908, -1.1067668199539185, -1.1692951917648315, -1.0037460327148438, -1.148607611656189, -1.0456465482711792, -1.168910264968872, -1.0472297668457031, -0.9853423833847046, -1.0026899576187134, -1.0178601741790771, -1.0726120471954346, -1.0783615112304688, -1.0563428401947021, -1.2032301425933838, -1.0832126140594482, -1.1008516550064087, -1.0897812843322754, -0.9427335262298584, -1.0629347562789917, -1.0640900135040283, -1.0356372594833374, -1.1162586212158203, -1.1284140348434448]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 16.2292 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -0.9752418395598471
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -0.9764019116037554
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -0.976025934198393
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -0.9738300026443116
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -0.9752066971887375
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2030, dtype=torch.float64), tensor(0.1081, dtype=torch.float64), tensor(0.0199, dtype=torch.float64), tensor(0.0287, dtype=torch.float64), tensor(0.0892, dtype=torch.float64), tensor(0.0170, dtype=torch.float64), 0, 0, tensor(0.5273, dtype=torch.float64), 19, 0, 1, 1, 1, 0, 96, 0.026777255213370505, 28.911828247976175, 0]
normalized proposed parameters for next round by BO: [tensor(0.2030, dtype=torch.float64), tensor(0.1081, dtype=torch.float64), tensor(0.0199, dtype=torch.float64), tensor(0.0287, dtype=torch.float64), tensor(0.0892, dtype=torch.float64), tensor(0.0170, dtype=torch.float64), tensor(0.0045, dtype=torch.float64), tensor(0.0022, dtype=torch.float64), tensor(0.5273, dtype=torch.float64), tensor(0.5968, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7484, dtype=torch.float64), tensor(0.2678, dtype=torch.float64), tensor(0.6023, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
commonsense_qa
evaluation dataset:
data domain:  commonsense_qa  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/commonsense_qa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 17, 0, 1, 0, 1, 1, 74, 0.06944379333880846, 10, 0]
Checking history sample input_X_between_0_1:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 0.53125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.578125, 0.6944379333880846, 0.20833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -0.897139310836792
Checking history sample input_X:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 18, 0, 0, 1, 0, 0, 88, 0.07338847024538249, 48, 1]
Checking history sample input_X_between_0_1:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 0.5625, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6875, 0.7338847024538249, 1.0, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1220359802246094
Checking history sample input_X:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 27, 1, 1, 0, 1, 0, 35, 0.056205119903528195, 6, 0]
Checking history sample input_X_between_0_1:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 0.84375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2734375, 0.5620511990352819, 0.125, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9592704772949219
Checking history sample input_X:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 22, 0, 0, 0, 0, 1, 14, 0.05785794551060587, 9, 1]
Checking history sample input_X_between_0_1:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 0.6875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.109375, 0.5785794551060587, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0981301069259644
Checking history sample input_X:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 15, 1, 0, 0, 1, 1, 33, 0.046509797776235, 40, 0]
Checking history sample input_X_between_0_1:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 0.46875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2578125, 0.46509797776234996, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9429119229316711
Checking history sample input_X:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 11, 0, 0, 1, 1, 1, 70, 0.005359902393564798, 44, 1]
Checking history sample input_X_between_0_1:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 0.34375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.546875, 0.05359902393564797, 0.9166666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8976500034332275
Checking history sample input_X:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 3, 1, 1, 0, 0, 1, 89, 0.004230716614147934, 21, 1]
Checking history sample input_X_between_0_1:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 0.09375, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6953125, 0.042307166141479335, 0.4375, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0746725797653198
Checking history sample input_X:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 11, 0, 0, 1, 0, 1, 128, 0.02126981773222546, 43, 0]
Checking history sample input_X_between_0_1:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 0.34375, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2126981773222546, 0.8958333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0224553346633911
Checking history sample input_X:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 4, 0, 1, 0, 0, 1, 51, 0.048394033913014715, 30, 0]
Checking history sample input_X_between_0_1:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3984375, 0.4839403391301471, 0.625, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3750804662704468
Checking history sample input_X:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 25, 1, 0, 1, 0, 0, 103, 0.021006146654874183, 41, 0]
Checking history sample input_X_between_0_1:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 0.78125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8046875, 0.2100614665487418, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9797114729881287
Checking history sample input_X:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 30, 0, 0, 0, 1, 0, 30, 0.06473585772462145, 22, 0]
Checking history sample input_X_between_0_1:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.234375, 0.6473585772462145, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8777164220809937
Checking history sample input_X:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 10, 0, 0, 1, 0, 1, 33, 0.053839147652140054, 30, 1]
Checking history sample input_X_between_0_1:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 0.3125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2578125, 0.5383914765214005, 0.625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.424627661705017
Checking history sample input_X:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 27, 0, 1, 1, 1, 0, 91, 0.026257080995186557, 27, 0]
Checking history sample input_X_between_0_1:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 0.84375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7109375, 0.26257080995186555, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.6500163078308105
Checking history sample input_X:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 28, 1, 1, 1, 0, 0, 23, 0.048743516472132736, 35, 1]
Checking history sample input_X_between_0_1:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 0.875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1796875, 0.48743516472132736, 0.7291666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9704539775848389
Checking history sample input_X:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 14, 1, 0, 1, 0, 0, 32, 0.04806841934255027, 48, 0]
Checking history sample input_X_between_0_1:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 0.4375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.48068419342550267, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1657739877700806
Checking history sample input_X:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 19, 0, 0, 1, 1, 0, 22, 0.09470699670511286, 20, 1]
Checking history sample input_X_between_0_1:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 0.59375, 0.0, 0.0, 1.0, 1.0, 0.0, 0.171875, 0.9470699670511286, 0.4166666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1796187162399292
Checking history sample input_X:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 2, 0, 1, 1, 1, 0, 72, 0.07202092774167915, 33, 1]
Checking history sample input_X_between_0_1:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5625, 0.7202092774167914, 0.6875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2933062314987183
Checking history sample input_X:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 15, 1, 0, 1, 0, 1, 69, 0.09577839904714924, 28, 0]
Checking history sample input_X_between_0_1:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 0.46875, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5390625, 0.9577839904714924, 0.5833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0850565433502197
Checking history sample input_X:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 30, 1, 1, 1, 0, 1, 19, 0.07443761643578949, 27, 0]
Checking history sample input_X_between_0_1:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 0.9375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1484375, 0.7443761643578948, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.7435973286628723
Checking history sample input_X:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 12, 1, 0, 0, 1, 1, 19, 0.0774188690692264, 1, 1]
Checking history sample input_X_between_0_1:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1484375, 0.774188690692264, 0.020833333333333332, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1769657135009766
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1339 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7549261450767517, 0.5223153233528137, 0.34494686126708984, 0.22022795677185059, 0.8649192452430725, 0.11607193946838379, 0.08206868171691895, 0.10216474533081055, 0.7195968627929688, 0.36592358350753784, 0.8032858371734619, 0.556303858757019, 0.931158185005188, 0.535922110080719, 0.39674752950668335, 0.5428948402404785, 0.42590945959091187, 0.5472617149353027, 0.9354628920555115]  ‚Üí  acq = -0.1733032707846589
X = [0.9950141310691833, 0.8505911827087402, 0.20876818895339966, 0.7469173669815063, 0.5558549165725708, 0.6175926923751831, 0.033005595207214355, 0.07938653230667114, 0.32334065437316895, 0.716880738735199, 0.040747880935668945, 0.4049155116081238, 0.5577225089073181, 0.9406478404998779, 0.11002945899963379, 0.11274168640375137, 0.7791347503662109, 0.26116126775741577, 0.05847817659378052]  ‚Üí  acq = -0.19648109288334292
X = [0.6601270437240601, 0.038965046405792236, 0.38540923595428467, 0.16049319505691528, 0.3265771269798279, 0.350848913192749, 0.2535751461982727, 0.09295397996902466, 0.6733603477478027, 0.8707625269889832, 0.01407933235168457, 0.06870031356811523, 0.8086802959442139, 0.5700405240058899, 0.08736544847488403, 0.9062260389328003, 0.3623340129852295, 0.7215187549591064, 0.11750108003616333]  ‚Üí  acq = -0.16049316564467486
X = [0.9082239270210266, 0.9917650818824768, 0.9099915623664856, 0.8330632448196411, 0.8657382726669312, 0.6571934223175049, 0.24284619092941284, 0.08867347240447998, 0.20606768131256104, 0.8301365971565247, 0.4698001742362976, 0.8798337578773499, 0.04068303108215332, 0.19657790660858154, 0.7941281199455261, 0.24681568145751953, 0.4398619532585144, 0.6199308633804321, 0.8852578401565552]  ‚Üí  acq = -0.21793367247788176
X = [0.7217139601707458, 0.03493863344192505, 0.2610248327255249, 0.9692310690879822, 0.7282414436340332, 0.6186767816543579, 0.23972588777542114, 0.5724396705627441, 0.7413028478622437, 0.9198618531227112, 0.6214724183082581, 0.07783794403076172, 0.3100129961967468, 0.1396312117576599, 0.024176061153411865, 0.9036850929260254, 0.494157612323761, 0.09651041030883789, 0.005554378032684326]  ‚Üí  acq = -0.17928167903770376
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.7401, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.2599, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 0.0, 47.99999999999998, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.7401, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.7938e-17, dtype=torch.float64), tensor(8.8002e-18, dtype=torch.float64), tensor(4.1239e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.0942e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2599, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.74
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.26

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (47.99999999999998,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  47.99999999999998
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.9115, 'grad_norm': 1.12343430519104, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.186650276184082, 'eval_runtime': 4.9572, 'eval_samples_per_second': 201.727, 'eval_steps_per_second': 12.709, 'epoch': 0.04}
{'loss': 1.0187, 'grad_norm': 0.5317832827568054, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9300342202186584, 'eval_runtime': 5.0249, 'eval_samples_per_second': 199.011, 'eval_steps_per_second': 12.538, 'epoch': 0.08}
{'loss': 0.8843, 'grad_norm': 0.2907907962799072, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.8994660973548889, 'eval_runtime': 4.9798, 'eval_samples_per_second': 200.813, 'eval_steps_per_second': 12.651, 'epoch': 0.12}
{'loss': 0.874, 'grad_norm': 0.23506370186805725, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8818889260292053, 'eval_runtime': 4.9854, 'eval_samples_per_second': 200.587, 'eval_steps_per_second': 12.637, 'epoch': 0.16}
{'loss': 0.8624, 'grad_norm': 0.23260226845741272, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8788335919380188, 'eval_runtime': 4.9916, 'eval_samples_per_second': 200.335, 'eval_steps_per_second': 12.621, 'epoch': 0.2}
{'loss': 0.8411, 'grad_norm': 0.21310032904148102, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8676171898841858, 'eval_runtime': 5.0114, 'eval_samples_per_second': 199.545, 'eval_steps_per_second': 12.571, 'epoch': 0.24}
{'loss': 0.8368, 'grad_norm': 0.22362379729747772, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8644298315048218, 'eval_runtime': 5.0177, 'eval_samples_per_second': 199.294, 'eval_steps_per_second': 12.556, 'epoch': 0.28}
{'loss': 0.8422, 'grad_norm': 0.2270456701517105, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8593065142631531, 'eval_runtime': 5.0111, 'eval_samples_per_second': 199.556, 'eval_steps_per_second': 12.572, 'epoch': 0.32}
{'loss': 0.84, 'grad_norm': 0.21233119070529938, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8508042693138123, 'eval_runtime': 5.0167, 'eval_samples_per_second': 199.335, 'eval_steps_per_second': 12.558, 'epoch': 0.36}
{'loss': 0.8283, 'grad_norm': 0.2257101982831955, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8474463820457458, 'eval_runtime': 4.9945, 'eval_samples_per_second': 200.221, 'eval_steps_per_second': 12.614, 'epoch': 0.4}
{'loss': 0.8214, 'grad_norm': 0.22539597749710083, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8463367819786072, 'eval_runtime': 5.0148, 'eval_samples_per_second': 199.412, 'eval_steps_per_second': 12.563, 'epoch': 0.44}
{'loss': 0.8063, 'grad_norm': 0.22203576564788818, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8427004218101501, 'eval_runtime': 5.0019, 'eval_samples_per_second': 199.926, 'eval_steps_per_second': 12.595, 'epoch': 0.48}
{'loss': 0.7901, 'grad_norm': 0.22752384841442108, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8393833041191101, 'eval_runtime': 5.0249, 'eval_samples_per_second': 199.009, 'eval_steps_per_second': 12.538, 'epoch': 0.52}
{'loss': 0.7906, 'grad_norm': 0.2355562448501587, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8382774591445923, 'eval_runtime': 4.9652, 'eval_samples_per_second': 201.4, 'eval_steps_per_second': 12.688, 'epoch': 0.56}
{'loss': 0.7804, 'grad_norm': 0.2703550457954407, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8350238800048828, 'eval_runtime': 4.9118, 'eval_samples_per_second': 203.592, 'eval_steps_per_second': 12.826, 'epoch': 0.6}
{'loss': 0.7845, 'grad_norm': 0.2512569725513458, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8313537836074829, 'eval_runtime': 5.0795, 'eval_samples_per_second': 196.869, 'eval_steps_per_second': 12.403, 'epoch': 0.64}
{'loss': 0.7909, 'grad_norm': 0.2556837201118469, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8273571133613586, 'eval_runtime': 5.0519, 'eval_samples_per_second': 197.944, 'eval_steps_per_second': 12.47, 'epoch': 0.68}
{'loss': 0.7693, 'grad_norm': 0.22807194292545319, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8263545036315918, 'eval_runtime': 5.0296, 'eval_samples_per_second': 198.822, 'eval_steps_per_second': 12.526, 'epoch': 0.72}
{'loss': 0.7624, 'grad_norm': 0.28822654485702515, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8265975117683411, 'eval_runtime': 5.2596, 'eval_samples_per_second': 190.13, 'eval_steps_per_second': 11.978, 'epoch': 0.76}
{'loss': 0.7739, 'grad_norm': 0.26894184947013855, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.822895884513855, 'eval_runtime': 16.4459, 'eval_samples_per_second': 60.805, 'eval_steps_per_second': 3.831, 'epoch': 0.8}
{'loss': 0.7369, 'grad_norm': 0.27609601616859436, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.823292076587677, 'eval_runtime': 14.8934, 'eval_samples_per_second': 67.144, 'eval_steps_per_second': 4.23, 'epoch': 0.84}
{'loss': 0.759, 'grad_norm': 0.2632274627685547, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8227108120918274, 'eval_runtime': 14.865, 'eval_samples_per_second': 67.272, 'eval_steps_per_second': 4.238, 'epoch': 0.88}
{'loss': 0.7508, 'grad_norm': 0.23033100366592407, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8211687207221985, 'eval_runtime': 11.8085, 'eval_samples_per_second': 84.685, 'eval_steps_per_second': 5.335, 'epoch': 0.92}
{'loss': 0.7398, 'grad_norm': 0.24673502147197723, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8203803896903992, 'eval_runtime': 12.5569, 'eval_samples_per_second': 79.637, 'eval_steps_per_second': 5.017, 'epoch': 0.96}
{'loss': 0.7233, 'grad_norm': 0.2983561158180237, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.81962651014328, 'eval_runtime': 11.9089, 'eval_samples_per_second': 83.971, 'eval_steps_per_second': 5.29, 'epoch': 1.0}
{'train_runtime': 401.0916, 'train_samples_per_second': 24.929, 'train_steps_per_second': 1.558, 'train_loss': 0.8927594970703125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.186650276184082, 0.9300342202186584, 0.8994660973548889, 0.8818889260292053, 0.8788335919380188, 0.8676171898841858, 0.8644298315048218, 0.8593065142631531, 0.8508042693138123, 0.8474463820457458, 0.8463367819786072, 0.8427004218101501, 0.8393833041191101, 0.8382774591445923, 0.8350238800048828, 0.8313537836074829, 0.8273571133613586, 0.8263545036315918, 0.8265975117683411, 0.822895884513855, 0.823292076587677, 0.8227108120918274, 0.8211687207221985, 0.8203803896903992, 0.81962651014328], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.186650276184082, 0.9300342202186584, 0.8994660973548889, 0.8818889260292053, 0.8788335919380188, 0.8676171898841858, 0.8644298315048218, 0.8593065142631531, 0.8508042693138123, 0.8474463820457458, 0.8463367819786072, 0.8427004218101501, 0.8393833041191101, 0.8382774591445923, 0.8350238800048828, 0.8313537836074829, 0.8273571133613586, 0.8263545036315918, 0.8265975117683411, 0.822895884513855, 0.823292076587677, 0.8227108120918274, 0.8211687207221985, 0.8203803896903992, 0.81962651014328]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2130283117294312
current iteration best possible eval_loss (full train run):  -0.81962651014328
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5782 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.5402211346591242
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.5243041594638272
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.5243045163934474
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.5746018023996697
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.524061982651334
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2883, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.7117, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2883, dtype=torch.float64), tensor(1.6469e-16, dtype=torch.float64), tensor(7.9511e-17, dtype=torch.float64), tensor(8.7688e-20, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.7855e-17, dtype=torch.float64), tensor(5.3226e-17, dtype=torch.float64), tensor(0.7117, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.288
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.712

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0756, 'grad_norm': 7.525890827178955, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.90714955329895, 'eval_runtime': 4.3193, 'eval_samples_per_second': 231.519, 'eval_steps_per_second': 14.586, 'epoch': 0.04}
{'loss': 2.4971, 'grad_norm': 3.914947986602783, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.116713762283325, 'eval_runtime': 4.3492, 'eval_samples_per_second': 229.928, 'eval_steps_per_second': 14.485, 'epoch': 0.08}
{'loss': 1.5168, 'grad_norm': 2.6374363899230957, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5130715370178223, 'eval_runtime': 4.3597, 'eval_samples_per_second': 229.372, 'eval_steps_per_second': 14.45, 'epoch': 0.12}
{'loss': 1.2549, 'grad_norm': 2.520747661590576, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3300209045410156, 'eval_runtime': 4.3488, 'eval_samples_per_second': 229.947, 'eval_steps_per_second': 14.487, 'epoch': 0.16}
{'loss': 1.1482, 'grad_norm': 2.632448196411133, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.229150414466858, 'eval_runtime': 4.3653, 'eval_samples_per_second': 229.079, 'eval_steps_per_second': 14.432, 'epoch': 0.2}
{'loss': 1.0855, 'grad_norm': 2.432079553604126, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1586205959320068, 'eval_runtime': 4.3852, 'eval_samples_per_second': 228.04, 'eval_steps_per_second': 14.367, 'epoch': 0.24}
{'loss': 1.0321, 'grad_norm': 2.0561110973358154, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.130741000175476, 'eval_runtime': 4.3924, 'eval_samples_per_second': 227.667, 'eval_steps_per_second': 14.343, 'epoch': 0.28}
{'loss': 1.0127, 'grad_norm': 2.1385457515716553, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1149035692214966, 'eval_runtime': 4.3844, 'eval_samples_per_second': 228.08, 'eval_steps_per_second': 14.369, 'epoch': 0.32}
{'loss': 0.9806, 'grad_norm': 1.8613146543502808, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.098684549331665, 'eval_runtime': 4.4186, 'eval_samples_per_second': 226.314, 'eval_steps_per_second': 14.258, 'epoch': 0.36}
{'loss': 0.9931, 'grad_norm': 1.9091261625289917, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.093524694442749, 'eval_runtime': 4.4183, 'eval_samples_per_second': 226.329, 'eval_steps_per_second': 14.259, 'epoch': 0.4}
{'loss': 0.9794, 'grad_norm': 2.181650400161743, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0908888578414917, 'eval_runtime': 4.4031, 'eval_samples_per_second': 227.114, 'eval_steps_per_second': 14.308, 'epoch': 0.44}
{'loss': 0.9858, 'grad_norm': 1.8359558582305908, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0825704336166382, 'eval_runtime': 4.3917, 'eval_samples_per_second': 227.703, 'eval_steps_per_second': 14.345, 'epoch': 0.48}
{'loss': 0.9676, 'grad_norm': 1.940982699394226, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.078197717666626, 'eval_runtime': 4.4061, 'eval_samples_per_second': 226.959, 'eval_steps_per_second': 14.298, 'epoch': 0.52}
{'loss': 0.9646, 'grad_norm': 2.3728504180908203, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0742979049682617, 'eval_runtime': 4.3974, 'eval_samples_per_second': 227.408, 'eval_steps_per_second': 14.327, 'epoch': 0.56}
{'loss': 0.9766, 'grad_norm': 1.8783975839614868, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.071108341217041, 'eval_runtime': 4.3937, 'eval_samples_per_second': 227.599, 'eval_steps_per_second': 14.339, 'epoch': 0.6}
{'loss': 0.9877, 'grad_norm': 1.9280716180801392, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0673859119415283, 'eval_runtime': 4.3898, 'eval_samples_per_second': 227.801, 'eval_steps_per_second': 14.351, 'epoch': 0.64}
{'loss': 0.9767, 'grad_norm': 1.9397633075714111, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.0644164085388184, 'eval_runtime': 4.3946, 'eval_samples_per_second': 227.55, 'eval_steps_per_second': 14.336, 'epoch': 0.68}
{'loss': 0.9448, 'grad_norm': 2.06665301322937, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0630929470062256, 'eval_runtime': 4.4027, 'eval_samples_per_second': 227.132, 'eval_steps_per_second': 14.309, 'epoch': 0.72}
{'loss': 0.9597, 'grad_norm': 1.8879433870315552, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0610134601593018, 'eval_runtime': 4.3926, 'eval_samples_per_second': 227.656, 'eval_steps_per_second': 14.342, 'epoch': 0.76}
{'loss': 0.9571, 'grad_norm': 2.2835018634796143, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.060024619102478, 'eval_runtime': 4.3921, 'eval_samples_per_second': 227.683, 'eval_steps_per_second': 14.344, 'epoch': 0.8}
{'loss': 0.9878, 'grad_norm': 1.813524842262268, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0579386949539185, 'eval_runtime': 4.3843, 'eval_samples_per_second': 228.089, 'eval_steps_per_second': 14.37, 'epoch': 0.84}
{'loss': 0.9508, 'grad_norm': 1.759682059288025, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0588183403015137, 'eval_runtime': 4.3955, 'eval_samples_per_second': 227.506, 'eval_steps_per_second': 14.333, 'epoch': 0.88}
{'loss': 0.9569, 'grad_norm': 2.062011480331421, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0595530271530151, 'eval_runtime': 4.3839, 'eval_samples_per_second': 228.109, 'eval_steps_per_second': 14.371, 'epoch': 0.92}
{'loss': 0.971, 'grad_norm': 2.074070692062378, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.058804988861084, 'eval_runtime': 4.3847, 'eval_samples_per_second': 228.067, 'eval_steps_per_second': 14.368, 'epoch': 0.96}
{'loss': 0.9612, 'grad_norm': 2.2371528148651123, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0578869581222534, 'eval_runtime': 4.374, 'eval_samples_per_second': 228.623, 'eval_steps_per_second': 14.403, 'epoch': 1.0}
{'train_runtime': 179.3187, 'train_samples_per_second': 55.761, 'train_steps_per_second': 3.485, 'train_loss': 1.2049796264648438, 'epoch': 1.0}
train_results:  {'eval_loss': [3.90714955329895, 2.116713762283325, 1.5130715370178223, 1.3300209045410156, 1.229150414466858, 1.1586205959320068, 1.130741000175476, 1.1149035692214966, 1.098684549331665, 1.093524694442749, 1.0908888578414917, 1.0825704336166382, 1.078197717666626, 1.0742979049682617, 1.071108341217041, 1.0673859119415283, 1.0644164085388184, 1.0630929470062256, 1.0610134601593018, 1.060024619102478, 1.0579386949539185, 1.0588183403015137, 1.0595530271530151, 1.058804988861084, 1.0578869581222534], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.90714955329895, 2.116713762283325, 1.5130715370178223, 1.3300209045410156, 1.229150414466858, 1.1586205959320068, 1.130741000175476, 1.1149035692214966, 1.098684549331665, 1.093524694442749, 1.0908888578414917, 1.0825704336166382, 1.078197717666626, 1.0742979049682617, 1.071108341217041, 1.0673859119415283, 1.0644164085388184, 1.0630929470062256, 1.0610134601593018, 1.060024619102478, 1.0579386949539185, 1.0588183403015137, 1.0595530271530151, 1.058804988861084, 1.0578869581222534]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.511947751045227
current iteration best possible eval_loss (full train run):  -1.0578869581222534
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.0026 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.7184693219621242
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.6441233475369833
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.6444015993532012
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.6436665107627426
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.6441435865636034
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2136, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.4172, dtype=torch.float64), tensor(0.3692, dtype=torch.float64), 29, 0, 0, 1, 1, 1, 128, 8.673617379884005e-20, 47.999999999999986, 0]
normalized proposed parameters for next round by BO: [tensor(0.2136, dtype=torch.float64), tensor(4.4368e-16, dtype=torch.float64), tensor(7.8473e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4172, dtype=torch.float64), tensor(0.3692, dtype=torch.float64), tensor(0.9131, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(8.6736e-19, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.214
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.417
  arc_challenge: 0.369

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (8.673617379884005e-20,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (47.999999999999986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  8.673617379884005e-20
lora alpha:  47.999999999999986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,258,752 || all params: 8,235,520,000 || trainable%: 2.4924
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5487, 'grad_norm': 0.4975184202194214, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.220993161201477, 'eval_runtime': 5.2013, 'eval_samples_per_second': 192.261, 'eval_steps_per_second': 12.112, 'epoch': 0.04}
{'loss': 1.1527, 'grad_norm': 0.23698240518569946, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9761558771133423, 'eval_runtime': 5.2014, 'eval_samples_per_second': 192.256, 'eval_steps_per_second': 12.112, 'epoch': 0.08}
{'loss': 1.107, 'grad_norm': 0.23133456707000732, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9338669180870056, 'eval_runtime': 5.1842, 'eval_samples_per_second': 192.894, 'eval_steps_per_second': 12.152, 'epoch': 0.12}
{'loss': 1.0416, 'grad_norm': 0.20574651658535004, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9246537089347839, 'eval_runtime': 5.1871, 'eval_samples_per_second': 192.784, 'eval_steps_per_second': 12.145, 'epoch': 0.16}
{'loss': 1.0331, 'grad_norm': 0.1859428733587265, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9116671681404114, 'eval_runtime': 5.1975, 'eval_samples_per_second': 192.402, 'eval_steps_per_second': 12.121, 'epoch': 0.2}
{'loss': 1.0896, 'grad_norm': 0.24276761710643768, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9118635058403015, 'eval_runtime': 5.2051, 'eval_samples_per_second': 192.118, 'eval_steps_per_second': 12.103, 'epoch': 0.24}
{'loss': 1.0082, 'grad_norm': 0.22870413959026337, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9004731178283691, 'eval_runtime': 5.21, 'eval_samples_per_second': 191.938, 'eval_steps_per_second': 12.092, 'epoch': 0.28}
{'loss': 0.9675, 'grad_norm': 0.23024995625019073, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9007092714309692, 'eval_runtime': 5.21, 'eval_samples_per_second': 191.938, 'eval_steps_per_second': 12.092, 'epoch': 0.32}
{'loss': 0.975, 'grad_norm': 0.2508765459060669, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8985130786895752, 'eval_runtime': 5.2036, 'eval_samples_per_second': 192.175, 'eval_steps_per_second': 12.107, 'epoch': 0.36}
{'loss': 0.991, 'grad_norm': 0.24350665509700775, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8902369141578674, 'eval_runtime': 5.2073, 'eval_samples_per_second': 192.037, 'eval_steps_per_second': 12.098, 'epoch': 0.4}
{'loss': 0.9321, 'grad_norm': 0.2262788861989975, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8929750323295593, 'eval_runtime': 5.201, 'eval_samples_per_second': 192.269, 'eval_steps_per_second': 12.113, 'epoch': 0.44}
{'loss': 0.9373, 'grad_norm': 0.28732264041900635, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8830334544181824, 'eval_runtime': 5.2027, 'eval_samples_per_second': 192.207, 'eval_steps_per_second': 12.109, 'epoch': 0.48}
{'loss': 0.9043, 'grad_norm': 0.2587510645389557, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8791205883026123, 'eval_runtime': 5.2003, 'eval_samples_per_second': 192.296, 'eval_steps_per_second': 12.115, 'epoch': 0.52}
{'loss': 0.8468, 'grad_norm': 0.2621593773365021, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8756409883499146, 'eval_runtime': 5.2031, 'eval_samples_per_second': 192.194, 'eval_steps_per_second': 12.108, 'epoch': 0.56}
{'loss': 0.9194, 'grad_norm': 0.3111867904663086, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8772943019866943, 'eval_runtime': 5.2032, 'eval_samples_per_second': 192.189, 'eval_steps_per_second': 12.108, 'epoch': 0.6}
{'loss': 0.8437, 'grad_norm': 0.2584246098995209, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8709728717803955, 'eval_runtime': 5.2108, 'eval_samples_per_second': 191.909, 'eval_steps_per_second': 12.09, 'epoch': 0.64}
{'loss': 0.8514, 'grad_norm': 0.23378078639507294, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8688684105873108, 'eval_runtime': 5.226, 'eval_samples_per_second': 191.35, 'eval_steps_per_second': 12.055, 'epoch': 0.68}
{'loss': 0.8284, 'grad_norm': 0.280327171087265, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8744389414787292, 'eval_runtime': 5.2305, 'eval_samples_per_second': 191.188, 'eval_steps_per_second': 12.045, 'epoch': 0.72}
{'loss': 0.853, 'grad_norm': 0.35583260655403137, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8652226328849792, 'eval_runtime': 5.2498, 'eval_samples_per_second': 190.482, 'eval_steps_per_second': 12.0, 'epoch': 0.76}
{'loss': 0.8355, 'grad_norm': 0.36308884620666504, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8646872043609619, 'eval_runtime': 5.2596, 'eval_samples_per_second': 190.128, 'eval_steps_per_second': 11.978, 'epoch': 0.8}
{'loss': 0.7872, 'grad_norm': 0.27016183733940125, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8645814061164856, 'eval_runtime': 5.2637, 'eval_samples_per_second': 189.981, 'eval_steps_per_second': 11.969, 'epoch': 0.84}
{'loss': 0.7627, 'grad_norm': 0.3247736394405365, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8627713918685913, 'eval_runtime': 5.2389, 'eval_samples_per_second': 190.88, 'eval_steps_per_second': 12.025, 'epoch': 0.88}
{'loss': 0.7722, 'grad_norm': 0.2095746099948883, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8614075779914856, 'eval_runtime': 5.2479, 'eval_samples_per_second': 190.554, 'eval_steps_per_second': 12.005, 'epoch': 0.92}
{'loss': 0.769, 'grad_norm': 0.34471118450164795, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8605424165725708, 'eval_runtime': 5.2335, 'eval_samples_per_second': 191.077, 'eval_steps_per_second': 12.038, 'epoch': 0.96}
{'loss': 0.8031, 'grad_norm': 0.33346664905548096, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8601453900337219, 'eval_runtime': 5.2803, 'eval_samples_per_second': 189.382, 'eval_steps_per_second': 11.931, 'epoch': 1.0}
{'train_runtime': 339.2196, 'train_samples_per_second': 29.474, 'train_steps_per_second': 1.842, 'train_loss': 0.9824144012451171, 'epoch': 1.0}
train_results:  {'eval_loss': [1.220993161201477, 0.9761558771133423, 0.9338669180870056, 0.9246537089347839, 0.9116671681404114, 0.9118635058403015, 0.9004731178283691, 0.9007092714309692, 0.8985130786895752, 0.8902369141578674, 0.8929750323295593, 0.8830334544181824, 0.8791205883026123, 0.8756409883499146, 0.8772943019866943, 0.8709728717803955, 0.8688684105873108, 0.8744389414787292, 0.8652226328849792, 0.8646872043609619, 0.8645814061164856, 0.8627713918685913, 0.8614075779914856, 0.8605424165725708, 0.8601453900337219], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.220993161201477, 0.9761558771133423, 0.9338669180870056, 0.9246537089347839, 0.9116671681404114, 0.9118635058403015, 0.9004731178283691, 0.9007092714309692, 0.8985130786895752, 0.8902369141578674, 0.8929750323295593, 0.8830334544181824, 0.8791205883026123, 0.8756409883499146, 0.8772943019866943, 0.8709728717803955, 0.8688684105873108, 0.8744389414787292, 0.8652226328849792, 0.8646872043609619, 0.8645814061164856, 0.8627713918685913, 0.8614075779914856, 0.8605424165725708, 0.8601453900337219]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.257813572883606
current iteration best possible eval_loss (full train run):  -0.8601453900337219
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5470 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.7225691092140665
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.8345470774207036
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.8282155409001604
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.8345112881860797
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.834479485594018
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3091, dtype=torch.float64), 0, 0, 0, tensor(0.1591, dtype=torch.float64), tensor(0.2309, dtype=torch.float64), 0, 0, tensor(0.3009, dtype=torch.float64), 32, 1, 0, 0, 1, 1, 128, 2.3592239273284567e-17, 22.53707608439276, 0]
normalized proposed parameters for next round by BO: [tensor(0.3091, dtype=torch.float64), tensor(9.7442e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1125e-16, dtype=torch.float64), tensor(0.1591, dtype=torch.float64), tensor(0.2309, dtype=torch.float64), tensor(2.4551e-16, dtype=torch.float64), tensor(9.0703e-17, dtype=torch.float64), tensor(0.3009, dtype=torch.float64), tensor(1.0000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(2.3592e-16, dtype=torch.float64), tensor(0.4695, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.309
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.159
  truthfulqa_gen: 0.231
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.301

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (2.3592239273284567e-17,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (22.53707608439276,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  128
lora dropout:  2.3592239273284567e-17
lora alpha:  22.53707608439276
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3526, 'grad_norm': 0.7341218590736389, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4932502508163452, 'eval_runtime': 5.24, 'eval_samples_per_second': 190.838, 'eval_steps_per_second': 12.023, 'epoch': 0.04}
{'loss': 1.1323, 'grad_norm': 0.5641270279884338, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0082931518554688, 'eval_runtime': 5.2183, 'eval_samples_per_second': 191.634, 'eval_steps_per_second': 12.073, 'epoch': 0.08}
{'loss': 0.9298, 'grad_norm': 0.2763427793979645, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9301764965057373, 'eval_runtime': 5.2278, 'eval_samples_per_second': 191.287, 'eval_steps_per_second': 12.051, 'epoch': 0.12}
{'loss': 0.8787, 'grad_norm': 0.17413777112960815, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.91115403175354, 'eval_runtime': 5.2331, 'eval_samples_per_second': 191.09, 'eval_steps_per_second': 12.039, 'epoch': 0.16}
{'loss': 0.8711, 'grad_norm': 0.1604926884174347, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9032442569732666, 'eval_runtime': 5.2342, 'eval_samples_per_second': 191.052, 'eval_steps_per_second': 12.036, 'epoch': 0.2}
{'loss': 0.8615, 'grad_norm': 0.16815142333507538, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9096212387084961, 'eval_runtime': 5.2453, 'eval_samples_per_second': 190.647, 'eval_steps_per_second': 12.011, 'epoch': 0.24}
{'loss': 0.8382, 'grad_norm': 0.1482759565114975, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8957735300064087, 'eval_runtime': 5.2421, 'eval_samples_per_second': 190.764, 'eval_steps_per_second': 12.018, 'epoch': 0.28}
{'loss': 0.833, 'grad_norm': 0.1569524109363556, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8930433392524719, 'eval_runtime': 5.245, 'eval_samples_per_second': 190.659, 'eval_steps_per_second': 12.012, 'epoch': 0.32}
{'loss': 0.8361, 'grad_norm': 0.16795870661735535, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.888213038444519, 'eval_runtime': 5.2733, 'eval_samples_per_second': 189.634, 'eval_steps_per_second': 11.947, 'epoch': 0.36}
{'loss': 0.8048, 'grad_norm': 0.4178089201450348, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8795490860939026, 'eval_runtime': 5.2474, 'eval_samples_per_second': 190.569, 'eval_steps_per_second': 12.006, 'epoch': 0.4}
{'loss': 0.7689, 'grad_norm': 0.18151290714740753, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8810728788375854, 'eval_runtime': 5.248, 'eval_samples_per_second': 190.547, 'eval_steps_per_second': 12.004, 'epoch': 0.44}
{'loss': 0.7692, 'grad_norm': 0.20826466381549835, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8778267502784729, 'eval_runtime': 5.2631, 'eval_samples_per_second': 190.001, 'eval_steps_per_second': 11.97, 'epoch': 0.48}
{'loss': 0.7545, 'grad_norm': 0.22692745923995972, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8741530776023865, 'eval_runtime': 5.2369, 'eval_samples_per_second': 190.954, 'eval_steps_per_second': 12.03, 'epoch': 0.52}
{'loss': 0.7393, 'grad_norm': 0.22922968864440918, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8748784065246582, 'eval_runtime': 5.2439, 'eval_samples_per_second': 190.698, 'eval_steps_per_second': 12.014, 'epoch': 0.56}
{'loss': 0.7535, 'grad_norm': 0.24206896126270294, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8710359930992126, 'eval_runtime': 5.2593, 'eval_samples_per_second': 190.138, 'eval_steps_per_second': 11.979, 'epoch': 0.6}
{'loss': 0.7129, 'grad_norm': 0.23771589994430542, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8677206039428711, 'eval_runtime': 5.2591, 'eval_samples_per_second': 190.147, 'eval_steps_per_second': 11.979, 'epoch': 0.64}
{'loss': 0.7147, 'grad_norm': 0.2605708837509155, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8708354830741882, 'eval_runtime': 5.2525, 'eval_samples_per_second': 190.385, 'eval_steps_per_second': 11.994, 'epoch': 0.68}
{'loss': 0.7209, 'grad_norm': 0.24051183462142944, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8669393062591553, 'eval_runtime': 5.2436, 'eval_samples_per_second': 190.71, 'eval_steps_per_second': 12.015, 'epoch': 0.72}
{'loss': 0.686, 'grad_norm': 0.2552134394645691, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8681366443634033, 'eval_runtime': 5.2484, 'eval_samples_per_second': 190.535, 'eval_steps_per_second': 12.004, 'epoch': 0.76}
{'loss': 0.7229, 'grad_norm': 0.2343176156282425, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8617976307868958, 'eval_runtime': 5.2231, 'eval_samples_per_second': 191.457, 'eval_steps_per_second': 12.062, 'epoch': 0.8}
{'loss': 0.6745, 'grad_norm': 0.232152059674263, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8622444868087769, 'eval_runtime': 5.2044, 'eval_samples_per_second': 192.145, 'eval_steps_per_second': 12.105, 'epoch': 0.84}
{'loss': 0.657, 'grad_norm': 0.2689785063266754, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8601513504981995, 'eval_runtime': 5.2038, 'eval_samples_per_second': 192.168, 'eval_steps_per_second': 12.107, 'epoch': 0.88}
{'loss': 0.6489, 'grad_norm': 0.2619325518608093, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8568810224533081, 'eval_runtime': 5.2071, 'eval_samples_per_second': 192.045, 'eval_steps_per_second': 12.099, 'epoch': 0.92}
{'loss': 0.6409, 'grad_norm': 0.28790631890296936, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8572431802749634, 'eval_runtime': 5.2075, 'eval_samples_per_second': 192.029, 'eval_steps_per_second': 12.098, 'epoch': 0.96}
{'loss': 0.657, 'grad_norm': 0.27101653814315796, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8568841814994812, 'eval_runtime': 5.198, 'eval_samples_per_second': 192.383, 'eval_steps_per_second': 12.12, 'epoch': 1.0}
{'train_runtime': 292.9055, 'train_samples_per_second': 34.134, 'train_steps_per_second': 2.134, 'train_loss': 0.878365756225586, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4932502508163452, 1.0082931518554688, 0.9301764965057373, 0.91115403175354, 0.9032442569732666, 0.9096212387084961, 0.8957735300064087, 0.8930433392524719, 0.888213038444519, 0.8795490860939026, 0.8810728788375854, 0.8778267502784729, 0.8741530776023865, 0.8748784065246582, 0.8710359930992126, 0.8677206039428711, 0.8708354830741882, 0.8669393062591553, 0.8681366443634033, 0.8617976307868958, 0.8622444868087769, 0.8601513504981995, 0.8568810224533081, 0.8572431802749634, 0.8568841814994812], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4932502508163452, 1.0082931518554688, 0.9301764965057373, 0.91115403175354, 0.9032442569732666, 0.9096212387084961, 0.8957735300064087, 0.8930433392524719, 0.888213038444519, 0.8795490860939026, 0.8810728788375854, 0.8778267502784729, 0.8741530776023865, 0.8748784065246582, 0.8710359930992126, 0.8677206039428711, 0.8708354830741882, 0.8669393062591553, 0.8681366443634033, 0.8617976307868958, 0.8622444868087769, 0.8601513504981995, 0.8568810224533081, 0.8572431802749634, 0.8568841814994812]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1117538213729858
current iteration best possible eval_loss (full train run):  -0.8568841814994812
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.5228 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.8367879604528022
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.833147665242986
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.8326589570636322
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.8327167197430523
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.8326874231956016
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3276, dtype=torch.float64), tensor(0.0926, dtype=torch.float64), 0, 0, 0, tensor(0.0116, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0.4578, dtype=torch.float64), 31, 0, 1, 1, 1, 0, 80, 0.054537139688529634, 27.358594500814252, 0]
normalized proposed parameters for next round by BO: [tensor(0.3276, dtype=torch.float64), tensor(0.0926, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2492e-17, dtype=torch.float64), tensor(0.0116, dtype=torch.float64), tensor(0.0530, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0.4578, dtype=torch.float64), tensor(0.9667, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6225, dtype=torch.float64), tensor(0.5454, dtype=torch.float64), tensor(0.5700, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.328
  gsm8k: 0.093
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.012
  wikitext: 0.053
  mmlu: 0.057
  arc_challenge: 0.458

LoRA Parameters:
  lora_r: (80,)
  lora_dropout: (0.054537139688529634,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (27.358594500814252,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  80
lora dropout:  0.054537139688529634
lora alpha:  27.358594500814252
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 104,120,320 || all params: 8,134,381,568 || trainable%: 1.2800
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7769, 'grad_norm': 0.6977941989898682, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2872817516326904, 'eval_runtime': 5.1029, 'eval_samples_per_second': 195.967, 'eval_steps_per_second': 12.346, 'epoch': 0.04}
{'loss': 1.1349, 'grad_norm': 0.5331122875213623, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9871676564216614, 'eval_runtime': 5.1232, 'eval_samples_per_second': 195.191, 'eval_steps_per_second': 12.297, 'epoch': 0.08}
{'loss': 1.0045, 'grad_norm': 0.4405313730239868, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9271242022514343, 'eval_runtime': 5.1107, 'eval_samples_per_second': 195.668, 'eval_steps_per_second': 12.327, 'epoch': 0.12}
{'loss': 0.946, 'grad_norm': 0.20313145220279694, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9179101586341858, 'eval_runtime': 5.1177, 'eval_samples_per_second': 195.401, 'eval_steps_per_second': 12.31, 'epoch': 0.16}
{'loss': 0.9248, 'grad_norm': 0.22596058249473572, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9023823142051697, 'eval_runtime': 5.1191, 'eval_samples_per_second': 195.347, 'eval_steps_per_second': 12.307, 'epoch': 0.2}
{'loss': 0.952, 'grad_norm': 0.20483256876468658, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8988686800003052, 'eval_runtime': 5.1367, 'eval_samples_per_second': 194.676, 'eval_steps_per_second': 12.265, 'epoch': 0.24}
{'loss': 0.9221, 'grad_norm': 0.20732331275939941, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8904614448547363, 'eval_runtime': 5.1293, 'eval_samples_per_second': 194.958, 'eval_steps_per_second': 12.282, 'epoch': 0.28}
{'loss': 0.8786, 'grad_norm': 0.2626943588256836, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8857259750366211, 'eval_runtime': 5.1217, 'eval_samples_per_second': 195.248, 'eval_steps_per_second': 12.301, 'epoch': 0.32}
{'loss': 0.8425, 'grad_norm': 0.2622245252132416, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8796758055686951, 'eval_runtime': 5.1281, 'eval_samples_per_second': 195.006, 'eval_steps_per_second': 12.285, 'epoch': 0.36}
{'loss': 0.8932, 'grad_norm': 0.24672897160053253, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8781141638755798, 'eval_runtime': 5.133, 'eval_samples_per_second': 194.819, 'eval_steps_per_second': 12.274, 'epoch': 0.4}
{'loss': 0.8454, 'grad_norm': 0.3658902943134308, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8749178647994995, 'eval_runtime': 5.1313, 'eval_samples_per_second': 194.883, 'eval_steps_per_second': 12.278, 'epoch': 0.44}
{'loss': 0.7925, 'grad_norm': 0.25124287605285645, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8751993775367737, 'eval_runtime': 5.1223, 'eval_samples_per_second': 195.224, 'eval_steps_per_second': 12.299, 'epoch': 0.48}
{'loss': 0.7646, 'grad_norm': 0.2827294170856476, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8667826652526855, 'eval_runtime': 5.1221, 'eval_samples_per_second': 195.231, 'eval_steps_per_second': 12.3, 'epoch': 0.52}
{'loss': 0.8067, 'grad_norm': 0.3508436679840088, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8632313013076782, 'eval_runtime': 5.1097, 'eval_samples_per_second': 195.706, 'eval_steps_per_second': 12.329, 'epoch': 0.56}
{'loss': 0.7763, 'grad_norm': 0.4032866656780243, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8618910312652588, 'eval_runtime': 5.1169, 'eval_samples_per_second': 195.431, 'eval_steps_per_second': 12.312, 'epoch': 0.6}
{'loss': 0.7362, 'grad_norm': 0.28215134143829346, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8604101538658142, 'eval_runtime': 5.1175, 'eval_samples_per_second': 195.407, 'eval_steps_per_second': 12.311, 'epoch': 0.64}
{'loss': 0.7158, 'grad_norm': 0.32457807660102844, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8619868159294128, 'eval_runtime': 5.1281, 'eval_samples_per_second': 195.006, 'eval_steps_per_second': 12.285, 'epoch': 0.68}
{'loss': 0.7537, 'grad_norm': 0.34755152463912964, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8555882573127747, 'eval_runtime': 5.133, 'eval_samples_per_second': 194.816, 'eval_steps_per_second': 12.273, 'epoch': 0.72}
{'loss': 0.7226, 'grad_norm': 0.3395750820636749, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8571521043777466, 'eval_runtime': 5.1187, 'eval_samples_per_second': 195.361, 'eval_steps_per_second': 12.308, 'epoch': 0.76}
{'loss': 0.7141, 'grad_norm': 0.40347784757614136, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8541929721832275, 'eval_runtime': 5.1176, 'eval_samples_per_second': 195.405, 'eval_steps_per_second': 12.311, 'epoch': 0.8}
{'loss': 0.67, 'grad_norm': 0.2613121271133423, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8513694405555725, 'eval_runtime': 5.1362, 'eval_samples_per_second': 194.698, 'eval_steps_per_second': 12.266, 'epoch': 0.84}
{'loss': 0.695, 'grad_norm': 0.37783291935920715, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8523330092430115, 'eval_runtime': 5.1504, 'eval_samples_per_second': 194.161, 'eval_steps_per_second': 12.232, 'epoch': 0.88}
{'loss': 0.6431, 'grad_norm': 0.43119415640830994, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8507068157196045, 'eval_runtime': 5.1493, 'eval_samples_per_second': 194.203, 'eval_steps_per_second': 12.235, 'epoch': 0.92}
{'loss': 0.6205, 'grad_norm': 0.3348754942417145, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8497369289398193, 'eval_runtime': 5.1658, 'eval_samples_per_second': 193.582, 'eval_steps_per_second': 12.196, 'epoch': 0.96}
{'loss': 0.6619, 'grad_norm': 0.3648964464664459, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.849241316318512, 'eval_runtime': 5.187, 'eval_samples_per_second': 192.791, 'eval_steps_per_second': 12.146, 'epoch': 1.0}
{'train_runtime': 332.6127, 'train_samples_per_second': 30.053, 'train_steps_per_second': 1.879, 'train_loss': 0.8877588973999023, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2872817516326904, 0.9871676564216614, 0.9271242022514343, 0.9179101586341858, 0.9023823142051697, 0.8988686800003052, 0.8904614448547363, 0.8857259750366211, 0.8796758055686951, 0.8781141638755798, 0.8749178647994995, 0.8751993775367737, 0.8667826652526855, 0.8632313013076782, 0.8618910312652588, 0.8604101538658142, 0.8619868159294128, 0.8555882573127747, 0.8571521043777466, 0.8541929721832275, 0.8513694405555725, 0.8523330092430115, 0.8507068157196045, 0.8497369289398193, 0.849241316318512], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2872817516326904, 0.9871676564216614, 0.9271242022514343, 0.9179101586341858, 0.9023823142051697, 0.8988686800003052, 0.8904614448547363, 0.8857259750366211, 0.8796758055686951, 0.8781141638755798, 0.8749178647994995, 0.8751993775367737, 0.8667826652526855, 0.8632313013076782, 0.8618910312652588, 0.8604101538658142, 0.8619868159294128, 0.8555882573127747, 0.8571521043777466, 0.8541929721832275, 0.8513694405555725, 0.8523330092430115, 0.8507068157196045, 0.8497369289398193, 0.849241316318512]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0477313995361328
current iteration best possible eval_loss (full train run):  -0.849241316318512
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.8641 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.828517406745872
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.8282597972234018
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.8282597972234018
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.8283099275795894
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.8282597972234018
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3811, dtype=torch.float64), 0, tensor(0.0613, dtype=torch.float64), 0, tensor(0.0341, dtype=torch.float64), 0, 0, tensor(0.0730, dtype=torch.float64), tensor(0.4504, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 128, 0.0, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.3811, dtype=torch.float64), tensor(2.9862e-18, dtype=torch.float64), tensor(0.0613, dtype=torch.float64), tensor(3.5816e-18, dtype=torch.float64), tensor(0.0341, dtype=torch.float64), tensor(6.6089e-18, dtype=torch.float64), tensor(5.2498e-18, dtype=torch.float64), tensor(0.0730, dtype=torch.float64), tensor(0.4504, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.381
  gsm8k: 0
  rowan_hellaswag: 0.061
  sciq: 0
  triviaqa: 0.034
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.073
  arc_challenge: 0.45

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  0.0
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6027, 'grad_norm': 0.44002699851989746, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1685190200805664, 'eval_runtime': 5.3578, 'eval_samples_per_second': 186.645, 'eval_steps_per_second': 11.759, 'epoch': 0.04}
{'loss': 1.1986, 'grad_norm': 0.7692946791648865, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9506756663322449, 'eval_runtime': 5.3573, 'eval_samples_per_second': 186.66, 'eval_steps_per_second': 11.76, 'epoch': 0.08}
{'loss': 1.0545, 'grad_norm': 0.25465232133865356, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9174971580505371, 'eval_runtime': 5.511, 'eval_samples_per_second': 181.455, 'eval_steps_per_second': 11.432, 'epoch': 0.12}
{'loss': 0.9937, 'grad_norm': 0.2198806256055832, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9011828899383545, 'eval_runtime': 5.7796, 'eval_samples_per_second': 173.022, 'eval_steps_per_second': 10.9, 'epoch': 0.16}
{'loss': 0.9722, 'grad_norm': 0.2580004334449768, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8950806856155396, 'eval_runtime': 7.3656, 'eval_samples_per_second': 135.766, 'eval_steps_per_second': 8.553, 'epoch': 0.2}
{'loss': 0.9969, 'grad_norm': 0.2093151956796646, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8818134665489197, 'eval_runtime': 6.2957, 'eval_samples_per_second': 158.838, 'eval_steps_per_second': 10.007, 'epoch': 0.24}
{'loss': 0.9641, 'grad_norm': 0.2870085835456848, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8774118423461914, 'eval_runtime': 5.74, 'eval_samples_per_second': 174.217, 'eval_steps_per_second': 10.976, 'epoch': 0.28}
{'loss': 0.928, 'grad_norm': 0.2512279152870178, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8770412802696228, 'eval_runtime': 6.1778, 'eval_samples_per_second': 161.87, 'eval_steps_per_second': 10.198, 'epoch': 0.32}
{'loss': 0.989, 'grad_norm': 0.25276824831962585, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8739571571350098, 'eval_runtime': 6.5493, 'eval_samples_per_second': 152.688, 'eval_steps_per_second': 9.619, 'epoch': 0.36}
{'loss': 0.8903, 'grad_norm': 0.25070497393608093, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8676891922950745, 'eval_runtime': 5.944, 'eval_samples_per_second': 168.237, 'eval_steps_per_second': 10.599, 'epoch': 0.4}
{'loss': 0.8675, 'grad_norm': 0.2580084204673767, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8649179935455322, 'eval_runtime': 7.2303, 'eval_samples_per_second': 138.307, 'eval_steps_per_second': 8.713, 'epoch': 0.44}
{'loss': 0.7951, 'grad_norm': 0.42822423577308655, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8625476360321045, 'eval_runtime': 5.573, 'eval_samples_per_second': 179.436, 'eval_steps_per_second': 11.304, 'epoch': 0.48}
{'loss': 0.8157, 'grad_norm': 0.26683688163757324, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8556389212608337, 'eval_runtime': 5.386, 'eval_samples_per_second': 185.666, 'eval_steps_per_second': 11.697, 'epoch': 0.52}
{'loss': 0.7639, 'grad_norm': 0.2694169878959656, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8513693809509277, 'eval_runtime': 5.5095, 'eval_samples_per_second': 181.505, 'eval_steps_per_second': 11.435, 'epoch': 0.56}
{'loss': 0.7901, 'grad_norm': 0.29718348383903503, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8517704606056213, 'eval_runtime': 14.1527, 'eval_samples_per_second': 70.658, 'eval_steps_per_second': 4.451, 'epoch': 0.6}
{'loss': 0.7374, 'grad_norm': 0.31405794620513916, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.847066342830658, 'eval_runtime': 10.821, 'eval_samples_per_second': 92.413, 'eval_steps_per_second': 5.822, 'epoch': 0.64}
{'loss': 0.7353, 'grad_norm': 0.34426409006118774, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8465203046798706, 'eval_runtime': 12.2791, 'eval_samples_per_second': 81.439, 'eval_steps_per_second': 5.131, 'epoch': 0.68}
{'loss': 0.7502, 'grad_norm': 0.4043554961681366, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8439250588417053, 'eval_runtime': 13.3647, 'eval_samples_per_second': 74.824, 'eval_steps_per_second': 4.714, 'epoch': 0.72}
{'loss': 0.708, 'grad_norm': 0.29694032669067383, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8405760526657104, 'eval_runtime': 13.0636, 'eval_samples_per_second': 76.549, 'eval_steps_per_second': 4.823, 'epoch': 0.76}
{'loss': 0.7517, 'grad_norm': 0.2572669982910156, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8396843075752258, 'eval_runtime': 11.0386, 'eval_samples_per_second': 90.591, 'eval_steps_per_second': 5.707, 'epoch': 0.8}
{'loss': 0.7402, 'grad_norm': 0.32718583941459656, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.837867259979248, 'eval_runtime': 12.217, 'eval_samples_per_second': 81.853, 'eval_steps_per_second': 5.157, 'epoch': 0.84}
{'loss': 0.704, 'grad_norm': 0.32994356751441956, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8356813192367554, 'eval_runtime': 13.3746, 'eval_samples_per_second': 74.769, 'eval_steps_per_second': 4.71, 'epoch': 0.88}
{'loss': 0.6752, 'grad_norm': 0.2370079606771469, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8348734974861145, 'eval_runtime': 13.2652, 'eval_samples_per_second': 75.385, 'eval_steps_per_second': 4.749, 'epoch': 0.92}
{'loss': 0.7425, 'grad_norm': 0.27978378534317017, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8347313404083252, 'eval_runtime': 12.7678, 'eval_samples_per_second': 78.322, 'eval_steps_per_second': 4.934, 'epoch': 0.96}
{'loss': 0.6728, 'grad_norm': 0.271490216255188, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8341493010520935, 'eval_runtime': 12.4498, 'eval_samples_per_second': 80.322, 'eval_steps_per_second': 5.06, 'epoch': 1.0}
{'train_runtime': 536.8999, 'train_samples_per_second': 18.624, 'train_steps_per_second': 1.164, 'train_loss': 0.913583251953125, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1685190200805664, 0.9506756663322449, 0.9174971580505371, 0.9011828899383545, 0.8950806856155396, 0.8818134665489197, 0.8774118423461914, 0.8770412802696228, 0.8739571571350098, 0.8676891922950745, 0.8649179935455322, 0.8625476360321045, 0.8556389212608337, 0.8513693809509277, 0.8517704606056213, 0.847066342830658, 0.8465203046798706, 0.8439250588417053, 0.8405760526657104, 0.8396843075752258, 0.837867259979248, 0.8356813192367554, 0.8348734974861145, 0.8347313404083252, 0.8341493010520935], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1685190200805664, 0.9506756663322449, 0.9174971580505371, 0.9011828899383545, 0.8950806856155396, 0.8818134665489197, 0.8774118423461914, 0.8770412802696228, 0.8739571571350098, 0.8676891922950745, 0.8649179935455322, 0.8625476360321045, 0.8556389212608337, 0.8513693809509277, 0.8517704606056213, 0.847066342830658, 0.8465203046798706, 0.8439250588417053, 0.8405760526657104, 0.8396843075752258, 0.837867259979248, 0.8356813192367554, 0.8348734974861145, 0.8347313404083252, 0.8341493010520935]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2225608825683594
current iteration best possible eval_loss (full train run):  -0.8341493010520935
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.9244 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.8329882342551205
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.8329189110146108
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.8562280470284067
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.8330693563261589
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.8331994763637445
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4716, dtype=torch.float64), tensor(0.1555, dtype=torch.float64), 0, tensor(0.0183, dtype=torch.float64), tensor(0.1104, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.0107, dtype=torch.float64), 0, tensor(0.1615, dtype=torch.float64), 20, 0, 1, 1, 1, 0, 105, 0.0375205533911588, 22.8886409676621, 0]
normalized proposed parameters for next round by BO: [tensor(0.4716, dtype=torch.float64), tensor(0.1555, dtype=torch.float64), tensor(1.5287e-17, dtype=torch.float64), tensor(0.0183, dtype=torch.float64), tensor(0.1104, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.0107, dtype=torch.float64), tensor(4.1265e-18, dtype=torch.float64), tensor(0.1615, dtype=torch.float64), tensor(0.6239, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8165, dtype=torch.float64), tensor(0.3752, dtype=torch.float64), tensor(0.4768, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.472
  gsm8k: 0.155
  rowan_hellaswag: 0
  sciq: 0.018
  triviaqa: 0.11
  truthfulqa_gen: 0.072
  wikitext: 0.011
  mmlu: 0
  arc_challenge: 0.161

LoRA Parameters:
  lora_r: (105,)
  lora_dropout: (0.0375205533911588,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (22.8886409676621,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  105
lora dropout:  0.0375205533911588
lora alpha:  22.8886409676621
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 88,166,400 || all params: 8,118,427,648 || trainable%: 1.0860
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0706, 'grad_norm': 1.0792521238327026, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6867079734802246, 'eval_runtime': 5.0753, 'eval_samples_per_second': 197.032, 'eval_steps_per_second': 12.413, 'epoch': 0.04}
{'loss': 1.2149, 'grad_norm': 0.3703450560569763, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9965294599533081, 'eval_runtime': 5.1009, 'eval_samples_per_second': 196.045, 'eval_steps_per_second': 12.351, 'epoch': 0.08}
{'loss': 0.9923, 'grad_norm': 0.1792839914560318, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9467652440071106, 'eval_runtime': 5.0873, 'eval_samples_per_second': 196.569, 'eval_steps_per_second': 12.384, 'epoch': 0.12}
{'loss': 0.9599, 'grad_norm': 0.16189435124397278, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9304673075675964, 'eval_runtime': 5.0824, 'eval_samples_per_second': 196.759, 'eval_steps_per_second': 12.396, 'epoch': 0.16}
{'loss': 0.9288, 'grad_norm': 0.15061184763908386, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9111939072608948, 'eval_runtime': 5.0954, 'eval_samples_per_second': 196.256, 'eval_steps_per_second': 12.364, 'epoch': 0.2}
{'loss': 0.9236, 'grad_norm': 0.16648489236831665, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9055507183074951, 'eval_runtime': 5.0947, 'eval_samples_per_second': 196.282, 'eval_steps_per_second': 12.366, 'epoch': 0.24}
{'loss': 0.9222, 'grad_norm': 0.1810561865568161, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.897889256477356, 'eval_runtime': 5.1022, 'eval_samples_per_second': 195.996, 'eval_steps_per_second': 12.348, 'epoch': 0.28}
{'loss': 0.8949, 'grad_norm': 0.15902723371982574, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8923164010047913, 'eval_runtime': 5.1132, 'eval_samples_per_second': 195.573, 'eval_steps_per_second': 12.321, 'epoch': 0.32}
{'loss': 0.9241, 'grad_norm': 0.16115708649158478, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8916390538215637, 'eval_runtime': 5.1207, 'eval_samples_per_second': 195.287, 'eval_steps_per_second': 12.303, 'epoch': 0.36}
{'loss': 0.8966, 'grad_norm': 0.1643364429473877, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8872114419937134, 'eval_runtime': 5.1182, 'eval_samples_per_second': 195.381, 'eval_steps_per_second': 12.309, 'epoch': 0.4}
{'loss': 0.883, 'grad_norm': 0.14616823196411133, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8824290037155151, 'eval_runtime': 5.1081, 'eval_samples_per_second': 195.767, 'eval_steps_per_second': 12.333, 'epoch': 0.44}
{'loss': 0.8864, 'grad_norm': 0.15864521265029907, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8805570006370544, 'eval_runtime': 5.1287, 'eval_samples_per_second': 194.983, 'eval_steps_per_second': 12.284, 'epoch': 0.48}
{'loss': 0.9014, 'grad_norm': 0.17594972252845764, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8748260736465454, 'eval_runtime': 5.1271, 'eval_samples_per_second': 195.041, 'eval_steps_per_second': 12.288, 'epoch': 0.52}
{'loss': 0.8603, 'grad_norm': 0.14979568123817444, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8744077086448669, 'eval_runtime': 5.13, 'eval_samples_per_second': 194.932, 'eval_steps_per_second': 12.281, 'epoch': 0.56}
{'loss': 0.879, 'grad_norm': 0.16245287656784058, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8696261644363403, 'eval_runtime': 5.1247, 'eval_samples_per_second': 195.134, 'eval_steps_per_second': 12.293, 'epoch': 0.6}
{'loss': 0.8754, 'grad_norm': 0.1550447940826416, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8684613108634949, 'eval_runtime': 5.1233, 'eval_samples_per_second': 195.187, 'eval_steps_per_second': 12.297, 'epoch': 0.64}
{'loss': 0.864, 'grad_norm': 0.18412891030311584, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8661064505577087, 'eval_runtime': 5.1362, 'eval_samples_per_second': 194.697, 'eval_steps_per_second': 12.266, 'epoch': 0.68}
{'loss': 0.8503, 'grad_norm': 0.17344708740711212, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8645542860031128, 'eval_runtime': 5.5356, 'eval_samples_per_second': 180.648, 'eval_steps_per_second': 11.381, 'epoch': 0.72}
{'loss': 0.8781, 'grad_norm': 0.20406287908554077, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8627722859382629, 'eval_runtime': 5.6066, 'eval_samples_per_second': 178.363, 'eval_steps_per_second': 11.237, 'epoch': 0.76}
{'loss': 0.8704, 'grad_norm': 0.15645481646060944, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.861018180847168, 'eval_runtime': 5.5478, 'eval_samples_per_second': 180.253, 'eval_steps_per_second': 11.356, 'epoch': 0.8}
{'loss': 0.8504, 'grad_norm': 0.17806999385356903, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8590282201766968, 'eval_runtime': 5.4801, 'eval_samples_per_second': 182.479, 'eval_steps_per_second': 11.496, 'epoch': 0.84}
{'loss': 0.8713, 'grad_norm': 0.188445582985878, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8567624092102051, 'eval_runtime': 5.541, 'eval_samples_per_second': 180.472, 'eval_steps_per_second': 11.37, 'epoch': 0.88}
{'loss': 0.8481, 'grad_norm': 0.20602263510227203, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8567065596580505, 'eval_runtime': 5.6395, 'eval_samples_per_second': 177.321, 'eval_steps_per_second': 11.171, 'epoch': 0.92}
{'loss': 0.8165, 'grad_norm': 0.19020698964595795, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8556360006332397, 'eval_runtime': 5.577, 'eval_samples_per_second': 179.307, 'eval_steps_per_second': 11.296, 'epoch': 0.96}
{'loss': 0.8535, 'grad_norm': 0.21800142526626587, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.854883074760437, 'eval_runtime': 5.5212, 'eval_samples_per_second': 181.121, 'eval_steps_per_second': 11.411, 'epoch': 1.0}
{'train_runtime': 307.048, 'train_samples_per_second': 32.555, 'train_steps_per_second': 2.036, 'train_loss': 0.9886436737060547, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6867079734802246, 0.9965294599533081, 0.9467652440071106, 0.9304673075675964, 0.9111939072608948, 0.9055507183074951, 0.897889256477356, 0.8923164010047913, 0.8916390538215637, 0.8872114419937134, 0.8824290037155151, 0.8805570006370544, 0.8748260736465454, 0.8744077086448669, 0.8696261644363403, 0.8684613108634949, 0.8661064505577087, 0.8645542860031128, 0.8627722859382629, 0.861018180847168, 0.8590282201766968, 0.8567624092102051, 0.8567065596580505, 0.8556360006332397, 0.854883074760437], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6867079734802246, 0.9965294599533081, 0.9467652440071106, 0.9304673075675964, 0.9111939072608948, 0.9055507183074951, 0.897889256477356, 0.8923164010047913, 0.8916390538215637, 0.8872114419937134, 0.8824290037155151, 0.8805570006370544, 0.8748260736465454, 0.8744077086448669, 0.8696261644363403, 0.8684613108634949, 0.8661064505577087, 0.8645542860031128, 0.8627722859382629, 0.861018180847168, 0.8590282201766968, 0.8567624092102051, 0.8567065596580505, 0.8556360006332397, 0.854883074760437]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1300960779190063
current iteration best possible eval_loss (full train run):  -0.854883074760437
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.8178 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.9179982399183345
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.8958128608298511
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.8954021643270051
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.8958286561179116
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.8984029350071997
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2406, dtype=torch.float64), tensor(0.1122, dtype=torch.float64), 0, 0, tensor(0.1401, dtype=torch.float64), tensor(0.1709, dtype=torch.float64), tensor(0.0128, dtype=torch.float64), 0, tensor(0.3158, dtype=torch.float64), 29, 0, 1, 1, 0, 0, 48, 0.0, 30.87623086263716, 0]
normalized proposed parameters for next round by BO: [tensor(0.2406, dtype=torch.float64), tensor(0.1122, dtype=torch.float64), tensor(0.0036, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1401, dtype=torch.float64), tensor(0.1709, dtype=torch.float64), tensor(0.0128, dtype=torch.float64), tensor(0.0040, dtype=torch.float64), tensor(0.3158, dtype=torch.float64), tensor(0.9160, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3787, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6433, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.241
  gsm8k: 0.112
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.14
  truthfulqa_gen: 0.171
  wikitext: 0.013
  mmlu: 0
  arc_challenge: 0.316

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (30.87623086263716,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  48
lora dropout:  0.0
lora alpha:  30.87623086263716
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 32,784,384 || all params: 8,063,045,632 || trainable%: 0.4066
length of training data:  9921
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9618, 'grad_norm': 0.9508705139160156, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5685014724731445, 'eval_runtime': 4.7174, 'eval_samples_per_second': 211.982, 'eval_steps_per_second': 13.355, 'epoch': 0.04}
{'loss': 1.2802, 'grad_norm': 0.49808934330940247, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1585476398468018, 'eval_runtime': 4.7473, 'eval_samples_per_second': 210.645, 'eval_steps_per_second': 13.271, 'epoch': 0.08}
{'loss': 1.1106, 'grad_norm': 0.3913072943687439, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 1.056991696357727, 'eval_runtime': 4.7145, 'eval_samples_per_second': 212.11, 'eval_steps_per_second': 13.363, 'epoch': 0.12}
{'loss': 1.0034, 'grad_norm': 0.33307701349258423, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 1.0166367292404175, 'eval_runtime': 4.711, 'eval_samples_per_second': 212.268, 'eval_steps_per_second': 13.373, 'epoch': 0.16}
{'loss': 0.9636, 'grad_norm': 0.29453083872795105, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.9776382446289062, 'eval_runtime': 4.7174, 'eval_samples_per_second': 211.979, 'eval_steps_per_second': 13.355, 'epoch': 0.2}
{'loss': 0.9125, 'grad_norm': 0.29248037934303284, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.9190302491188049, 'eval_runtime': 4.7284, 'eval_samples_per_second': 211.488, 'eval_steps_per_second': 13.324, 'epoch': 0.24}
{'loss': 0.8945, 'grad_norm': 0.31764504313468933, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.9121757745742798, 'eval_runtime': 4.7275, 'eval_samples_per_second': 211.53, 'eval_steps_per_second': 13.326, 'epoch': 0.28}
{'loss': 0.8511, 'grad_norm': 0.3732106387615204, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.8995769619941711, 'eval_runtime': 4.7703, 'eval_samples_per_second': 209.63, 'eval_steps_per_second': 13.207, 'epoch': 0.32}
{'loss': 0.8641, 'grad_norm': 0.2970786988735199, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.9005732536315918, 'eval_runtime': 4.7482, 'eval_samples_per_second': 210.604, 'eval_steps_per_second': 13.268, 'epoch': 0.36}
{'loss': 0.8443, 'grad_norm': 0.3128563463687897, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.8986390829086304, 'eval_runtime': 4.7535, 'eval_samples_per_second': 210.372, 'eval_steps_per_second': 13.253, 'epoch': 0.4}
{'loss': 0.8381, 'grad_norm': 0.31270307302474976, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.8931069374084473, 'eval_runtime': 4.7545, 'eval_samples_per_second': 210.325, 'eval_steps_per_second': 13.25, 'epoch': 0.44}
{'loss': 0.7994, 'grad_norm': 0.29842323064804077, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.8920547962188721, 'eval_runtime': 4.7377, 'eval_samples_per_second': 211.071, 'eval_steps_per_second': 13.297, 'epoch': 0.48}
{'loss': 0.8332, 'grad_norm': 0.3903596103191376, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.8865739703178406, 'eval_runtime': 4.733, 'eval_samples_per_second': 211.284, 'eval_steps_per_second': 13.311, 'epoch': 0.52}
{'loss': 0.7774, 'grad_norm': 0.3333366811275482, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.889330267906189, 'eval_runtime': 4.7324, 'eval_samples_per_second': 211.31, 'eval_steps_per_second': 13.313, 'epoch': 0.56}
{'loss': 0.8049, 'grad_norm': 0.3643062114715576, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.8848925828933716, 'eval_runtime': 4.7373, 'eval_samples_per_second': 211.09, 'eval_steps_per_second': 13.299, 'epoch': 0.6}
{'loss': 0.7756, 'grad_norm': 0.3488231599330902, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.8804894089698792, 'eval_runtime': 4.7324, 'eval_samples_per_second': 211.309, 'eval_steps_per_second': 13.312, 'epoch': 0.64}
{'loss': 0.7896, 'grad_norm': 0.4273090660572052, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.8763946294784546, 'eval_runtime': 4.7391, 'eval_samples_per_second': 211.01, 'eval_steps_per_second': 13.294, 'epoch': 0.68}
{'loss': 0.7672, 'grad_norm': 0.38709914684295654, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.8771823048591614, 'eval_runtime': 4.7347, 'eval_samples_per_second': 211.207, 'eval_steps_per_second': 13.306, 'epoch': 0.72}
{'loss': 0.7346, 'grad_norm': 0.3504101037979126, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.876652717590332, 'eval_runtime': 4.7348, 'eval_samples_per_second': 211.202, 'eval_steps_per_second': 13.306, 'epoch': 0.76}
{'loss': 0.7377, 'grad_norm': 0.44098591804504395, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.8735035061836243, 'eval_runtime': 4.7331, 'eval_samples_per_second': 211.276, 'eval_steps_per_second': 13.31, 'epoch': 0.81}
{'loss': 0.783, 'grad_norm': 0.38930845260620117, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.8707129955291748, 'eval_runtime': 4.7401, 'eval_samples_per_second': 210.967, 'eval_steps_per_second': 13.291, 'epoch': 0.85}
{'loss': 0.7231, 'grad_norm': 0.48140427470207214, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.8702326416969299, 'eval_runtime': 4.743, 'eval_samples_per_second': 210.838, 'eval_steps_per_second': 13.283, 'epoch': 0.89}
{'loss': 0.7361, 'grad_norm': 0.41923531889915466, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.8683362007141113, 'eval_runtime': 4.7277, 'eval_samples_per_second': 211.52, 'eval_steps_per_second': 13.326, 'epoch': 0.93}
{'loss': 0.7327, 'grad_norm': 0.4174238443374634, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.868259847164154, 'eval_runtime': 4.7375, 'eval_samples_per_second': 211.08, 'eval_steps_per_second': 13.298, 'epoch': 0.97}
{'train_runtime': 280.1887, 'train_samples_per_second': 35.408, 'train_steps_per_second': 2.216, 'train_loss': 0.9318305505454636, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5685014724731445, 1.1585476398468018, 1.056991696357727, 1.0166367292404175, 0.9776382446289062, 0.9190302491188049, 0.9121757745742798, 0.8995769619941711, 0.9005732536315918, 0.8986390829086304, 0.8931069374084473, 0.8920547962188721, 0.8865739703178406, 0.889330267906189, 0.8848925828933716, 0.8804894089698792, 0.8763946294784546, 0.8771823048591614, 0.876652717590332, 0.8735035061836243, 0.8707129955291748, 0.8702326416969299, 0.8683362007141113, 0.868259847164154], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.5685014724731445, 1.1585476398468018, 1.056991696357727, 1.0166367292404175, 0.9776382446289062, 0.9190302491188049, 0.9121757745742798, 0.8995769619941711, 0.9005732536315918, 0.8986390829086304, 0.8931069374084473, 0.8920547962188721, 0.8865739703178406, 0.889330267906189, 0.8848925828933716, 0.8804894089698792, 0.8763946294784546, 0.8771823048591614, 0.876652717590332, 0.8735035061836243, 0.8707129955291748, 0.8702326416969299, 0.8683362007141113, 0.868259847164154]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0372769832611084
current iteration best possible eval_loss (full train run):  -0.868259847164154
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 7.4069 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1560913324356079, 0.29771536588668823, 0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.4007338583469391, 0.8963236808776855, 0.5591384768486023, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.42323532700538635, 0.003319978713989258, 0.666995644569397, 0.6627236008644104]  ‚Üí  acq = -0.874015109450878
X = [0.6140679717063904, 0.7597888708114624, 0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.5511543154716492, 0.10114860534667969, 0.46051692962646484, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6117270588874817, 0.06090492010116577, 0.29019129276275635, 0.544792890548706]  ‚Üí  acq = -0.8742207371735055
X = [0.4161165952682495, 0.6534545421600342, 0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9124553799629211, 0.5554662346839905, 0.09747803211212158, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9407435655593872, 0.9420399069786072, 0.717787504196167, 0.5669505000114441]  ‚Üí  acq = -0.8740004008697556
X = [0.11413401365280151, 0.7658460736274719, 0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.20504699647426605, 0.40315020084381104, 0.9066953659057617, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7133153080940247, 0.3376033306121826, 0.9497083425521851, 0.04203289747238159]  ‚Üí  acq = -0.8740597673154116
X = [0.7649766802787781, 0.17325276136398315, 0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7676264643669128, 0.04699522256851196, 0.9299392104148865, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.02683671936392784, 0.7503892779350281, 0.20525000989437103, 0.7006362080574036]  ‚Üí  acq = -0.8740015723366175
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1435, dtype=torch.float64), tensor(0.1414, dtype=torch.float64), tensor(0.0851, dtype=torch.float64), 0, tensor(0.1095, dtype=torch.float64), tensor(0.1010, dtype=torch.float64), 0, 0, tensor(0.4195, dtype=torch.float64), 29, 0, 1, 1, 1, 0, 46, 0.062078816198449016, 28.184796008208647, 0]
normalized proposed parameters for next round by BO: [tensor(0.1435, dtype=torch.float64), tensor(0.1414, dtype=torch.float64), tensor(0.0851, dtype=torch.float64), tensor(7.9443e-18, dtype=torch.float64), tensor(0.1095, dtype=torch.float64), tensor(0.1010, dtype=torch.float64), tensor(7.8028e-18, dtype=torch.float64), tensor(7.3244e-17, dtype=torch.float64), tensor(0.4195, dtype=torch.float64), tensor(0.8996, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3606, dtype=torch.float64), tensor(0.6208, dtype=torch.float64), tensor(0.5872, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.144
  gsm8k: 0.141
  rowan_hellaswag: 0.085
  sciq: 0
  triviaqa: 0.11
  truthfulqa_gen: 0.101
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.42

LoRA Parameters:
  lora_r: (46,)
  lora_dropout: (0.062078816198449016,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (28.184796008208647,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  46
lora dropout:  0.062078816198449016
lora alpha:  28.184796008208647
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 56,006,656 || all params: 8,086,267,904 || trainable%: 0.6926
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8885, 'grad_norm': 0.996090829372406, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4508613348007202, 'eval_runtime': 5.1121, 'eval_samples_per_second': 195.613, 'eval_steps_per_second': 12.324, 'epoch': 0.04}
{'loss': 1.2274, 'grad_norm': 0.3423600196838379, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.032228708267212, 'eval_runtime': 5.1399, 'eval_samples_per_second': 194.555, 'eval_steps_per_second': 12.257, 'epoch': 0.08}
{'loss': 1.092, 'grad_norm': 0.36933574080467224, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9747461080551147, 'eval_runtime': 5.1212, 'eval_samples_per_second': 195.268, 'eval_steps_per_second': 12.302, 'epoch': 0.12}
{'loss': 0.9742, 'grad_norm': 0.36756616830825806, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9523500800132751, 'eval_runtime': 5.133, 'eval_samples_per_second': 194.818, 'eval_steps_per_second': 12.274, 'epoch': 0.16}
{'loss': 1.0247, 'grad_norm': 0.2698669135570526, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9406601190567017, 'eval_runtime': 5.1393, 'eval_samples_per_second': 194.578, 'eval_steps_per_second': 12.258, 'epoch': 0.2}
{'loss': 1.0262, 'grad_norm': 0.31054118275642395, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.94182288646698, 'eval_runtime': 5.1512, 'eval_samples_per_second': 194.13, 'eval_steps_per_second': 12.23, 'epoch': 0.24}
{'loss': 0.9362, 'grad_norm': 0.2953876852989197, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9228995442390442, 'eval_runtime': 5.1575, 'eval_samples_per_second': 193.894, 'eval_steps_per_second': 12.215, 'epoch': 0.28}
{'loss': 0.9356, 'grad_norm': 0.25614669919013977, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9182075262069702, 'eval_runtime': 5.1531, 'eval_samples_per_second': 194.06, 'eval_steps_per_second': 12.226, 'epoch': 0.32}
{'loss': 0.9449, 'grad_norm': 0.2859329581260681, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.918180525302887, 'eval_runtime': 5.1543, 'eval_samples_per_second': 194.014, 'eval_steps_per_second': 12.223, 'epoch': 0.36}
{'loss': 0.9485, 'grad_norm': 0.33081701397895813, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9056603908538818, 'eval_runtime': 5.1495, 'eval_samples_per_second': 194.195, 'eval_steps_per_second': 12.234, 'epoch': 0.4}
{'loss': 0.8928, 'grad_norm': 0.3840606212615967, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9080038070678711, 'eval_runtime': 5.15, 'eval_samples_per_second': 194.176, 'eval_steps_per_second': 12.233, 'epoch': 0.44}
{'loss': 0.9139, 'grad_norm': 0.3104936480522156, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8995554447174072, 'eval_runtime': 5.186, 'eval_samples_per_second': 192.827, 'eval_steps_per_second': 12.148, 'epoch': 0.48}
{'loss': 0.8665, 'grad_norm': 0.41124579310417175, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8996593356132507, 'eval_runtime': 5.1975, 'eval_samples_per_second': 192.402, 'eval_steps_per_second': 12.121, 'epoch': 0.52}
{'loss': 0.8705, 'grad_norm': 0.3299935460090637, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8994978070259094, 'eval_runtime': 5.1861, 'eval_samples_per_second': 192.822, 'eval_steps_per_second': 12.148, 'epoch': 0.56}
{'loss': 0.8732, 'grad_norm': 0.3679512143135071, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8944712281227112, 'eval_runtime': 5.1857, 'eval_samples_per_second': 192.84, 'eval_steps_per_second': 12.149, 'epoch': 0.6}
{'loss': 0.8886, 'grad_norm': 0.44224005937576294, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8903669714927673, 'eval_runtime': 5.2114, 'eval_samples_per_second': 191.886, 'eval_steps_per_second': 12.089, 'epoch': 0.64}
{'loss': 0.8022, 'grad_norm': 0.48803266882896423, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8877654671669006, 'eval_runtime': 5.187, 'eval_samples_per_second': 192.792, 'eval_steps_per_second': 12.146, 'epoch': 0.68}
{'loss': 0.8426, 'grad_norm': 0.38596150279045105, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8901682496070862, 'eval_runtime': 5.2003, 'eval_samples_per_second': 192.296, 'eval_steps_per_second': 12.115, 'epoch': 0.72}
{'loss': 0.8214, 'grad_norm': 0.5209805369377136, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.885125994682312, 'eval_runtime': 5.2202, 'eval_samples_per_second': 191.564, 'eval_steps_per_second': 12.069, 'epoch': 0.76}
{'loss': 0.8283, 'grad_norm': 0.47319456934928894, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8821684718132019, 'eval_runtime': 5.1881, 'eval_samples_per_second': 192.749, 'eval_steps_per_second': 12.143, 'epoch': 0.8}
{'loss': 0.7326, 'grad_norm': 0.2905220687389374, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8848544359207153, 'eval_runtime': 5.1952, 'eval_samples_per_second': 192.486, 'eval_steps_per_second': 12.127, 'epoch': 0.84}
{'loss': 0.8065, 'grad_norm': 0.48422831296920776, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8824547529220581, 'eval_runtime': 5.1787, 'eval_samples_per_second': 193.098, 'eval_steps_per_second': 12.165, 'epoch': 0.88}
{'loss': 0.762, 'grad_norm': 0.4666471481323242, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8815729022026062, 'eval_runtime': 5.1823, 'eval_samples_per_second': 192.963, 'eval_steps_per_second': 12.157, 'epoch': 0.92}
{'loss': 0.7625, 'grad_norm': 0.36903294920921326, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8812119364738464, 'eval_runtime': 5.2092, 'eval_samples_per_second': 191.97, 'eval_steps_per_second': 12.094, 'epoch': 0.96}
{'loss': 0.8268, 'grad_norm': 0.6058237552642822, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8815487027168274, 'eval_runtime': 5.1932, 'eval_samples_per_second': 192.561, 'eval_steps_per_second': 12.131, 'epoch': 1.0}
{'train_runtime': 349.4537, 'train_samples_per_second': 28.61, 'train_steps_per_second': 1.789, 'train_loss': 0.9795449340820312, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4508613348007202, 1.032228708267212, 0.9747461080551147, 0.9523500800132751, 0.9406601190567017, 0.94182288646698, 0.9228995442390442, 0.9182075262069702, 0.918180525302887, 0.9056603908538818, 0.9080038070678711, 0.8995554447174072, 0.8996593356132507, 0.8994978070259094, 0.8944712281227112, 0.8903669714927673, 0.8877654671669006, 0.8901682496070862, 0.885125994682312, 0.8821684718132019, 0.8848544359207153, 0.8824547529220581, 0.8815729022026062, 0.8812119364738464, 0.8815487027168274], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4508613348007202, 1.032228708267212, 0.9747461080551147, 0.9523500800132751, 0.9406601190567017, 0.94182288646698, 0.9228995442390442, 0.9182075262069702, 0.918180525302887, 0.9056603908538818, 0.9080038070678711, 0.8995554447174072, 0.8996593356132507, 0.8994978070259094, 0.8944712281227112, 0.8903669714927673, 0.8877654671669006, 0.8901682496070862, 0.885125994682312, 0.8821684718132019, 0.8848544359207153, 0.8824547529220581, 0.8815729022026062, 0.8812119364738464, 0.8815487027168274]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0111750364303589
current iteration best possible eval_loss (full train run):  -0.8815487027168274
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1498 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.817896595349727
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.796567214679463
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.8179256276564726
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.8185450138954475
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.8077481804351971
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2293, dtype=torch.float64), 0, 0, 0, tensor(0.2105, dtype=torch.float64), 0, tensor(0.0424, dtype=torch.float64), 0, tensor(0.5178, dtype=torch.float64), 19, 1, 0, 0, 1, 1, 25, 0.024107270189119283, 29.3441185561731, 1]
normalized proposed parameters for next round by BO: [tensor(0.2293, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.2223e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2105, dtype=torch.float64), tensor(1.3741e-17, dtype=torch.float64), tensor(0.0424, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5178, dtype=torch.float64), tensor(0.5861, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1969, dtype=torch.float64), tensor(0.2411, dtype=torch.float64), tensor(0.6113, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.229
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.21
  truthfulqa_gen: 0
  wikitext: 0.042
  mmlu: 0
  arc_challenge: 0.518

LoRA Parameters:
  lora_r: (25,)
  lora_dropout: (0.024107270189119283,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (29.3441185561731,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  25
lora dropout:  0.024107270189119283
lora alpha:  29.3441185561731
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 21,401,600 || all params: 8,051,662,848 || trainable%: 0.2658
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1731, 'grad_norm': 2.4285709857940674, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6919583082199097, 'eval_runtime': 4.9412, 'eval_samples_per_second': 202.38, 'eval_steps_per_second': 12.75, 'epoch': 0.04}
{'loss': 1.2327, 'grad_norm': 0.9323102235794067, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0725293159484863, 'eval_runtime': 4.9395, 'eval_samples_per_second': 202.451, 'eval_steps_per_second': 12.754, 'epoch': 0.08}
{'loss': 1.0414, 'grad_norm': 0.515311598777771, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9996650218963623, 'eval_runtime': 4.933, 'eval_samples_per_second': 202.715, 'eval_steps_per_second': 12.771, 'epoch': 0.12}
{'loss': 0.9676, 'grad_norm': 0.49936166405677795, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9495462775230408, 'eval_runtime': 4.9163, 'eval_samples_per_second': 203.405, 'eval_steps_per_second': 12.815, 'epoch': 0.16}
{'loss': 0.9454, 'grad_norm': 0.47462227940559387, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9230198860168457, 'eval_runtime': 4.9271, 'eval_samples_per_second': 202.957, 'eval_steps_per_second': 12.786, 'epoch': 0.2}
{'loss': 0.8879, 'grad_norm': 0.4407764971256256, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9236990809440613, 'eval_runtime': 4.9241, 'eval_samples_per_second': 203.082, 'eval_steps_per_second': 12.794, 'epoch': 0.24}
{'loss': 0.8641, 'grad_norm': 0.465575248003006, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9166598916053772, 'eval_runtime': 4.9392, 'eval_samples_per_second': 202.46, 'eval_steps_per_second': 12.755, 'epoch': 0.28}
{'loss': 0.8392, 'grad_norm': 0.48806682229042053, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9084970951080322, 'eval_runtime': 4.9398, 'eval_samples_per_second': 202.438, 'eval_steps_per_second': 12.754, 'epoch': 0.32}
{'loss': 0.8675, 'grad_norm': 0.4958760738372803, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9043660759925842, 'eval_runtime': 4.9375, 'eval_samples_per_second': 202.534, 'eval_steps_per_second': 12.76, 'epoch': 0.36}
{'loss': 0.8518, 'grad_norm': 0.6520659327507019, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8986127376556396, 'eval_runtime': 4.9493, 'eval_samples_per_second': 202.048, 'eval_steps_per_second': 12.729, 'epoch': 0.4}
{'loss': 0.791, 'grad_norm': 0.6536400318145752, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9027628302574158, 'eval_runtime': 4.9449, 'eval_samples_per_second': 202.229, 'eval_steps_per_second': 12.74, 'epoch': 0.44}
{'loss': 0.8046, 'grad_norm': 0.6516337394714355, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8956044912338257, 'eval_runtime': 4.9481, 'eval_samples_per_second': 202.098, 'eval_steps_per_second': 12.732, 'epoch': 0.48}
{'loss': 0.7451, 'grad_norm': 0.6721874475479126, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8913301825523376, 'eval_runtime': 4.9499, 'eval_samples_per_second': 202.024, 'eval_steps_per_second': 12.727, 'epoch': 0.52}
{'loss': 0.6914, 'grad_norm': 0.700011670589447, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8888954520225525, 'eval_runtime': 4.9603, 'eval_samples_per_second': 201.603, 'eval_steps_per_second': 12.701, 'epoch': 0.56}
{'loss': 0.7454, 'grad_norm': 0.8758996725082397, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8865888714790344, 'eval_runtime': 4.9594, 'eval_samples_per_second': 201.638, 'eval_steps_per_second': 12.703, 'epoch': 0.6}
{'loss': 0.6827, 'grad_norm': 0.8062095642089844, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8867715001106262, 'eval_runtime': 4.9888, 'eval_samples_per_second': 200.449, 'eval_steps_per_second': 12.628, 'epoch': 0.64}
{'loss': 0.7, 'grad_norm': 0.887916088104248, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8801838159561157, 'eval_runtime': 4.9916, 'eval_samples_per_second': 200.338, 'eval_steps_per_second': 12.621, 'epoch': 0.68}
{'loss': 0.6607, 'grad_norm': 0.8545514345169067, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8826696872711182, 'eval_runtime': 5.0019, 'eval_samples_per_second': 199.923, 'eval_steps_per_second': 12.595, 'epoch': 0.72}
{'loss': 0.686, 'grad_norm': 1.001723289489746, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8779626488685608, 'eval_runtime': 4.9737, 'eval_samples_per_second': 201.057, 'eval_steps_per_second': 12.667, 'epoch': 0.76}
{'loss': 0.6486, 'grad_norm': 0.835029125213623, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8779312968254089, 'eval_runtime': 4.981, 'eval_samples_per_second': 200.762, 'eval_steps_per_second': 12.648, 'epoch': 0.8}
{'loss': 0.6218, 'grad_norm': 0.884343147277832, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8767225742340088, 'eval_runtime': 4.9928, 'eval_samples_per_second': 200.289, 'eval_steps_per_second': 12.618, 'epoch': 0.84}
{'loss': 0.5756, 'grad_norm': 1.0546377897262573, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8763908743858337, 'eval_runtime': 4.9778, 'eval_samples_per_second': 200.892, 'eval_steps_per_second': 12.656, 'epoch': 0.88}
{'loss': 0.5782, 'grad_norm': 0.8867575526237488, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8768519163131714, 'eval_runtime': 4.9998, 'eval_samples_per_second': 200.008, 'eval_steps_per_second': 12.601, 'epoch': 0.92}
{'loss': 0.5898, 'grad_norm': 0.9396873116493225, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.87646484375, 'eval_runtime': 4.9637, 'eval_samples_per_second': 201.461, 'eval_steps_per_second': 12.692, 'epoch': 0.96}
{'loss': 0.6122, 'grad_norm': 0.7933056354522705, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8760395646095276, 'eval_runtime': 4.9619, 'eval_samples_per_second': 201.534, 'eval_steps_per_second': 12.697, 'epoch': 1.0}
{'train_runtime': 291.6196, 'train_samples_per_second': 34.284, 'train_steps_per_second': 2.143, 'train_loss': 0.8721514587402344, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6919583082199097, 1.0725293159484863, 0.9996650218963623, 0.9495462775230408, 0.9230198860168457, 0.9236990809440613, 0.9166598916053772, 0.9084970951080322, 0.9043660759925842, 0.8986127376556396, 0.9027628302574158, 0.8956044912338257, 0.8913301825523376, 0.8888954520225525, 0.8865888714790344, 0.8867715001106262, 0.8801838159561157, 0.8826696872711182, 0.8779626488685608, 0.8779312968254089, 0.8767225742340088, 0.8763908743858337, 0.8768519163131714, 0.87646484375, 0.8760395646095276], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6919583082199097, 1.0725293159484863, 0.9996650218963623, 0.9495462775230408, 0.9230198860168457, 0.9236990809440613, 0.9166598916053772, 0.9084970951080322, 0.9043660759925842, 0.8986127376556396, 0.9027628302574158, 0.8956044912338257, 0.8913301825523376, 0.8888954520225525, 0.8865888714790344, 0.8867715001106262, 0.8801838159561157, 0.8826696872711182, 0.8779626488685608, 0.8779312968254089, 0.8767225742340088, 0.8763908743858337, 0.8768519163131714, 0.87646484375, 0.8760395646095276]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.096990704536438
current iteration best possible eval_loss (full train run):  -0.8760395646095276
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.4603 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.8665382599661547
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.866538193804026
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.86654456965281
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.8665382092432291
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.8665382092395773
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1259, dtype=torch.float64), tensor(0.0568, dtype=torch.float64), tensor(0.0180, dtype=torch.float64), tensor(0.0136, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.1061, dtype=torch.float64), tensor(0.0156, dtype=torch.float64), tensor(0.1119, dtype=torch.float64), tensor(0.4560, dtype=torch.float64), 28, 0, 1, 1, 1, 0, 79, 0.02542816838099518, 26.7801172475475, 0]
normalized proposed parameters for next round by BO: [tensor(0.1259, dtype=torch.float64), tensor(0.0568, dtype=torch.float64), tensor(0.0180, dtype=torch.float64), tensor(0.0136, dtype=torch.float64), tensor(0.0962, dtype=torch.float64), tensor(0.1061, dtype=torch.float64), tensor(0.0156, dtype=torch.float64), tensor(0.1119, dtype=torch.float64), tensor(0.4560, dtype=torch.float64), tensor(0.8768, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6211, dtype=torch.float64), tensor(0.2543, dtype=torch.float64), tensor(0.5579, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.126
  gsm8k: 0.057
  rowan_hellaswag: 0.018
  sciq: 0.014
  triviaqa: 0.096
  truthfulqa_gen: 0.106
  wikitext: 0.016
  mmlu: 0.112
  arc_challenge: 0.456

LoRA Parameters:
  lora_r: (79,)
  lora_dropout: (0.02542816838099518,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (26.7801172475475,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  79
lora dropout:  0.02542816838099518
lora alpha:  26.7801172475475
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 92,868,608 || all params: 8,123,129,856 || trainable%: 1.1433
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8789, 'grad_norm': 0.8279304504394531, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5112756490707397, 'eval_runtime': 5.3288, 'eval_samples_per_second': 187.661, 'eval_steps_per_second': 11.823, 'epoch': 0.04}
{'loss': 1.2336, 'grad_norm': 0.2707323729991913, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0342004299163818, 'eval_runtime': 5.3114, 'eval_samples_per_second': 188.274, 'eval_steps_per_second': 11.861, 'epoch': 0.08}
{'loss': 1.0604, 'grad_norm': 0.2571713924407959, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9731699228286743, 'eval_runtime': 5.3205, 'eval_samples_per_second': 187.953, 'eval_steps_per_second': 11.841, 'epoch': 0.12}
{'loss': 1.0124, 'grad_norm': 0.20109066367149353, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.955784797668457, 'eval_runtime': 5.337, 'eval_samples_per_second': 187.371, 'eval_steps_per_second': 11.804, 'epoch': 0.16}
{'loss': 0.9922, 'grad_norm': 0.2039453238248825, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9338129758834839, 'eval_runtime': 5.3398, 'eval_samples_per_second': 187.273, 'eval_steps_per_second': 11.798, 'epoch': 0.2}
{'loss': 0.9456, 'grad_norm': 0.23381176590919495, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9337503910064697, 'eval_runtime': 5.336, 'eval_samples_per_second': 187.405, 'eval_steps_per_second': 11.807, 'epoch': 0.24}
{'loss': 0.9212, 'grad_norm': 0.2227543741464615, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9322302341461182, 'eval_runtime': 5.3397, 'eval_samples_per_second': 187.277, 'eval_steps_per_second': 11.798, 'epoch': 0.28}
{'loss': 0.939, 'grad_norm': 0.23043645918369293, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9247798919677734, 'eval_runtime': 5.3361, 'eval_samples_per_second': 187.403, 'eval_steps_per_second': 11.806, 'epoch': 0.32}
{'loss': 0.8796, 'grad_norm': 0.2155524343252182, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.926562488079071, 'eval_runtime': 5.3404, 'eval_samples_per_second': 187.252, 'eval_steps_per_second': 11.797, 'epoch': 0.36}
{'loss': 0.9018, 'grad_norm': 0.23586170375347137, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9145443439483643, 'eval_runtime': 5.3618, 'eval_samples_per_second': 186.504, 'eval_steps_per_second': 11.75, 'epoch': 0.4}
{'loss': 0.8998, 'grad_norm': 0.2859255075454712, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9200934171676636, 'eval_runtime': 5.385, 'eval_samples_per_second': 185.699, 'eval_steps_per_second': 11.699, 'epoch': 0.44}
{'loss': 0.8949, 'grad_norm': 0.3004662096500397, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9094099402427673, 'eval_runtime': 5.4199, 'eval_samples_per_second': 184.505, 'eval_steps_per_second': 11.624, 'epoch': 0.48}
{'loss': 0.8602, 'grad_norm': 0.31467944383621216, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9104050397872925, 'eval_runtime': 5.3938, 'eval_samples_per_second': 185.398, 'eval_steps_per_second': 11.68, 'epoch': 0.52}
{'loss': 0.8117, 'grad_norm': 0.3687657415866852, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9039334058761597, 'eval_runtime': 5.4042, 'eval_samples_per_second': 185.041, 'eval_steps_per_second': 11.658, 'epoch': 0.56}
{'loss': 0.854, 'grad_norm': 0.2851709723472595, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9012983441352844, 'eval_runtime': 5.3903, 'eval_samples_per_second': 185.518, 'eval_steps_per_second': 11.688, 'epoch': 0.6}
{'loss': 0.7889, 'grad_norm': 0.3082588315010071, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9035462737083435, 'eval_runtime': 5.4379, 'eval_samples_per_second': 183.895, 'eval_steps_per_second': 11.585, 'epoch': 0.64}
{'loss': 0.8426, 'grad_norm': 0.31639930605888367, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9059895873069763, 'eval_runtime': 5.4145, 'eval_samples_per_second': 184.688, 'eval_steps_per_second': 11.635, 'epoch': 0.68}
{'loss': 0.8326, 'grad_norm': 0.32646694779396057, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8932633996009827, 'eval_runtime': 5.3488, 'eval_samples_per_second': 186.959, 'eval_steps_per_second': 11.778, 'epoch': 0.72}
{'loss': 0.7227, 'grad_norm': 0.33271220326423645, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8982101082801819, 'eval_runtime': 5.3506, 'eval_samples_per_second': 186.894, 'eval_steps_per_second': 11.774, 'epoch': 0.76}
{'loss': 0.7956, 'grad_norm': 0.353929728269577, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8942720890045166, 'eval_runtime': 5.3487, 'eval_samples_per_second': 186.961, 'eval_steps_per_second': 11.779, 'epoch': 0.8}
{'loss': 0.6961, 'grad_norm': 0.4500483572483063, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.893722653388977, 'eval_runtime': 5.3444, 'eval_samples_per_second': 187.113, 'eval_steps_per_second': 11.788, 'epoch': 0.84}
{'loss': 0.7382, 'grad_norm': 0.3501178026199341, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8906413316726685, 'eval_runtime': 5.349, 'eval_samples_per_second': 186.95, 'eval_steps_per_second': 11.778, 'epoch': 0.88}
{'loss': 0.7063, 'grad_norm': 0.4725310504436493, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8905431628227234, 'eval_runtime': 5.3505, 'eval_samples_per_second': 186.897, 'eval_steps_per_second': 11.775, 'epoch': 0.92}
{'loss': 0.719, 'grad_norm': 0.411740779876709, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8900318741798401, 'eval_runtime': 5.3447, 'eval_samples_per_second': 187.1, 'eval_steps_per_second': 11.787, 'epoch': 0.96}
{'loss': 0.6872, 'grad_norm': 0.5266860723495483, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8897671103477478, 'eval_runtime': 5.3488, 'eval_samples_per_second': 186.956, 'eval_steps_per_second': 11.778, 'epoch': 1.0}
{'train_runtime': 340.6152, 'train_samples_per_second': 29.344, 'train_steps_per_second': 1.835, 'train_loss': 0.9445860321044922, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5112756490707397, 1.0342004299163818, 0.9731699228286743, 0.955784797668457, 0.9338129758834839, 0.9337503910064697, 0.9322302341461182, 0.9247798919677734, 0.926562488079071, 0.9145443439483643, 0.9200934171676636, 0.9094099402427673, 0.9104050397872925, 0.9039334058761597, 0.9012983441352844, 0.9035462737083435, 0.9059895873069763, 0.8932633996009827, 0.8982101082801819, 0.8942720890045166, 0.893722653388977, 0.8906413316726685, 0.8905431628227234, 0.8900318741798401, 0.8897671103477478], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5112756490707397, 1.0342004299163818, 0.9731699228286743, 0.955784797668457, 0.9338129758834839, 0.9337503910064697, 0.9322302341461182, 0.9247798919677734, 0.926562488079071, 0.9145443439483643, 0.9200934171676636, 0.9094099402427673, 0.9104050397872925, 0.9039334058761597, 0.9012983441352844, 0.9035462737083435, 0.9059895873069763, 0.8932633996009827, 0.8982101082801819, 0.8942720890045166, 0.893722653388977, 0.8906413316726685, 0.8905431628227234, 0.8900318741798401, 0.8897671103477478]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.062602162361145
current iteration best possible eval_loss (full train run):  -0.8897671103477478
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.7051 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.9025566298548416
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.9025566306334107
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.9025607909670827
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9025566298548923
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.9025566298548405
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0736, dtype=torch.float64), 0, tensor(0.0744, dtype=torch.float64), tensor(0.0977, dtype=torch.float64), tensor(0.2683, dtype=torch.float64), 0, 0, 0, tensor(0.4717, dtype=torch.float64), 18, 1, 1, 0, 1, 1, 79, 0.016039293956993113, 20.24325026056652, 0]
normalized proposed parameters for next round by BO: [tensor(0.0736, dtype=torch.float64), tensor(1.0045e-18, dtype=torch.float64), tensor(0.0744, dtype=torch.float64), tensor(0.0977, dtype=torch.float64), tensor(0.2683, dtype=torch.float64), tensor(8.2667e-19, dtype=torch.float64), tensor(0.0054, dtype=torch.float64), tensor(0.0089, dtype=torch.float64), tensor(0.4717, dtype=torch.float64), tensor(0.5650, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6207, dtype=torch.float64), tensor(0.1604, dtype=torch.float64), tensor(0.4217, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.074
  gsm8k: 0
  rowan_hellaswag: 0.074
  sciq: 0.098
  triviaqa: 0.268
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.472

LoRA Parameters:
  lora_r: (79,)
  lora_dropout: (0.016039293956993113,)
  num_layers_to_apply: (18,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (20.24325026056652,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  18
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  79
lora dropout:  0.016039293956993113
lora alpha:  20.24325026056652
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 71,350,272 || all params: 8,101,611,520 || trainable%: 0.8807
length of training data:  9854
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3793, 'grad_norm': 0.8849818110466003, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0794224739074707, 'eval_runtime': 5.0945, 'eval_samples_per_second': 196.291, 'eval_steps_per_second': 12.366, 'epoch': 0.04}
{'loss': 1.4224, 'grad_norm': 0.3650478422641754, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1395041942596436, 'eval_runtime': 5.0819, 'eval_samples_per_second': 196.775, 'eval_steps_per_second': 12.397, 'epoch': 0.08}
{'loss': 1.1402, 'grad_norm': 0.19763541221618652, 'learning_rate': 0.0002872791519434629, 'epoch': 0.12}
{'eval_loss': 1.0855296850204468, 'eval_runtime': 5.0784, 'eval_samples_per_second': 196.911, 'eval_steps_per_second': 12.405, 'epoch': 0.12}
{'loss': 1.1242, 'grad_norm': 0.20622557401657104, 'learning_rate': 0.00027402826855123675, 'epoch': 0.16}
{'eval_loss': 1.0375771522521973, 'eval_runtime': 5.0856, 'eval_samples_per_second': 196.634, 'eval_steps_per_second': 12.388, 'epoch': 0.16}
{'loss': 1.0937, 'grad_norm': 0.20800995826721191, 'learning_rate': 0.00026077738515901055, 'epoch': 0.2}
{'eval_loss': 1.0161904096603394, 'eval_runtime': 5.1064, 'eval_samples_per_second': 195.833, 'eval_steps_per_second': 12.338, 'epoch': 0.2}
{'loss': 1.0793, 'grad_norm': 0.1895466446876526, 'learning_rate': 0.0002475265017667844, 'epoch': 0.24}
{'eval_loss': 1.0177041292190552, 'eval_runtime': 5.1048, 'eval_samples_per_second': 195.895, 'eval_steps_per_second': 12.341, 'epoch': 0.24}
{'loss': 1.0549, 'grad_norm': 0.21218475699424744, 'learning_rate': 0.00023427561837455828, 'epoch': 0.28}
{'eval_loss': 1.0149433612823486, 'eval_runtime': 5.1134, 'eval_samples_per_second': 195.565, 'eval_steps_per_second': 12.321, 'epoch': 0.28}
{'loss': 1.0628, 'grad_norm': 0.25073719024658203, 'learning_rate': 0.00022102473498233213, 'epoch': 0.32}
{'eval_loss': 1.0014421939849854, 'eval_runtime': 5.1273, 'eval_samples_per_second': 195.033, 'eval_steps_per_second': 12.287, 'epoch': 0.32}
{'loss': 0.9736, 'grad_norm': 0.23452474176883698, 'learning_rate': 0.00020777385159010599, 'epoch': 0.37}
{'eval_loss': 0.9917701482772827, 'eval_runtime': 5.1179, 'eval_samples_per_second': 195.392, 'eval_steps_per_second': 12.31, 'epoch': 0.37}
{'loss': 0.9717, 'grad_norm': 0.24320386350154877, 'learning_rate': 0.00019452296819787987, 'epoch': 0.41}
{'eval_loss': 0.9821100831031799, 'eval_runtime': 5.1238, 'eval_samples_per_second': 195.169, 'eval_steps_per_second': 12.296, 'epoch': 0.41}
{'loss': 0.9315, 'grad_norm': 0.25036969780921936, 'learning_rate': 0.0001812720848056537, 'epoch': 0.45}
{'eval_loss': 0.9815006852149963, 'eval_runtime': 5.1588, 'eval_samples_per_second': 193.845, 'eval_steps_per_second': 12.212, 'epoch': 0.45}
{'loss': 0.9437, 'grad_norm': 0.24288801848888397, 'learning_rate': 0.00016802120141342754, 'epoch': 0.49}
{'eval_loss': 0.9850463271141052, 'eval_runtime': 5.1325, 'eval_samples_per_second': 194.835, 'eval_steps_per_second': 12.275, 'epoch': 0.49}
{'loss': 0.9684, 'grad_norm': 0.27768003940582275, 'learning_rate': 0.0001547703180212014, 'epoch': 0.53}
{'eval_loss': 0.9667312502861023, 'eval_runtime': 5.1608, 'eval_samples_per_second': 193.768, 'eval_steps_per_second': 12.207, 'epoch': 0.53}
{'loss': 0.8791, 'grad_norm': 0.341597318649292, 'learning_rate': 0.00014151943462897525, 'epoch': 0.57}
{'eval_loss': 0.9636225700378418, 'eval_runtime': 5.1594, 'eval_samples_per_second': 193.821, 'eval_steps_per_second': 12.211, 'epoch': 0.57}
{'loss': 0.9068, 'grad_norm': 0.36068713665008545, 'learning_rate': 0.0001282685512367491, 'epoch': 0.61}
{'eval_loss': 0.9682258367538452, 'eval_runtime': 5.141, 'eval_samples_per_second': 194.514, 'eval_steps_per_second': 12.254, 'epoch': 0.61}
{'loss': 0.9234, 'grad_norm': 0.37427932024002075, 'learning_rate': 0.00011501766784452296, 'epoch': 0.65}
{'eval_loss': 0.9644904732704163, 'eval_runtime': 5.1266, 'eval_samples_per_second': 195.063, 'eval_steps_per_second': 12.289, 'epoch': 0.65}
{'loss': 0.8323, 'grad_norm': 0.23656131327152252, 'learning_rate': 0.00010176678445229682, 'epoch': 0.69}
{'eval_loss': 0.9629277586936951, 'eval_runtime': 5.1309, 'eval_samples_per_second': 194.898, 'eval_steps_per_second': 12.279, 'epoch': 0.69}
{'loss': 0.8539, 'grad_norm': 0.34071964025497437, 'learning_rate': 8.851590106007066e-05, 'epoch': 0.73}
{'eval_loss': 0.958816409111023, 'eval_runtime': 5.1452, 'eval_samples_per_second': 194.357, 'eval_steps_per_second': 12.244, 'epoch': 0.73}
{'loss': 0.8576, 'grad_norm': 0.40123239159584045, 'learning_rate': 7.526501766784451e-05, 'epoch': 0.77}
{'eval_loss': 0.9557112455368042, 'eval_runtime': 5.1486, 'eval_samples_per_second': 194.228, 'eval_steps_per_second': 12.236, 'epoch': 0.77}
{'loss': 0.8368, 'grad_norm': 0.4219658374786377, 'learning_rate': 6.201413427561837e-05, 'epoch': 0.81}
{'eval_loss': 0.9575438499450684, 'eval_runtime': 5.1421, 'eval_samples_per_second': 194.475, 'eval_steps_per_second': 12.252, 'epoch': 0.81}
{'loss': 0.8041, 'grad_norm': 0.33822011947631836, 'learning_rate': 4.876325088339222e-05, 'epoch': 0.85}
{'eval_loss': 0.9543906450271606, 'eval_runtime': 5.133, 'eval_samples_per_second': 194.819, 'eval_steps_per_second': 12.274, 'epoch': 0.85}
{'loss': 0.8639, 'grad_norm': 0.37988153100013733, 'learning_rate': 3.551236749116607e-05, 'epoch': 0.89}
{'eval_loss': 0.9514685869216919, 'eval_runtime': 5.1332, 'eval_samples_per_second': 194.809, 'eval_steps_per_second': 12.273, 'epoch': 0.89}
{'loss': 0.7531, 'grad_norm': 0.3471997082233429, 'learning_rate': 2.2261484098939926e-05, 'epoch': 0.93}
{'eval_loss': 0.9520180821418762, 'eval_runtime': 5.1407, 'eval_samples_per_second': 194.525, 'eval_steps_per_second': 12.255, 'epoch': 0.93}
{'loss': 0.7593, 'grad_norm': 0.39405229687690735, 'learning_rate': 9.010600706713779e-06, 'epoch': 0.97}
{'eval_loss': 0.9534769058227539, 'eval_runtime': 5.1305, 'eval_samples_per_second': 194.914, 'eval_steps_per_second': 12.28, 'epoch': 0.97}
{'train_runtime': 287.9628, 'train_samples_per_second': 34.22, 'train_steps_per_second': 2.139, 'train_loss': 1.053087432663162, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0794224739074707, 1.1395041942596436, 1.0855296850204468, 1.0375771522521973, 1.0161904096603394, 1.0177041292190552, 1.0149433612823486, 1.0014421939849854, 0.9917701482772827, 0.9821100831031799, 0.9815006852149963, 0.9850463271141052, 0.9667312502861023, 0.9636225700378418, 0.9682258367538452, 0.9644904732704163, 0.9629277586936951, 0.958816409111023, 0.9557112455368042, 0.9575438499450684, 0.9543906450271606, 0.9514685869216919, 0.9520180821418762, 0.9534769058227539], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.0794224739074707, 1.1395041942596436, 1.0855296850204468, 1.0375771522521973, 1.0161904096603394, 1.0177041292190552, 1.0149433612823486, 1.0014421939849854, 0.9917701482772827, 0.9821100831031799, 0.9815006852149963, 0.9850463271141052, 0.9667312502861023, 0.9636225700378418, 0.9682258367538452, 0.9644904732704163, 0.9629277586936951, 0.958816409111023, 0.9557112455368042, 0.9575438499450684, 0.9543906450271606, 0.9514685869216919, 0.9520180821418762, 0.9534769058227539]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.094516396522522
current iteration best possible eval_loss (full train run):  -0.9534769058227539
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.1781 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1795889139175415, 0.5639668703079224, 0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.6092051267623901, 0.8282220363616943, 0.8343875408172607, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.22647850215435028, 0.034817516803741455, 0.2369377166032791, 0.8301550149917603]  ‚Üí  acq = -0.9303796017106626
X = [0.3627225160598755, 0.4275026321411133, 0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.20957553386688232, 0.29544466733932495, 0.45958811044692993, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7996509075164795, 0.16252559423446655, 0.23583632707595825, 0.46233898401260376]  ‚Üí  acq = -0.9303717432835145
X = [0.3497363328933716, 0.9076562523841858, 0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.451727032661438, 0.6221595406532288, 0.021270394325256348, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.990592360496521, 0.6705264449119568, 0.5292245149612427, 0.4145408272743225]  ‚Üí  acq = -0.9307136619679193
X = [0.8545095920562744, 0.7290428876876831, 0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.1896294802427292, 0.13708847761154175, 0.2295888066291809, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.42133286595344543, 0.9889391660690308, 0.27955883741378784, 0.647262454032898]  ‚Üí  acq = -0.9303811232777593
X = [0.25802141427993774, 0.15090978145599365, 0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.16085723042488098, 0.13615381717681885, 0.3176853060722351, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7902160882949829, 0.012897372245788574, 0.5332701206207275, 0.154333233833313]  ‚Üí  acq = -0.9303530547671544
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2605, dtype=torch.float64), tensor(0.1039, dtype=torch.float64), tensor(0.0739, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(0.1796, dtype=torch.float64), tensor(0.1543, dtype=torch.float64), tensor(0.0217, dtype=torch.float64), tensor(0.0696, dtype=torch.float64), tensor(0.1052, dtype=torch.float64), 29, 0, 1, 1, 1, 0, 92, 0.072817089845866, 26.26312643423737, 0]
normalized proposed parameters for next round by BO: [tensor(0.2605, dtype=torch.float64), tensor(0.1039, dtype=torch.float64), tensor(0.0739, dtype=torch.float64), tensor(0.0314, dtype=torch.float64), tensor(0.1796, dtype=torch.float64), tensor(0.1543, dtype=torch.float64), tensor(0.0217, dtype=torch.float64), tensor(0.0696, dtype=torch.float64), tensor(0.1052, dtype=torch.float64), tensor(0.8981, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7168, dtype=torch.float64), tensor(0.7282, dtype=torch.float64), tensor(0.5471, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.26
  gsm8k: 0.104
  rowan_hellaswag: 0.074
  sciq: 0.031
  triviaqa: 0.18
  truthfulqa_gen: 0.154
  wikitext: 0.022
  mmlu: 0.07
  arc_challenge: 0.105

LoRA Parameters:
  lora_r: (92,)
  lora_dropout: (0.072817089845866,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (26.26312643423737,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  92
lora dropout:  0.072817089845866
lora alpha:  26.26312643423737
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 112,013,312 || all params: 8,142,274,560 || trainable%: 1.3757
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0905, 'grad_norm': 0.9710673093795776, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6062849760055542, 'eval_runtime': 5.1887, 'eval_samples_per_second': 192.726, 'eval_steps_per_second': 12.142, 'epoch': 0.04}
{'loss': 1.3989, 'grad_norm': 0.2823736369609833, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9807713627815247, 'eval_runtime': 5.1971, 'eval_samples_per_second': 192.416, 'eval_steps_per_second': 12.122, 'epoch': 0.08}
{'loss': 1.1799, 'grad_norm': 0.2171279788017273, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.944368839263916, 'eval_runtime': 5.2257, 'eval_samples_per_second': 191.362, 'eval_steps_per_second': 12.056, 'epoch': 0.12}
{'loss': 1.0566, 'grad_norm': 0.198380246758461, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9211612343788147, 'eval_runtime': 5.2114, 'eval_samples_per_second': 191.888, 'eval_steps_per_second': 12.089, 'epoch': 0.16}
{'loss': 1.1254, 'grad_norm': 0.20401491224765778, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9139406681060791, 'eval_runtime': 5.2029, 'eval_samples_per_second': 192.201, 'eval_steps_per_second': 12.109, 'epoch': 0.2}
{'loss': 1.1016, 'grad_norm': 0.24026672542095184, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9003297090530396, 'eval_runtime': 5.1934, 'eval_samples_per_second': 192.551, 'eval_steps_per_second': 12.131, 'epoch': 0.24}
{'loss': 1.1197, 'grad_norm': 0.20084711909294128, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8955932855606079, 'eval_runtime': 5.21, 'eval_samples_per_second': 191.939, 'eval_steps_per_second': 12.092, 'epoch': 0.28}
{'loss': 1.0713, 'grad_norm': 0.22034043073654175, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8888779282569885, 'eval_runtime': 5.2184, 'eval_samples_per_second': 191.631, 'eval_steps_per_second': 12.073, 'epoch': 0.32}
{'loss': 1.1005, 'grad_norm': 0.20174385607242584, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8898588418960571, 'eval_runtime': 5.2056, 'eval_samples_per_second': 192.102, 'eval_steps_per_second': 12.102, 'epoch': 0.36}
{'loss': 1.0048, 'grad_norm': 0.20822300016880035, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8835700750350952, 'eval_runtime': 5.1898, 'eval_samples_per_second': 192.686, 'eval_steps_per_second': 12.139, 'epoch': 0.4}
{'loss': 1.0189, 'grad_norm': 0.20364321768283844, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8846001029014587, 'eval_runtime': 5.1872, 'eval_samples_per_second': 192.781, 'eval_steps_per_second': 12.145, 'epoch': 0.44}
{'loss': 1.0848, 'grad_norm': 0.18697534501552582, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.876726508140564, 'eval_runtime': 5.1821, 'eval_samples_per_second': 192.973, 'eval_steps_per_second': 12.157, 'epoch': 0.48}
{'loss': 1.0351, 'grad_norm': 0.20810455083847046, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8805789947509766, 'eval_runtime': 5.1914, 'eval_samples_per_second': 192.628, 'eval_steps_per_second': 12.136, 'epoch': 0.52}
{'loss': 1.0663, 'grad_norm': 0.26329198479652405, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8752921223640442, 'eval_runtime': 5.1899, 'eval_samples_per_second': 192.682, 'eval_steps_per_second': 12.139, 'epoch': 0.56}
{'loss': 1.019, 'grad_norm': 0.2040795385837555, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8718026280403137, 'eval_runtime': 5.1843, 'eval_samples_per_second': 192.892, 'eval_steps_per_second': 12.152, 'epoch': 0.6}
{'loss': 1.0614, 'grad_norm': 0.19923575222492218, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8724048137664795, 'eval_runtime': 5.1797, 'eval_samples_per_second': 193.063, 'eval_steps_per_second': 12.163, 'epoch': 0.64}
{'loss': 1.1071, 'grad_norm': 0.17682723701000214, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8696858882904053, 'eval_runtime': 5.1993, 'eval_samples_per_second': 192.333, 'eval_steps_per_second': 12.117, 'epoch': 0.68}
{'loss': 1.0351, 'grad_norm': 0.18448735773563385, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8637658953666687, 'eval_runtime': 5.1865, 'eval_samples_per_second': 192.808, 'eval_steps_per_second': 12.147, 'epoch': 0.72}
{'loss': 1.0509, 'grad_norm': 0.23104475438594818, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8680760264396667, 'eval_runtime': 5.1839, 'eval_samples_per_second': 192.906, 'eval_steps_per_second': 12.153, 'epoch': 0.76}
{'loss': 1.0454, 'grad_norm': 0.2171628177165985, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8646746873855591, 'eval_runtime': 5.1871, 'eval_samples_per_second': 192.786, 'eval_steps_per_second': 12.145, 'epoch': 0.8}
{'loss': 1.0207, 'grad_norm': 0.20037700235843658, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8600040078163147, 'eval_runtime': 5.1869, 'eval_samples_per_second': 192.792, 'eval_steps_per_second': 12.146, 'epoch': 0.84}
{'loss': 0.963, 'grad_norm': 0.20005826652050018, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8600937724113464, 'eval_runtime': 5.1834, 'eval_samples_per_second': 192.925, 'eval_steps_per_second': 12.154, 'epoch': 0.88}
{'loss': 1.028, 'grad_norm': 0.22709697484970093, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8610429763793945, 'eval_runtime': 5.2143, 'eval_samples_per_second': 191.781, 'eval_steps_per_second': 12.082, 'epoch': 0.92}
{'loss': 1.0618, 'grad_norm': 0.22849346697330475, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8580069541931152, 'eval_runtime': 5.217, 'eval_samples_per_second': 191.682, 'eval_steps_per_second': 12.076, 'epoch': 0.96}
{'loss': 1.0069, 'grad_norm': 0.26138830184936523, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8581597208976746, 'eval_runtime': 5.2166, 'eval_samples_per_second': 191.696, 'eval_steps_per_second': 12.077, 'epoch': 1.0}
{'train_runtime': 348.2176, 'train_samples_per_second': 28.703, 'train_steps_per_second': 1.795, 'train_loss': 1.1541549224853516, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6062849760055542, 0.9807713627815247, 0.944368839263916, 0.9211612343788147, 0.9139406681060791, 0.9003297090530396, 0.8955932855606079, 0.8888779282569885, 0.8898588418960571, 0.8835700750350952, 0.8846001029014587, 0.876726508140564, 0.8805789947509766, 0.8752921223640442, 0.8718026280403137, 0.8724048137664795, 0.8696858882904053, 0.8637658953666687, 0.8680760264396667, 0.8646746873855591, 0.8600040078163147, 0.8600937724113464, 0.8610429763793945, 0.8580069541931152, 0.8581597208976746], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.6062849760055542, 0.9807713627815247, 0.944368839263916, 0.9211612343788147, 0.9139406681060791, 0.9003297090530396, 0.8955932855606079, 0.8888779282569885, 0.8898588418960571, 0.8835700750350952, 0.8846001029014587, 0.876726508140564, 0.8805789947509766, 0.8752921223640442, 0.8718026280403137, 0.8724048137664795, 0.8696858882904053, 0.8637658953666687, 0.8680760264396667, 0.8646746873855591, 0.8600040078163147, 0.8600937724113464, 0.8610429763793945, 0.8580069541931152, 0.8581597208976746]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0757626295089722
current iteration best possible eval_loss (full train run):  -0.8581597208976746
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1966 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.8662045668133573
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.8662060977386468
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.8662045668411131
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.8661610850292457
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.8662065514185575
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3093, dtype=torch.float64), 0, tensor(0.0953, dtype=torch.float64), tensor(0.0143, dtype=torch.float64), tensor(0.0282, dtype=torch.float64), tensor(0.0460, dtype=torch.float64), 0, 0, tensor(0.5070, dtype=torch.float64), 26, 0, 1, 1, 1, 1, 90, 0.025776919894303137, 21.534762447005512, 0]
normalized proposed parameters for next round by BO: [tensor(0.3093, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0953, dtype=torch.float64), tensor(0.0143, dtype=torch.float64), tensor(0.0282, dtype=torch.float64), tensor(0.0460, dtype=torch.float64), tensor(8.5590e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5070, dtype=torch.float64), tensor(0.8108, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.7048, dtype=torch.float64), tensor(0.2578, dtype=torch.float64), tensor(0.4486, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.309
  gsm8k: 0
  rowan_hellaswag: 0.095
  sciq: 0.014
  triviaqa: 0.028
  truthfulqa_gen: 0.046
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.507

LoRA Parameters:
  lora_r: (90,)
  lora_dropout: (0.025776919894303137,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (21.534762447005512,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  90
lora dropout:  0.025776919894303137
lora alpha:  21.534762447005512
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 141,373,440 || all params: 8,171,634,688 || trainable%: 1.7301
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1171, 'grad_norm': 0.8116852045059204, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4613701105117798, 'eval_runtime': 5.4309, 'eval_samples_per_second': 184.13, 'eval_steps_per_second': 11.6, 'epoch': 0.04}
{'loss': 1.2944, 'grad_norm': 0.3308667540550232, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9847235083580017, 'eval_runtime': 5.445, 'eval_samples_per_second': 183.654, 'eval_steps_per_second': 11.57, 'epoch': 0.08}
{'loss': 1.0531, 'grad_norm': 0.16585808992385864, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9307388663291931, 'eval_runtime': 5.4766, 'eval_samples_per_second': 182.595, 'eval_steps_per_second': 11.503, 'epoch': 0.12}
{'loss': 1.0267, 'grad_norm': 0.18077363073825836, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9192312955856323, 'eval_runtime': 5.5047, 'eval_samples_per_second': 181.664, 'eval_steps_per_second': 11.445, 'epoch': 0.16}
{'loss': 0.9794, 'grad_norm': 0.18387995660305023, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9167382717132568, 'eval_runtime': 5.4603, 'eval_samples_per_second': 183.139, 'eval_steps_per_second': 11.538, 'epoch': 0.2}
{'loss': 0.9895, 'grad_norm': 0.1740412414073944, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8997688889503479, 'eval_runtime': 5.4401, 'eval_samples_per_second': 183.82, 'eval_steps_per_second': 11.581, 'epoch': 0.24}
{'loss': 0.9849, 'grad_norm': 0.19834385812282562, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8986534476280212, 'eval_runtime': 5.4421, 'eval_samples_per_second': 183.754, 'eval_steps_per_second': 11.576, 'epoch': 0.28}
{'loss': 0.9347, 'grad_norm': 0.20204444229602814, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8909097909927368, 'eval_runtime': 5.4203, 'eval_samples_per_second': 184.491, 'eval_steps_per_second': 11.623, 'epoch': 0.32}
{'loss': 0.9405, 'grad_norm': 0.24968701601028442, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8883640170097351, 'eval_runtime': 5.4205, 'eval_samples_per_second': 184.484, 'eval_steps_per_second': 11.622, 'epoch': 0.36}
{'loss': 0.9275, 'grad_norm': 0.2078285664319992, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8884899616241455, 'eval_runtime': 5.4193, 'eval_samples_per_second': 184.526, 'eval_steps_per_second': 11.625, 'epoch': 0.4}
{'loss': 0.832, 'grad_norm': 0.26608482003211975, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8848046064376831, 'eval_runtime': 5.4219, 'eval_samples_per_second': 184.436, 'eval_steps_per_second': 11.619, 'epoch': 0.44}
{'loss': 0.8026, 'grad_norm': 0.29373136162757874, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8786516189575195, 'eval_runtime': 5.4221, 'eval_samples_per_second': 184.429, 'eval_steps_per_second': 11.619, 'epoch': 0.48}
{'loss': 0.8298, 'grad_norm': 0.29130199551582336, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8715803027153015, 'eval_runtime': 5.4359, 'eval_samples_per_second': 183.962, 'eval_steps_per_second': 11.59, 'epoch': 0.52}
{'loss': 0.8614, 'grad_norm': 0.28855034708976746, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8745467662811279, 'eval_runtime': 5.432, 'eval_samples_per_second': 184.094, 'eval_steps_per_second': 11.598, 'epoch': 0.56}
{'loss': 0.8265, 'grad_norm': 0.26481395959854126, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8677009344100952, 'eval_runtime': 5.4245, 'eval_samples_per_second': 184.349, 'eval_steps_per_second': 11.614, 'epoch': 0.6}
{'loss': 0.7548, 'grad_norm': 0.28918734192848206, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8666013479232788, 'eval_runtime': 5.4273, 'eval_samples_per_second': 184.254, 'eval_steps_per_second': 11.608, 'epoch': 0.64}
{'loss': 0.7807, 'grad_norm': 0.29702475666999817, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8715798258781433, 'eval_runtime': 5.4333, 'eval_samples_per_second': 184.05, 'eval_steps_per_second': 11.595, 'epoch': 0.68}
{'loss': 0.8172, 'grad_norm': 0.2931143641471863, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8664471507072449, 'eval_runtime': 5.446, 'eval_samples_per_second': 183.62, 'eval_steps_per_second': 11.568, 'epoch': 0.72}
{'loss': 0.8511, 'grad_norm': 0.24664746224880219, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8627715110778809, 'eval_runtime': 5.4303, 'eval_samples_per_second': 184.152, 'eval_steps_per_second': 11.602, 'epoch': 0.76}
{'loss': 0.7299, 'grad_norm': 0.35412511229515076, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8644673824310303, 'eval_runtime': 5.4333, 'eval_samples_per_second': 184.05, 'eval_steps_per_second': 11.595, 'epoch': 0.8}
{'loss': 0.7089, 'grad_norm': 0.3024786710739136, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8611975312232971, 'eval_runtime': 5.4291, 'eval_samples_per_second': 184.193, 'eval_steps_per_second': 11.604, 'epoch': 0.84}
{'loss': 0.6701, 'grad_norm': 0.2986665964126587, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8613382577896118, 'eval_runtime': 5.4252, 'eval_samples_per_second': 184.325, 'eval_steps_per_second': 11.613, 'epoch': 0.88}
{'loss': 0.6975, 'grad_norm': 0.21802739799022675, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8580591082572937, 'eval_runtime': 5.4266, 'eval_samples_per_second': 184.278, 'eval_steps_per_second': 11.61, 'epoch': 0.92}
{'loss': 0.7006, 'grad_norm': 0.2002999186515808, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8598569631576538, 'eval_runtime': 5.428, 'eval_samples_per_second': 184.23, 'eval_steps_per_second': 11.606, 'epoch': 0.96}
{'loss': 0.7489, 'grad_norm': 0.28398576378822327, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8592767119407654, 'eval_runtime': 5.4237, 'eval_samples_per_second': 184.375, 'eval_steps_per_second': 11.616, 'epoch': 1.0}
{'train_runtime': 344.6369, 'train_samples_per_second': 29.007, 'train_steps_per_second': 1.814, 'train_loss': 0.954392611694336, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4613701105117798, 0.9847235083580017, 0.9307388663291931, 0.9192312955856323, 0.9167382717132568, 0.8997688889503479, 0.8986534476280212, 0.8909097909927368, 0.8883640170097351, 0.8884899616241455, 0.8848046064376831, 0.8786516189575195, 0.8715803027153015, 0.8745467662811279, 0.8677009344100952, 0.8666013479232788, 0.8715798258781433, 0.8664471507072449, 0.8627715110778809, 0.8644673824310303, 0.8611975312232971, 0.8613382577896118, 0.8580591082572937, 0.8598569631576538, 0.8592767119407654], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4613701105117798, 0.9847235083580017, 0.9307388663291931, 0.9192312955856323, 0.9167382717132568, 0.8997688889503479, 0.8986534476280212, 0.8909097909927368, 0.8883640170097351, 0.8884899616241455, 0.8848046064376831, 0.8786516189575195, 0.8715803027153015, 0.8745467662811279, 0.8677009344100952, 0.8666013479232788, 0.8715798258781433, 0.8664471507072449, 0.8627715110778809, 0.8644673824310303, 0.8611975312232971, 0.8613382577896118, 0.8580591082572937, 0.8598569631576538, 0.8592767119407654]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0688362121582031
current iteration best possible eval_loss (full train run):  -0.8592767119407654
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.3691 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.4685550332069397, 0.8636659979820251, 0.1409301459789276, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.0774579644203186, 0.5328817963600159, 0.1593477874994278, 0.8951139450073242, 0.22685877978801727, 0.6831576228141785]  ‚Üí  acq = -0.9351602506095482
X = [0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.211955726146698, 0.7839422821998596, 0.6941977143287659, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.052415549755096436, 0.9363643527030945, 0.48458048701286316, 0.07482516765594482, 0.7481347322463989, 0.8363668322563171]  ‚Üí  acq = -0.9522866164144435
X = [0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.565816342830658, 0.37286609411239624, 0.8525202870368958, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.3118453025817871, 0.8762340545654297, 0.13084112107753754, 0.6519256234169006, 0.5938699245452881, 0.9983730316162109]  ‚Üí  acq = -0.935281502233366
X = [0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.4944447875022888, 0.0025587081909179688, 0.5481817126274109, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.02311795949935913, 0.9936825633049011, 0.12430374324321747, 0.053788065910339355, 0.3728521466255188, 0.5949426293373108]  ‚Üí  acq = -0.9424621946221177
X = [0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.37545084953308105, 0.7189667820930481, 0.8034883737564087, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4807974100112915, 0.8887658715248108, 0.9803124666213989, 0.013015925884246826, 0.34956714510917664, 0.09932535886764526]  ‚Üí  acq = -0.9343693665436694
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1209, dtype=torch.float64), tensor(0.2025, dtype=torch.float64), tensor(0.0110, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), tensor(0.1260, dtype=torch.float64), tensor(0.1864, dtype=torch.float64), tensor(0.0327, dtype=torch.float64), tensor(0.0632, dtype=torch.float64), tensor(0.1667, dtype=torch.float64), 30, 1, 1, 1, 0, 1, 53, 0.06908466535580444, 32.924057223706086, 0]
normalized proposed parameters for next round by BO: [tensor(0.1209, dtype=torch.float64), tensor(0.2025, dtype=torch.float64), tensor(0.0110, dtype=torch.float64), tensor(0.0905, dtype=torch.float64), tensor(0.1260, dtype=torch.float64), tensor(0.1864, dtype=torch.float64), tensor(0.0327, dtype=torch.float64), tensor(0.0632, dtype=torch.float64), tensor(0.1667, dtype=torch.float64), tensor(0.9354, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4111, dtype=torch.float64), tensor(0.6908, dtype=torch.float64), tensor(0.6859, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.121
  gsm8k: 0.203
  rowan_hellaswag: 0.011
  sciq: 0.091
  triviaqa: 0.126
  truthfulqa_gen: 0.186
  wikitext: 0.033
  mmlu: 0.063
  arc_challenge: 0.167

LoRA Parameters:
  lora_r: (53,)
  lora_dropout: (0.06908466535580444,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (32.924057223706086,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  53
lora dropout:  0.06908466535580444
lora alpha:  32.924057223706086
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 79,779,840 || all params: 8,110,041,088 || trainable%: 0.9837
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6936, 'grad_norm': 0.7277773022651672, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.481226921081543, 'eval_runtime': 5.4532, 'eval_samples_per_second': 183.38, 'eval_steps_per_second': 11.553, 'epoch': 0.04}
{'loss': 1.2882, 'grad_norm': 0.37738358974456787, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1588870286941528, 'eval_runtime': 5.459, 'eval_samples_per_second': 183.185, 'eval_steps_per_second': 11.541, 'epoch': 0.08}
{'loss': 1.104, 'grad_norm': 0.33595573902130127, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0400807857513428, 'eval_runtime': 5.4369, 'eval_samples_per_second': 183.93, 'eval_steps_per_second': 11.588, 'epoch': 0.12}
{'loss': 1.0243, 'grad_norm': 0.32969531416893005, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0011844635009766, 'eval_runtime': 5.4381, 'eval_samples_per_second': 183.888, 'eval_steps_per_second': 11.585, 'epoch': 0.16}
{'loss': 0.9656, 'grad_norm': 0.2784135639667511, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9355860948562622, 'eval_runtime': 5.4373, 'eval_samples_per_second': 183.916, 'eval_steps_per_second': 11.587, 'epoch': 0.2}
{'loss': 0.9751, 'grad_norm': 0.31623339653015137, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.936859130859375, 'eval_runtime': 5.4479, 'eval_samples_per_second': 183.557, 'eval_steps_per_second': 11.564, 'epoch': 0.24}
{'loss': 0.9338, 'grad_norm': 0.2485727220773697, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9328967332839966, 'eval_runtime': 5.4511, 'eval_samples_per_second': 183.448, 'eval_steps_per_second': 11.557, 'epoch': 0.28}
{'loss': 0.9156, 'grad_norm': 0.24271880090236664, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9210047721862793, 'eval_runtime': 5.4513, 'eval_samples_per_second': 183.442, 'eval_steps_per_second': 11.557, 'epoch': 0.32}
{'loss': 0.9422, 'grad_norm': 0.3102954030036926, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.920120120048523, 'eval_runtime': 5.4451, 'eval_samples_per_second': 183.652, 'eval_steps_per_second': 11.57, 'epoch': 0.36}
{'loss': 0.9046, 'grad_norm': 0.30022722482681274, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9138956665992737, 'eval_runtime': 5.4467, 'eval_samples_per_second': 183.598, 'eval_steps_per_second': 11.567, 'epoch': 0.4}
{'loss': 0.9833, 'grad_norm': 0.30277490615844727, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9120607972145081, 'eval_runtime': 5.4683, 'eval_samples_per_second': 182.873, 'eval_steps_per_second': 11.521, 'epoch': 0.44}
{'loss': 0.8942, 'grad_norm': 0.3084560036659241, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9105277061462402, 'eval_runtime': 5.4707, 'eval_samples_per_second': 182.791, 'eval_steps_per_second': 11.516, 'epoch': 0.48}
{'loss': 0.9058, 'grad_norm': 0.2704246938228607, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9111878871917725, 'eval_runtime': 5.4747, 'eval_samples_per_second': 182.659, 'eval_steps_per_second': 11.508, 'epoch': 0.52}
{'loss': 0.8652, 'grad_norm': 0.3044726252555847, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8994629383087158, 'eval_runtime': 5.4953, 'eval_samples_per_second': 181.974, 'eval_steps_per_second': 11.464, 'epoch': 0.56}
{'loss': 0.8971, 'grad_norm': 0.31196045875549316, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8917133212089539, 'eval_runtime': 5.508, 'eval_samples_per_second': 181.555, 'eval_steps_per_second': 11.438, 'epoch': 0.6}
{'loss': 0.903, 'grad_norm': 0.359943151473999, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8985998630523682, 'eval_runtime': 5.5191, 'eval_samples_per_second': 181.189, 'eval_steps_per_second': 11.415, 'epoch': 0.64}
{'loss': 0.8883, 'grad_norm': 0.2910909652709961, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8984876871109009, 'eval_runtime': 5.5236, 'eval_samples_per_second': 181.041, 'eval_steps_per_second': 11.406, 'epoch': 0.68}
{'loss': 0.978, 'grad_norm': 0.2821730375289917, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8949646353721619, 'eval_runtime': 5.5345, 'eval_samples_per_second': 180.685, 'eval_steps_per_second': 11.383, 'epoch': 0.72}
{'loss': 0.8379, 'grad_norm': 0.3542884588241577, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.892177402973175, 'eval_runtime': 5.5216, 'eval_samples_per_second': 181.106, 'eval_steps_per_second': 11.41, 'epoch': 0.76}
{'loss': 0.8742, 'grad_norm': 0.28797847032546997, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8917922377586365, 'eval_runtime': 5.5447, 'eval_samples_per_second': 180.352, 'eval_steps_per_second': 11.362, 'epoch': 0.8}
{'loss': 0.8213, 'grad_norm': 0.37103334069252014, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8906386494636536, 'eval_runtime': 5.5311, 'eval_samples_per_second': 180.797, 'eval_steps_per_second': 11.39, 'epoch': 0.84}
{'loss': 0.8508, 'grad_norm': 0.2673603594303131, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8904376029968262, 'eval_runtime': 5.5366, 'eval_samples_per_second': 180.617, 'eval_steps_per_second': 11.379, 'epoch': 0.88}
{'loss': 0.8338, 'grad_norm': 0.4133446514606476, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8863239288330078, 'eval_runtime': 5.5492, 'eval_samples_per_second': 180.205, 'eval_steps_per_second': 11.353, 'epoch': 0.92}
{'loss': 0.8838, 'grad_norm': 0.364938884973526, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8857269287109375, 'eval_runtime': 5.5383, 'eval_samples_per_second': 180.561, 'eval_steps_per_second': 11.375, 'epoch': 0.96}
{'loss': 0.8647, 'grad_norm': 0.3392321467399597, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8854354023933411, 'eval_runtime': 5.546, 'eval_samples_per_second': 180.309, 'eval_steps_per_second': 11.359, 'epoch': 1.0}
{'train_runtime': 376.1525, 'train_samples_per_second': 26.574, 'train_steps_per_second': 1.662, 'train_loss': 1.0011389892578124, 'epoch': 1.0}
train_results:  {'eval_loss': [1.481226921081543, 1.1588870286941528, 1.0400807857513428, 1.0011844635009766, 0.9355860948562622, 0.936859130859375, 0.9328967332839966, 0.9210047721862793, 0.920120120048523, 0.9138956665992737, 0.9120607972145081, 0.9105277061462402, 0.9111878871917725, 0.8994629383087158, 0.8917133212089539, 0.8985998630523682, 0.8984876871109009, 0.8949646353721619, 0.892177402973175, 0.8917922377586365, 0.8906386494636536, 0.8904376029968262, 0.8863239288330078, 0.8857269287109375, 0.8854354023933411], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.481226921081543, 1.1588870286941528, 1.0400807857513428, 1.0011844635009766, 0.9355860948562622, 0.936859130859375, 0.9328967332839966, 0.9210047721862793, 0.920120120048523, 0.9138956665992737, 0.9120607972145081, 0.9105277061462402, 0.9111878871917725, 0.8994629383087158, 0.8917133212089539, 0.8985998630523682, 0.8984876871109009, 0.8949646353721619, 0.892177402973175, 0.8917922377586365, 0.8906386494636536, 0.8904376029968262, 0.8863239288330078, 0.8857269287109375, 0.8854354023933411]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.053370475769043
current iteration best possible eval_loss (full train run):  -0.8854354023933411
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.7760 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.8981085419654846, 0.3290713429450989, 0.8147203326225281, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.710469663143158, 0.5243701934814453, 0.3434617817401886, 0.006528973579406738, 0.11192161589860916, 0.21730327606201172]  ‚Üí  acq = -0.9073035434945212
X = [0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4398396611213684, 0.49664074182510376, 0.11438293755054474, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.07886964082717896, 0.25031810998916626, 0.8195444941520691, 0.7197057604789734, 0.2722628116607666, 0.20842111110687256]  ‚Üí  acq = -0.907220297945503
X = [0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.37660378217697144, 0.07552939653396606, 0.3019024431705475, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.931312084197998, 0.1334356665611267, 0.15485918521881104, 0.5709797143936157, 0.6226682662963867, 0.9664523601531982]  ‚Üí  acq = -0.90721837445727
X = [0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.10048657655715942, 0.28943711519241333, 0.07310955226421356, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8318466544151306, 0.620255172252655, 0.11587437987327576, 0.9232467412948608, 0.3529142141342163, 0.6604408025741577]  ‚Üí  acq = -0.9084873704192287
X = [0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6771580576896667, 0.5186182260513306, 0.29587042331695557, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.1786932349205017, 0.0007928609848022461, 0.8421242237091064, 0.3439427614212036, 0.7353686094284058, 0.32386642694473267]  ‚Üí  acq = -0.9068295794651264
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2541, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), 0, tensor(0.1197, dtype=torch.float64), tensor(0.2554, dtype=torch.float64), tensor(0.1691, dtype=torch.float64), 0, 0, tensor(0.1785, dtype=torch.float64), 20, 0, 1, 1, 1, 0, 103, 0.03067025029176279, 24.890203149902185, 0]
normalized proposed parameters for next round by BO: [tensor(0.2541, dtype=torch.float64), tensor(0.0200, dtype=torch.float64), tensor(1.0985e-18, dtype=torch.float64), tensor(0.1197, dtype=torch.float64), tensor(0.2554, dtype=torch.float64), tensor(0.1691, dtype=torch.float64), tensor(0.0032, dtype=torch.float64), tensor(1.1160e-18, dtype=torch.float64), tensor(0.1785, dtype=torch.float64), tensor(0.6315, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.8011, dtype=torch.float64), tensor(0.3067, dtype=torch.float64), tensor(0.5185, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.254
  gsm8k: 0.02
  rowan_hellaswag: 0
  sciq: 0.12
  triviaqa: 0.255
  truthfulqa_gen: 0.169
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.179

LoRA Parameters:
  lora_r: (103,)
  lora_dropout: (0.03067025029176279,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (24.890203149902185,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  103
lora dropout:  0.03067025029176279
lora alpha:  24.890203149902185
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 86,487,040 || all params: 8,116,748,288 || trainable%: 1.0655
length of training data:  9965
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.5196, 'grad_norm': 1.1214038133621216, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.751106858253479, 'eval_runtime': 5.0573, 'eval_samples_per_second': 197.734, 'eval_steps_per_second': 12.457, 'epoch': 0.04}
{'loss': 1.195, 'grad_norm': 0.23822751641273499, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0232809782028198, 'eval_runtime': 5.0801, 'eval_samples_per_second': 196.845, 'eval_steps_per_second': 12.401, 'epoch': 0.08}
{'loss': 1.0045, 'grad_norm': 0.22059212625026703, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 0.9709067940711975, 'eval_runtime': 5.0679, 'eval_samples_per_second': 197.321, 'eval_steps_per_second': 12.431, 'epoch': 0.12}
{'loss': 0.9486, 'grad_norm': 0.19516858458518982, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.9448879957199097, 'eval_runtime': 5.0746, 'eval_samples_per_second': 197.058, 'eval_steps_per_second': 12.415, 'epoch': 0.16}
{'loss': 0.9274, 'grad_norm': 0.222047820687294, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.93916255235672, 'eval_runtime': 5.0826, 'eval_samples_per_second': 196.748, 'eval_steps_per_second': 12.395, 'epoch': 0.2}
{'loss': 0.9273, 'grad_norm': 0.18070465326309204, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.9333634376525879, 'eval_runtime': 5.0965, 'eval_samples_per_second': 196.215, 'eval_steps_per_second': 12.362, 'epoch': 0.24}
{'loss': 0.9238, 'grad_norm': 0.20551079511642456, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.9186486601829529, 'eval_runtime': 5.1026, 'eval_samples_per_second': 195.979, 'eval_steps_per_second': 12.347, 'epoch': 0.28}
{'loss': 0.9206, 'grad_norm': 0.2250814437866211, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.9125478267669678, 'eval_runtime': 5.0898, 'eval_samples_per_second': 196.47, 'eval_steps_per_second': 12.378, 'epoch': 0.32}
{'loss': 0.8971, 'grad_norm': 0.1954004168510437, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.9041239023208618, 'eval_runtime': 5.0877, 'eval_samples_per_second': 196.553, 'eval_steps_per_second': 12.383, 'epoch': 0.36}
{'loss': 0.9146, 'grad_norm': 0.18300597369670868, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.9052953124046326, 'eval_runtime': 5.0928, 'eval_samples_per_second': 196.357, 'eval_steps_per_second': 12.37, 'epoch': 0.4}
{'loss': 0.8772, 'grad_norm': 0.2162802666425705, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.9073314666748047, 'eval_runtime': 5.0846, 'eval_samples_per_second': 196.671, 'eval_steps_per_second': 12.39, 'epoch': 0.44}
{'loss': 0.8808, 'grad_norm': 0.22007207572460175, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.9048371911048889, 'eval_runtime': 5.0849, 'eval_samples_per_second': 196.661, 'eval_steps_per_second': 12.39, 'epoch': 0.48}
{'loss': 0.865, 'grad_norm': 0.22802823781967163, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.8986945152282715, 'eval_runtime': 5.0901, 'eval_samples_per_second': 196.461, 'eval_steps_per_second': 12.377, 'epoch': 0.52}
{'loss': 0.8766, 'grad_norm': 0.21318453550338745, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.8957188725471497, 'eval_runtime': 5.0886, 'eval_samples_per_second': 196.517, 'eval_steps_per_second': 12.381, 'epoch': 0.56}
{'loss': 0.8539, 'grad_norm': 0.22280815243721008, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.8913975954055786, 'eval_runtime': 5.0907, 'eval_samples_per_second': 196.435, 'eval_steps_per_second': 12.375, 'epoch': 0.6}
{'loss': 0.869, 'grad_norm': 0.20877762138843536, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.8912686705589294, 'eval_runtime': 5.0889, 'eval_samples_per_second': 196.504, 'eval_steps_per_second': 12.38, 'epoch': 0.64}
{'loss': 0.8545, 'grad_norm': 0.2185954451560974, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.8861883282661438, 'eval_runtime': 5.0923, 'eval_samples_per_second': 196.373, 'eval_steps_per_second': 12.372, 'epoch': 0.68}
{'loss': 0.8489, 'grad_norm': 0.20485204458236694, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.8872275948524475, 'eval_runtime': 5.1075, 'eval_samples_per_second': 195.79, 'eval_steps_per_second': 12.335, 'epoch': 0.72}
{'loss': 0.8314, 'grad_norm': 0.22927118837833405, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.8838879466056824, 'eval_runtime': 5.1103, 'eval_samples_per_second': 195.683, 'eval_steps_per_second': 12.328, 'epoch': 0.76}
{'loss': 0.8473, 'grad_norm': 0.21821936964988708, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.8845174312591553, 'eval_runtime': 5.1148, 'eval_samples_per_second': 195.512, 'eval_steps_per_second': 12.317, 'epoch': 0.8}
{'loss': 0.8296, 'grad_norm': 0.23599208891391754, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.88032066822052, 'eval_runtime': 5.1242, 'eval_samples_per_second': 195.152, 'eval_steps_per_second': 12.295, 'epoch': 0.84}
{'loss': 0.8257, 'grad_norm': 0.2422938197851181, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.8829870820045471, 'eval_runtime': 5.1182, 'eval_samples_per_second': 195.382, 'eval_steps_per_second': 12.309, 'epoch': 0.88}
{'loss': 0.8468, 'grad_norm': 0.22807122766971588, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.8787543177604675, 'eval_runtime': 5.125, 'eval_samples_per_second': 195.121, 'eval_steps_per_second': 12.293, 'epoch': 0.92}
{'loss': 0.8206, 'grad_norm': 0.2852003276348114, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.8782474398612976, 'eval_runtime': 5.1268, 'eval_samples_per_second': 195.053, 'eval_steps_per_second': 12.288, 'epoch': 0.96}
{'train_runtime': 253.5671, 'train_samples_per_second': 39.299, 'train_steps_per_second': 2.457, 'train_loss': 0.9974628130084631, 'epoch': 1.0}
train_results:  {'eval_loss': [1.751106858253479, 1.0232809782028198, 0.9709067940711975, 0.9448879957199097, 0.93916255235672, 0.9333634376525879, 0.9186486601829529, 0.9125478267669678, 0.9041239023208618, 0.9052953124046326, 0.9073314666748047, 0.9048371911048889, 0.8986945152282715, 0.8957188725471497, 0.8913975954055786, 0.8912686705589294, 0.8861883282661438, 0.8872275948524475, 0.8838879466056824, 0.8845174312591553, 0.88032066822052, 0.8829870820045471, 0.8787543177604675, 0.8782474398612976], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.751106858253479, 1.0232809782028198, 0.9709067940711975, 0.9448879957199097, 0.93916255235672, 0.9333634376525879, 0.9186486601829529, 0.9125478267669678, 0.9041239023208618, 0.9052953124046326, 0.9073314666748047, 0.9048371911048889, 0.8986945152282715, 0.8957188725471497, 0.8913975954055786, 0.8912686705589294, 0.8861883282661438, 0.8872275948524475, 0.8838879466056824, 0.8845174312591553, 0.88032066822052, 0.8829870820045471, 0.8787543177604675, 0.8782474398612976]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1377488374710083
current iteration best possible eval_loss (full train run):  -0.8782474398612976
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.3213 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.007365286350250244, 0.05544036626815796, 0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9870830178260803, 0.754863977432251, 0.9104870557785034, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8787160515785217, 0.16884422302246094, 0.48048388957977295, 0.9369502067565918]  ‚Üí  acq = -0.9397696481980409
X = [0.06936377286911011, 0.632781982421875, 0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7510114908218384, 0.5261915326118469, 0.9568371772766113, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.13144083321094513, 0.18786925077438354, 0.9205564260482788, 0.21238505840301514]  ‚Üí  acq = -0.9393301374092946
X = [0.1816890835762024, 0.5671297311782837, 0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.3449631333351135, 0.8766421675682068, 0.5072073936462402, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.30366525053977966, 0.8914388418197632, 0.9394388198852539, 0.7902622818946838]  ‚Üí  acq = -0.9436853981068302
X = [0.4756810665130615, 0.8949254751205444, 0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.4715011417865753, 0.27662307024002075, 0.1057593822479248, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.19461123645305634, 0.8357580900192261, 0.8589680194854736, 0.6040682792663574]  ‚Üí  acq = -0.9410550169769556
X = [0.14415353536605835, 0.1752747893333435, 0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.6041354537010193, 0.18114352226257324, 0.5152944922447205, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.49458131194114685, 0.7254683971405029, 0.4729866683483124, 0.9238991737365723]  ‚Üí  acq = -0.9394713721212476
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3233, dtype=torch.float64), tensor(0.0583, dtype=torch.float64), tensor(0.0660, dtype=torch.float64), tensor(0.0222, dtype=torch.float64), tensor(0.0967, dtype=torch.float64), tensor(0.0640, dtype=torch.float64), tensor(0.0122, dtype=torch.float64), 0, tensor(0.3543, dtype=torch.float64), 32, 0, 1, 1, 1, 0, 29, 0.011977913276068813, 22.773041631722986, 0]
normalized proposed parameters for next round by BO: [tensor(0.3233, dtype=torch.float64), tensor(0.0583, dtype=torch.float64), tensor(0.0660, dtype=torch.float64), tensor(0.0222, dtype=torch.float64), tensor(0.0967, dtype=torch.float64), tensor(0.0640, dtype=torch.float64), tensor(0.0122, dtype=torch.float64), tensor(0.0029, dtype=torch.float64), tensor(0.3543, dtype=torch.float64), tensor(0.9917, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2253, dtype=torch.float64), tensor(0.1198, dtype=torch.float64), tensor(0.4744, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.323
  gsm8k: 0.058
  rowan_hellaswag: 0.066
  sciq: 0.022
  triviaqa: 0.097
  truthfulqa_gen: 0.064
  wikitext: 0.012
  mmlu: 0
  arc_challenge: 0.354

LoRA Parameters:
  lora_r: (29,)
  lora_dropout: (0.011977913276068813,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (22.773041631722986,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  29
lora dropout:  0.011977913276068813
lora alpha:  22.773041631722986
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 38,961,152 || all params: 8,069,222,400 || trainable%: 0.4828
length of training data:  9968
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9607, 'grad_norm': 2.0325300693511963, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3452221155166626, 'eval_runtime': 5.3443, 'eval_samples_per_second': 187.114, 'eval_steps_per_second': 11.788, 'epoch': 0.04}
{'loss': 1.2269, 'grad_norm': 0.8008610010147095, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9666345715522766, 'eval_runtime': 5.3503, 'eval_samples_per_second': 186.906, 'eval_steps_per_second': 11.775, 'epoch': 0.08}
{'loss': 1.0461, 'grad_norm': 0.40120819211006165, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 0.9185416102409363, 'eval_runtime': 5.3268, 'eval_samples_per_second': 187.729, 'eval_steps_per_second': 11.827, 'epoch': 0.12}
{'loss': 0.9932, 'grad_norm': 0.31555959582328796, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.9131560325622559, 'eval_runtime': 5.3335, 'eval_samples_per_second': 187.496, 'eval_steps_per_second': 11.812, 'epoch': 0.16}
{'loss': 1.0301, 'grad_norm': 0.37188920378685, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.9062482118606567, 'eval_runtime': 5.3394, 'eval_samples_per_second': 187.287, 'eval_steps_per_second': 11.799, 'epoch': 0.2}
{'loss': 0.9455, 'grad_norm': 0.35176369547843933, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.8947286605834961, 'eval_runtime': 5.3807, 'eval_samples_per_second': 185.849, 'eval_steps_per_second': 11.708, 'epoch': 0.24}
{'loss': 0.9837, 'grad_norm': 0.37398403882980347, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.8852790594100952, 'eval_runtime': 5.3827, 'eval_samples_per_second': 185.782, 'eval_steps_per_second': 11.704, 'epoch': 0.28}
{'loss': 1.0151, 'grad_norm': 0.3192504048347473, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.8831451535224915, 'eval_runtime': 5.3613, 'eval_samples_per_second': 186.523, 'eval_steps_per_second': 11.751, 'epoch': 0.32}
{'loss': 1.0065, 'grad_norm': 0.35850656032562256, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.8817638158798218, 'eval_runtime': 5.3469, 'eval_samples_per_second': 187.024, 'eval_steps_per_second': 11.783, 'epoch': 0.36}
{'loss': 0.8993, 'grad_norm': 0.3575974404811859, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.8718990683555603, 'eval_runtime': 5.3548, 'eval_samples_per_second': 186.747, 'eval_steps_per_second': 11.765, 'epoch': 0.4}
{'loss': 0.9314, 'grad_norm': 0.3748713433742523, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.8693573474884033, 'eval_runtime': 5.345, 'eval_samples_per_second': 187.092, 'eval_steps_per_second': 11.787, 'epoch': 0.44}
{'loss': 0.8993, 'grad_norm': 0.4719164967536926, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.8686671853065491, 'eval_runtime': 5.3493, 'eval_samples_per_second': 186.941, 'eval_steps_per_second': 11.777, 'epoch': 0.48}
{'loss': 0.9562, 'grad_norm': 0.37633317708969116, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.867952287197113, 'eval_runtime': 5.3504, 'eval_samples_per_second': 186.9, 'eval_steps_per_second': 11.775, 'epoch': 0.52}
{'loss': 0.8252, 'grad_norm': 0.36824119091033936, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.863709032535553, 'eval_runtime': 5.353, 'eval_samples_per_second': 186.812, 'eval_steps_per_second': 11.769, 'epoch': 0.56}
{'loss': 0.8894, 'grad_norm': 0.4802224636077881, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.8633620738983154, 'eval_runtime': 5.3595, 'eval_samples_per_second': 186.584, 'eval_steps_per_second': 11.755, 'epoch': 0.6}
{'loss': 0.8831, 'grad_norm': 0.49081364274024963, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.8631157875061035, 'eval_runtime': 5.3681, 'eval_samples_per_second': 186.286, 'eval_steps_per_second': 11.736, 'epoch': 0.64}
{'loss': 0.8956, 'grad_norm': 0.4317547678947449, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.8562968969345093, 'eval_runtime': 5.3708, 'eval_samples_per_second': 186.192, 'eval_steps_per_second': 11.73, 'epoch': 0.68}
{'loss': 0.8529, 'grad_norm': 0.46899619698524475, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.8582009673118591, 'eval_runtime': 5.3807, 'eval_samples_per_second': 185.849, 'eval_steps_per_second': 11.708, 'epoch': 0.72}
{'loss': 0.8258, 'grad_norm': 0.40875446796417236, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.8563544750213623, 'eval_runtime': 5.3713, 'eval_samples_per_second': 186.175, 'eval_steps_per_second': 11.729, 'epoch': 0.76}
{'loss': 0.7995, 'grad_norm': 0.36126208305358887, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.8517613410949707, 'eval_runtime': 5.364, 'eval_samples_per_second': 186.428, 'eval_steps_per_second': 11.745, 'epoch': 0.8}
{'loss': 0.8091, 'grad_norm': 0.509630560874939, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.8521649241447449, 'eval_runtime': 5.3768, 'eval_samples_per_second': 185.985, 'eval_steps_per_second': 11.717, 'epoch': 0.84}
{'loss': 0.8175, 'grad_norm': 0.42204877734184265, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.8495818972587585, 'eval_runtime': 5.3999, 'eval_samples_per_second': 185.189, 'eval_steps_per_second': 11.667, 'epoch': 0.88}
{'loss': 0.8365, 'grad_norm': 0.46772676706314087, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.8493123650550842, 'eval_runtime': 5.3816, 'eval_samples_per_second': 185.819, 'eval_steps_per_second': 11.707, 'epoch': 0.92}
{'loss': 0.8399, 'grad_norm': 0.5990897417068481, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.8490031957626343, 'eval_runtime': 5.3622, 'eval_samples_per_second': 186.492, 'eval_steps_per_second': 11.749, 'epoch': 0.96}
{'train_runtime': 354.4909, 'train_samples_per_second': 28.119, 'train_steps_per_second': 1.757, 'train_loss': 1.0002423251230108, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3452221155166626, 0.9666345715522766, 0.9185416102409363, 0.9131560325622559, 0.9062482118606567, 0.8947286605834961, 0.8852790594100952, 0.8831451535224915, 0.8817638158798218, 0.8718990683555603, 0.8693573474884033, 0.8686671853065491, 0.867952287197113, 0.863709032535553, 0.8633620738983154, 0.8631157875061035, 0.8562968969345093, 0.8582009673118591, 0.8563544750213623, 0.8517613410949707, 0.8521649241447449, 0.8495818972587585, 0.8493123650550842, 0.8490031957626343], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.3452221155166626, 0.9666345715522766, 0.9185416102409363, 0.9131560325622559, 0.9062482118606567, 0.8947286605834961, 0.8852790594100952, 0.8831451535224915, 0.8817638158798218, 0.8718990683555603, 0.8693573474884033, 0.8686671853065491, 0.867952287197113, 0.863709032535553, 0.8633620738983154, 0.8631157875061035, 0.8562968969345093, 0.8582009673118591, 0.8563544750213623, 0.8517613410949707, 0.8521649241447449, 0.8495818972587585, 0.8493123650550842, 0.8490031957626343]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9729693531990051
current iteration best possible eval_loss (full train run):  -0.8490031957626343
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 15.5947 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -0.9654416249804442
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -0.9651309181939934
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -0.9651425648952137
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -0.9651302273170521
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -0.9651305169701434
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2084, dtype=torch.float64), tensor(0.0361, dtype=torch.float64), tensor(0.0682, dtype=torch.float64), 0, tensor(0.0611, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0.0525, dtype=torch.float64), 0, tensor(0.4964, dtype=torch.float64), 21, 0, 1, 1, 1, 0, 94, 2.1639116771163212e-19, 13.280565128719214, 0]
normalized proposed parameters for next round by BO: [tensor(0.2084, dtype=torch.float64), tensor(0.0361, dtype=torch.float64), tensor(0.0682, dtype=torch.float64), tensor(1.1341e-18, dtype=torch.float64), tensor(0.0611, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0.0525, dtype=torch.float64), tensor(0.0071, dtype=torch.float64), tensor(0.4964, dtype=torch.float64), tensor(0.6433, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7331, dtype=torch.float64), tensor(2.1639e-18, dtype=torch.float64), tensor(0.2767, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.208
  gsm8k: 0.036
  rowan_hellaswag: 0.068
  sciq: 0
  triviaqa: 0.061
  truthfulqa_gen: 0.07
  wikitext: 0.052
  mmlu: 0
  arc_challenge: 0.496

LoRA Parameters:
  lora_r: (94,)
  lora_dropout: (2.1639116771163212e-19,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (13.280565128719214,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  94
lora dropout:  2.1639116771163212e-19
lora alpha:  13.280565128719214
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 82,876,416 || all params: 8,113,137,664 || trainable%: 1.0215
length of training data:  9926
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3887, 'grad_norm': 0.5803701281547546, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.04327392578125, 'eval_runtime': 4.9325, 'eval_samples_per_second': 202.736, 'eval_steps_per_second': 12.772, 'epoch': 0.04}
{'loss': 1.4218, 'grad_norm': 0.18621115386486053, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0884919166564941, 'eval_runtime': 4.948, 'eval_samples_per_second': 202.103, 'eval_steps_per_second': 12.732, 'epoch': 0.08}
{'loss': 1.2332, 'grad_norm': 0.174655944108963, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9936171770095825, 'eval_runtime': 4.9539, 'eval_samples_per_second': 201.86, 'eval_steps_per_second': 12.717, 'epoch': 0.12}
{'loss': 1.0899, 'grad_norm': 0.14333313703536987, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.9735602736473083, 'eval_runtime': 4.9533, 'eval_samples_per_second': 201.887, 'eval_steps_per_second': 12.719, 'epoch': 0.16}
{'loss': 1.0801, 'grad_norm': 0.15483854711055756, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.9530768990516663, 'eval_runtime': 4.9598, 'eval_samples_per_second': 201.621, 'eval_steps_per_second': 12.702, 'epoch': 0.2}
{'loss': 1.0953, 'grad_norm': 0.16380007565021515, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.9507291913032532, 'eval_runtime': 4.9593, 'eval_samples_per_second': 201.64, 'eval_steps_per_second': 12.703, 'epoch': 0.24}
{'loss': 1.0458, 'grad_norm': 0.16134785115718842, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.9455332159996033, 'eval_runtime': 4.9701, 'eval_samples_per_second': 201.201, 'eval_steps_per_second': 12.676, 'epoch': 0.28}
{'loss': 1.0453, 'grad_norm': 0.1603868007659912, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.9433292150497437, 'eval_runtime': 4.9679, 'eval_samples_per_second': 201.294, 'eval_steps_per_second': 12.681, 'epoch': 0.32}
{'loss': 0.983, 'grad_norm': 0.16847871243953705, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.9377845525741577, 'eval_runtime': 4.9816, 'eval_samples_per_second': 200.74, 'eval_steps_per_second': 12.647, 'epoch': 0.36}
{'loss': 0.9835, 'grad_norm': 0.16859662532806396, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.9241327047348022, 'eval_runtime': 4.9786, 'eval_samples_per_second': 200.858, 'eval_steps_per_second': 12.654, 'epoch': 0.4}
{'loss': 0.9815, 'grad_norm': 0.19450032711029053, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.915203332901001, 'eval_runtime': 4.9722, 'eval_samples_per_second': 201.118, 'eval_steps_per_second': 12.67, 'epoch': 0.44}
{'loss': 1.0106, 'grad_norm': 0.19561262428760529, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.9169589877128601, 'eval_runtime': 4.9768, 'eval_samples_per_second': 200.933, 'eval_steps_per_second': 12.659, 'epoch': 0.48}
{'loss': 0.9605, 'grad_norm': 0.19512663781642914, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.9105383157730103, 'eval_runtime': 4.9718, 'eval_samples_per_second': 201.134, 'eval_steps_per_second': 12.671, 'epoch': 0.52}
{'loss': 0.9032, 'grad_norm': 0.215586319565773, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.9092235565185547, 'eval_runtime': 4.9744, 'eval_samples_per_second': 201.029, 'eval_steps_per_second': 12.665, 'epoch': 0.56}
{'loss': 0.9793, 'grad_norm': 0.2659744322299957, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.9108939170837402, 'eval_runtime': 4.9754, 'eval_samples_per_second': 200.988, 'eval_steps_per_second': 12.662, 'epoch': 0.6}
{'loss': 0.9256, 'grad_norm': 0.24966782331466675, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.9098071455955505, 'eval_runtime': 4.9735, 'eval_samples_per_second': 201.064, 'eval_steps_per_second': 12.667, 'epoch': 0.64}
{'loss': 0.9477, 'grad_norm': 0.2872793972492218, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.9021154642105103, 'eval_runtime': 4.9821, 'eval_samples_per_second': 200.717, 'eval_steps_per_second': 12.645, 'epoch': 0.68}
{'loss': 0.9022, 'grad_norm': 0.24374651908874512, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.9032186269760132, 'eval_runtime': 4.9966, 'eval_samples_per_second': 200.134, 'eval_steps_per_second': 12.608, 'epoch': 0.72}
{'loss': 0.9137, 'grad_norm': 0.3225601017475128, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.8997893333435059, 'eval_runtime': 5.0012, 'eval_samples_per_second': 199.951, 'eval_steps_per_second': 12.597, 'epoch': 0.76}
{'loss': 0.8855, 'grad_norm': 0.27451252937316895, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.8998258113861084, 'eval_runtime': 5.0064, 'eval_samples_per_second': 199.745, 'eval_steps_per_second': 12.584, 'epoch': 0.81}
{'loss': 0.8567, 'grad_norm': 0.27783650159835815, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.8973256945610046, 'eval_runtime': 4.9683, 'eval_samples_per_second': 201.274, 'eval_steps_per_second': 12.68, 'epoch': 0.85}
{'loss': 0.8435, 'grad_norm': 0.32973992824554443, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.8977536559104919, 'eval_runtime': 4.9801, 'eval_samples_per_second': 200.799, 'eval_steps_per_second': 12.65, 'epoch': 0.89}
{'loss': 0.8516, 'grad_norm': 0.2883496880531311, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.8982921242713928, 'eval_runtime': 4.9752, 'eval_samples_per_second': 200.998, 'eval_steps_per_second': 12.663, 'epoch': 0.93}
{'loss': 0.7818, 'grad_norm': 0.2940415143966675, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.8960000276565552, 'eval_runtime': 4.9694, 'eval_samples_per_second': 201.231, 'eval_steps_per_second': 12.678, 'epoch': 0.97}
{'train_runtime': 294.3309, 'train_samples_per_second': 33.724, 'train_steps_per_second': 2.11, 'train_loss': 1.0794079576329525, 'epoch': 1.0}
train_results:  {'eval_loss': [2.04327392578125, 1.0884919166564941, 0.9936171770095825, 0.9735602736473083, 0.9530768990516663, 0.9507291913032532, 0.9455332159996033, 0.9433292150497437, 0.9377845525741577, 0.9241327047348022, 0.915203332901001, 0.9169589877128601, 0.9105383157730103, 0.9092235565185547, 0.9108939170837402, 0.9098071455955505, 0.9021154642105103, 0.9032186269760132, 0.8997893333435059, 0.8998258113861084, 0.8973256945610046, 0.8977536559104919, 0.8982921242713928, 0.8960000276565552], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.04327392578125, 1.0884919166564941, 0.9936171770095825, 0.9735602736473083, 0.9530768990516663, 0.9507291913032532, 0.9455332159996033, 0.9433292150497437, 0.9377845525741577, 0.9241327047348022, 0.915203332901001, 0.9169589877128601, 0.9105383157730103, 0.9092235565185547, 0.9108939170837402, 0.9098071455955505, 0.9021154642105103, 0.9032186269760132, 0.8997893333435059, 0.8998258113861084, 0.8973256945610046, 0.8977536559104919, 0.8982921242713928, 0.8960000276565552]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0756399631500244
current iteration best possible eval_loss (full train run):  -0.8960000276565552
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.8661 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9994975924491882, 0.07707124948501587, 0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5594547390937805, 0.1537771224975586, 0.817682683467865, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.14512693881988525, 0.883480966091156, 0.9173967838287354, 0.8848122358322144]  ‚Üí  acq = -0.9654493576008929
X = [0.9514877796173096, 0.9132158160209656, 0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.41754159331321716, 0.28531861305236816, 0.5411667227745056, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.23015734553337097, 0.9975385665893555, 0.2174660712480545, 0.13808739185333252]  ‚Üí  acq = -0.962650749030059
X = [0.39850908517837524, 0.8170029520988464, 0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5720815062522888, 0.12135124206542969, 0.050965309143066406, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8528783321380615, 0.15156877040863037, 0.1581156700849533, 0.8321003913879395]  ‚Üí  acq = -0.9628545056962592
X = [0.28970110416412354, 0.6408820152282715, 0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7433760762214661, 0.9123662114143372, 0.8424274921417236, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5417225956916809, 0.7911179065704346, 0.9902287721633911, 0.020802080631256104]  ‚Üí  acq = -0.9625806642405649
X = [0.5538846254348755, 0.17085957527160645, 0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6919615864753723, 0.1586219072341919, 0.2204926609992981, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.45917418599128723, 0.6860421299934387, 0.8713036775588989, 0.5007997155189514]  ‚Üí  acq = -0.9625834128026167
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2233, dtype=torch.float64), tensor(0.2466, dtype=torch.float64), tensor(0.0230, dtype=torch.float64), 0, tensor(0.0658, dtype=torch.float64), tensor(0.0552, dtype=torch.float64), tensor(0.0135, dtype=torch.float64), tensor(0.0862, dtype=torch.float64), tensor(0.2865, dtype=torch.float64), 29, 1, 1, 1, 0, 1, 17, 0.041637887961377594, 25.471841049248305, 0]
normalized proposed parameters for next round by BO: [tensor(0.2233, dtype=torch.float64), tensor(0.2466, dtype=torch.float64), tensor(0.0230, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0658, dtype=torch.float64), tensor(0.0552, dtype=torch.float64), tensor(0.0135, dtype=torch.float64), tensor(0.0862, dtype=torch.float64), tensor(0.2865, dtype=torch.float64), tensor(0.8927, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1313, dtype=torch.float64), tensor(0.4164, dtype=torch.float64), tensor(0.5307, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.223
  gsm8k: 0.247
  rowan_hellaswag: 0.023
  sciq: 0
  triviaqa: 0.066
  truthfulqa_gen: 0.055
  wikitext: 0.014
  mmlu: 0.086
  arc_challenge: 0.286

LoRA Parameters:
  lora_r: (17,)
  lora_dropout: (0.041637887961377594,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (25.471841049248305,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  17
lora dropout:  0.041637887961377594
lora alpha:  25.471841049248305
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 24,736,768 || all params: 8,054,998,016 || trainable%: 0.3071
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7597, 'grad_norm': 1.0662841796875, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4816391468048096, 'eval_runtime': 5.3226, 'eval_samples_per_second': 187.877, 'eval_steps_per_second': 11.836, 'epoch': 0.04}
{'loss': 1.2788, 'grad_norm': 0.5406418442726135, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1330639123916626, 'eval_runtime': 5.3232, 'eval_samples_per_second': 187.856, 'eval_steps_per_second': 11.835, 'epoch': 0.08}
{'loss': 1.1567, 'grad_norm': 0.39401471614837646, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0291035175323486, 'eval_runtime': 5.3221, 'eval_samples_per_second': 187.896, 'eval_steps_per_second': 11.837, 'epoch': 0.12}
{'loss': 1.0727, 'grad_norm': 0.43758389353752136, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9781768321990967, 'eval_runtime': 5.3473, 'eval_samples_per_second': 187.009, 'eval_steps_per_second': 11.782, 'epoch': 0.16}
{'loss': 0.986, 'grad_norm': 0.37738072872161865, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9175411462783813, 'eval_runtime': 5.3509, 'eval_samples_per_second': 186.885, 'eval_steps_per_second': 11.774, 'epoch': 0.2}
{'loss': 0.9568, 'grad_norm': 0.3454183042049408, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9107245802879333, 'eval_runtime': 5.3945, 'eval_samples_per_second': 185.375, 'eval_steps_per_second': 11.679, 'epoch': 0.24}
{'loss': 0.9828, 'grad_norm': 0.39548560976982117, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9038738012313843, 'eval_runtime': 5.3985, 'eval_samples_per_second': 185.238, 'eval_steps_per_second': 11.67, 'epoch': 0.28}
{'loss': 0.8703, 'grad_norm': 0.40723177790641785, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9025953412055969, 'eval_runtime': 5.3534, 'eval_samples_per_second': 186.799, 'eval_steps_per_second': 11.768, 'epoch': 0.32}
{'loss': 0.9618, 'grad_norm': 0.40534958243370056, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.893438458442688, 'eval_runtime': 5.3848, 'eval_samples_per_second': 185.708, 'eval_steps_per_second': 11.7, 'epoch': 0.36}
{'loss': 0.9188, 'grad_norm': 0.3455917537212372, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8925198912620544, 'eval_runtime': 5.3713, 'eval_samples_per_second': 186.175, 'eval_steps_per_second': 11.729, 'epoch': 0.4}
{'loss': 0.9322, 'grad_norm': 0.38791200518608093, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8901022672653198, 'eval_runtime': 5.3474, 'eval_samples_per_second': 187.006, 'eval_steps_per_second': 11.781, 'epoch': 0.44}
{'loss': 0.8688, 'grad_norm': 0.35500776767730713, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8846384882926941, 'eval_runtime': 5.3554, 'eval_samples_per_second': 186.727, 'eval_steps_per_second': 11.764, 'epoch': 0.48}
{'loss': 0.8718, 'grad_norm': 0.4049747586250305, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8786662220954895, 'eval_runtime': 5.3498, 'eval_samples_per_second': 186.921, 'eval_steps_per_second': 11.776, 'epoch': 0.52}
{'loss': 0.8742, 'grad_norm': 0.43323850631713867, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8785301446914673, 'eval_runtime': 5.3503, 'eval_samples_per_second': 186.906, 'eval_steps_per_second': 11.775, 'epoch': 0.56}
{'loss': 0.9019, 'grad_norm': 0.4300311505794525, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8764715194702148, 'eval_runtime': 5.3556, 'eval_samples_per_second': 186.722, 'eval_steps_per_second': 11.763, 'epoch': 0.6}
{'loss': 0.8466, 'grad_norm': 0.3929518163204193, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8768631815910339, 'eval_runtime': 5.3528, 'eval_samples_per_second': 186.82, 'eval_steps_per_second': 11.77, 'epoch': 0.64}
{'loss': 0.863, 'grad_norm': 0.45569178462028503, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8767639398574829, 'eval_runtime': 5.3511, 'eval_samples_per_second': 186.878, 'eval_steps_per_second': 11.773, 'epoch': 0.68}
{'loss': 0.7932, 'grad_norm': 0.43972229957580566, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.870193600654602, 'eval_runtime': 5.3531, 'eval_samples_per_second': 186.807, 'eval_steps_per_second': 11.769, 'epoch': 0.72}
{'loss': 0.8433, 'grad_norm': 0.5099510550498962, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8722806572914124, 'eval_runtime': 5.3477, 'eval_samples_per_second': 186.998, 'eval_steps_per_second': 11.781, 'epoch': 0.76}
{'loss': 0.8221, 'grad_norm': 0.5758350491523743, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.870915949344635, 'eval_runtime': 5.3518, 'eval_samples_per_second': 186.853, 'eval_steps_per_second': 11.772, 'epoch': 0.8}
{'loss': 0.8722, 'grad_norm': 0.40029776096343994, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8683567047119141, 'eval_runtime': 5.3425, 'eval_samples_per_second': 187.179, 'eval_steps_per_second': 11.792, 'epoch': 0.84}
{'loss': 0.8806, 'grad_norm': 0.46978941559791565, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.868554949760437, 'eval_runtime': 5.356, 'eval_samples_per_second': 186.707, 'eval_steps_per_second': 11.763, 'epoch': 0.88}
{'loss': 0.8345, 'grad_norm': 0.5058150887489319, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8665763139724731, 'eval_runtime': 5.3482, 'eval_samples_per_second': 186.98, 'eval_steps_per_second': 11.78, 'epoch': 0.92}
{'loss': 0.831, 'grad_norm': 0.5886812806129456, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8652100563049316, 'eval_runtime': 5.3445, 'eval_samples_per_second': 187.107, 'eval_steps_per_second': 11.788, 'epoch': 0.96}
{'loss': 0.8032, 'grad_norm': 0.6867861747741699, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8649935722351074, 'eval_runtime': 5.3418, 'eval_samples_per_second': 187.203, 'eval_steps_per_second': 11.794, 'epoch': 1.0}
{'train_runtime': 362.8207, 'train_samples_per_second': 27.551, 'train_steps_per_second': 1.723, 'train_loss': 0.991326821899414, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4816391468048096, 1.1330639123916626, 1.0291035175323486, 0.9781768321990967, 0.9175411462783813, 0.9107245802879333, 0.9038738012313843, 0.9025953412055969, 0.893438458442688, 0.8925198912620544, 0.8901022672653198, 0.8846384882926941, 0.8786662220954895, 0.8785301446914673, 0.8764715194702148, 0.8768631815910339, 0.8767639398574829, 0.870193600654602, 0.8722806572914124, 0.870915949344635, 0.8683567047119141, 0.868554949760437, 0.8665763139724731, 0.8652100563049316, 0.8649935722351074], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4816391468048096, 1.1330639123916626, 1.0291035175323486, 0.9781768321990967, 0.9175411462783813, 0.9107245802879333, 0.9038738012313843, 0.9025953412055969, 0.893438458442688, 0.8925198912620544, 0.8901022672653198, 0.8846384882926941, 0.8786662220954895, 0.8785301446914673, 0.8764715194702148, 0.8768631815910339, 0.8767639398574829, 0.870193600654602, 0.8722806572914124, 0.870915949344635, 0.8683567047119141, 0.868554949760437, 0.8665763139724731, 0.8652100563049316, 0.8649935722351074]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0635651350021362
current iteration best possible eval_loss (full train run):  -0.8649935722351074
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 12.5754 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.9702307518661093
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9705664598284475
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.9743710443884065
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9700300450556275
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.970031761182728
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1406, dtype=torch.float64), tensor(0.1643, dtype=torch.float64), tensor(0.0674, dtype=torch.float64), 0, tensor(0.1356, dtype=torch.float64), tensor(0.1103, dtype=torch.float64), tensor(0.0458, dtype=torch.float64), 0, tensor(0.3285, dtype=torch.float64), 23, 0, 1, 1, 1, 0, 77, 0.03175110524279667, 34.31481369003796, 0]
normalized proposed parameters for next round by BO: [tensor(0.1406, dtype=torch.float64), tensor(0.1643, dtype=torch.float64), tensor(0.0674, dtype=torch.float64), tensor(3.6843e-19, dtype=torch.float64), tensor(0.1356, dtype=torch.float64), tensor(0.1103, dtype=torch.float64), tensor(0.0458, dtype=torch.float64), tensor(0.0074, dtype=torch.float64), tensor(0.3285, dtype=torch.float64), tensor(0.7271, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6047, dtype=torch.float64), tensor(0.3175, dtype=torch.float64), tensor(0.7149, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.141
  gsm8k: 0.164
  rowan_hellaswag: 0.067
  sciq: 0
  triviaqa: 0.136
  truthfulqa_gen: 0.11
  wikitext: 0.046
  mmlu: 0
  arc_challenge: 0.328

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (0.03175110524279667,)
  num_layers_to_apply: (23,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (34.31481369003796,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  23
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  77
lora dropout:  0.03175110524279667
lora alpha:  34.31481369003796
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 74,353,664 || all params: 8,104,614,912 || trainable%: 0.9174
length of training data:  9921
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8984, 'grad_norm': 0.855090856552124, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3889130353927612, 'eval_runtime': 5.2017, 'eval_samples_per_second': 192.243, 'eval_steps_per_second': 12.111, 'epoch': 0.04}
{'loss': 1.2671, 'grad_norm': 0.35553431510925293, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.027113437652588, 'eval_runtime': 5.1805, 'eval_samples_per_second': 193.033, 'eval_steps_per_second': 12.161, 'epoch': 0.08}
{'loss': 1.1051, 'grad_norm': 0.2969476282596588, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9732204675674438, 'eval_runtime': 5.1844, 'eval_samples_per_second': 192.888, 'eval_steps_per_second': 12.152, 'epoch': 0.12}
{'loss': 1.0382, 'grad_norm': 0.2278018593788147, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.973008930683136, 'eval_runtime': 5.1544, 'eval_samples_per_second': 194.009, 'eval_steps_per_second': 12.223, 'epoch': 0.16}
{'loss': 1.07, 'grad_norm': 0.2529902756214142, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.9497121572494507, 'eval_runtime': 5.1461, 'eval_samples_per_second': 194.322, 'eval_steps_per_second': 12.242, 'epoch': 0.2}
{'loss': 1.0412, 'grad_norm': 0.27109387516975403, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.9365198612213135, 'eval_runtime': 5.1781, 'eval_samples_per_second': 193.12, 'eval_steps_per_second': 12.167, 'epoch': 0.24}
{'loss': 1.0291, 'grad_norm': 0.2796882688999176, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.9239736199378967, 'eval_runtime': 5.155, 'eval_samples_per_second': 193.987, 'eval_steps_per_second': 12.221, 'epoch': 0.28}
{'loss': 1.0455, 'grad_norm': 0.2428705245256424, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.9183393716812134, 'eval_runtime': 5.1578, 'eval_samples_per_second': 193.881, 'eval_steps_per_second': 12.214, 'epoch': 0.32}
{'loss': 1.0424, 'grad_norm': 0.24599240720272064, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.9197480082511902, 'eval_runtime': 5.1696, 'eval_samples_per_second': 193.438, 'eval_steps_per_second': 12.187, 'epoch': 0.36}
{'loss': 0.9941, 'grad_norm': 0.24317651987075806, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.922045111656189, 'eval_runtime': 5.1702, 'eval_samples_per_second': 193.416, 'eval_steps_per_second': 12.185, 'epoch': 0.4}
{'loss': 0.9831, 'grad_norm': 0.26941820979118347, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.9175111055374146, 'eval_runtime': 5.1426, 'eval_samples_per_second': 194.453, 'eval_steps_per_second': 12.251, 'epoch': 0.44}
{'loss': 0.9905, 'grad_norm': 0.2945536971092224, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.9135849475860596, 'eval_runtime': 5.157, 'eval_samples_per_second': 193.911, 'eval_steps_per_second': 12.216, 'epoch': 0.48}
{'loss': 0.9192, 'grad_norm': 0.27533048391342163, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.9097509980201721, 'eval_runtime': 5.1706, 'eval_samples_per_second': 193.401, 'eval_steps_per_second': 12.184, 'epoch': 0.52}
{'loss': 0.9592, 'grad_norm': 0.27444443106651306, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.9149358868598938, 'eval_runtime': 5.1799, 'eval_samples_per_second': 193.053, 'eval_steps_per_second': 12.162, 'epoch': 0.56}
{'loss': 0.9746, 'grad_norm': 0.3398832678794861, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.9050813913345337, 'eval_runtime': 5.2084, 'eval_samples_per_second': 191.998, 'eval_steps_per_second': 12.096, 'epoch': 0.6}
{'loss': 0.8693, 'grad_norm': 0.23579074442386627, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.9013047814369202, 'eval_runtime': 5.1743, 'eval_samples_per_second': 193.263, 'eval_steps_per_second': 12.176, 'epoch': 0.64}
{'loss': 0.9121, 'grad_norm': 0.355016827583313, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.8969241380691528, 'eval_runtime': 5.1696, 'eval_samples_per_second': 193.438, 'eval_steps_per_second': 12.187, 'epoch': 0.68}
{'loss': 0.9206, 'grad_norm': 0.2994401454925537, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.8974013924598694, 'eval_runtime': 5.1797, 'eval_samples_per_second': 193.062, 'eval_steps_per_second': 12.163, 'epoch': 0.72}
{'loss': 0.8834, 'grad_norm': 0.34664058685302734, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.8966379165649414, 'eval_runtime': 5.1845, 'eval_samples_per_second': 192.882, 'eval_steps_per_second': 12.152, 'epoch': 0.76}
{'loss': 0.901, 'grad_norm': 0.32464832067489624, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.8979721665382385, 'eval_runtime': 5.153, 'eval_samples_per_second': 194.062, 'eval_steps_per_second': 12.226, 'epoch': 0.81}
{'loss': 0.9285, 'grad_norm': 0.296708881855011, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.89362633228302, 'eval_runtime': 5.1541, 'eval_samples_per_second': 194.019, 'eval_steps_per_second': 12.223, 'epoch': 0.85}
{'loss': 0.8504, 'grad_norm': 0.33317601680755615, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.8921285271644592, 'eval_runtime': 5.1463, 'eval_samples_per_second': 194.313, 'eval_steps_per_second': 12.242, 'epoch': 0.89}
{'loss': 0.884, 'grad_norm': 0.31624093651771545, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.8924281001091003, 'eval_runtime': 5.1397, 'eval_samples_per_second': 194.562, 'eval_steps_per_second': 12.257, 'epoch': 0.93}
{'loss': 0.8832, 'grad_norm': 0.4033883213996887, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.8905983567237854, 'eval_runtime': 5.1387, 'eval_samples_per_second': 194.602, 'eval_steps_per_second': 12.26, 'epoch': 0.97}
{'train_runtime': 330.7668, 'train_samples_per_second': 29.994, 'train_steps_per_second': 1.877, 'train_loss': 1.0503572964629881, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3889130353927612, 1.027113437652588, 0.9732204675674438, 0.973008930683136, 0.9497121572494507, 0.9365198612213135, 0.9239736199378967, 0.9183393716812134, 0.9197480082511902, 0.922045111656189, 0.9175111055374146, 0.9135849475860596, 0.9097509980201721, 0.9149358868598938, 0.9050813913345337, 0.9013047814369202, 0.8969241380691528, 0.8974013924598694, 0.8966379165649414, 0.8979721665382385, 0.89362633228302, 0.8921285271644592, 0.8924281001091003, 0.8905983567237854], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.3889130353927612, 1.027113437652588, 0.9732204675674438, 0.973008930683136, 0.9497121572494507, 0.9365198612213135, 0.9239736199378967, 0.9183393716812134, 0.9197480082511902, 0.922045111656189, 0.9175111055374146, 0.9135849475860596, 0.9097509980201721, 0.9149358868598938, 0.9050813913345337, 0.9013047814369202, 0.8969241380691528, 0.8974013924598694, 0.8966379165649414, 0.8979721665382385, 0.89362633228302, 0.8921285271644592, 0.8924281001091003, 0.8905983567237854]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1366990804672241
current iteration best possible eval_loss (full train run):  -0.8905983567237854
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5461 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7657198309898376, 0.9410857558250427, 0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.32434797286987305, 0.1894705891609192, 0.7398009896278381, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.25163692235946655, 0.6103376746177673, 0.06646472215652466, 0.1532604694366455]  ‚Üí  acq = -0.9031017570631819
X = [0.9276310801506042, 0.01835566759109497, 0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.4864007830619812, 0.852687656879425, 0.4021565914154053, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7941768169403076, 0.03410828113555908, 0.9558417797088623, 0.6570152640342712]  ‚Üí  acq = -0.9069700353759949
X = [0.3898862600326538, 0.1672157645225525, 0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8220118284225464, 0.09181463718414307, 0.7922564744949341, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8501660823822021, 0.8251820206642151, 0.8375023603439331, 0.5925764441490173]  ‚Üí  acq = -0.9068039745533748
X = [0.7604454159736633, 0.9365105628967285, 0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.14734964072704315, 0.7609574198722839, 0.3722277879714966, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9713605642318726, 0.19831812381744385, 0.38370370864868164, 0.6161256432533264]  ‚Üí  acq = -0.9081913614407718
X = [0.0232393741607666, 0.5331325531005859, 0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8101420402526855, 0.707656979560852, 0.6279451251029968, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.42141979932785034, 0.38733386993408203, 0.770684003829956, 0.287418007850647]  ‚Üí  acq = -0.9069717369326169
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3401, dtype=torch.float64), tensor(0.0548, dtype=torch.float64), tensor(0.0560, dtype=torch.float64), tensor(0.0623, dtype=torch.float64), tensor(0.1083, dtype=torch.float64), 0, tensor(0.0121, dtype=torch.float64), tensor(0.0522, dtype=torch.float64), tensor(0.3084, dtype=torch.float64), 30, 0, 1, 1, 1, 0, 12, 0.0815183629370597, 13.528202502597676, 0]
normalized proposed parameters for next round by BO: [tensor(0.3401, dtype=torch.float64), tensor(0.0548, dtype=torch.float64), tensor(0.0560, dtype=torch.float64), tensor(0.0623, dtype=torch.float64), tensor(0.1083, dtype=torch.float64), tensor(0.0060, dtype=torch.float64), tensor(0.0121, dtype=torch.float64), tensor(0.0522, dtype=torch.float64), tensor(0.3084, dtype=torch.float64), tensor(0.9468, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0959, dtype=torch.float64), tensor(0.8152, dtype=torch.float64), tensor(0.2818, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.34
  gsm8k: 0.055
  rowan_hellaswag: 0.056
  sciq: 0.062
  triviaqa: 0.108
  truthfulqa_gen: 0
  wikitext: 0.012
  mmlu: 0.052
  arc_challenge: 0.308

LoRA Parameters:
  lora_r: (12,)
  lora_dropout: (0.0815183629370597,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (13.528202502597676,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  12
lora dropout:  0.0815183629370597
lora alpha:  13.528202502597676
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 15,114,240 || all params: 8,045,375,488 || trainable%: 0.1879
length of training data:  9936
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2585, 'grad_norm': 1.9502235651016235, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7130882740020752, 'eval_runtime': 5.0902, 'eval_samples_per_second': 196.456, 'eval_steps_per_second': 12.377, 'epoch': 0.04}
{'loss': 1.3979, 'grad_norm': 0.7817786335945129, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0132019519805908, 'eval_runtime': 5.0907, 'eval_samples_per_second': 196.436, 'eval_steps_per_second': 12.375, 'epoch': 0.08}
{'loss': 1.1611, 'grad_norm': 0.4881855547428131, 'learning_rate': 0.00028739054290718036, 'epoch': 0.12}
{'eval_loss': 0.9245817065238953, 'eval_runtime': 5.1085, 'eval_samples_per_second': 195.753, 'eval_steps_per_second': 12.332, 'epoch': 0.12}
{'loss': 1.0685, 'grad_norm': 0.3951011300086975, 'learning_rate': 0.0002742556917688266, 'epoch': 0.16}
{'eval_loss': 0.9161971211433411, 'eval_runtime': 5.0904, 'eval_samples_per_second': 196.448, 'eval_steps_per_second': 12.376, 'epoch': 0.16}
{'loss': 1.1124, 'grad_norm': 0.3870837986469269, 'learning_rate': 0.00026112084063047283, 'epoch': 0.2}
{'eval_loss': 0.9032008051872253, 'eval_runtime': 5.1121, 'eval_samples_per_second': 195.615, 'eval_steps_per_second': 12.324, 'epoch': 0.2}
{'loss': 1.0722, 'grad_norm': 0.33596107363700867, 'learning_rate': 0.00024798598949211904, 'epoch': 0.24}
{'eval_loss': 0.8969981074333191, 'eval_runtime': 5.1037, 'eval_samples_per_second': 195.938, 'eval_steps_per_second': 12.344, 'epoch': 0.24}
{'loss': 0.9989, 'grad_norm': 0.4184059500694275, 'learning_rate': 0.0002348511383537653, 'epoch': 0.28}
{'eval_loss': 0.8908953666687012, 'eval_runtime': 5.1005, 'eval_samples_per_second': 196.057, 'eval_steps_per_second': 12.352, 'epoch': 0.28}
{'loss': 1.0145, 'grad_norm': 0.39508482813835144, 'learning_rate': 0.00022171628721541153, 'epoch': 0.32}
{'eval_loss': 0.888870120048523, 'eval_runtime': 5.1035, 'eval_samples_per_second': 195.946, 'eval_steps_per_second': 12.345, 'epoch': 0.32}
{'loss': 0.9957, 'grad_norm': 0.36306995153427124, 'learning_rate': 0.00020858143607705777, 'epoch': 0.36}
{'eval_loss': 0.8832657933235168, 'eval_runtime': 5.103, 'eval_samples_per_second': 195.964, 'eval_steps_per_second': 12.346, 'epoch': 0.36}
{'loss': 0.9458, 'grad_norm': 0.3513646423816681, 'learning_rate': 0.00019544658493870403, 'epoch': 0.4}
{'eval_loss': 0.8806279301643372, 'eval_runtime': 5.1088, 'eval_samples_per_second': 195.739, 'eval_steps_per_second': 12.332, 'epoch': 0.4}
{'loss': 0.9574, 'grad_norm': 0.3803786635398865, 'learning_rate': 0.00018231173380035023, 'epoch': 0.44}
{'eval_loss': 0.872157096862793, 'eval_runtime': 5.1002, 'eval_samples_per_second': 196.071, 'eval_steps_per_second': 12.352, 'epoch': 0.44}
{'loss': 1.0215, 'grad_norm': 0.430951327085495, 'learning_rate': 0.00016917688266199647, 'epoch': 0.48}
{'eval_loss': 0.874026894569397, 'eval_runtime': 5.1131, 'eval_samples_per_second': 195.576, 'eval_steps_per_second': 12.321, 'epoch': 0.48}
{'loss': 0.9546, 'grad_norm': 0.3846909701824188, 'learning_rate': 0.00015604203152364273, 'epoch': 0.52}
{'eval_loss': 0.870152473449707, 'eval_runtime': 5.1279, 'eval_samples_per_second': 195.012, 'eval_steps_per_second': 12.286, 'epoch': 0.52}
{'loss': 1.0079, 'grad_norm': 0.38760414719581604, 'learning_rate': 0.00014290718038528896, 'epoch': 0.56}
{'eval_loss': 0.8719069361686707, 'eval_runtime': 5.1381, 'eval_samples_per_second': 194.625, 'eval_steps_per_second': 12.261, 'epoch': 0.56}
{'loss': 0.9189, 'grad_norm': 0.4027036428451538, 'learning_rate': 0.0001297723292469352, 'epoch': 0.6}
{'eval_loss': 0.8705433011054993, 'eval_runtime': 5.1334, 'eval_samples_per_second': 194.802, 'eval_steps_per_second': 12.273, 'epoch': 0.6}
{'loss': 0.9507, 'grad_norm': 0.5217306613922119, 'learning_rate': 0.00011663747810858143, 'epoch': 0.64}
{'eval_loss': 0.8669660091400146, 'eval_runtime': 5.1401, 'eval_samples_per_second': 194.547, 'eval_steps_per_second': 12.256, 'epoch': 0.64}
{'loss': 0.9614, 'grad_norm': 0.4558264911174774, 'learning_rate': 0.00010350262697022766, 'epoch': 0.68}
{'eval_loss': 0.864714503288269, 'eval_runtime': 5.1896, 'eval_samples_per_second': 192.693, 'eval_steps_per_second': 12.14, 'epoch': 0.68}
{'loss': 0.9337, 'grad_norm': 0.4578380584716797, 'learning_rate': 9.03677758318739e-05, 'epoch': 0.72}
{'eval_loss': 0.8633034229278564, 'eval_runtime': 5.182, 'eval_samples_per_second': 192.977, 'eval_steps_per_second': 12.158, 'epoch': 0.72}
{'loss': 0.9231, 'grad_norm': 0.5488055944442749, 'learning_rate': 7.723292469352013e-05, 'epoch': 0.76}
{'eval_loss': 0.8637439012527466, 'eval_runtime': 5.18, 'eval_samples_per_second': 193.052, 'eval_steps_per_second': 12.162, 'epoch': 0.76}
{'loss': 0.9662, 'grad_norm': 0.46728745102882385, 'learning_rate': 6.409807355516636e-05, 'epoch': 0.81}
{'eval_loss': 0.8608450293540955, 'eval_runtime': 5.1927, 'eval_samples_per_second': 192.58, 'eval_steps_per_second': 12.133, 'epoch': 0.81}
{'loss': 0.9016, 'grad_norm': 0.5135708451271057, 'learning_rate': 5.096322241681261e-05, 'epoch': 0.85}
{'eval_loss': 0.857552707195282, 'eval_runtime': 5.1967, 'eval_samples_per_second': 192.43, 'eval_steps_per_second': 12.123, 'epoch': 0.85}
{'loss': 0.9078, 'grad_norm': 0.6586117744445801, 'learning_rate': 3.782837127845884e-05, 'epoch': 0.89}
{'eval_loss': 0.8575543165206909, 'eval_runtime': 5.2074, 'eval_samples_per_second': 192.034, 'eval_steps_per_second': 12.098, 'epoch': 0.89}
{'loss': 0.9271, 'grad_norm': 0.5021814107894897, 'learning_rate': 2.4693520140105075e-05, 'epoch': 0.93}
{'eval_loss': 0.855583906173706, 'eval_runtime': 5.1737, 'eval_samples_per_second': 193.283, 'eval_steps_per_second': 12.177, 'epoch': 0.93}
{'loss': 0.8646, 'grad_norm': 0.49887382984161377, 'learning_rate': 1.1558669001751312e-05, 'epoch': 0.97}
{'eval_loss': 0.8555662631988525, 'eval_runtime': 5.1919, 'eval_samples_per_second': 192.609, 'eval_steps_per_second': 12.134, 'epoch': 0.97}
{'train_runtime': 332.4737, 'train_samples_per_second': 29.885, 'train_steps_per_second': 1.868, 'train_loss': 1.0852131743745912, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7130882740020752, 1.0132019519805908, 0.9245817065238953, 0.9161971211433411, 0.9032008051872253, 0.8969981074333191, 0.8908953666687012, 0.888870120048523, 0.8832657933235168, 0.8806279301643372, 0.872157096862793, 0.874026894569397, 0.870152473449707, 0.8719069361686707, 0.8705433011054993, 0.8669660091400146, 0.864714503288269, 0.8633034229278564, 0.8637439012527466, 0.8608450293540955, 0.857552707195282, 0.8575543165206909, 0.855583906173706, 0.8555662631988525], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7130882740020752, 1.0132019519805908, 0.9245817065238953, 0.9161971211433411, 0.9032008051872253, 0.8969981074333191, 0.8908953666687012, 0.888870120048523, 0.8832657933235168, 0.8806279301643372, 0.872157096862793, 0.874026894569397, 0.870152473449707, 0.8719069361686707, 0.8705433011054993, 0.8669660091400146, 0.864714503288269, 0.8633034229278564, 0.8637439012527466, 0.8608450293540955, 0.857552707195282, 0.8575543165206909, 0.855583906173706, 0.8555662631988525]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.939906656742096
current iteration best possible eval_loss (full train run):  -0.8555662631988525
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.1567 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6823287010192871, 0.1862812042236328, 0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8836188316345215, 0.9964625239372253, 0.7934750914573669, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.33229848742485046, 0.9432199001312256, 0.3315914273262024, 0.7924636006355286]  ‚Üí  acq = -0.9491278145337224
X = [0.6971794366836548, 0.0029845833778381348, 0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9298456907272339, 0.9158058762550354, 0.6235672831535339, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5776915550231934, 0.40536046028137207, 0.8601958751678467, 0.07649117708206177]  ‚Üí  acq = -0.9215083648455877
X = [0.7746965289115906, 0.45509761571884155, 0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.08178042620420456, 0.07738137245178223, 0.8753153681755066, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7854319214820862, 0.9972329139709473, 0.8323233127593994, 0.5278875827789307]  ‚Üí  acq = -0.9493990015410648
X = [0.6284062266349792, 0.25792115926742554, 0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.653922438621521, 0.4825098514556885, 0.12023776769638062, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.750758707523346, 0.8871928453445435, 0.8711596727371216, 0.8900622725486755]  ‚Üí  acq = -0.9528664982429014
X = [0.7428956627845764, 0.7939770817756653, 0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.10134205967187881, 0.695570707321167, 0.8957255482673645, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2861731946468353, 0.8315107822418213, 0.7387340068817139, 0.4935872554779053]  ‚Üí  acq = -0.9552075224667658
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2546, dtype=torch.float64), 0, tensor(0.0105, dtype=torch.float64), tensor(0.0330, dtype=torch.float64), tensor(0.0988, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), 0, 0, tensor(0.5786, dtype=torch.float64), 27, 0, 1, 1, 1, 0, 38, 0.0762605739383984, 24.59820174443584, 0]
normalized proposed parameters for next round by BO: [tensor(0.2546, dtype=torch.float64), tensor(7.3871e-17, dtype=torch.float64), tensor(0.0105, dtype=torch.float64), tensor(0.0330, dtype=torch.float64), tensor(0.0988, dtype=torch.float64), tensor(0.0238, dtype=torch.float64), tensor(0.0008, dtype=torch.float64), tensor(2.7861e-17, dtype=torch.float64), tensor(0.5786, dtype=torch.float64), tensor(0.8337, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2977, dtype=torch.float64), tensor(0.7626, dtype=torch.float64), tensor(0.5125, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.255
  gsm8k: 0
  rowan_hellaswag: 0.01
  sciq: 0.033
  triviaqa: 0.099
  truthfulqa_gen: 0.024
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.579

LoRA Parameters:
  lora_r: (38,)
  lora_dropout: (0.0762605739383984,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (24.59820174443584,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  38
lora dropout:  0.0762605739383984
lora alpha:  24.59820174443584
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 43,075,584 || all params: 8,073,336,832 || trainable%: 0.5336
length of training data:  9988
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9937, 'grad_norm': 1.5780571699142456, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.42317533493042, 'eval_runtime': 5.0448, 'eval_samples_per_second': 198.223, 'eval_steps_per_second': 12.488, 'epoch': 0.04}
{'loss': 1.124, 'grad_norm': 0.4369214177131653, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9809886813163757, 'eval_runtime': 5.0569, 'eval_samples_per_second': 197.749, 'eval_steps_per_second': 12.458, 'epoch': 0.08}
{'loss': 0.918, 'grad_norm': 0.36209478974342346, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9350100755691528, 'eval_runtime': 5.0441, 'eval_samples_per_second': 198.252, 'eval_steps_per_second': 12.49, 'epoch': 0.12}
{'loss': 0.8937, 'grad_norm': 0.30847063660621643, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9226059317588806, 'eval_runtime': 5.0563, 'eval_samples_per_second': 197.774, 'eval_steps_per_second': 12.46, 'epoch': 0.16}
{'loss': 0.8775, 'grad_norm': 0.47149553894996643, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9138686656951904, 'eval_runtime': 5.0579, 'eval_samples_per_second': 197.71, 'eval_steps_per_second': 12.456, 'epoch': 0.2}
{'loss': 0.8392, 'grad_norm': 0.34410330653190613, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9131352305412292, 'eval_runtime': 5.0701, 'eval_samples_per_second': 197.235, 'eval_steps_per_second': 12.426, 'epoch': 0.24}
{'loss': 0.7971, 'grad_norm': 0.3678300678730011, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9007899165153503, 'eval_runtime': 5.0797, 'eval_samples_per_second': 196.863, 'eval_steps_per_second': 12.402, 'epoch': 0.28}
{'loss': 0.7773, 'grad_norm': 0.43890368938446045, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8967302441596985, 'eval_runtime': 5.0736, 'eval_samples_per_second': 197.098, 'eval_steps_per_second': 12.417, 'epoch': 0.32}
{'loss': 0.7802, 'grad_norm': 0.41856324672698975, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8914839625358582, 'eval_runtime': 5.0666, 'eval_samples_per_second': 197.37, 'eval_steps_per_second': 12.434, 'epoch': 0.36}
{'loss': 0.7329, 'grad_norm': 0.4446454644203186, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8881372213363647, 'eval_runtime': 5.0655, 'eval_samples_per_second': 197.412, 'eval_steps_per_second': 12.437, 'epoch': 0.4}
{'loss': 0.682, 'grad_norm': 0.4964134395122528, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8911541700363159, 'eval_runtime': 5.0671, 'eval_samples_per_second': 197.353, 'eval_steps_per_second': 12.433, 'epoch': 0.44}
{'loss': 0.7024, 'grad_norm': 0.4164743423461914, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.886114239692688, 'eval_runtime': 5.0836, 'eval_samples_per_second': 196.713, 'eval_steps_per_second': 12.393, 'epoch': 0.48}
{'loss': 0.6532, 'grad_norm': 0.6296411752700806, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8853902816772461, 'eval_runtime': 5.0659, 'eval_samples_per_second': 197.398, 'eval_steps_per_second': 12.436, 'epoch': 0.52}
{'loss': 0.6214, 'grad_norm': 0.7190059423446655, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8834004998207092, 'eval_runtime': 5.0712, 'eval_samples_per_second': 197.19, 'eval_steps_per_second': 12.423, 'epoch': 0.56}
{'loss': 0.5876, 'grad_norm': 0.5257669687271118, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8778191208839417, 'eval_runtime': 5.0644, 'eval_samples_per_second': 197.457, 'eval_steps_per_second': 12.44, 'epoch': 0.6}
{'loss': 0.5922, 'grad_norm': 0.5560916066169739, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8753769397735596, 'eval_runtime': 5.0503, 'eval_samples_per_second': 198.009, 'eval_steps_per_second': 12.475, 'epoch': 0.64}
{'loss': 0.5717, 'grad_norm': 0.6065452694892883, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8726003766059875, 'eval_runtime': 5.0472, 'eval_samples_per_second': 198.128, 'eval_steps_per_second': 12.482, 'epoch': 0.68}
{'loss': 0.5598, 'grad_norm': 0.5822005271911621, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.870516836643219, 'eval_runtime': 5.0448, 'eval_samples_per_second': 198.226, 'eval_steps_per_second': 12.488, 'epoch': 0.72}
{'loss': 0.5713, 'grad_norm': 0.5071665644645691, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8677466511726379, 'eval_runtime': 5.0523, 'eval_samples_per_second': 197.929, 'eval_steps_per_second': 12.47, 'epoch': 0.76}
{'loss': 0.5126, 'grad_norm': 0.41048988699913025, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.870195746421814, 'eval_runtime': 5.0547, 'eval_samples_per_second': 197.835, 'eval_steps_per_second': 12.464, 'epoch': 0.8}
{'loss': 0.4826, 'grad_norm': 0.4648474156856537, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8656287789344788, 'eval_runtime': 5.0496, 'eval_samples_per_second': 198.036, 'eval_steps_per_second': 12.476, 'epoch': 0.84}
{'loss': 0.4867, 'grad_norm': 0.4768526554107666, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8656009435653687, 'eval_runtime': 5.0514, 'eval_samples_per_second': 197.964, 'eval_steps_per_second': 12.472, 'epoch': 0.88}
{'loss': 0.4772, 'grad_norm': 0.6865487694740295, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8660167455673218, 'eval_runtime': 5.0599, 'eval_samples_per_second': 197.631, 'eval_steps_per_second': 12.451, 'epoch': 0.92}
{'loss': 0.4827, 'grad_norm': 0.3462744951248169, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8646384477615356, 'eval_runtime': 5.0737, 'eval_samples_per_second': 197.093, 'eval_steps_per_second': 12.417, 'epoch': 0.96}
{'loss': 0.5358, 'grad_norm': 0.8263363838195801, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8643702268600464, 'eval_runtime': 5.0774, 'eval_samples_per_second': 196.951, 'eval_steps_per_second': 12.408, 'epoch': 1.0}
{'train_runtime': 283.6525, 'train_samples_per_second': 35.212, 'train_steps_per_second': 2.203, 'train_loss': 0.7701067825317383, 'epoch': 1.0}
train_results:  {'eval_loss': [1.42317533493042, 0.9809886813163757, 0.9350100755691528, 0.9226059317588806, 0.9138686656951904, 0.9131352305412292, 0.9007899165153503, 0.8967302441596985, 0.8914839625358582, 0.8881372213363647, 0.8911541700363159, 0.886114239692688, 0.8853902816772461, 0.8834004998207092, 0.8778191208839417, 0.8753769397735596, 0.8726003766059875, 0.870516836643219, 0.8677466511726379, 0.870195746421814, 0.8656287789344788, 0.8656009435653687, 0.8660167455673218, 0.8646384477615356, 0.8643702268600464], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.42317533493042, 0.9809886813163757, 0.9350100755691528, 0.9226059317588806, 0.9138686656951904, 0.9131352305412292, 0.9007899165153503, 0.8967302441596985, 0.8914839625358582, 0.8881372213363647, 0.8911541700363159, 0.886114239692688, 0.8853902816772461, 0.8834004998207092, 0.8778191208839417, 0.8753769397735596, 0.8726003766059875, 0.870516836643219, 0.8677466511726379, 0.870195746421814, 0.8656287789344788, 0.8656009435653687, 0.8660167455673218, 0.8646384477615356, 0.8643702268600464]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9987301826477051
current iteration best possible eval_loss (full train run):  -0.8643702268600464
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.0288 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.32162636518478394, 0.057854533195495605, 0.7804635167121887, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6484256982803345, 0.3577408194541931, 0.8824917078018188, 0.5400984883308411, 0.805059552192688, 0.1653779149055481]  ‚Üí  acq = -0.9525274132903371
X = [0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9492530822753906, 0.8898367881774902, 0.32775449752807617, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.22656208276748657, 0.2050917148590088, 0.03848014771938324, 0.8115118741989136, 0.24837057292461395, 0.07812052965164185]  ‚Üí  acq = -0.9525316646423464
X = [0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9890984892845154, 0.6373879313468933, 0.9511483907699585, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6272949576377869, 0.44919973611831665, 0.97850102186203, 0.45290279388427734, 0.3849879801273346, 0.6524207592010498]  ‚Üí  acq = -0.9525938599667338
X = [0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.892720639705658, 0.9170647859573364, 0.6738178133964539, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.857653796672821, 0.164650559425354, 0.13199880719184875, 0.7966952323913574, 0.4646103084087372, 0.6951091885566711]  ‚Üí  acq = -0.9525274063864853
X = [0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.022762298583984375, 0.7526708841323853, 0.2420748919248581, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.08876413106918335, 0.07692551612854004, 0.7160918116569519, 0.04362046718597412, 0.07799573242664337, 0.41990405321121216]  ‚Üí  acq = -0.9797216054059321
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1618, dtype=torch.float64), tensor(0.0701, dtype=torch.float64), tensor(0.0135, dtype=torch.float64), tensor(0.0584, dtype=torch.float64), tensor(0.1719, dtype=torch.float64), tensor(0.1523, dtype=torch.float64), 0, tensor(0.0771, dtype=torch.float64), tensor(0.2945, dtype=torch.float64), 31, 1, 1, 1, 0, 1, 26, 0.07732271774220917, 19.03655897334575, 0]
normalized proposed parameters for next round by BO: [tensor(0.1618, dtype=torch.float64), tensor(0.0701, dtype=torch.float64), tensor(0.0135, dtype=torch.float64), tensor(0.0584, dtype=torch.float64), tensor(0.1719, dtype=torch.float64), tensor(0.1523, dtype=torch.float64), tensor(0.0004, dtype=torch.float64), tensor(0.0771, dtype=torch.float64), tensor(0.2945, dtype=torch.float64), tensor(0.9586, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2031, dtype=torch.float64), tensor(0.7732, dtype=torch.float64), tensor(0.3966, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.162
  gsm8k: 0.07
  rowan_hellaswag: 0.014
  sciq: 0.058
  triviaqa: 0.172
  truthfulqa_gen: 0.152
  wikitext: 0
  mmlu: 0.077
  arc_challenge: 0.294

LoRA Parameters:
  lora_r: (26,)
  lora_dropout: (0.07732271774220917,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (19.03655897334575,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  26
lora dropout:  0.07732271774220917
lora alpha:  19.03655897334575
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 40,441,856 || all params: 8,070,703,104 || trainable%: 0.5011
length of training data:  9992
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0481, 'grad_norm': 0.6560429334640503, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7946912050247192, 'eval_runtime': 5.2779, 'eval_samples_per_second': 189.469, 'eval_steps_per_second': 11.937, 'epoch': 0.04}
{'loss': 1.3878, 'grad_norm': 0.4858068525791168, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.176790475845337, 'eval_runtime': 5.2714, 'eval_samples_per_second': 189.702, 'eval_steps_per_second': 11.951, 'epoch': 0.08}
{'loss': 1.1807, 'grad_norm': 0.37370604276657104, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0591118335723877, 'eval_runtime': 5.2696, 'eval_samples_per_second': 189.767, 'eval_steps_per_second': 11.955, 'epoch': 0.12}
{'loss': 1.0568, 'grad_norm': 0.42732810974121094, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9967431426048279, 'eval_runtime': 5.2827, 'eval_samples_per_second': 189.295, 'eval_steps_per_second': 11.926, 'epoch': 0.16}
{'loss': 0.9962, 'grad_norm': 0.45216694474220276, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9368647933006287, 'eval_runtime': 5.2868, 'eval_samples_per_second': 189.15, 'eval_steps_per_second': 11.916, 'epoch': 0.2}
{'loss': 0.9445, 'grad_norm': 0.3190489113330841, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9236616492271423, 'eval_runtime': 5.2933, 'eval_samples_per_second': 188.919, 'eval_steps_per_second': 11.902, 'epoch': 0.24}
{'loss': 0.9484, 'grad_norm': 0.30477550625801086, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.921282172203064, 'eval_runtime': 5.2923, 'eval_samples_per_second': 188.956, 'eval_steps_per_second': 11.904, 'epoch': 0.28}
{'loss': 0.931, 'grad_norm': 0.36602962017059326, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9039044380187988, 'eval_runtime': 5.293, 'eval_samples_per_second': 188.93, 'eval_steps_per_second': 11.903, 'epoch': 0.32}
{'loss': 0.9127, 'grad_norm': 0.3417542278766632, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9072306156158447, 'eval_runtime': 5.2884, 'eval_samples_per_second': 189.095, 'eval_steps_per_second': 11.913, 'epoch': 0.36}
{'loss': 0.8982, 'grad_norm': 0.3334760367870331, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9023603796958923, 'eval_runtime': 5.2796, 'eval_samples_per_second': 189.41, 'eval_steps_per_second': 11.933, 'epoch': 0.4}
{'loss': 0.9051, 'grad_norm': 0.3140943944454193, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8979498147964478, 'eval_runtime': 5.2725, 'eval_samples_per_second': 189.662, 'eval_steps_per_second': 11.949, 'epoch': 0.44}
{'loss': 0.8353, 'grad_norm': 0.3374118208885193, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9015189409255981, 'eval_runtime': 5.268, 'eval_samples_per_second': 189.827, 'eval_steps_per_second': 11.959, 'epoch': 0.48}
{'loss': 0.8594, 'grad_norm': 0.3456331491470337, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9037944078445435, 'eval_runtime': 5.2684, 'eval_samples_per_second': 189.811, 'eval_steps_per_second': 11.958, 'epoch': 0.52}
{'loss': 0.8878, 'grad_norm': 0.44210973381996155, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8962213397026062, 'eval_runtime': 5.2721, 'eval_samples_per_second': 189.677, 'eval_steps_per_second': 11.95, 'epoch': 0.56}
{'loss': 0.8477, 'grad_norm': 0.43210849165916443, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8906925320625305, 'eval_runtime': 5.2702, 'eval_samples_per_second': 189.747, 'eval_steps_per_second': 11.954, 'epoch': 0.6}
{'loss': 0.8827, 'grad_norm': 0.49002715945243835, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8902501463890076, 'eval_runtime': 5.2663, 'eval_samples_per_second': 189.887, 'eval_steps_per_second': 11.963, 'epoch': 0.64}
{'loss': 0.8366, 'grad_norm': 0.5323971509933472, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8885782361030579, 'eval_runtime': 5.2848, 'eval_samples_per_second': 189.222, 'eval_steps_per_second': 11.921, 'epoch': 0.68}
{'loss': 0.8192, 'grad_norm': 0.48744043707847595, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8834359049797058, 'eval_runtime': 5.2653, 'eval_samples_per_second': 189.923, 'eval_steps_per_second': 11.965, 'epoch': 0.72}
{'loss': 0.8328, 'grad_norm': 0.44310519099235535, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8875452876091003, 'eval_runtime': 5.2612, 'eval_samples_per_second': 190.069, 'eval_steps_per_second': 11.974, 'epoch': 0.76}
{'loss': 0.8442, 'grad_norm': 0.4424147307872772, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8807427883148193, 'eval_runtime': 5.2794, 'eval_samples_per_second': 189.416, 'eval_steps_per_second': 11.933, 'epoch': 0.8}
{'loss': 0.7685, 'grad_norm': 0.4898991584777832, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8835044503211975, 'eval_runtime': 5.2759, 'eval_samples_per_second': 189.541, 'eval_steps_per_second': 11.941, 'epoch': 0.84}
{'loss': 0.7766, 'grad_norm': 0.48349568247795105, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8783493041992188, 'eval_runtime': 5.2775, 'eval_samples_per_second': 189.482, 'eval_steps_per_second': 11.937, 'epoch': 0.88}
{'loss': 0.7712, 'grad_norm': 0.4818328022956848, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8833590149879456, 'eval_runtime': 5.2915, 'eval_samples_per_second': 188.984, 'eval_steps_per_second': 11.906, 'epoch': 0.92}
{'loss': 0.7911, 'grad_norm': 0.4278843104839325, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8801787495613098, 'eval_runtime': 5.2938, 'eval_samples_per_second': 188.902, 'eval_steps_per_second': 11.901, 'epoch': 0.96}
{'loss': 0.76, 'grad_norm': 0.5880209803581238, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8780717253684998, 'eval_runtime': 5.2939, 'eval_samples_per_second': 188.896, 'eval_steps_per_second': 11.9, 'epoch': 1.0}
{'train_runtime': 336.428, 'train_samples_per_second': 29.7, 'train_steps_per_second': 1.858, 'train_loss': 0.9889061462402344, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7946912050247192, 1.176790475845337, 1.0591118335723877, 0.9967431426048279, 0.9368647933006287, 0.9236616492271423, 0.921282172203064, 0.9039044380187988, 0.9072306156158447, 0.9023603796958923, 0.8979498147964478, 0.9015189409255981, 0.9037944078445435, 0.8962213397026062, 0.8906925320625305, 0.8902501463890076, 0.8885782361030579, 0.8834359049797058, 0.8875452876091003, 0.8807427883148193, 0.8835044503211975, 0.8783493041992188, 0.8833590149879456, 0.8801787495613098, 0.8780717253684998], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7946912050247192, 1.176790475845337, 1.0591118335723877, 0.9967431426048279, 0.9368647933006287, 0.9236616492271423, 0.921282172203064, 0.9039044380187988, 0.9072306156158447, 0.9023603796958923, 0.8979498147964478, 0.9015189409255981, 0.9037944078445435, 0.8962213397026062, 0.8906925320625305, 0.8902501463890076, 0.8885782361030579, 0.8834359049797058, 0.8875452876091003, 0.8807427883148193, 0.8835044503211975, 0.8783493041992188, 0.8833590149879456, 0.8801787495613098, 0.8780717253684998]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9676089286804199
current iteration best possible eval_loss (full train run):  -0.8780717253684998
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.2771 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.9578397736036843
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.9577562544035241
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.9577331641724803
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.9605594379971107
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.9578241426745518
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3265, dtype=torch.float64), 0, tensor(0.0369, dtype=torch.float64), tensor(0.1102, dtype=torch.float64), tensor(0.1277, dtype=torch.float64), 0, 0, 0, tensor(0.3969, dtype=torch.float64), 27, 0, 1, 1, 1, 0, 44, 0.026190721199171065, 19.517203497739562, 0]
normalized proposed parameters for next round by BO: [tensor(0.3265, dtype=torch.float64), tensor(2.5860e-18, dtype=torch.float64), tensor(0.0369, dtype=torch.float64), tensor(0.1102, dtype=torch.float64), tensor(0.1277, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0019, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3969, dtype=torch.float64), tensor(0.8435, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3416, dtype=torch.float64), tensor(0.2619, dtype=torch.float64), tensor(0.4066, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.327
  gsm8k: 0
  rowan_hellaswag: 0.037
  sciq: 0.11
  triviaqa: 0.128
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.397

LoRA Parameters:
  lora_r: (44,)
  lora_dropout: (0.026190721199171065,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (19.517203497739562,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  44
lora dropout:  0.026190721199171065
lora alpha:  19.517203497739562
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 49,876,992 || all params: 8,080,138,240 || trainable%: 0.6173
length of training data:  9979
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3514, 'grad_norm': 1.4610369205474854, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.543499231338501, 'eval_runtime': 5.033, 'eval_samples_per_second': 198.691, 'eval_steps_per_second': 12.518, 'epoch': 0.04}
{'loss': 1.1993, 'grad_norm': 0.4679499566555023, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9810157418251038, 'eval_runtime': 5.068, 'eval_samples_per_second': 197.318, 'eval_steps_per_second': 12.431, 'epoch': 0.08}
{'loss': 1.0027, 'grad_norm': 0.26863476634025574, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.9321606159210205, 'eval_runtime': 5.0502, 'eval_samples_per_second': 198.013, 'eval_steps_per_second': 12.475, 'epoch': 0.12}
{'loss': 0.9438, 'grad_norm': 0.26243874430656433, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.9200404286384583, 'eval_runtime': 5.0615, 'eval_samples_per_second': 197.57, 'eval_steps_per_second': 12.447, 'epoch': 0.16}
{'loss': 0.991, 'grad_norm': 0.2160857766866684, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.9087101817131042, 'eval_runtime': 5.0701, 'eval_samples_per_second': 197.235, 'eval_steps_per_second': 12.426, 'epoch': 0.2}
{'loss': 0.9483, 'grad_norm': 0.25033673644065857, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9006929397583008, 'eval_runtime': 5.1022, 'eval_samples_per_second': 195.993, 'eval_steps_per_second': 12.348, 'epoch': 0.24}
{'loss': 0.9295, 'grad_norm': 0.24588178098201752, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.8969159126281738, 'eval_runtime': 5.0759, 'eval_samples_per_second': 197.01, 'eval_steps_per_second': 12.412, 'epoch': 0.28}
{'loss': 0.9206, 'grad_norm': 0.25725024938583374, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.8911617994308472, 'eval_runtime': 5.0823, 'eval_samples_per_second': 196.762, 'eval_steps_per_second': 12.396, 'epoch': 0.32}
{'loss': 0.866, 'grad_norm': 0.2722564935684204, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.8824900984764099, 'eval_runtime': 5.0738, 'eval_samples_per_second': 197.091, 'eval_steps_per_second': 12.417, 'epoch': 0.36}
{'loss': 0.9584, 'grad_norm': 0.26737865805625916, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.8812896609306335, 'eval_runtime': 5.0741, 'eval_samples_per_second': 197.08, 'eval_steps_per_second': 12.416, 'epoch': 0.4}
{'loss': 0.8336, 'grad_norm': 0.29062870144844055, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.8802569508552551, 'eval_runtime': 5.0674, 'eval_samples_per_second': 197.339, 'eval_steps_per_second': 12.432, 'epoch': 0.44}
{'loss': 0.8763, 'grad_norm': 0.28994932770729065, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.8772819638252258, 'eval_runtime': 5.0654, 'eval_samples_per_second': 197.416, 'eval_steps_per_second': 12.437, 'epoch': 0.48}
{'loss': 0.8401, 'grad_norm': 0.28227218985557556, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.8722978234291077, 'eval_runtime': 5.069, 'eval_samples_per_second': 197.277, 'eval_steps_per_second': 12.428, 'epoch': 0.52}
{'loss': 0.8345, 'grad_norm': 0.33940190076828003, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.8736241459846497, 'eval_runtime': 5.0639, 'eval_samples_per_second': 197.478, 'eval_steps_per_second': 12.441, 'epoch': 0.56}
{'loss': 0.8355, 'grad_norm': 0.35862213373184204, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.8676271438598633, 'eval_runtime': 5.0584, 'eval_samples_per_second': 197.692, 'eval_steps_per_second': 12.455, 'epoch': 0.6}
{'loss': 0.822, 'grad_norm': 0.3617459237575531, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.8664706945419312, 'eval_runtime': 5.0676, 'eval_samples_per_second': 197.33, 'eval_steps_per_second': 12.432, 'epoch': 0.64}
{'loss': 0.8337, 'grad_norm': 0.3853569030761719, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.8661924004554749, 'eval_runtime': 5.0615, 'eval_samples_per_second': 197.571, 'eval_steps_per_second': 12.447, 'epoch': 0.68}
{'loss': 0.7739, 'grad_norm': 0.44424185156822205, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.8652200698852539, 'eval_runtime': 5.067, 'eval_samples_per_second': 197.356, 'eval_steps_per_second': 12.433, 'epoch': 0.72}
{'loss': 0.8378, 'grad_norm': 0.3849342167377472, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.8641132712364197, 'eval_runtime': 5.0802, 'eval_samples_per_second': 196.842, 'eval_steps_per_second': 12.401, 'epoch': 0.76}
{'loss': 0.7433, 'grad_norm': 0.3342379331588745, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.8617953658103943, 'eval_runtime': 5.0651, 'eval_samples_per_second': 197.43, 'eval_steps_per_second': 12.438, 'epoch': 0.8}
{'loss': 0.7663, 'grad_norm': 0.4603406488895416, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.8589308857917786, 'eval_runtime': 5.0605, 'eval_samples_per_second': 197.61, 'eval_steps_per_second': 12.449, 'epoch': 0.84}
{'loss': 0.7707, 'grad_norm': 0.4498641788959503, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.8599865436553955, 'eval_runtime': 5.0553, 'eval_samples_per_second': 197.813, 'eval_steps_per_second': 12.462, 'epoch': 0.88}
{'loss': 0.7521, 'grad_norm': 0.5090017914772034, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.8574238419532776, 'eval_runtime': 5.0744, 'eval_samples_per_second': 197.068, 'eval_steps_per_second': 12.415, 'epoch': 0.92}
{'loss': 0.7307, 'grad_norm': 0.4168474078178406, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.8555524349212646, 'eval_runtime': 5.0918, 'eval_samples_per_second': 196.394, 'eval_steps_per_second': 12.373, 'epoch': 0.96}
{'train_runtime': 296.3758, 'train_samples_per_second': 33.67, 'train_steps_per_second': 2.105, 'train_loss': 0.9630327805494651, 'epoch': 1.0}
train_results:  {'eval_loss': [1.543499231338501, 0.9810157418251038, 0.9321606159210205, 0.9200404286384583, 0.9087101817131042, 0.9006929397583008, 0.8969159126281738, 0.8911617994308472, 0.8824900984764099, 0.8812896609306335, 0.8802569508552551, 0.8772819638252258, 0.8722978234291077, 0.8736241459846497, 0.8676271438598633, 0.8664706945419312, 0.8661924004554749, 0.8652200698852539, 0.8641132712364197, 0.8617953658103943, 0.8589308857917786, 0.8599865436553955, 0.8574238419532776, 0.8555524349212646], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.543499231338501, 0.9810157418251038, 0.9321606159210205, 0.9200404286384583, 0.9087101817131042, 0.9006929397583008, 0.8969159126281738, 0.8911617994308472, 0.8824900984764099, 0.8812896609306335, 0.8802569508552551, 0.8772819638252258, 0.8722978234291077, 0.8736241459846497, 0.8676271438598633, 0.8664706945419312, 0.8661924004554749, 0.8652200698852539, 0.8641132712364197, 0.8617953658103943, 0.8589308857917786, 0.8599865436553955, 0.8574238419532776, 0.8555524349212646]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.982107937335968
current iteration best possible eval_loss (full train run):  -0.8555524349212646
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199, -0.982107937335968]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 8.0459 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3668936491012573, 0.891264021396637, 0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.8238186836242676, 0.9171960949897766, 0.8835806846618652, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.45111533999443054, 0.2139490246772766, 0.9017742872238159, 0.11642980575561523]  ‚Üí  acq = -0.9623717086345136
X = [0.10986042022705078, 0.20953714847564697, 0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9494266510009766, 0.8764203786849976, 0.9284661412239075, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6602510213851929, 0.18004006147384644, 0.9259414672851562, 0.9654239416122437]  ‚Üí  acq = -0.963941576222585
X = [0.17275136709213257, 0.6849347949028015, 0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.31699109077453613, 0.9090394377708435, 0.2634289264678955, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.03518377244472504, 0.6907520294189453, 0.12004825472831726, 0.8972193002700806]  ‚Üí  acq = -0.968462330360333
X = [0.6180925965309143, 0.16254359483718872, 0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4304628074169159, 0.8458959460258484, 0.5030843019485474, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.715866208076477, 0.002808213233947754, 0.6724920272827148, 0.5655561685562134]  ‚Üí  acq = -0.9623648957774822
X = [0.32551831007003784, 0.7355300188064575, 0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.8032635450363159, 0.7881297469139099, 0.10880237817764282, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.07442650943994522, 0.0027261972427368164, 0.3981722295284271, 0.7018656134605408]  ‚Üí  acq = -0.9622566945949776
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2507, dtype=torch.float64), tensor(0.0171, dtype=torch.float64), tensor(0.0237, dtype=torch.float64), tensor(0.0824, dtype=torch.float64), tensor(0.0577, dtype=torch.float64), tensor(0.0893, dtype=torch.float64), tensor(0.0130, dtype=torch.float64), tensor(0.0480, dtype=torch.float64), tensor(0.4181, dtype=torch.float64), 30, 1, 1, 1, 0, 1, 38, 0.06506273815184536, 29.7933962262941, 0]
normalized proposed parameters for next round by BO: [tensor(0.2507, dtype=torch.float64), tensor(0.0171, dtype=torch.float64), tensor(0.0237, dtype=torch.float64), tensor(0.0824, dtype=torch.float64), tensor(0.0577, dtype=torch.float64), tensor(0.0893, dtype=torch.float64), tensor(0.0130, dtype=torch.float64), tensor(0.0480, dtype=torch.float64), tensor(0.4181, dtype=torch.float64), tensor(0.9328, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2934, dtype=torch.float64), tensor(0.6506, dtype=torch.float64), tensor(0.6207, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.251
  gsm8k: 0.017
  rowan_hellaswag: 0.024
  sciq: 0.082
  triviaqa: 0.058
  truthfulqa_gen: 0.089
  wikitext: 0.013
  mmlu: 0.048
  arc_challenge: 0.418

LoRA Parameters:
  lora_r: (38,)
  lora_dropout: (0.06506273815184536,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (29.7933962262941,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  38
lora dropout:  0.06506273815184536
lora alpha:  29.7933962262941
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 57,200,640 || all params: 8,087,461,888 || trainable%: 0.7073
length of training data:  9993
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0001, 'grad_norm': 0.9561519026756287, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4154108762741089, 'eval_runtime': 5.2599, 'eval_samples_per_second': 190.118, 'eval_steps_per_second': 11.977, 'epoch': 0.04}
{'loss': 1.2813, 'grad_norm': 0.31846824288368225, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1219924688339233, 'eval_runtime': 5.2681, 'eval_samples_per_second': 189.821, 'eval_steps_per_second': 11.959, 'epoch': 0.08}
{'loss': 1.1573, 'grad_norm': 0.40403395891189575, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0074999332427979, 'eval_runtime': 5.272, 'eval_samples_per_second': 189.682, 'eval_steps_per_second': 11.95, 'epoch': 0.12}
{'loss': 1.0128, 'grad_norm': 0.31277358531951904, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9435647130012512, 'eval_runtime': 5.2734, 'eval_samples_per_second': 189.633, 'eval_steps_per_second': 11.947, 'epoch': 0.16}
{'loss': 0.955, 'grad_norm': 0.3823537528514862, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9066716432571411, 'eval_runtime': 5.2705, 'eval_samples_per_second': 189.736, 'eval_steps_per_second': 11.953, 'epoch': 0.2}
{'loss': 0.9025, 'grad_norm': 0.32114285230636597, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9049117565155029, 'eval_runtime': 5.2738, 'eval_samples_per_second': 189.617, 'eval_steps_per_second': 11.946, 'epoch': 0.24}
{'loss': 0.9537, 'grad_norm': 0.38788720965385437, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8991190195083618, 'eval_runtime': 5.2872, 'eval_samples_per_second': 189.136, 'eval_steps_per_second': 11.916, 'epoch': 0.28}
{'loss': 0.8979, 'grad_norm': 0.339770644903183, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.895255446434021, 'eval_runtime': 5.2902, 'eval_samples_per_second': 189.03, 'eval_steps_per_second': 11.909, 'epoch': 0.32}
{'loss': 0.904, 'grad_norm': 0.47117117047309875, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8956064581871033, 'eval_runtime': 5.29, 'eval_samples_per_second': 189.037, 'eval_steps_per_second': 11.909, 'epoch': 0.36}
{'loss': 0.8743, 'grad_norm': 0.385513037443161, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8845195174217224, 'eval_runtime': 5.2878, 'eval_samples_per_second': 189.114, 'eval_steps_per_second': 11.914, 'epoch': 0.4}
{'loss': 0.8059, 'grad_norm': 0.38059425354003906, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8878911733627319, 'eval_runtime': 5.2991, 'eval_samples_per_second': 188.711, 'eval_steps_per_second': 11.889, 'epoch': 0.44}
{'loss': 0.8168, 'grad_norm': 0.4266352951526642, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8788188695907593, 'eval_runtime': 5.2916, 'eval_samples_per_second': 188.977, 'eval_steps_per_second': 11.906, 'epoch': 0.48}
{'loss': 0.7509, 'grad_norm': 0.38594990968704224, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8752567172050476, 'eval_runtime': 5.2894, 'eval_samples_per_second': 189.057, 'eval_steps_per_second': 11.911, 'epoch': 0.52}
{'loss': 0.7851, 'grad_norm': 0.4730338752269745, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8751645088195801, 'eval_runtime': 5.2952, 'eval_samples_per_second': 188.851, 'eval_steps_per_second': 11.898, 'epoch': 0.56}
{'loss': 0.7249, 'grad_norm': 0.4953203797340393, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8702257871627808, 'eval_runtime': 5.2834, 'eval_samples_per_second': 189.272, 'eval_steps_per_second': 11.924, 'epoch': 0.6}
{'loss': 0.7564, 'grad_norm': 0.5258464813232422, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8707704544067383, 'eval_runtime': 5.2696, 'eval_samples_per_second': 189.769, 'eval_steps_per_second': 11.955, 'epoch': 0.64}
{'loss': 0.7176, 'grad_norm': 0.5492975115776062, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8656477332115173, 'eval_runtime': 5.2669, 'eval_samples_per_second': 189.866, 'eval_steps_per_second': 11.962, 'epoch': 0.68}
{'loss': 0.7552, 'grad_norm': 0.4040178656578064, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8646059036254883, 'eval_runtime': 5.2731, 'eval_samples_per_second': 189.64, 'eval_steps_per_second': 11.947, 'epoch': 0.72}
{'loss': 0.7875, 'grad_norm': 0.4247490465641022, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.862259566783905, 'eval_runtime': 5.2683, 'eval_samples_per_second': 189.815, 'eval_steps_per_second': 11.958, 'epoch': 0.76}
{'loss': 0.7266, 'grad_norm': 0.3453899323940277, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8616213202476501, 'eval_runtime': 5.2632, 'eval_samples_per_second': 189.999, 'eval_steps_per_second': 11.97, 'epoch': 0.8}
{'loss': 0.6589, 'grad_norm': 0.46440714597702026, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8598122000694275, 'eval_runtime': 5.2648, 'eval_samples_per_second': 189.942, 'eval_steps_per_second': 11.966, 'epoch': 0.84}
{'loss': 0.6686, 'grad_norm': 0.46230369806289673, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8572201728820801, 'eval_runtime': 5.2674, 'eval_samples_per_second': 189.846, 'eval_steps_per_second': 11.96, 'epoch': 0.88}
{'loss': 0.6333, 'grad_norm': 0.38795265555381775, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8576419949531555, 'eval_runtime': 5.2681, 'eval_samples_per_second': 189.823, 'eval_steps_per_second': 11.959, 'epoch': 0.92}
{'loss': 0.611, 'grad_norm': 0.5384256839752197, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8563432693481445, 'eval_runtime': 5.2703, 'eval_samples_per_second': 189.744, 'eval_steps_per_second': 11.954, 'epoch': 0.96}
{'loss': 0.5771, 'grad_norm': 0.4447772800922394, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8567988276481628, 'eval_runtime': 5.2566, 'eval_samples_per_second': 190.238, 'eval_steps_per_second': 11.985, 'epoch': 1.0}
{'train_runtime': 324.6433, 'train_samples_per_second': 30.781, 'train_steps_per_second': 1.925, 'train_loss': 0.9085924362182617, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4154108762741089, 1.1219924688339233, 1.0074999332427979, 0.9435647130012512, 0.9066716432571411, 0.9049117565155029, 0.8991190195083618, 0.895255446434021, 0.8956064581871033, 0.8845195174217224, 0.8878911733627319, 0.8788188695907593, 0.8752567172050476, 0.8751645088195801, 0.8702257871627808, 0.8707704544067383, 0.8656477332115173, 0.8646059036254883, 0.862259566783905, 0.8616213202476501, 0.8598122000694275, 0.8572201728820801, 0.8576419949531555, 0.8563432693481445, 0.8567988276481628], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4154108762741089, 1.1219924688339233, 1.0074999332427979, 0.9435647130012512, 0.9066716432571411, 0.9049117565155029, 0.8991190195083618, 0.895255446434021, 0.8956064581871033, 0.8845195174217224, 0.8878911733627319, 0.8788188695907593, 0.8752567172050476, 0.8751645088195801, 0.8702257871627808, 0.8707704544067383, 0.8656477332115173, 0.8646059036254883, 0.862259566783905, 0.8616213202476501, 0.8598122000694275, 0.8572201728820801, 0.8576419949531555, 0.8563432693481445, 0.8567988276481628]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0346444845199585
current iteration best possible eval_loss (full train run):  -0.8567988276481628
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199, -0.982107937335968, -1.0346444845199585]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 14.9772 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -0.9942060347708789
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -0.994206145769483
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -0.9941865722868083
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -0.994206034741368
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -0.9942060355090331
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2115, dtype=torch.float64), 0, tensor(0.0490, dtype=torch.float64), tensor(0.1038, dtype=torch.float64), tensor(0.0868, dtype=torch.float64), tensor(0.1406, dtype=torch.float64), tensor(0.0218, dtype=torch.float64), tensor(0.0899, dtype=torch.float64), tensor(0.2967, dtype=torch.float64), 27, 1, 1, 1, 0, 1, 74, 0.06512548986251684, 25.462670447928538, 0]
normalized proposed parameters for next round by BO: [tensor(0.2115, dtype=torch.float64), tensor(4.4467e-18, dtype=torch.float64), tensor(0.0490, dtype=torch.float64), tensor(0.1038, dtype=torch.float64), tensor(0.0868, dtype=torch.float64), tensor(0.1406, dtype=torch.float64), tensor(0.0218, dtype=torch.float64), tensor(0.0899, dtype=torch.float64), tensor(0.2967, dtype=torch.float64), tensor(0.8589, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5768, dtype=torch.float64), tensor(0.6513, dtype=torch.float64), tensor(0.5305, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.212
  gsm8k: 0
  rowan_hellaswag: 0.049
  sciq: 0.104
  triviaqa: 0.087
  truthfulqa_gen: 0.141
  wikitext: 0.022
  mmlu: 0.09
  arc_challenge: 0.297

LoRA Parameters:
  lora_r: (74,)
  lora_dropout: (0.06512548986251684,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (25.462670447928538,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  74
lora dropout:  0.06512548986251684
lora alpha:  25.462670447928538
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 100,251,648 || all params: 8,130,512,896 || trainable%: 1.2330
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2032, 'grad_norm': 0.6867437362670898, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.59879732131958, 'eval_runtime': 5.2225, 'eval_samples_per_second': 191.479, 'eval_steps_per_second': 12.063, 'epoch': 0.04}
{'loss': 1.4069, 'grad_norm': 0.2700536847114563, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1708967685699463, 'eval_runtime': 5.2476, 'eval_samples_per_second': 190.564, 'eval_steps_per_second': 12.006, 'epoch': 0.08}
{'loss': 1.2555, 'grad_norm': 0.3026876449584961, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0761929750442505, 'eval_runtime': 5.246, 'eval_samples_per_second': 190.622, 'eval_steps_per_second': 12.009, 'epoch': 0.12}
{'loss': 1.1651, 'grad_norm': 0.2217598706483841, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9995973110198975, 'eval_runtime': 5.2729, 'eval_samples_per_second': 189.649, 'eval_steps_per_second': 11.948, 'epoch': 0.16}
{'loss': 1.1725, 'grad_norm': 0.21393880248069763, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9277350902557373, 'eval_runtime': 5.2574, 'eval_samples_per_second': 190.208, 'eval_steps_per_second': 11.983, 'epoch': 0.2}
{'loss': 1.0154, 'grad_norm': 0.250527024269104, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9223369359970093, 'eval_runtime': 5.3156, 'eval_samples_per_second': 188.127, 'eval_steps_per_second': 11.852, 'epoch': 0.24}
{'loss': 1.0626, 'grad_norm': 0.2168201357126236, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9173497557640076, 'eval_runtime': 5.318, 'eval_samples_per_second': 188.042, 'eval_steps_per_second': 11.847, 'epoch': 0.28}
{'loss': 1.0071, 'grad_norm': 0.22854971885681152, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9073230028152466, 'eval_runtime': 5.319, 'eval_samples_per_second': 188.004, 'eval_steps_per_second': 11.844, 'epoch': 0.32}
{'loss': 1.0211, 'grad_norm': 0.2315671294927597, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9002126455307007, 'eval_runtime': 5.3177, 'eval_samples_per_second': 188.05, 'eval_steps_per_second': 11.847, 'epoch': 0.36}
{'loss': 0.9659, 'grad_norm': 0.2560186982154846, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9003315567970276, 'eval_runtime': 5.3123, 'eval_samples_per_second': 188.242, 'eval_steps_per_second': 11.859, 'epoch': 0.4}
{'loss': 0.934, 'grad_norm': 0.2506403923034668, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8987483978271484, 'eval_runtime': 5.3242, 'eval_samples_per_second': 187.822, 'eval_steps_per_second': 11.833, 'epoch': 0.44}
{'loss': 0.9428, 'grad_norm': 0.23168149590492249, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.88981693983078, 'eval_runtime': 5.3133, 'eval_samples_per_second': 188.208, 'eval_steps_per_second': 11.857, 'epoch': 0.48}
{'loss': 0.9602, 'grad_norm': 0.24867691099643707, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8862292766571045, 'eval_runtime': 5.3169, 'eval_samples_per_second': 188.081, 'eval_steps_per_second': 11.849, 'epoch': 0.52}
{'loss': 1.0027, 'grad_norm': 0.2436111867427826, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8859026432037354, 'eval_runtime': 5.3247, 'eval_samples_per_second': 187.804, 'eval_steps_per_second': 11.832, 'epoch': 0.56}
{'loss': 0.9567, 'grad_norm': 0.2692786455154419, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8830093145370483, 'eval_runtime': 5.3249, 'eval_samples_per_second': 187.798, 'eval_steps_per_second': 11.831, 'epoch': 0.6}
{'loss': 0.9193, 'grad_norm': 0.338400274515152, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8857988119125366, 'eval_runtime': 5.3321, 'eval_samples_per_second': 187.542, 'eval_steps_per_second': 11.815, 'epoch': 0.64}
{'loss': 0.9633, 'grad_norm': 0.2987630367279053, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8826861381530762, 'eval_runtime': 5.32, 'eval_samples_per_second': 187.968, 'eval_steps_per_second': 11.842, 'epoch': 0.68}
{'loss': 0.9021, 'grad_norm': 0.37842878699302673, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.874700665473938, 'eval_runtime': 5.2745, 'eval_samples_per_second': 189.592, 'eval_steps_per_second': 11.944, 'epoch': 0.72}
{'loss': 0.8919, 'grad_norm': 0.32087305188179016, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8751266002655029, 'eval_runtime': 5.2768, 'eval_samples_per_second': 189.508, 'eval_steps_per_second': 11.939, 'epoch': 0.76}
{'loss': 0.9207, 'grad_norm': 0.3358466327190399, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8779947757720947, 'eval_runtime': 5.2851, 'eval_samples_per_second': 189.212, 'eval_steps_per_second': 11.92, 'epoch': 0.8}
{'loss': 0.9112, 'grad_norm': 0.3967948853969574, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8746506571769714, 'eval_runtime': 5.2771, 'eval_samples_per_second': 189.498, 'eval_steps_per_second': 11.938, 'epoch': 0.84}
{'loss': 0.9334, 'grad_norm': 0.2741338610649109, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8730124235153198, 'eval_runtime': 5.2756, 'eval_samples_per_second': 189.55, 'eval_steps_per_second': 11.942, 'epoch': 0.88}
{'loss': 0.8748, 'grad_norm': 0.4564572274684906, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8708646297454834, 'eval_runtime': 5.2656, 'eval_samples_per_second': 189.91, 'eval_steps_per_second': 11.964, 'epoch': 0.92}
{'loss': 0.8309, 'grad_norm': 0.3391316533088684, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8702665567398071, 'eval_runtime': 5.2693, 'eval_samples_per_second': 189.777, 'eval_steps_per_second': 11.956, 'epoch': 0.96}
{'loss': 0.9452, 'grad_norm': 0.33080729842185974, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8694586753845215, 'eval_runtime': 5.2724, 'eval_samples_per_second': 189.666, 'eval_steps_per_second': 11.949, 'epoch': 1.0}
{'train_runtime': 323.972, 'train_samples_per_second': 30.855, 'train_steps_per_second': 1.929, 'train_loss': 1.0865726135253906, 'epoch': 1.0}
train_results:  {'eval_loss': [1.59879732131958, 1.1708967685699463, 1.0761929750442505, 0.9995973110198975, 0.9277350902557373, 0.9223369359970093, 0.9173497557640076, 0.9073230028152466, 0.9002126455307007, 0.9003315567970276, 0.8987483978271484, 0.88981693983078, 0.8862292766571045, 0.8859026432037354, 0.8830093145370483, 0.8857988119125366, 0.8826861381530762, 0.874700665473938, 0.8751266002655029, 0.8779947757720947, 0.8746506571769714, 0.8730124235153198, 0.8708646297454834, 0.8702665567398071, 0.8694586753845215], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.59879732131958, 1.1708967685699463, 1.0761929750442505, 0.9995973110198975, 0.9277350902557373, 0.9223369359970093, 0.9173497557640076, 0.9073230028152466, 0.9002126455307007, 0.9003315567970276, 0.8987483978271484, 0.88981693983078, 0.8862292766571045, 0.8859026432037354, 0.8830093145370483, 0.8857988119125366, 0.8826861381530762, 0.874700665473938, 0.8751266002655029, 0.8779947757720947, 0.8746506571769714, 0.8730124235153198, 0.8708646297454834, 0.8702665567398071, 0.8694586753845215]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0614299774169922
current iteration best possible eval_loss (full train run):  -0.8694586753845215
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199, -0.982107937335968, -1.0346444845199585, -1.0614299774169922]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 10.2829 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.2951089143753052, 0.03446310758590698, 0.8156396746635437, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5905086398124695, 0.24681228399276733, 0.3105791509151459, 0.43591630458831787, 0.09187254309654236, 0.4111078381538391]  ‚Üí  acq = -0.9211687536660771
X = [0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.24910348653793335, 0.1305062174797058, 0.756322979927063, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2366807460784912, 0.26675117015838623, 0.7178916931152344, 0.1087694764137268, 0.77919602394104, 0.36252284049987793]  ‚Üí  acq = -0.9222437731497352
X = [0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8292636871337891, 0.2568463683128357, 0.6638046503067017, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7841399908065796, 0.044145405292510986, 0.16194474697113037, 0.9866487383842468, 0.5886383056640625, 0.49270403385162354]  ‚Üí  acq = -0.9202295254101326
X = [0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2230069637298584, 0.3171401023864746, 0.8722177743911743, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6486951112747192, 0.34602898359298706, 0.024289045482873917, 0.7405623197555542, 0.7914584875106812, 0.7650286555290222]  ‚Üí  acq = -0.9202292146764959
X = [0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.25201165676116943, 0.7089584469795227, 0.6091451644897461, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.8982568979263306, 0.3606852889060974, 0.7456476092338562, 0.591159462928772, 0.9080792665481567, 0.865877628326416]  ‚Üí  acq = -0.9203019130584085
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1384, dtype=torch.float64), tensor(0.0733, dtype=torch.float64), tensor(0.0366, dtype=torch.float64), 0, tensor(0.2052, dtype=torch.float64), tensor(0.0891, dtype=torch.float64), 0, 0, tensor(0.4442, dtype=torch.float64), 28, 0, 1, 1, 1, 0, 91, 0.0823522268851013, 5.705668760349102, 0]
normalized proposed parameters for next round by BO: [tensor(0.1384, dtype=torch.float64), tensor(0.0733, dtype=torch.float64), tensor(0.0366, dtype=torch.float64), tensor(4.5203e-20, dtype=torch.float64), tensor(0.2052, dtype=torch.float64), tensor(0.0891, dtype=torch.float64), tensor(0.0068, dtype=torch.float64), tensor(0.0063, dtype=torch.float64), tensor(0.4442, dtype=torch.float64), tensor(0.8765, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7099, dtype=torch.float64), tensor(0.8235, dtype=torch.float64), tensor(0.1189, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.138
  gsm8k: 0.073
  rowan_hellaswag: 0.037
  sciq: 0
  triviaqa: 0.205
  truthfulqa_gen: 0.089
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.444

LoRA Parameters:
  lora_r: (91,)
  lora_dropout: (0.0823522268851013,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (5.705668760349102,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  91
lora dropout:  0.0823522268851013
lora alpha:  5.705668760349102
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 106,975,232 || all params: 8,137,236,480 || trainable%: 1.3146
length of training data:  9866
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4877, 'grad_norm': 0.321913480758667, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4379496574401855, 'eval_runtime': 5.3197, 'eval_samples_per_second': 187.981, 'eval_steps_per_second': 11.843, 'epoch': 0.04}
{'loss': 1.5203, 'grad_norm': 0.13562026619911194, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.106984257698059, 'eval_runtime': 5.3206, 'eval_samples_per_second': 187.95, 'eval_steps_per_second': 11.841, 'epoch': 0.08}
{'loss': 1.1351, 'grad_norm': 0.13336722552776337, 'learning_rate': 0.00028730158730158725, 'epoch': 0.12}
{'eval_loss': 1.0045710802078247, 'eval_runtime': 5.3177, 'eval_samples_per_second': 188.051, 'eval_steps_per_second': 11.847, 'epoch': 0.12}
{'loss': 1.0471, 'grad_norm': 0.11249924451112747, 'learning_rate': 0.00027407407407407404, 'epoch': 0.16}
{'eval_loss': 0.9827277660369873, 'eval_runtime': 5.3185, 'eval_samples_per_second': 188.024, 'eval_steps_per_second': 11.846, 'epoch': 0.16}
{'loss': 1.0012, 'grad_norm': 0.09662855416536331, 'learning_rate': 0.00026084656084656083, 'epoch': 0.2}
{'eval_loss': 0.954525887966156, 'eval_runtime': 5.3229, 'eval_samples_per_second': 187.867, 'eval_steps_per_second': 11.836, 'epoch': 0.2}
{'loss': 0.9511, 'grad_norm': 0.1093287393450737, 'learning_rate': 0.00024761904761904757, 'epoch': 0.24}
{'eval_loss': 0.9473373889923096, 'eval_runtime': 5.323, 'eval_samples_per_second': 187.862, 'eval_steps_per_second': 11.835, 'epoch': 0.24}
{'loss': 0.9112, 'grad_norm': 0.10618160665035248, 'learning_rate': 0.0002343915343915344, 'epoch': 0.28}
{'eval_loss': 0.9576842188835144, 'eval_runtime': 5.3241, 'eval_samples_per_second': 187.825, 'eval_steps_per_second': 11.833, 'epoch': 0.28}
{'loss': 0.9449, 'grad_norm': 0.11256837099790573, 'learning_rate': 0.00022116402116402112, 'epoch': 0.32}
{'eval_loss': 0.9295133352279663, 'eval_runtime': 5.3268, 'eval_samples_per_second': 187.729, 'eval_steps_per_second': 11.827, 'epoch': 0.32}
{'loss': 0.9625, 'grad_norm': 0.128647118806839, 'learning_rate': 0.00020793650793650791, 'epoch': 0.36}
{'eval_loss': 0.9284226894378662, 'eval_runtime': 5.3266, 'eval_samples_per_second': 187.737, 'eval_steps_per_second': 11.827, 'epoch': 0.36}
{'loss': 0.9253, 'grad_norm': 0.11529859900474548, 'learning_rate': 0.00019470899470899468, 'epoch': 0.41}
{'eval_loss': 0.9285083413124084, 'eval_runtime': 5.3241, 'eval_samples_per_second': 187.825, 'eval_steps_per_second': 11.833, 'epoch': 0.41}
{'loss': 0.8876, 'grad_norm': 0.13353312015533447, 'learning_rate': 0.00018148148148148147, 'epoch': 0.45}
{'eval_loss': 0.9252845644950867, 'eval_runtime': 5.3359, 'eval_samples_per_second': 187.41, 'eval_steps_per_second': 11.807, 'epoch': 0.45}
{'loss': 0.9546, 'grad_norm': 0.12353312969207764, 'learning_rate': 0.00016825396825396823, 'epoch': 0.49}
{'eval_loss': 0.9257598519325256, 'eval_runtime': 5.3253, 'eval_samples_per_second': 187.782, 'eval_steps_per_second': 11.83, 'epoch': 0.49}
{'loss': 0.9417, 'grad_norm': 0.12248984724283218, 'learning_rate': 0.00015502645502645502, 'epoch': 0.53}
{'eval_loss': 0.9292643666267395, 'eval_runtime': 5.3278, 'eval_samples_per_second': 187.694, 'eval_steps_per_second': 11.825, 'epoch': 0.53}
{'loss': 0.8891, 'grad_norm': 0.12573978304862976, 'learning_rate': 0.00014179894179894179, 'epoch': 0.57}
{'eval_loss': 0.9217687845230103, 'eval_runtime': 5.3241, 'eval_samples_per_second': 187.824, 'eval_steps_per_second': 11.833, 'epoch': 0.57}
{'loss': 0.8486, 'grad_norm': 0.16888655722141266, 'learning_rate': 0.00012857142857142855, 'epoch': 0.61}
{'eval_loss': 0.9106135964393616, 'eval_runtime': 5.3217, 'eval_samples_per_second': 187.908, 'eval_steps_per_second': 11.838, 'epoch': 0.61}
{'loss': 0.861, 'grad_norm': 0.1380956918001175, 'learning_rate': 0.00011534391534391533, 'epoch': 0.65}
{'eval_loss': 0.9222375750541687, 'eval_runtime': 5.3254, 'eval_samples_per_second': 187.781, 'eval_steps_per_second': 11.83, 'epoch': 0.65}
{'loss': 0.8133, 'grad_norm': 0.18153536319732666, 'learning_rate': 0.0001021164021164021, 'epoch': 0.69}
{'eval_loss': 0.9084914922714233, 'eval_runtime': 5.3311, 'eval_samples_per_second': 187.58, 'eval_steps_per_second': 11.818, 'epoch': 0.69}
{'loss': 0.8692, 'grad_norm': 0.19871997833251953, 'learning_rate': 8.888888888888888e-05, 'epoch': 0.73}
{'eval_loss': 0.9118963479995728, 'eval_runtime': 5.3688, 'eval_samples_per_second': 186.261, 'eval_steps_per_second': 11.734, 'epoch': 0.73}
{'loss': 0.8785, 'grad_norm': 0.1306285709142685, 'learning_rate': 7.566137566137566e-05, 'epoch': 0.77}
{'eval_loss': 0.9058831334114075, 'eval_runtime': 5.3816, 'eval_samples_per_second': 185.82, 'eval_steps_per_second': 11.707, 'epoch': 0.77}
{'loss': 0.8391, 'grad_norm': 0.185797318816185, 'learning_rate': 6.243386243386242e-05, 'epoch': 0.81}
{'eval_loss': 0.9057208299636841, 'eval_runtime': 5.3653, 'eval_samples_per_second': 186.385, 'eval_steps_per_second': 11.742, 'epoch': 0.81}
{'loss': 0.8041, 'grad_norm': 0.18209202587604523, 'learning_rate': 4.92063492063492e-05, 'epoch': 0.85}
{'eval_loss': 0.9076109528541565, 'eval_runtime': 5.3584, 'eval_samples_per_second': 186.621, 'eval_steps_per_second': 11.757, 'epoch': 0.85}
{'loss': 0.8314, 'grad_norm': 0.1926194578409195, 'learning_rate': 3.5978835978835974e-05, 'epoch': 0.89}
{'eval_loss': 0.9060485363006592, 'eval_runtime': 5.3557, 'eval_samples_per_second': 186.715, 'eval_steps_per_second': 11.763, 'epoch': 0.89}
{'loss': 0.8153, 'grad_norm': 0.21275921165943146, 'learning_rate': 2.2751322751322748e-05, 'epoch': 0.93}
{'eval_loss': 0.9057148694992065, 'eval_runtime': 5.3542, 'eval_samples_per_second': 186.769, 'eval_steps_per_second': 11.766, 'epoch': 0.93}
{'loss': 0.8213, 'grad_norm': 0.2199384570121765, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.97}
{'eval_loss': 0.9063730239868164, 'eval_runtime': 5.3459, 'eval_samples_per_second': 187.058, 'eval_steps_per_second': 11.785, 'epoch': 0.97}
{'train_runtime': 331.6397, 'train_samples_per_second': 29.749, 'train_steps_per_second': 1.86, 'train_loss': 1.0330108583842916, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4379496574401855, 1.106984257698059, 1.0045710802078247, 0.9827277660369873, 0.954525887966156, 0.9473373889923096, 0.9576842188835144, 0.9295133352279663, 0.9284226894378662, 0.9285083413124084, 0.9252845644950867, 0.9257598519325256, 0.9292643666267395, 0.9217687845230103, 0.9106135964393616, 0.9222375750541687, 0.9084914922714233, 0.9118963479995728, 0.9058831334114075, 0.9057208299636841, 0.9076109528541565, 0.9060485363006592, 0.9057148694992065, 0.9063730239868164], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.4379496574401855, 1.106984257698059, 1.0045710802078247, 0.9827277660369873, 0.954525887966156, 0.9473373889923096, 0.9576842188835144, 0.9295133352279663, 0.9284226894378662, 0.9285083413124084, 0.9252845644950867, 0.9257598519325256, 0.9292643666267395, 0.9217687845230103, 0.9106135964393616, 0.9222375750541687, 0.9084914922714233, 0.9118963479995728, 0.9058831334114075, 0.9057208299636841, 0.9076109528541565, 0.9060485363006592, 0.9057148694992065, 0.9063730239868164]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.00323486328125
current iteration best possible eval_loss (full train run):  -0.9063730239868164
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199, -0.982107937335968, -1.0346444845199585, -1.0614299774169922, -1.00323486328125]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 113.0002 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.647038459777832, 0.3565073609352112, 0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.6441194415092468, 0.6647308468818665, 0.9432829022407532, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.47025445103645325, 0.5915921330451965, 0.563302755355835, 0.236403226852417]  ‚Üí  acq = -0.9835188396228922
X = [0.7882768511772156, 0.0865660309791565, 0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.06684881448745728, 0.476356565952301, 0.49730604887008667, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.06370062381029129, 0.8247895836830139, 0.642969012260437, 0.2869639992713928]  ‚Üí  acq = -0.9832021087231626
X = [0.9537772536277771, 0.5254051685333252, 0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5333559513092041, 0.6058185696601868, 0.9840016961097717, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.07915887981653214, 0.6141131520271301, 0.8602699041366577, 0.6692355275154114]  ‚Üí  acq = -0.9849096233710728
X = [0.10701495409011841, 0.8236895203590393, 0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6551105976104736, 0.23645484447479248, 0.007582306861877441, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5136890411376953, 0.0011958479881286621, 0.378928005695343, 0.21825557947158813]  ‚Üí  acq = -0.9365001937886633
X = [0.37967562675476074, 0.21945631504058838, 0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4969014823436737, 0.5960436463356018, 0.011648118495941162, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.918420135974884, 0.3473445177078247, 0.7110291719436646, 0.30779868364334106]  ‚Üí  acq = -0.9835555138305084
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1073, dtype=torch.float64), tensor(0.1679, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), 0, tensor(0.2010, dtype=torch.float64), tensor(0.0322, dtype=torch.float64), 0, tensor(0.1538, dtype=torch.float64), tensor(0.2959, dtype=torch.float64), 32, 1, 1, 1, 0, 1, 16, 0.07602175225199254, 18.056245332364714, 0]
normalized proposed parameters for next round by BO: [tensor(0.1073, dtype=torch.float64), tensor(0.1679, dtype=torch.float64), tensor(0.0420, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2010, dtype=torch.float64), tensor(0.0322, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1538, dtype=torch.float64), tensor(0.2959, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1268, dtype=torch.float64), tensor(0.7602, dtype=torch.float64), tensor(0.3762, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.107
  gsm8k: 0.168
  rowan_hellaswag: 0.042
  sciq: 0
  triviaqa: 0.201
  truthfulqa_gen: 0.032
  wikitext: 0
  mmlu: 0.154
  arc_challenge: 0.296

LoRA Parameters:
  lora_r: (16,)
  lora_dropout: (0.07602175225199254,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (18.056245332364714,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  16
lora dropout:  0.07602175225199254
lora alpha:  18.056245332364714
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 25,690,112 || all params: 8,055,951,360 || trainable%: 0.3189
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9041, 'grad_norm': 1.0880951881408691, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7751233577728271, 'eval_runtime': 15.2687, 'eval_samples_per_second': 65.493, 'eval_steps_per_second': 4.126, 'epoch': 0.04}
{'loss': 1.4408, 'grad_norm': 0.4440923035144806, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1974704265594482, 'eval_runtime': 13.6211, 'eval_samples_per_second': 73.416, 'eval_steps_per_second': 4.625, 'epoch': 0.08}
{'loss': 1.2438, 'grad_norm': 0.4063468873500824, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0709807872772217, 'eval_runtime': 14.1756, 'eval_samples_per_second': 70.544, 'eval_steps_per_second': 4.444, 'epoch': 0.12}
{'loss': 1.1684, 'grad_norm': 0.43914031982421875, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.006339430809021, 'eval_runtime': 15.3649, 'eval_samples_per_second': 65.083, 'eval_steps_per_second': 4.1, 'epoch': 0.16}
{'loss': 1.0345, 'grad_norm': 0.3005969524383545, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9434419870376587, 'eval_runtime': 17.2821, 'eval_samples_per_second': 57.863, 'eval_steps_per_second': 3.645, 'epoch': 0.2}
{'loss': 1.0201, 'grad_norm': 0.3448522388935089, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9472153186798096, 'eval_runtime': 17.6095, 'eval_samples_per_second': 56.788, 'eval_steps_per_second': 3.578, 'epoch': 0.24}
{'loss': 1.0463, 'grad_norm': 0.3336625099182129, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9307483434677124, 'eval_runtime': 18.055, 'eval_samples_per_second': 55.386, 'eval_steps_per_second': 3.489, 'epoch': 0.28}
{'loss': 0.9649, 'grad_norm': 0.33028650283813477, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9272800087928772, 'eval_runtime': 14.926, 'eval_samples_per_second': 66.997, 'eval_steps_per_second': 4.221, 'epoch': 0.32}
{'loss': 0.9703, 'grad_norm': 0.35978856682777405, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9257124662399292, 'eval_runtime': 15.3442, 'eval_samples_per_second': 65.171, 'eval_steps_per_second': 4.106, 'epoch': 0.36}
{'loss': 0.9442, 'grad_norm': 0.3681145906448364, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9181835651397705, 'eval_runtime': 13.7402, 'eval_samples_per_second': 72.779, 'eval_steps_per_second': 4.585, 'epoch': 0.4}
{'loss': 1.0121, 'grad_norm': 0.4172132611274719, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9216928482055664, 'eval_runtime': 13.5484, 'eval_samples_per_second': 73.81, 'eval_steps_per_second': 4.65, 'epoch': 0.44}
{'loss': 0.9578, 'grad_norm': 0.5010836124420166, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9115439057350159, 'eval_runtime': 13.9281, 'eval_samples_per_second': 71.797, 'eval_steps_per_second': 4.523, 'epoch': 0.48}
{'loss': 0.9123, 'grad_norm': 0.36617591977119446, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9162153601646423, 'eval_runtime': 16.6191, 'eval_samples_per_second': 60.172, 'eval_steps_per_second': 3.791, 'epoch': 0.52}
{'loss': 1.0134, 'grad_norm': 0.42464888095855713, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9035646319389343, 'eval_runtime': 20.2437, 'eval_samples_per_second': 49.398, 'eval_steps_per_second': 3.112, 'epoch': 0.56}
{'loss': 0.9311, 'grad_norm': 0.4208090007305145, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9047597646713257, 'eval_runtime': 15.166, 'eval_samples_per_second': 65.937, 'eval_steps_per_second': 4.154, 'epoch': 0.6}
{'loss': 0.8992, 'grad_norm': 0.4394800364971161, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9034203886985779, 'eval_runtime': 17.4748, 'eval_samples_per_second': 57.225, 'eval_steps_per_second': 3.605, 'epoch': 0.64}
{'loss': 0.8922, 'grad_norm': 0.4958834946155548, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9041629433631897, 'eval_runtime': 11.7213, 'eval_samples_per_second': 85.315, 'eval_steps_per_second': 5.375, 'epoch': 0.68}
{'loss': 0.9352, 'grad_norm': 0.5311241149902344, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9036260843276978, 'eval_runtime': 13.9965, 'eval_samples_per_second': 71.446, 'eval_steps_per_second': 4.501, 'epoch': 0.72}
{'loss': 0.8883, 'grad_norm': 0.5832762122154236, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8964862823486328, 'eval_runtime': 12.19, 'eval_samples_per_second': 82.034, 'eval_steps_per_second': 5.168, 'epoch': 0.76}
{'loss': 0.9185, 'grad_norm': 0.4681793451309204, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8996084928512573, 'eval_runtime': 12.0254, 'eval_samples_per_second': 83.157, 'eval_steps_per_second': 5.239, 'epoch': 0.8}
{'loss': 0.9399, 'grad_norm': 0.5028872489929199, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8988909721374512, 'eval_runtime': 13.2059, 'eval_samples_per_second': 75.724, 'eval_steps_per_second': 4.771, 'epoch': 0.84}
{'loss': 0.8699, 'grad_norm': 0.3645498752593994, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8981017470359802, 'eval_runtime': 14.0133, 'eval_samples_per_second': 71.361, 'eval_steps_per_second': 4.496, 'epoch': 0.88}
{'loss': 0.9051, 'grad_norm': 0.6485730409622192, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8958882093429565, 'eval_runtime': 13.1144, 'eval_samples_per_second': 76.252, 'eval_steps_per_second': 4.804, 'epoch': 0.92}
{'loss': 0.871, 'grad_norm': 0.5008791089057922, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8946162462234497, 'eval_runtime': 12.1154, 'eval_samples_per_second': 82.539, 'eval_steps_per_second': 5.2, 'epoch': 0.96}
{'loss': 0.9322, 'grad_norm': 0.554470419883728, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8943676948547363, 'eval_runtime': 12.6351, 'eval_samples_per_second': 79.145, 'eval_steps_per_second': 4.986, 'epoch': 1.0}
{'train_runtime': 924.9797, 'train_samples_per_second': 10.807, 'train_steps_per_second': 0.676, 'train_loss': 1.0646288879394532, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7751233577728271, 1.1974704265594482, 1.0709807872772217, 1.006339430809021, 0.9434419870376587, 0.9472153186798096, 0.9307483434677124, 0.9272800087928772, 0.9257124662399292, 0.9181835651397705, 0.9216928482055664, 0.9115439057350159, 0.9162153601646423, 0.9035646319389343, 0.9047597646713257, 0.9034203886985779, 0.9041629433631897, 0.9036260843276978, 0.8964862823486328, 0.8996084928512573, 0.8988909721374512, 0.8981017470359802, 0.8958882093429565, 0.8946162462234497, 0.8943676948547363], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7751233577728271, 1.1974704265594482, 1.0709807872772217, 1.006339430809021, 0.9434419870376587, 0.9472153186798096, 0.9307483434677124, 0.9272800087928772, 0.9257124662399292, 0.9181835651397705, 0.9216928482055664, 0.9115439057350159, 0.9162153601646423, 0.9035646319389343, 0.9047597646713257, 0.9034203886985779, 0.9041629433631897, 0.9036260843276978, 0.8964862823486328, 0.8996084928512573, 0.8988909721374512, 0.8981017470359802, 0.8958882093429565, 0.8946162462234497, 0.8943676948547363]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9628822207450867
current iteration best possible eval_loss (full train run):  -0.8943676948547363
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199, -0.982107937335968, -1.0346444845199585, -1.0614299774169922, -1.00323486328125, -0.9628822207450867]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9138 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -0.993182882758826
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -0.9924727081217668
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -0.9928647822430302
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -0.9923768174186991
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.0070353069870108
proposed candidate layer mask is:  tensor([0., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2277, dtype=torch.float64), tensor(0.1864, dtype=torch.float64), 0, tensor(0.1122, dtype=torch.float64), 0, tensor(0.2681, dtype=torch.float64), 0, tensor(0.2056, dtype=torch.float64), 0, 13, 0, 0, 0, 1, 0, 77, 1.5612511283791162e-17, 31.7135323065686, 0]
normalized proposed parameters for next round by BO: [tensor(0.2277, dtype=torch.float64), tensor(0.1864, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1122, dtype=torch.float64), tensor(7.8118e-17, dtype=torch.float64), tensor(0.2681, dtype=torch.float64), tensor(2.2662e-17, dtype=torch.float64), tensor(0.2056, dtype=torch.float64), tensor(5.4048e-17, dtype=torch.float64), tensor(0.4156, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5998, dtype=torch.float64), tensor(1.5613e-16, dtype=torch.float64), tensor(0.6607, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.228
  gsm8k: 0.186
  rowan_hellaswag: 0
  sciq: 0.112
  triviaqa: 0
  truthfulqa_gen: 0.268
  wikitext: 0
  mmlu: 0.206
  arc_challenge: 0

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (1.5612511283791162e-17,)
  num_layers_to_apply: (13,)
  five_dim_vector: ([0, 0, 0, 1, 0],)
  lora_alpha: (31.7135323065686,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  13
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 0, 1, 0]
lora rank:  77
lora dropout:  1.5612511283791162e-17
lora alpha:  31.7135323065686
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 18,450,432 || all params: 8,048,711,680 || trainable%: 0.2292
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.4048, 'grad_norm': 1.3695340156555176, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.6456658840179443, 'eval_runtime': 4.5071, 'eval_samples_per_second': 221.873, 'eval_steps_per_second': 13.978, 'epoch': 0.04}
{'loss': 1.5715, 'grad_norm': 0.55642169713974, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3392748832702637, 'eval_runtime': 4.531, 'eval_samples_per_second': 220.703, 'eval_steps_per_second': 13.904, 'epoch': 0.08}
{'loss': 1.2825, 'grad_norm': 0.26058539748191833, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.077479362487793, 'eval_runtime': 4.5235, 'eval_samples_per_second': 221.066, 'eval_steps_per_second': 13.927, 'epoch': 0.12}
{'loss': 1.1398, 'grad_norm': 0.2698513865470886, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0666776895523071, 'eval_runtime': 4.5297, 'eval_samples_per_second': 220.764, 'eval_steps_per_second': 13.908, 'epoch': 0.16}
{'loss': 1.094, 'grad_norm': 0.19528637826442719, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0289978981018066, 'eval_runtime': 4.5615, 'eval_samples_per_second': 219.227, 'eval_steps_per_second': 13.811, 'epoch': 0.2}
{'loss': 1.0885, 'grad_norm': 0.23275014758110046, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.033994197845459, 'eval_runtime': 4.5585, 'eval_samples_per_second': 219.372, 'eval_steps_per_second': 13.82, 'epoch': 0.24}
{'loss': 1.0672, 'grad_norm': 0.2628803551197052, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.009340763092041, 'eval_runtime': 4.5681, 'eval_samples_per_second': 218.908, 'eval_steps_per_second': 13.791, 'epoch': 0.28}
{'loss': 1.0498, 'grad_norm': 0.2104351967573166, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9966275691986084, 'eval_runtime': 4.5784, 'eval_samples_per_second': 218.418, 'eval_steps_per_second': 13.76, 'epoch': 0.32}
{'loss': 1.067, 'grad_norm': 0.21009410917758942, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9934569597244263, 'eval_runtime': 4.5774, 'eval_samples_per_second': 218.466, 'eval_steps_per_second': 13.763, 'epoch': 0.36}
{'loss': 1.039, 'grad_norm': 0.19895461201667786, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9848235845565796, 'eval_runtime': 4.5939, 'eval_samples_per_second': 217.68, 'eval_steps_per_second': 13.714, 'epoch': 0.4}
{'loss': 1.0336, 'grad_norm': 0.18380311131477356, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9866816401481628, 'eval_runtime': 4.5904, 'eval_samples_per_second': 217.847, 'eval_steps_per_second': 13.724, 'epoch': 0.44}
{'loss': 0.9859, 'grad_norm': 0.21639908850193024, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9789808392524719, 'eval_runtime': 4.5769, 'eval_samples_per_second': 218.49, 'eval_steps_per_second': 13.765, 'epoch': 0.48}
{'loss': 0.9816, 'grad_norm': 0.1857774257659912, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9702711701393127, 'eval_runtime': 4.578, 'eval_samples_per_second': 218.434, 'eval_steps_per_second': 13.761, 'epoch': 0.52}
{'loss': 1.0096, 'grad_norm': 0.19771708548069, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9715785384178162, 'eval_runtime': 4.5826, 'eval_samples_per_second': 218.217, 'eval_steps_per_second': 13.748, 'epoch': 0.56}
{'loss': 1.0255, 'grad_norm': 0.19008609652519226, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9729074239730835, 'eval_runtime': 4.5778, 'eval_samples_per_second': 218.444, 'eval_steps_per_second': 13.762, 'epoch': 0.6}
{'loss': 0.9938, 'grad_norm': 0.18444131314754486, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9645115733146667, 'eval_runtime': 4.5775, 'eval_samples_per_second': 218.458, 'eval_steps_per_second': 13.763, 'epoch': 0.64}
{'loss': 1.0189, 'grad_norm': 0.19338178634643555, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9626822471618652, 'eval_runtime': 4.5755, 'eval_samples_per_second': 218.553, 'eval_steps_per_second': 13.769, 'epoch': 0.68}
{'loss': 1.0006, 'grad_norm': 0.1891472041606903, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.962056577205658, 'eval_runtime': 4.5735, 'eval_samples_per_second': 218.65, 'eval_steps_per_second': 13.775, 'epoch': 0.72}
{'loss': 0.9777, 'grad_norm': 0.22577695548534393, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9631420969963074, 'eval_runtime': 4.5769, 'eval_samples_per_second': 218.489, 'eval_steps_per_second': 13.765, 'epoch': 0.76}
{'loss': 1.0074, 'grad_norm': 0.19381491839885712, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9601367115974426, 'eval_runtime': 4.5749, 'eval_samples_per_second': 218.584, 'eval_steps_per_second': 13.771, 'epoch': 0.8}
{'loss': 1.0267, 'grad_norm': 0.20582568645477295, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9597365856170654, 'eval_runtime': 4.575, 'eval_samples_per_second': 218.579, 'eval_steps_per_second': 13.77, 'epoch': 0.84}
{'loss': 1.0191, 'grad_norm': 0.21009385585784912, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9554292559623718, 'eval_runtime': 4.5764, 'eval_samples_per_second': 218.513, 'eval_steps_per_second': 13.766, 'epoch': 0.88}
{'loss': 1.0092, 'grad_norm': 0.2018333524465561, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9523929357528687, 'eval_runtime': 4.583, 'eval_samples_per_second': 218.197, 'eval_steps_per_second': 13.746, 'epoch': 0.92}
{'loss': 1.0, 'grad_norm': 0.21810904145240784, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9555061459541321, 'eval_runtime': 4.5784, 'eval_samples_per_second': 218.415, 'eval_steps_per_second': 13.76, 'epoch': 0.96}
{'loss': 0.9659, 'grad_norm': 0.23321057856082916, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.954516589641571, 'eval_runtime': 4.5849, 'eval_samples_per_second': 218.108, 'eval_steps_per_second': 13.741, 'epoch': 1.0}
{'train_runtime': 249.2002, 'train_samples_per_second': 40.12, 'train_steps_per_second': 2.508, 'train_loss': 1.15437666015625, 'epoch': 1.0}
train_results:  {'eval_loss': [2.6456658840179443, 1.3392748832702637, 1.077479362487793, 1.0666776895523071, 1.0289978981018066, 1.033994197845459, 1.009340763092041, 0.9966275691986084, 0.9934569597244263, 0.9848235845565796, 0.9866816401481628, 0.9789808392524719, 0.9702711701393127, 0.9715785384178162, 0.9729074239730835, 0.9645115733146667, 0.9626822471618652, 0.962056577205658, 0.9631420969963074, 0.9601367115974426, 0.9597365856170654, 0.9554292559623718, 0.9523929357528687, 0.9555061459541321, 0.954516589641571], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.6456658840179443, 1.3392748832702637, 1.077479362487793, 1.0666776895523071, 1.0289978981018066, 1.033994197845459, 1.009340763092041, 0.9966275691986084, 0.9934569597244263, 0.9848235845565796, 0.9866816401481628, 0.9789808392524719, 0.9702711701393127, 0.9715785384178162, 0.9729074239730835, 0.9645115733146667, 0.9626822471618652, 0.962056577205658, 0.9631420969963074, 0.9601367115974426, 0.9597365856170654, 0.9554292559623718, 0.9523929357528687, 0.9555061459541321, 0.954516589641571]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2226285934448242
current iteration best possible eval_loss (full train run):  -0.954516589641571
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199, -0.982107937335968, -1.0346444845199585, -1.0614299774169922, -1.00323486328125, -0.9628822207450867, -1.2226285934448242]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.8432 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3237239718437195, 0.6742079257965088, 0.8490332961082458, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.35023945569992065, 0.971827507019043, 0.9065044522285461, 0.298198401927948, 0.8475511074066162, 0.498738169670105]  ‚Üí  acq = -0.9997511543634399
X = [0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.947229266166687, 0.2025597095489502, 0.20687411725521088, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6102912425994873, 0.7515861392021179, 0.5552695989608765, 0.20585447549819946, 0.31215184926986694, 0.8506918549537659]  ‚Üí  acq = -0.9992356588647321
X = [0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.41811567544937134, 0.9807340502738953, 0.09085434675216675, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5667123198509216, 0.3193025588989258, 0.46271342039108276, 0.4887549877166748, 0.49947670102119446, 0.9963791966438293]  ‚Üí  acq = -1.0001924661860586
X = [0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.55754554271698, 0.605756938457489, 0.40601593255996704, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.11392313241958618, 0.7689643502235413, 0.682531476020813, 0.6047636270523071, 0.5154639482498169, 0.24933886528015137]  ‚Üí  acq = -0.9992361723051315
X = [0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.034960031509399414, 0.3871777653694153, 0.954418957233429, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.2306498885154724, 0.10478633642196655, 0.9865217208862305, 0.0338512659072876, 0.21921201050281525, 0.2958201766014099]  ‚Üí  acq = -0.9992809943384316
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2610, dtype=torch.float64), 0, tensor(0.0669, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.1490, dtype=torch.float64), tensor(0.0265, dtype=torch.float64), tensor(0.0338, dtype=torch.float64), 0, tensor(0.4291, dtype=torch.float64), 27, 0, 1, 1, 1, 0, 39, 0.017633400910146618, 13.766332433033838, 0]
normalized proposed parameters for next round by BO: [tensor(0.2610, dtype=torch.float64), tensor(5.6235e-19, dtype=torch.float64), tensor(0.0669, dtype=torch.float64), tensor(0.0283, dtype=torch.float64), tensor(0.1490, dtype=torch.float64), tensor(0.0265, dtype=torch.float64), tensor(0.0338, dtype=torch.float64), tensor(0.0054, dtype=torch.float64), tensor(0.4291, dtype=torch.float64), tensor(0.8537, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3031, dtype=torch.float64), tensor(0.1763, dtype=torch.float64), tensor(0.2868, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.261
  gsm8k: 0
  rowan_hellaswag: 0.067
  sciq: 0.028
  triviaqa: 0.149
  truthfulqa_gen: 0.026
  wikitext: 0.034
  mmlu: 0
  arc_challenge: 0.429

LoRA Parameters:
  lora_r: (39,)
  lora_dropout: (0.017633400910146618,)
  num_layers_to_apply: (27,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (13.766332433033838,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  27
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  39
lora dropout:  0.017633400910146618
lora alpha:  13.766332433033838
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 44,209,152 || all params: 8,074,470,400 || trainable%: 0.5475
length of training data:  9943
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.369, 'grad_norm': 0.9932626485824585, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.793535590171814, 'eval_runtime': 5.1961, 'eval_samples_per_second': 192.453, 'eval_steps_per_second': 12.125, 'epoch': 0.04}
{'loss': 1.4081, 'grad_norm': 0.42410293221473694, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.030124306678772, 'eval_runtime': 5.2059, 'eval_samples_per_second': 192.089, 'eval_steps_per_second': 12.102, 'epoch': 0.08}
{'loss': 1.128, 'grad_norm': 0.27250564098358154, 'learning_rate': 0.0002874125874125874, 'epoch': 0.12}
{'eval_loss': 0.9591856002807617, 'eval_runtime': 5.2015, 'eval_samples_per_second': 192.251, 'eval_steps_per_second': 12.112, 'epoch': 0.12}
{'loss': 1.0459, 'grad_norm': 0.24610310792922974, 'learning_rate': 0.00027430069930069927, 'epoch': 0.16}
{'eval_loss': 0.9363948702812195, 'eval_runtime': 5.215, 'eval_samples_per_second': 191.755, 'eval_steps_per_second': 12.081, 'epoch': 0.16}
{'loss': 1.007, 'grad_norm': 0.2510286271572113, 'learning_rate': 0.00026118881118881114, 'epoch': 0.2}
{'eval_loss': 0.92425537109375, 'eval_runtime': 5.2277, 'eval_samples_per_second': 191.29, 'eval_steps_per_second': 12.051, 'epoch': 0.2}
{'loss': 1.0495, 'grad_norm': 0.26169368624687195, 'learning_rate': 0.000248076923076923, 'epoch': 0.24}
{'eval_loss': 0.9230200052261353, 'eval_runtime': 5.2332, 'eval_samples_per_second': 191.087, 'eval_steps_per_second': 12.039, 'epoch': 0.24}
{'loss': 0.9895, 'grad_norm': 0.25710996985435486, 'learning_rate': 0.00023496503496503495, 'epoch': 0.28}
{'eval_loss': 0.9075000286102295, 'eval_runtime': 5.2356, 'eval_samples_per_second': 191.001, 'eval_steps_per_second': 12.033, 'epoch': 0.28}
{'loss': 1.0204, 'grad_norm': 0.2720426023006439, 'learning_rate': 0.00022185314685314682, 'epoch': 0.32}
{'eval_loss': 0.9041957259178162, 'eval_runtime': 5.264, 'eval_samples_per_second': 189.971, 'eval_steps_per_second': 11.968, 'epoch': 0.32}
{'loss': 0.9869, 'grad_norm': 0.21353504061698914, 'learning_rate': 0.00020874125874125872, 'epoch': 0.36}
{'eval_loss': 0.901642918586731, 'eval_runtime': 5.2807, 'eval_samples_per_second': 189.37, 'eval_steps_per_second': 11.93, 'epoch': 0.36}
{'loss': 0.9869, 'grad_norm': 0.24488094449043274, 'learning_rate': 0.0001956293706293706, 'epoch': 0.4}
{'eval_loss': 0.9034518599510193, 'eval_runtime': 5.2478, 'eval_samples_per_second': 190.555, 'eval_steps_per_second': 12.005, 'epoch': 0.4}
{'loss': 1.0434, 'grad_norm': 0.2800076901912689, 'learning_rate': 0.00018251748251748253, 'epoch': 0.44}
{'eval_loss': 0.8931277990341187, 'eval_runtime': 5.2513, 'eval_samples_per_second': 190.429, 'eval_steps_per_second': 11.997, 'epoch': 0.44}
{'loss': 0.9798, 'grad_norm': 0.28377097845077515, 'learning_rate': 0.0001694055944055944, 'epoch': 0.48}
{'eval_loss': 0.8899610638618469, 'eval_runtime': 5.2366, 'eval_samples_per_second': 190.963, 'eval_steps_per_second': 12.031, 'epoch': 0.48}
{'loss': 1.0127, 'grad_norm': 0.28849029541015625, 'learning_rate': 0.00015629370629370628, 'epoch': 0.52}
{'eval_loss': 0.8972601294517517, 'eval_runtime': 5.2384, 'eval_samples_per_second': 190.896, 'eval_steps_per_second': 12.026, 'epoch': 0.52}
{'loss': 1.0147, 'grad_norm': 0.2799836993217468, 'learning_rate': 0.00014318181818181818, 'epoch': 0.56}
{'eval_loss': 0.894225001335144, 'eval_runtime': 5.2639, 'eval_samples_per_second': 189.973, 'eval_steps_per_second': 11.968, 'epoch': 0.56}
{'loss': 0.9314, 'grad_norm': 0.3282516598701477, 'learning_rate': 0.00013006993006993005, 'epoch': 0.6}
{'eval_loss': 0.884151041507721, 'eval_runtime': 5.2475, 'eval_samples_per_second': 190.567, 'eval_steps_per_second': 12.006, 'epoch': 0.6}
{'loss': 0.8965, 'grad_norm': 0.35042527318000793, 'learning_rate': 0.00011695804195804194, 'epoch': 0.64}
{'eval_loss': 0.8832000494003296, 'eval_runtime': 5.2475, 'eval_samples_per_second': 190.566, 'eval_steps_per_second': 12.006, 'epoch': 0.64}
{'loss': 0.9381, 'grad_norm': 0.3473520874977112, 'learning_rate': 0.00010384615384615383, 'epoch': 0.68}
{'eval_loss': 0.8870722651481628, 'eval_runtime': 5.2574, 'eval_samples_per_second': 190.208, 'eval_steps_per_second': 11.983, 'epoch': 0.68}
{'loss': 0.9242, 'grad_norm': 0.4320349395275116, 'learning_rate': 9.073426573426573e-05, 'epoch': 0.72}
{'eval_loss': 0.8807086944580078, 'eval_runtime': 5.2556, 'eval_samples_per_second': 190.275, 'eval_steps_per_second': 11.987, 'epoch': 0.72}
{'loss': 0.8878, 'grad_norm': 0.4630073010921478, 'learning_rate': 7.762237762237762e-05, 'epoch': 0.76}
{'eval_loss': 0.8789854645729065, 'eval_runtime': 5.2593, 'eval_samples_per_second': 190.138, 'eval_steps_per_second': 11.979, 'epoch': 0.76}
{'loss': 0.9169, 'grad_norm': 0.49279627203941345, 'learning_rate': 6.45104895104895e-05, 'epoch': 0.8}
{'eval_loss': 0.8753612041473389, 'eval_runtime': 5.265, 'eval_samples_per_second': 189.934, 'eval_steps_per_second': 11.966, 'epoch': 0.8}
{'loss': 0.8415, 'grad_norm': 0.46116384863853455, 'learning_rate': 5.1398601398601395e-05, 'epoch': 0.84}
{'eval_loss': 0.8739888668060303, 'eval_runtime': 5.2363, 'eval_samples_per_second': 190.973, 'eval_steps_per_second': 12.031, 'epoch': 0.84}
{'loss': 0.8185, 'grad_norm': 0.5463302135467529, 'learning_rate': 3.828671328671328e-05, 'epoch': 0.88}
{'eval_loss': 0.8719968795776367, 'eval_runtime': 5.2316, 'eval_samples_per_second': 191.146, 'eval_steps_per_second': 12.042, 'epoch': 0.88}
{'loss': 0.8109, 'grad_norm': 0.45773226022720337, 'learning_rate': 2.5174825174825174e-05, 'epoch': 0.92}
{'eval_loss': 0.8721391558647156, 'eval_runtime': 5.2562, 'eval_samples_per_second': 190.252, 'eval_steps_per_second': 11.986, 'epoch': 0.92}
{'loss': 0.7987, 'grad_norm': 0.48461318016052246, 'learning_rate': 1.206293706293706e-05, 'epoch': 0.96}
{'eval_loss': 0.8707839250564575, 'eval_runtime': 5.2696, 'eval_samples_per_second': 189.768, 'eval_steps_per_second': 11.955, 'epoch': 0.96}
{'train_runtime': 320.7816, 'train_samples_per_second': 30.996, 'train_steps_per_second': 1.939, 'train_loss': 1.0684803383143384, 'epoch': 1.0}
train_results:  {'eval_loss': [1.793535590171814, 1.030124306678772, 0.9591856002807617, 0.9363948702812195, 0.92425537109375, 0.9230200052261353, 0.9075000286102295, 0.9041957259178162, 0.901642918586731, 0.9034518599510193, 0.8931277990341187, 0.8899610638618469, 0.8972601294517517, 0.894225001335144, 0.884151041507721, 0.8832000494003296, 0.8870722651481628, 0.8807086944580078, 0.8789854645729065, 0.8753612041473389, 0.8739888668060303, 0.8719968795776367, 0.8721391558647156, 0.8707839250564575], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.793535590171814, 1.030124306678772, 0.9591856002807617, 0.9363948702812195, 0.92425537109375, 0.9230200052261353, 0.9075000286102295, 0.9041957259178162, 0.901642918586731, 0.9034518599510193, 0.8931277990341187, 0.8899610638618469, 0.8972601294517517, 0.894225001335144, 0.884151041507721, 0.8832000494003296, 0.8870722651481628, 0.8807086944580078, 0.8789854645729065, 0.8753612041473389, 0.8739888668060303, 0.8719968795776367, 0.8721391558647156, 0.8707839250564575]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9546741247177124
current iteration best possible eval_loss (full train run):  -0.8707839250564575
max eval_loss so far:  -0.81962651014328
BO observations:  [-1.2130283117294312, -1.511947751045227, -1.257813572883606, -1.1117538213729858, -1.0477313995361328, -1.2225608825683594, -1.1300960779190063, -1.0372769832611084, -1.0111750364303589, -1.096990704536438, -1.062602162361145, -1.094516396522522, -1.0757626295089722, -1.0688362121582031, -1.053370475769043, -1.1377488374710083, -0.9729693531990051, -1.0756399631500244, -1.0635651350021362, -1.1366990804672241, -0.939906656742096, -0.9987301826477051, -0.9676089286804199, -0.982107937335968, -1.0346444845199585, -1.0614299774169922, -1.00323486328125, -0.9628822207450867, -1.2226285934448242, -0.9546741247177124]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.1689 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.3357044458389282, 0.4876623749732971, 0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7566351294517517, 0.44088155031204224, 0.21595293283462524, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7294671535491943, 0.9934723973274231, 0.20285475254058838, 0.606965959072113]  ‚Üí  acq = -1.0158492620796962
X = [0.7454677820205688, 0.4424137473106384, 0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8704531192779541, 0.009343624114990234, 0.477742075920105, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7311053276062012, 0.7334456443786621, 0.34133514761924744, 0.4159647822380066]  ‚Üí  acq = -1.01586342048367
X = [0.6070147752761841, 0.050814270973205566, 0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.06286248564720154, 0.38677525520324707, 0.3574179410934448, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5247937440872192, 0.7738797068595886, 0.6983144283294678, 0.9456675052642822]  ‚Üí  acq = -1.0158492620797017
X = [0.05165296792984009, 0.43891578912734985, 0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.34409016370773315, 0.6168457269668579, 0.23861968517303467, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.4990995526313782, 0.5673343539237976, 0.3437628448009491, 0.6652377843856812]  ‚Üí  acq = -1.0171004926765288
X = [0.05172693729400635, 0.2110685110092163, 0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.34519943594932556, 0.5801001787185669, 0.783306360244751, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9125829935073853, 0.5967274308204651, 0.9407432079315186, 0.06500035524368286]  ‚Üí  acq = -1.015849407828666
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1424, dtype=torch.float64), tensor(0.0738, dtype=torch.float64), 0, 0, tensor(0.1951, dtype=torch.float64), tensor(0.1589, dtype=torch.float64), tensor(0.0273, dtype=torch.float64), tensor(0.1396, dtype=torch.float64), tensor(0.2623, dtype=torch.float64), 26, 1, 1, 1, 0, 1, 19, 0.07449659480395525, 24.515042176031926, 0]
normalized proposed parameters for next round by BO: [tensor(0.1424, dtype=torch.float64), tensor(0.0738, dtype=torch.float64), tensor(8.4295e-17, dtype=torch.float64), tensor(0.0007, dtype=torch.float64), tensor(0.1951, dtype=torch.float64), tensor(0.1589, dtype=torch.float64), tensor(0.0273, dtype=torch.float64), tensor(0.1396, dtype=torch.float64), tensor(0.2623, dtype=torch.float64), tensor(0.8095, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1498, dtype=torch.float64), tensor(0.7450, dtype=torch.float64), tensor(0.5107, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328]
running BO on both data and model
commonsense_qa
gsm8k
rowan_hellaswag
sciq
triviaqa
truthfulqa_gen
wikitext
mmlu
arc_challenge
commonsense_qa
evaluation dataset:
data domain:  commonsense_qa  weight:  1.0
We are at initial step. BO_params["optimize_method"]: mixed
fidelity is not required
JoBS: Predictor loaded successfully from trained_predictor_fixed_20train_10val/eval_loss_H25_50_T625_curve/commonsense_qa/performance_mlp_20samples.pth
Fitting GP with prior observations collected for JoBS performance predictor...
Checking history sample input_X:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 17, 0, 1, 0, 1, 1, 74, 0.06944379333880846, 10, 0]
Checking history sample input_X_between_0_1:  [0.20003999473376882, 0.1083454358590159, 0.00723477289764326, 0.05413581824796855, 0.09706676470193586, 0.32412602821066344, 0.02425853963902149, 0.11888293976332241, 0.06590970594666017, 0.53125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.578125, 0.6944379333880846, 0.20833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -0.897139310836792
Checking history sample input_X:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 18, 0, 0, 1, 0, 0, 88, 0.07338847024538249, 48, 1]
Checking history sample input_X_between_0_1:  [0.05425921275168307, 0.004704349113610214, 0.12227551165346144, 0.1773054125041735, 0.10089833667899943, 0.032654505447894784, 0.0733834480513842, 0.1210596527368918, 0.3134595710619015, 0.5625, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6875, 0.7338847024538249, 1.0, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1220359802246094
Checking history sample input_X:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 27, 1, 1, 0, 1, 0, 35, 0.056205119903528195, 6, 0]
Checking history sample input_X_between_0_1:  [0.08495996789146838, 0.27329141216913805, 0.04813553263649738, 0.04828551327241618, 0.17400435980543916, 0.12451667510356126, 0.0002743814209281285, 0.03676533017316656, 0.20976682752738474, 0.84375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2734375, 0.5620511990352819, 0.125, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9592704772949219
Checking history sample input_X:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 22, 0, 0, 0, 0, 1, 14, 0.05785794551060587, 9, 1]
Checking history sample input_X_between_0_1:  [0.1420011571440345, 0.12105841595468493, 0.05326714851482698, 0.1973052975876618, 0.04189760435512395, 0.039224463931579495, 0.0012993657662880658, 0.16994386390604246, 0.23400268283975784, 0.6875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.109375, 0.5785794551060587, 0.1875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0981301069259644
Checking history sample input_X:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 15, 1, 0, 0, 1, 1, 33, 0.046509797776235, 40, 0]
Checking history sample input_X_between_0_1:  [0.2686100344981053, 0.13299950068402477, 0.00564174894898137, 0.1728648144773241, 0.23011344337522563, 0.026368589969785697, 0.047067281740930014, 0.06990460458041238, 0.04642998172521059, 0.46875, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2578125, 0.46509797776234996, 0.8333333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9429119229316711
Checking history sample input_X:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 11, 0, 0, 1, 1, 1, 70, 0.005359902393564798, 44, 1]
Checking history sample input_X_between_0_1:  [0.11117510777785206, 0.2258890357711354, 0.005722237032572739, 0.002336540575102911, 0.17159605685876256, 0.04262758729603535, 0.07840436792955673, 0.07836060119109041, 0.2838884655678919, 0.34375, 0.0, 0.0, 1.0, 1.0, 1.0, 0.546875, 0.05359902393564797, 0.9166666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.8976500034332275
Checking history sample input_X:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 3, 1, 1, 0, 0, 1, 89, 0.004230716614147934, 21, 1]
Checking history sample input_X_between_0_1:  [0.04139998319495745, 0.09636308661984504, 0.004740664519095928, 0.4223660406979466, 0.027720833538401227, 0.15563197559643432, 0.004137601604551855, 0.09720095113078629, 0.15043886309798135, 0.09375, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6953125, 0.042307166141479335, 0.4375, 1.0]
Checking history sample eval_loss at 625 steps:  -1.0746725797653198
Checking history sample input_X:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 11, 0, 0, 1, 0, 1, 128, 0.02126981773222546, 43, 0]
Checking history sample input_X_between_0_1:  [0.10372857645461106, 0.23793425053982767, 0.06811739150600891, 0.002032876449058414, 0.021239424269207285, 0.29634592154375927, 0.12003494689388468, 0.024165237989516176, 0.12640137435412666, 0.34375, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2126981773222546, 0.8958333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0224553346633911
Checking history sample input_X:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 4, 0, 1, 0, 0, 1, 51, 0.048394033913014715, 30, 0]
Checking history sample input_X_between_0_1:  [0.08012566767179982, 0.057572839499048525, 0.11177751116664468, 0.21871890637782535, 0.11139467243597961, 0.07498093805358813, 0.026474116803898187, 0.306520309166552, 0.012435038824663777, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3984375, 0.4839403391301471, 0.625, 0.0]
Checking history sample eval_loss at 625 steps:  -1.3750804662704468
Checking history sample input_X:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 25, 1, 0, 1, 0, 0, 103, 0.021006146654874183, 41, 0]
Checking history sample input_X_between_0_1:  [0.14918755499557265, 0.23648603996446563, 0.07125047682032595, 0.18527759547309525, 0.18003157515660598, 0.026298341356892935, 0.01103638165892208, 0.08600067926471924, 0.054431355309400464, 0.78125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8046875, 0.2100614665487418, 0.8541666666666666, 0.0]
Checking history sample eval_loss at 625 steps:  -0.9797114729881287
Checking history sample input_X:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 30, 0, 0, 0, 1, 0, 30, 0.06473585772462145, 22, 0]
Checking history sample input_X_between_0_1:  [0.0044503346439791255, 0.60716165759248, 0.006250310360176903, 0.009585463298271762, 0.0619125571659312, 0.023260206786986197, 0.006412588456246008, 0.13911221715934086, 0.14185466453658804, 0.9375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.234375, 0.6473585772462145, 0.4583333333333333, 0.0]
Checking history sample eval_loss at 625 steps:  -0.8777164220809937
Checking history sample input_X:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 10, 0, 0, 1, 0, 1, 33, 0.053839147652140054, 30, 1]
Checking history sample input_X_between_0_1:  [0.05273444050284431, 0.028239325550327918, 0.3341503385135226, 0.06901655994638124, 0.13634994326027716, 0.2728213594972425, 0.05572499702425396, 0.04895003564066391, 0.002013000064486339, 0.3125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2578125, 0.5383914765214005, 0.625, 1.0]
Checking history sample eval_loss at 625 steps:  -1.424627661705017
Checking history sample input_X:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 27, 0, 1, 1, 1, 0, 91, 0.026257080995186557, 27, 0]
Checking history sample input_X_between_0_1:  [0.25385255835091364, 0.0009177305686203449, 0.06945173899107342, 0.026190910643971766, 0.09799039175792759, 0.05553764986436874, 0.005779635230493405, 0.005262005553708621, 0.48501737903892245, 0.84375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7109375, 0.26257080995186555, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.6500163078308105
Checking history sample input_X:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 28, 1, 1, 1, 0, 0, 23, 0.048743516472132736, 35, 1]
Checking history sample input_X_between_0_1:  [0.13970539805798826, 0.13799353791085264, 0.030840123234993296, 0.15981096781087378, 0.22828025279642375, 0.05747728505974879, 0.04673246511758954, 0.13693845829107115, 0.062221511720458846, 0.875, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1796875, 0.48743516472132736, 0.7291666666666666, 1.0]
Checking history sample eval_loss at 625 steps:  -0.9704539775848389
Checking history sample input_X:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 14, 1, 0, 1, 0, 0, 32, 0.04806841934255027, 48, 0]
Checking history sample input_X_between_0_1:  [0.009273309850789937, 0.12033009973160307, 0.20476155213197156, 0.06677930707685785, 0.06580472030575982, 0.19585549901456453, 0.004105123415989387, 0.10174651099724911, 0.2313438774752146, 0.4375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.48068419342550267, 1.0, 0.0]
Checking history sample eval_loss at 625 steps:  -1.1657739877700806
Checking history sample input_X:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 19, 0, 0, 1, 1, 0, 22, 0.09470699670511286, 20, 1]
Checking history sample input_X_between_0_1:  [0.03804832254451998, 0.15152237071497315, 0.10857636400539114, 0.1842065793025251, 0.03713753947971202, 0.01618255759668317, 0.18298053859634877, 0.13346412722563092, 0.14788160053421576, 0.59375, 0.0, 0.0, 1.0, 1.0, 0.0, 0.171875, 0.9470699670511286, 0.4166666666666667, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1796187162399292
Checking history sample input_X:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 2, 0, 1, 1, 1, 0, 72, 0.07202092774167915, 33, 1]
Checking history sample input_X_between_0_1:  [0.041998281687862175, 0.2081750080051493, 0.1820984920317298, 0.16634450709516352, 0.10035614523741775, 0.08515984945102976, 0.08363369934012171, 0.11096798552395735, 0.02126603162756878, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5625, 0.7202092774167914, 0.6875, 1.0]
Checking history sample eval_loss at 625 steps:  -1.2933062314987183
Checking history sample input_X:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 15, 1, 0, 1, 0, 1, 69, 0.09577839904714924, 28, 0]
Checking history sample input_X_between_0_1:  [0.09109103929620392, 0.03575281495728391, 0.0648299817947527, 0.03152671202591101, 0.475476277421419, 0.09635241838304927, 0.05537541918007681, 0.016016856456574777, 0.13357848048472876, 0.46875, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5390625, 0.9577839904714924, 0.5833333333333334, 0.0]
Checking history sample eval_loss at 625 steps:  -1.0850565433502197
Checking history sample input_X:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 30, 1, 1, 1, 0, 1, 19, 0.07443761643578949, 27, 0]
Checking history sample input_X_between_0_1:  [0.26447218601539785, 0.04132834916694956, 0.021122593691824853, 0.027798869536712244, 0.08048088295403641, 0.10499508095981182, 0.019780273182535578, 0.14256020985450102, 0.2974615546382306, 0.9375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1484375, 0.7443761643578948, 0.5625, 0.0]
Checking history sample eval_loss at 625 steps:  -0.7435973286628723
Checking history sample input_X:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 12, 1, 0, 0, 1, 1, 19, 0.0774188690692264, 1, 1]
Checking history sample input_X_between_0_1:  [0.0011737374384470668, 0.2586261853390816, 0.059689576420591174, 0.00621390408520013, 0.11424978295517868, 0.2106315360131096, 0.17329139798982746, 0.04363384145023723, 0.13249003830832717, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1484375, 0.774188690692264, 0.020833333333333332, 1.0]
Checking history sample eval_loss at 625 steps:  -1.1769657135009766
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.3895 seconds
/home/alfred/Data-Mixing/BO.py:952: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4460054636001587, 0.503385066986084, 0.21726703643798828, 0.926043689250946, 0.9865227937698364, 0.9670451879501343, 0.6644757390022278, 0.007784128189086914, 0.12566906213760376, 0.8420848846435547, 0.6150030493736267, 0.7533425092697144, 0.04900914430618286, 0.29997825622558594, 0.004106998443603516, 0.27856332063674927, 0.858315110206604, 0.24935758113861084, 0.8471353650093079]  ‚Üí  acq = -0.22620788554440596
X = [0.6644539833068848, 0.012049376964569092, 0.6841586828231812, 0.7169187068939209, 0.9304714798927307, 0.6177476644515991, 0.8118157386779785, 0.8121294379234314, 0.7555518746376038, 0.07539018988609314, 0.9956803917884827, 0.9973446130752563, 0.6631196141242981, 0.950596809387207, 0.5734815001487732, 0.041506290435791016, 0.28539377450942993, 0.596328854560852, 0.9679235219955444]  ‚Üí  acq = -0.2200690315193491
X = [0.4400825500488281, 0.5658528208732605, 0.5425959229469299, 0.07132965326309204, 0.6977618336677551, 0.5552058219909668, 0.47397464513778687, 0.49641871452331543, 0.832179069519043, 0.21283124387264252, 0.0706908106803894, 0.3680011034011841, 0.5517967343330383, 0.8888658881187439, 0.03772050142288208, 0.30586808919906616, 0.7379536032676697, 0.47106170654296875, 0.42960864305496216]  ‚Üí  acq = -0.2288611146303019
X = [0.02293151617050171, 0.025504589080810547, 0.19622057676315308, 0.6123491525650024, 0.4694807529449463, 0.6615718603134155, 0.4360886216163635, 0.7734907269477844, 0.8508146405220032, 0.4204011559486389, 0.46971970796585083, 0.5794605612754822, 0.6292523145675659, 0.4632487893104553, 0.34889644384384155, 0.36618366837501526, 0.9968879222869873, 0.7180415391921997, 0.13323795795440674]  ‚Üí  acq = -0.2997374523263444
X = [0.6309345364570618, 0.9406264424324036, 0.23603194952011108, 0.6345602869987488, 0.14248567819595337, 0.5869790315628052, 0.919349193572998, 0.8825827240943909, 0.585145890712738, 0.3645155727863312, 0.4618048071861267, 0.3529096841812134, 0.9825409054756165, 0.9595580697059631, 0.9000720977783203, 0.17132039368152618, 0.7687629461288452, 0.3583839237689972, 0.4338597059249878]  ‚Üí  acq = -0.20818477275178016
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
input_X after fitting GP with prior data: [tensor(0.7400, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.2600, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 128, 1.517883041479706e-18, 48.0, 0]
input_X_between_0_1 after fitting GP with prior data: [tensor(0.7400, dtype=torch.float64), tensor(8.1445e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(5.6958e-18, dtype=torch.float64), tensor(1.6627e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.3052e-17, dtype=torch.float64), tensor(0.2600, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1.5179e-17, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity: None




======== BO iteration:  0  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.74
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.26

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (1.517883041479706e-18,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([0, 1, 0, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 0]
lora rank:  128
lora dropout:  1.517883041479706e-18
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 96,468,992 || all params: 8,126,730,240 || trainable%: 1.1871
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.9238, 'grad_norm': 1.7128593921661377, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.20035719871521, 'eval_runtime': 4.8421, 'eval_samples_per_second': 206.521, 'eval_steps_per_second': 13.011, 'epoch': 0.04}
{'loss': 1.0137, 'grad_norm': 0.5933307409286499, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9400253891944885, 'eval_runtime': 4.8615, 'eval_samples_per_second': 205.698, 'eval_steps_per_second': 12.959, 'epoch': 0.08}
{'loss': 0.894, 'grad_norm': 0.416863352060318, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9072219729423523, 'eval_runtime': 4.8476, 'eval_samples_per_second': 206.288, 'eval_steps_per_second': 12.996, 'epoch': 0.12}
{'loss': 0.8827, 'grad_norm': 0.24676533043384552, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.8911901712417603, 'eval_runtime': 4.8583, 'eval_samples_per_second': 205.834, 'eval_steps_per_second': 12.968, 'epoch': 0.16}
{'loss': 0.8839, 'grad_norm': 0.22668030858039856, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8861795663833618, 'eval_runtime': 4.8794, 'eval_samples_per_second': 204.945, 'eval_steps_per_second': 12.912, 'epoch': 0.2}
{'loss': 0.8466, 'grad_norm': 0.23085930943489075, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8744786977767944, 'eval_runtime': 4.8979, 'eval_samples_per_second': 204.17, 'eval_steps_per_second': 12.863, 'epoch': 0.24}
{'loss': 0.8403, 'grad_norm': 0.2536031901836395, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8721190094947815, 'eval_runtime': 4.8863, 'eval_samples_per_second': 204.656, 'eval_steps_per_second': 12.893, 'epoch': 0.28}
{'loss': 0.8171, 'grad_norm': 0.22085796296596527, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8690412640571594, 'eval_runtime': 4.8888, 'eval_samples_per_second': 204.55, 'eval_steps_per_second': 12.887, 'epoch': 0.32}
{'loss': 0.832, 'grad_norm': 0.20439891517162323, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8606541752815247, 'eval_runtime': 4.8897, 'eval_samples_per_second': 204.511, 'eval_steps_per_second': 12.884, 'epoch': 0.36}
{'loss': 0.809, 'grad_norm': 0.24766133725643158, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8575916886329651, 'eval_runtime': 4.8883, 'eval_samples_per_second': 204.572, 'eval_steps_per_second': 12.888, 'epoch': 0.4}
{'loss': 0.8307, 'grad_norm': 0.25860071182250977, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8579093217849731, 'eval_runtime': 4.8894, 'eval_samples_per_second': 204.526, 'eval_steps_per_second': 12.885, 'epoch': 0.44}
{'loss': 0.8101, 'grad_norm': 0.2407670021057129, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8521983623504639, 'eval_runtime': 4.8927, 'eval_samples_per_second': 204.386, 'eval_steps_per_second': 12.876, 'epoch': 0.48}
{'loss': 0.8072, 'grad_norm': 0.2184036821126938, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.849325954914093, 'eval_runtime': 4.8983, 'eval_samples_per_second': 204.153, 'eval_steps_per_second': 12.862, 'epoch': 0.52}
{'loss': 0.7917, 'grad_norm': 0.23631377518177032, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8450456261634827, 'eval_runtime': 4.8981, 'eval_samples_per_second': 204.162, 'eval_steps_per_second': 12.862, 'epoch': 0.56}
{'loss': 0.7947, 'grad_norm': 0.28129586577415466, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8435075879096985, 'eval_runtime': 4.9325, 'eval_samples_per_second': 202.737, 'eval_steps_per_second': 12.772, 'epoch': 0.6}
{'loss': 0.7854, 'grad_norm': 0.2530982792377472, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8413898944854736, 'eval_runtime': 4.9154, 'eval_samples_per_second': 203.441, 'eval_steps_per_second': 12.817, 'epoch': 0.64}
{'loss': 0.7849, 'grad_norm': 0.23641687631607056, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.839468240737915, 'eval_runtime': 4.9176, 'eval_samples_per_second': 203.349, 'eval_steps_per_second': 12.811, 'epoch': 0.68}
{'loss': 0.7607, 'grad_norm': 0.24364499747753143, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8377384543418884, 'eval_runtime': 4.9429, 'eval_samples_per_second': 202.312, 'eval_steps_per_second': 12.746, 'epoch': 0.72}
{'loss': 0.7664, 'grad_norm': 0.25972628593444824, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8357263207435608, 'eval_runtime': 4.9524, 'eval_samples_per_second': 201.923, 'eval_steps_per_second': 12.721, 'epoch': 0.76}
{'loss': 0.7539, 'grad_norm': 0.2736654281616211, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.833787739276886, 'eval_runtime': 4.9206, 'eval_samples_per_second': 203.226, 'eval_steps_per_second': 12.803, 'epoch': 0.8}
{'loss': 0.7659, 'grad_norm': 0.3505919575691223, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8323937058448792, 'eval_runtime': 4.9069, 'eval_samples_per_second': 203.796, 'eval_steps_per_second': 12.839, 'epoch': 0.84}
{'loss': 0.7341, 'grad_norm': 0.28253433108329773, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8306311368942261, 'eval_runtime': 4.8995, 'eval_samples_per_second': 204.102, 'eval_steps_per_second': 12.858, 'epoch': 0.88}
{'loss': 0.7309, 'grad_norm': 0.26751741766929626, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.830501914024353, 'eval_runtime': 4.8993, 'eval_samples_per_second': 204.113, 'eval_steps_per_second': 12.859, 'epoch': 0.92}
{'loss': 0.7398, 'grad_norm': 0.27341026067733765, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8289157748222351, 'eval_runtime': 4.8968, 'eval_samples_per_second': 204.215, 'eval_steps_per_second': 12.866, 'epoch': 0.96}
{'loss': 0.7171, 'grad_norm': 0.33179059624671936, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8283027410507202, 'eval_runtime': 4.8997, 'eval_samples_per_second': 204.093, 'eval_steps_per_second': 12.858, 'epoch': 1.0}
{'train_runtime': 264.0193, 'train_samples_per_second': 37.872, 'train_steps_per_second': 2.367, 'train_loss': 0.8926691925048829, 'epoch': 1.0}
train_results:  {'eval_loss': [1.20035719871521, 0.9400253891944885, 0.9072219729423523, 0.8911901712417603, 0.8861795663833618, 0.8744786977767944, 0.8721190094947815, 0.8690412640571594, 0.8606541752815247, 0.8575916886329651, 0.8579093217849731, 0.8521983623504639, 0.849325954914093, 0.8450456261634827, 0.8435075879096985, 0.8413898944854736, 0.839468240737915, 0.8377384543418884, 0.8357263207435608, 0.833787739276886, 0.8323937058448792, 0.8306311368942261, 0.830501914024353, 0.8289157748222351, 0.8283027410507202], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.20035719871521, 0.9400253891944885, 0.9072219729423523, 0.8911901712417603, 0.8861795663833618, 0.8744786977767944, 0.8721190094947815, 0.8690412640571594, 0.8606541752815247, 0.8575916886329651, 0.8579093217849731, 0.8521983623504639, 0.849325954914093, 0.8450456261634827, 0.8435075879096985, 0.8413898944854736, 0.839468240737915, 0.8377384543418884, 0.8357263207435608, 0.833787739276886, 0.8323937058448792, 0.8306311368942261, 0.830501914024353, 0.8289157748222351, 0.8283027410507202]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2132879495620728
current iteration best possible eval_loss (full train run):  -0.8283027410507202
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.3100 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7826806306838989, 0.9742068648338318, 0.5234155654907227, 0.18105673789978027, 0.5273805260658264, 0.21370339393615723, 0.09195005893707275, 0.31287604570388794, 0.8331720232963562, 0.9290022253990173, 0.6879664659500122, 0.5085357427597046, 0.47773629426956177, 0.1947622299194336, 0.22680413722991943, 0.9227204918861389, 0.7185676097869873, 0.6147770881652832, 0.4975128173828125]  ‚Üí  acq = -0.5391018652065698
X = [0.4750671982765198, 0.07905298471450806, 0.8066083788871765, 0.9066379070281982, 0.05567741394042969, 0.1928083300590515, 0.32484060525894165, 0.4433099627494812, 0.2890252470970154, 0.9325734376907349, 0.5378451347351074, 0.12646150588989258, 0.34055233001708984, 0.9862186908721924, 0.7511762380599976, 0.620393693447113, 0.17488831281661987, 0.5298567414283752, 0.6103439927101135]  ‚Üí  acq = -0.5226482940139258
X = [0.4159814119338989, 0.07608544826507568, 0.9775634407997131, 0.9005048274993896, 0.4587010145187378, 0.32811009883880615, 0.8625470995903015, 0.05485790967941284, 0.7054996490478516, 0.9007022976875305, 0.8941611647605896, 0.006784617900848389, 0.9708253741264343, 0.9738534092903137, 0.9028333425521851, 0.6683018207550049, 0.03373575210571289, 0.7236707210540771, 0.05047386884689331]  ‚Üí  acq = -0.5226489575539082
X = [0.04051196575164795, 0.34857720136642456, 0.9334995746612549, 0.08477455377578735, 0.1721893548965454, 0.18077385425567627, 0.1510382890701294, 0.11512458324432373, 0.49603205919265747, 0.08502563089132309, 0.8605102300643921, 0.8794232606887817, 0.0070765018463134766, 0.7316026091575623, 0.8372434377670288, 0.20095951855182648, 0.11787313222885132, 0.6393579244613647, 0.8474140167236328]  ‚Üí  acq = -0.5741976897682142
X = [0.38952839374542236, 0.3167262077331543, 0.3646835684776306, 0.687441885471344, 0.5183271765708923, 0.2513403296470642, 0.7209311127662659, 0.06702560186386108, 0.2037135362625122, 0.3290117681026459, 0.6907155513763428, 0.8127150535583496, 0.4432212710380554, 0.06876933574676514, 0.07623255252838135, 0.7192770838737488, 0.5579009056091309, 0.18385685980319977, 0.11628907918930054]  ‚Üí  acq = -0.5223528377583446
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2898, dtype=torch.float64), 0, 0, 0, 0, 0, 0, 0, tensor(0.7102, dtype=torch.float64), 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2898, dtype=torch.float64), tensor(4.1705e-16, dtype=torch.float64), tensor(1.0390e-16, dtype=torch.float64), tensor(1.8685e-17, dtype=torch.float64), tensor(2.1366e-15, dtype=torch.float64), tensor(8.3717e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.9305e-18, dtype=torch.float64), tensor(0.7102, dtype=torch.float64), tensor(0.0413, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  1  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.29
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.71

LoRA Parameters:
  lora_r: (2,)
  lora_dropout: (0.1,)
  num_layers_to_apply: (1,)
  five_dim_vector: ([1, 0, 1, 0, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  1
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 0]
lora rank:  2
lora dropout:  0.1
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 53,248 || all params: 8,030,314,496 || trainable%: 0.0007
length of training data:  9999
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 4.0556, 'grad_norm': 7.179890155792236, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 3.990398406982422, 'eval_runtime': 4.415, 'eval_samples_per_second': 226.5, 'eval_steps_per_second': 14.269, 'epoch': 0.04}
{'loss': 2.3484, 'grad_norm': 4.307501316070557, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 2.0943517684936523, 'eval_runtime': 4.4429, 'eval_samples_per_second': 225.076, 'eval_steps_per_second': 14.18, 'epoch': 0.08}
{'loss': 1.4595, 'grad_norm': 2.735462188720703, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.5037999153137207, 'eval_runtime': 4.4705, 'eval_samples_per_second': 223.688, 'eval_steps_per_second': 14.092, 'epoch': 0.12}
{'loss': 1.221, 'grad_norm': 1.9771614074707031, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.3103983402252197, 'eval_runtime': 4.5004, 'eval_samples_per_second': 222.203, 'eval_steps_per_second': 13.999, 'epoch': 0.16}
{'loss': 1.1047, 'grad_norm': 1.9189460277557373, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.2098537683486938, 'eval_runtime': 4.4793, 'eval_samples_per_second': 223.248, 'eval_steps_per_second': 14.065, 'epoch': 0.2}
{'loss': 1.0187, 'grad_norm': 2.1124064922332764, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.1399122476577759, 'eval_runtime': 4.4733, 'eval_samples_per_second': 223.55, 'eval_steps_per_second': 14.084, 'epoch': 0.24}
{'loss': 1.0035, 'grad_norm': 2.759960412979126, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 1.1235395669937134, 'eval_runtime': 4.4499, 'eval_samples_per_second': 224.726, 'eval_steps_per_second': 14.158, 'epoch': 0.28}
{'loss': 0.9968, 'grad_norm': 2.211729049682617, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 1.1093251705169678, 'eval_runtime': 4.4239, 'eval_samples_per_second': 226.046, 'eval_steps_per_second': 14.241, 'epoch': 0.32}
{'loss': 1.0046, 'grad_norm': 2.2762291431427, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 1.0963777303695679, 'eval_runtime': 4.4665, 'eval_samples_per_second': 223.889, 'eval_steps_per_second': 14.105, 'epoch': 0.36}
{'loss': 0.9964, 'grad_norm': 2.357469320297241, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 1.092567801475525, 'eval_runtime': 4.4985, 'eval_samples_per_second': 222.294, 'eval_steps_per_second': 14.005, 'epoch': 0.4}
{'loss': 0.9715, 'grad_norm': 2.4865100383758545, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 1.0952022075653076, 'eval_runtime': 4.4711, 'eval_samples_per_second': 223.66, 'eval_steps_per_second': 14.091, 'epoch': 0.44}
{'loss': 0.9577, 'grad_norm': 1.8701558113098145, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 1.0843132734298706, 'eval_runtime': 4.4798, 'eval_samples_per_second': 223.224, 'eval_steps_per_second': 14.063, 'epoch': 0.48}
{'loss': 0.9893, 'grad_norm': 1.915305256843567, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 1.081109642982483, 'eval_runtime': 4.4807, 'eval_samples_per_second': 223.177, 'eval_steps_per_second': 14.06, 'epoch': 0.52}
{'loss': 0.9468, 'grad_norm': 2.139662981033325, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 1.0808666944503784, 'eval_runtime': 4.4656, 'eval_samples_per_second': 223.932, 'eval_steps_per_second': 14.108, 'epoch': 0.56}
{'loss': 0.9615, 'grad_norm': 1.8199011087417603, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 1.0829181671142578, 'eval_runtime': 4.4752, 'eval_samples_per_second': 223.453, 'eval_steps_per_second': 14.078, 'epoch': 0.6}
{'loss': 0.9497, 'grad_norm': 2.390780448913574, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 1.0758284330368042, 'eval_runtime': 4.4585, 'eval_samples_per_second': 224.292, 'eval_steps_per_second': 14.13, 'epoch': 0.64}
{'loss': 0.9679, 'grad_norm': 2.11903977394104, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 1.072386384010315, 'eval_runtime': 4.4501, 'eval_samples_per_second': 224.714, 'eval_steps_per_second': 14.157, 'epoch': 0.68}
{'loss': 0.9604, 'grad_norm': 2.0277059078216553, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 1.0708564519882202, 'eval_runtime': 4.4562, 'eval_samples_per_second': 224.405, 'eval_steps_per_second': 14.137, 'epoch': 0.72}
{'loss': 0.9558, 'grad_norm': 2.185256242752075, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 1.0680456161499023, 'eval_runtime': 4.4861, 'eval_samples_per_second': 222.909, 'eval_steps_per_second': 14.043, 'epoch': 0.76}
{'loss': 0.9701, 'grad_norm': 2.242678165435791, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 1.0676064491271973, 'eval_runtime': 4.4623, 'eval_samples_per_second': 224.097, 'eval_steps_per_second': 14.118, 'epoch': 0.8}
{'loss': 0.9703, 'grad_norm': 1.759384036064148, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 1.0661572217941284, 'eval_runtime': 4.4534, 'eval_samples_per_second': 224.548, 'eval_steps_per_second': 14.147, 'epoch': 0.84}
{'loss': 0.9379, 'grad_norm': 1.9794245958328247, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 1.0670361518859863, 'eval_runtime': 4.4389, 'eval_samples_per_second': 225.281, 'eval_steps_per_second': 14.193, 'epoch': 0.88}
{'loss': 0.9246, 'grad_norm': 2.1198768615722656, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 1.0692781209945679, 'eval_runtime': 4.4378, 'eval_samples_per_second': 225.339, 'eval_steps_per_second': 14.196, 'epoch': 0.92}
{'loss': 0.9391, 'grad_norm': 2.6136972904205322, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 1.0674240589141846, 'eval_runtime': 4.4406, 'eval_samples_per_second': 225.197, 'eval_steps_per_second': 14.187, 'epoch': 0.96}
{'loss': 0.9713, 'grad_norm': 2.6796646118164062, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 1.0663707256317139, 'eval_runtime': 4.4364, 'eval_samples_per_second': 225.406, 'eval_steps_per_second': 14.201, 'epoch': 1.0}
{'train_runtime': 180.7938, 'train_samples_per_second': 55.306, 'train_steps_per_second': 3.457, 'train_loss': 1.183324917602539, 'epoch': 1.0}
train_results:  {'eval_loss': [3.990398406982422, 2.0943517684936523, 1.5037999153137207, 1.3103983402252197, 1.2098537683486938, 1.1399122476577759, 1.1235395669937134, 1.1093251705169678, 1.0963777303695679, 1.092567801475525, 1.0952022075653076, 1.0843132734298706, 1.081109642982483, 1.0808666944503784, 1.0829181671142578, 1.0758284330368042, 1.072386384010315, 1.0708564519882202, 1.0680456161499023, 1.0676064491271973, 1.0661572217941284, 1.0670361518859863, 1.0692781209945679, 1.0674240589141846, 1.0663707256317139], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [3.990398406982422, 2.0943517684936523, 1.5037999153137207, 1.3103983402252197, 1.2098537683486938, 1.1399122476577759, 1.1235395669937134, 1.1093251705169678, 1.0963777303695679, 1.092567801475525, 1.0952022075653076, 1.0843132734298706, 1.081109642982483, 1.0808666944503784, 1.0829181671142578, 1.0758284330368042, 1.072386384010315, 1.0708564519882202, 1.0680456161499023, 1.0676064491271973, 1.0661572217941284, 1.0670361518859863, 1.0692781209945679, 1.0674240589141846, 1.0663707256317139]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.5139060020446777
current iteration best possible eval_loss (full train run):  -1.0663707256317139
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.1175 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.12540125846862793, 0.8852415680885315, 0.29567885398864746, 0.13903272151947021, 0.6396822929382324, 0.8770517110824585, 0.4980446696281433, 0.41083741188049316, 0.4408547878265381, 0.3182459771633148, 0.7194241881370544, 0.04319125413894653, 0.7420942187309265, 0.7179659008979797, 0.5257965922355652, 0.7050647735595703, 0.6451549530029297, 0.8105471134185791, 0.5732041597366333]  ‚Üí  acq = -0.724604451227777
X = [0.4870757460594177, 0.0006139278411865234, 0.7226466536521912, 0.6739351153373718, 0.5888557434082031, 0.5384756922721863, 0.817223310470581, 0.5232639908790588, 0.6168438792228699, 0.6196032762527466, 0.7255858778953552, 0.24855589866638184, 0.7509732246398926, 0.02502620220184326, 0.2894328832626343, 0.9496669173240662, 0.8085587620735168, 0.20722834765911102, 0.36882972717285156]  ‚Üí  acq = -0.6441976365666593
X = [0.798983633518219, 0.8843463659286499, 0.7710047364234924, 0.21985924243927002, 0.6831211447715759, 0.5281556844711304, 0.5095335841178894, 0.6907831430435181, 0.5326343774795532, 0.20761679112911224, 0.383426308631897, 0.0802617073059082, 0.7871682047843933, 0.21052950620651245, 0.050669074058532715, 0.8629186749458313, 0.040459275245666504, 0.6426770687103271, 0.9872360825538635]  ‚Üí  acq = -0.6445070092644274
X = [0.7496808767318726, 0.058696627616882324, 0.31605440378189087, 0.7841656804084778, 0.05163168907165527, 0.7293487191200256, 0.4531790614128113, 0.05231660604476929, 0.0616917610168457, 0.20189379155635834, 0.2742578983306885, 0.3373923897743225, 0.5551875829696655, 0.2517983913421631, 0.2105121612548828, 0.8294119238853455, 0.5374197959899902, 0.07103338837623596, 0.4950082302093506]  ‚Üí  acq = -0.6437065895582793
X = [0.8153727054595947, 0.4259818196296692, 0.212477445602417, 0.6852957010269165, 0.6809787154197693, 0.5394172072410583, 0.20402950048446655, 0.6490947604179382, 0.1939259171485901, 0.959380567073822, 0.11465698480606079, 0.5397877097129822, 0.8247414231300354, 0.7748119831085205, 0.13258826732635498, 0.09844227880239487, 0.1842997670173645, 0.08216378092765808, 0.8576348423957825]  ‚Üí  acq = -0.6442198586533661
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2135, dtype=torch.float64), 0, 0, 0, 0, 0, 0, tensor(0.4145, dtype=torch.float64), tensor(0.3720, dtype=torch.float64), 29, 0, 0, 1, 1, 1, 128, 8.673617379884035e-19, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.2135, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1.8137e-16, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4145, dtype=torch.float64), tensor(0.3720, dtype=torch.float64), tensor(0.9131, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(8.6736e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  2  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.213
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.415
  arc_challenge: 0.372

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (8.673617379884035e-19,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  128
lora dropout:  8.673617379884035e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,258,752 || all params: 8,235,520,000 || trainable%: 2.4924
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5659, 'grad_norm': 0.5162070989608765, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.2234236001968384, 'eval_runtime': 5.2717, 'eval_samples_per_second': 189.693, 'eval_steps_per_second': 11.951, 'epoch': 0.04}
{'loss': 1.1576, 'grad_norm': 0.25074177980422974, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9764691591262817, 'eval_runtime': 5.2633, 'eval_samples_per_second': 189.994, 'eval_steps_per_second': 11.97, 'epoch': 0.08}
{'loss': 1.0516, 'grad_norm': 0.20954838395118713, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9433419108390808, 'eval_runtime': 5.2806, 'eval_samples_per_second': 189.374, 'eval_steps_per_second': 11.931, 'epoch': 0.12}
{'loss': 1.0361, 'grad_norm': 0.26574215292930603, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9342995882034302, 'eval_runtime': 5.3134, 'eval_samples_per_second': 188.204, 'eval_steps_per_second': 11.857, 'epoch': 0.16}
{'loss': 1.0346, 'grad_norm': 0.20483289659023285, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9185476899147034, 'eval_runtime': 5.2895, 'eval_samples_per_second': 189.053, 'eval_steps_per_second': 11.91, 'epoch': 0.2}
{'loss': 1.0271, 'grad_norm': 0.22511962056159973, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.928764283657074, 'eval_runtime': 5.2837, 'eval_samples_per_second': 189.262, 'eval_steps_per_second': 11.924, 'epoch': 0.24}
{'loss': 0.9617, 'grad_norm': 0.18940339982509613, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9126558899879456, 'eval_runtime': 5.2867, 'eval_samples_per_second': 189.153, 'eval_steps_per_second': 11.917, 'epoch': 0.28}
{'loss': 1.0053, 'grad_norm': 0.24812808632850647, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9121569991111755, 'eval_runtime': 5.2851, 'eval_samples_per_second': 189.21, 'eval_steps_per_second': 11.92, 'epoch': 0.32}
{'loss': 0.9094, 'grad_norm': 0.2658897936344147, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9034304022789001, 'eval_runtime': 5.2885, 'eval_samples_per_second': 189.089, 'eval_steps_per_second': 11.913, 'epoch': 0.36}
{'loss': 0.9591, 'grad_norm': 0.22865326702594757, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8972463607788086, 'eval_runtime': 5.3118, 'eval_samples_per_second': 188.26, 'eval_steps_per_second': 11.86, 'epoch': 0.4}
{'loss': 0.9381, 'grad_norm': 0.22847431898117065, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9033331274986267, 'eval_runtime': 5.2977, 'eval_samples_per_second': 188.76, 'eval_steps_per_second': 11.892, 'epoch': 0.44}
{'loss': 0.9289, 'grad_norm': 0.30776551365852356, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8964508175849915, 'eval_runtime': 5.2968, 'eval_samples_per_second': 188.792, 'eval_steps_per_second': 11.894, 'epoch': 0.48}
{'loss': 0.9559, 'grad_norm': 0.26263532042503357, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8912531733512878, 'eval_runtime': 5.2927, 'eval_samples_per_second': 188.938, 'eval_steps_per_second': 11.903, 'epoch': 0.52}
{'loss': 0.8409, 'grad_norm': 0.26487433910369873, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8896157741546631, 'eval_runtime': 5.2876, 'eval_samples_per_second': 189.122, 'eval_steps_per_second': 11.915, 'epoch': 0.56}
{'loss': 0.8676, 'grad_norm': 0.26985040307044983, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8899272084236145, 'eval_runtime': 5.2903, 'eval_samples_per_second': 189.025, 'eval_steps_per_second': 11.909, 'epoch': 0.6}
{'loss': 0.8521, 'grad_norm': 0.30253857374191284, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8844196200370789, 'eval_runtime': 5.2841, 'eval_samples_per_second': 189.248, 'eval_steps_per_second': 11.923, 'epoch': 0.64}
{'loss': 0.8402, 'grad_norm': 0.2773488461971283, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8763958811759949, 'eval_runtime': 5.2832, 'eval_samples_per_second': 189.281, 'eval_steps_per_second': 11.925, 'epoch': 0.68}
{'loss': 0.8795, 'grad_norm': 0.28015655279159546, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.883054792881012, 'eval_runtime': 5.2811, 'eval_samples_per_second': 189.354, 'eval_steps_per_second': 11.929, 'epoch': 0.72}
{'loss': 0.8835, 'grad_norm': 0.2872267961502075, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8754818439483643, 'eval_runtime': 5.2852, 'eval_samples_per_second': 189.207, 'eval_steps_per_second': 11.92, 'epoch': 0.76}
{'loss': 0.8359, 'grad_norm': 0.2870900630950928, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8735339641571045, 'eval_runtime': 5.2831, 'eval_samples_per_second': 189.283, 'eval_steps_per_second': 11.925, 'epoch': 0.8}
{'loss': 0.8028, 'grad_norm': 0.30223017930984497, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8734024167060852, 'eval_runtime': 5.2854, 'eval_samples_per_second': 189.201, 'eval_steps_per_second': 11.92, 'epoch': 0.84}
{'loss': 0.7888, 'grad_norm': 0.31890392303466797, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8712112307548523, 'eval_runtime': 5.2929, 'eval_samples_per_second': 188.931, 'eval_steps_per_second': 11.903, 'epoch': 0.88}
{'loss': 0.7844, 'grad_norm': 0.24605527520179749, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.870474100112915, 'eval_runtime': 5.353, 'eval_samples_per_second': 186.81, 'eval_steps_per_second': 11.769, 'epoch': 0.92}
{'loss': 0.7793, 'grad_norm': 0.23007962107658386, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8695114254951477, 'eval_runtime': 5.3376, 'eval_samples_per_second': 187.35, 'eval_steps_per_second': 11.803, 'epoch': 0.96}
{'loss': 0.7726, 'grad_norm': 0.2607749104499817, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8693589568138123, 'eval_runtime': 5.3191, 'eval_samples_per_second': 188.001, 'eval_steps_per_second': 11.844, 'epoch': 1.0}
{'train_runtime': 341.9481, 'train_samples_per_second': 29.238, 'train_steps_per_second': 1.828, 'train_loss': 0.9783609649658204, 'epoch': 1.0}
train_results:  {'eval_loss': [1.2234236001968384, 0.9764691591262817, 0.9433419108390808, 0.9342995882034302, 0.9185476899147034, 0.928764283657074, 0.9126558899879456, 0.9121569991111755, 0.9034304022789001, 0.8972463607788086, 0.9033331274986267, 0.8964508175849915, 0.8912531733512878, 0.8896157741546631, 0.8899272084236145, 0.8844196200370789, 0.8763958811759949, 0.883054792881012, 0.8754818439483643, 0.8735339641571045, 0.8734024167060852, 0.8712112307548523, 0.870474100112915, 0.8695114254951477, 0.8693589568138123], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.2234236001968384, 0.9764691591262817, 0.9433419108390808, 0.9342995882034302, 0.9185476899147034, 0.928764283657074, 0.9126558899879456, 0.9121569991111755, 0.9034304022789001, 0.8972463607788086, 0.9033331274986267, 0.8964508175849915, 0.8912531733512878, 0.8896157741546631, 0.8899272084236145, 0.8844196200370789, 0.8763958811759949, 0.883054792881012, 0.8754818439483643, 0.8735339641571045, 0.8734024167060852, 0.8712112307548523, 0.870474100112915, 0.8695114254951477, 0.8693589568138123]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.257798433303833
current iteration best possible eval_loss (full train run):  -0.8693589568138123
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.4590 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9304684400558472, 0.6001928448677063, 0.07802873849868774, 0.07184088230133057, 0.20331209897994995, 0.4108502268791199, 0.1417362093925476, 0.07630598545074463, 0.8343492150306702, 0.6439042091369629, 0.5720279216766357, 0.19384700059890747, 0.2411465048789978, 0.5439621806144714, 0.8785960674285889, 0.7869397401809692, 0.10385686159133911, 0.19263038039207458, 0.07206535339355469]  ‚Üí  acq = -0.7192551890545303
X = [0.9350422620773315, 0.7864449620246887, 0.9500066637992859, 0.18607103824615479, 0.6031085848808289, 0.2918567657470703, 0.2331099510192871, 0.2678210139274597, 0.02810537815093994, 0.4030059278011322, 0.5877178907394409, 0.6469395160675049, 0.8880372643470764, 0.4331531524658203, 0.8628382086753845, 0.20435945689678192, 0.16291123628616333, 0.577731728553772, 0.34905433654785156]  ‚Üí  acq = -0.8334391140654343
X = [0.8309503197669983, 0.5928624272346497, 0.11796188354492188, 0.6550196409225464, 0.5562008619308472, 0.7371083498001099, 0.055686235427856445, 0.4309970736503601, 0.9301089644432068, 0.8630064129829407, 0.010585129261016846, 0.051930129528045654, 0.4764968156814575, 0.9831361174583435, 0.5003925561904907, 0.9841403961181641, 0.11054939031600952, 0.19516916573047638, 0.7232905030250549]  ‚Üí  acq = -0.8269616106293278
X = [0.658439576625824, 0.13676774501800537, 0.5683725476264954, 0.9242382049560547, 0.6179104447364807, 0.2626451849937439, 0.6538912653923035, 0.08030986785888672, 0.728330671787262, 0.6187694668769836, 0.6138982772827148, 0.23371434211730957, 0.6261714696884155, 0.4185717701911926, 0.0390055775642395, 0.08426979929208755, 0.9996876120567322, 0.7565531730651855, 0.8432244062423706]  ‚Üí  acq = -0.8333918077149631
X = [0.8400817513465881, 0.7823814153671265, 0.7090836763381958, 0.12987852096557617, 0.05340629816055298, 0.6297608613967896, 0.7674234509468079, 0.4919307827949524, 0.7522366642951965, 0.638248860836029, 0.3141288757324219, 0.5084896087646484, 0.506928563117981, 0.4593488574028015, 0.9671209454536438, 0.11121711134910583, 0.006412029266357422, 0.5255816578865051, 0.5448671579360962]  ‚Üí  acq = -0.8333789641840497
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3151, dtype=torch.float64), 0, 0, 0, tensor(0.1617, dtype=torch.float64), tensor(0.2234, dtype=torch.float64), 0, 0, tensor(0.2998, dtype=torch.float64), 32, 1, 0, 0, 1, 1, 128, 0.0, 22.507652426013916, 0]
normalized proposed parameters for next round by BO: [tensor(0.3151, dtype=torch.float64), tensor(5.5675e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1617, dtype=torch.float64), tensor(0.2234, dtype=torch.float64), tensor(1.1753e-16, dtype=torch.float64), tensor(4.9084e-17, dtype=torch.float64), tensor(0.2998, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4689, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  3  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.315
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.162
  truthfulqa_gen: 0.223
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.3

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (0.0,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (22.507652426013916,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  128
lora dropout:  0.0
lora alpha:  22.507652426013916
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 184,549,376 || all params: 8,214,810,624 || trainable%: 2.2465
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3929, 'grad_norm': 0.8026558756828308, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5040674209594727, 'eval_runtime': 5.2195, 'eval_samples_per_second': 191.588, 'eval_steps_per_second': 12.07, 'epoch': 0.04}
{'loss': 1.143, 'grad_norm': 0.4897219240665436, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0183861255645752, 'eval_runtime': 5.2354, 'eval_samples_per_second': 191.008, 'eval_steps_per_second': 12.034, 'epoch': 0.08}
{'loss': 0.9337, 'grad_norm': 0.266063928604126, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9381167888641357, 'eval_runtime': 5.2287, 'eval_samples_per_second': 191.251, 'eval_steps_per_second': 12.049, 'epoch': 0.12}
{'loss': 0.8906, 'grad_norm': 0.6508128643035889, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9243330359458923, 'eval_runtime': 5.241, 'eval_samples_per_second': 190.804, 'eval_steps_per_second': 12.021, 'epoch': 0.16}
{'loss': 0.8761, 'grad_norm': 0.1488657146692276, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9143970608711243, 'eval_runtime': 5.2877, 'eval_samples_per_second': 189.119, 'eval_steps_per_second': 11.915, 'epoch': 0.2}
{'loss': 0.8596, 'grad_norm': 0.16304722428321838, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9185562133789062, 'eval_runtime': 5.3134, 'eval_samples_per_second': 188.204, 'eval_steps_per_second': 11.857, 'epoch': 0.24}
{'loss': 0.8373, 'grad_norm': 0.1460723876953125, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9041658043861389, 'eval_runtime': 5.2994, 'eval_samples_per_second': 188.701, 'eval_steps_per_second': 11.888, 'epoch': 0.28}
{'loss': 0.8351, 'grad_norm': 0.14903824031352997, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9063487648963928, 'eval_runtime': 5.2461, 'eval_samples_per_second': 190.617, 'eval_steps_per_second': 12.009, 'epoch': 0.32}
{'loss': 0.8024, 'grad_norm': 0.16577784717082977, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8980498909950256, 'eval_runtime': 5.2467, 'eval_samples_per_second': 190.595, 'eval_steps_per_second': 12.007, 'epoch': 0.36}
{'loss': 0.8074, 'grad_norm': 0.16532671451568604, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8945652842521667, 'eval_runtime': 5.2505, 'eval_samples_per_second': 190.459, 'eval_steps_per_second': 11.999, 'epoch': 0.4}
{'loss': 0.7978, 'grad_norm': 0.15353485941886902, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8884025812149048, 'eval_runtime': 5.2515, 'eval_samples_per_second': 190.422, 'eval_steps_per_second': 11.997, 'epoch': 0.44}
{'loss': 0.7846, 'grad_norm': 0.16538335382938385, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8864280581474304, 'eval_runtime': 5.2477, 'eval_samples_per_second': 190.56, 'eval_steps_per_second': 12.005, 'epoch': 0.48}
{'loss': 0.7692, 'grad_norm': 0.192817822098732, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8830583095550537, 'eval_runtime': 5.2517, 'eval_samples_per_second': 190.416, 'eval_steps_per_second': 11.996, 'epoch': 0.52}
{'loss': 0.7412, 'grad_norm': 0.21442179381847382, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8851628303527832, 'eval_runtime': 5.2498, 'eval_samples_per_second': 190.484, 'eval_steps_per_second': 12.0, 'epoch': 0.56}
{'loss': 0.7472, 'grad_norm': 0.21278116106987, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.886059582233429, 'eval_runtime': 5.252, 'eval_samples_per_second': 190.402, 'eval_steps_per_second': 11.995, 'epoch': 0.6}
{'loss': 0.7656, 'grad_norm': 0.25314611196517944, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8796650767326355, 'eval_runtime': 5.2485, 'eval_samples_per_second': 190.532, 'eval_steps_per_second': 12.004, 'epoch': 0.64}
{'loss': 0.7031, 'grad_norm': 0.1978495866060257, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8753961324691772, 'eval_runtime': 5.2587, 'eval_samples_per_second': 190.16, 'eval_steps_per_second': 11.98, 'epoch': 0.68}
{'loss': 0.7182, 'grad_norm': 0.1933818906545639, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8749682903289795, 'eval_runtime': 5.2671, 'eval_samples_per_second': 189.859, 'eval_steps_per_second': 11.961, 'epoch': 0.72}
{'loss': 0.7076, 'grad_norm': 0.2073114812374115, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8740702271461487, 'eval_runtime': 5.2779, 'eval_samples_per_second': 189.468, 'eval_steps_per_second': 11.936, 'epoch': 0.76}
{'loss': 0.6964, 'grad_norm': 0.2355177104473114, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8700084090232849, 'eval_runtime': 5.2706, 'eval_samples_per_second': 189.731, 'eval_steps_per_second': 11.953, 'epoch': 0.8}
{'loss': 0.682, 'grad_norm': 0.21592412889003754, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8704216480255127, 'eval_runtime': 5.2994, 'eval_samples_per_second': 188.7, 'eval_steps_per_second': 11.888, 'epoch': 0.84}
{'loss': 0.6542, 'grad_norm': 0.290164589881897, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8687536716461182, 'eval_runtime': 5.2538, 'eval_samples_per_second': 190.339, 'eval_steps_per_second': 11.991, 'epoch': 0.88}
{'loss': 0.653, 'grad_norm': 0.25130853056907654, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8670089244842529, 'eval_runtime': 5.2495, 'eval_samples_per_second': 190.494, 'eval_steps_per_second': 12.001, 'epoch': 0.92}
{'loss': 0.6505, 'grad_norm': 0.21622629463672638, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8674851059913635, 'eval_runtime': 5.2601, 'eval_samples_per_second': 190.109, 'eval_steps_per_second': 11.977, 'epoch': 0.96}
{'loss': 0.6613, 'grad_norm': 0.29733434319496155, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8668621778488159, 'eval_runtime': 5.2515, 'eval_samples_per_second': 190.422, 'eval_steps_per_second': 11.997, 'epoch': 1.0}
{'train_runtime': 288.0856, 'train_samples_per_second': 34.705, 'train_steps_per_second': 2.169, 'train_loss': 0.8843918273925782, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5040674209594727, 1.0183861255645752, 0.9381167888641357, 0.9243330359458923, 0.9143970608711243, 0.9185562133789062, 0.9041658043861389, 0.9063487648963928, 0.8980498909950256, 0.8945652842521667, 0.8884025812149048, 0.8864280581474304, 0.8830583095550537, 0.8851628303527832, 0.886059582233429, 0.8796650767326355, 0.8753961324691772, 0.8749682903289795, 0.8740702271461487, 0.8700084090232849, 0.8704216480255127, 0.8687536716461182, 0.8670089244842529, 0.8674851059913635, 0.8668621778488159], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5040674209594727, 1.0183861255645752, 0.9381167888641357, 0.9243330359458923, 0.9143970608711243, 0.9185562133789062, 0.9041658043861389, 0.9063487648963928, 0.8980498909950256, 0.8945652842521667, 0.8884025812149048, 0.8864280581474304, 0.8830583095550537, 0.8851628303527832, 0.886059582233429, 0.8796650767326355, 0.8753961324691772, 0.8749682903289795, 0.8740702271461487, 0.8700084090232849, 0.8704216480255127, 0.8687536716461182, 0.8670089244842529, 0.8674851059913635, 0.8668621778488159]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1118427515029907
current iteration best possible eval_loss (full train run):  -0.8668621778488159
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 1.6843 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8574292659759521, 0.7720642685890198, 0.34553974866867065, 0.3679484724998474, 0.7239366173744202, 0.0920833945274353, 0.18859398365020752, 0.6747823357582092, 0.07535994052886963, 0.40952304005622864, 0.6871200203895569, 0.11235320568084717, 0.3087785840034485, 0.8571810126304626, 0.2481454610824585, 0.4782077968120575, 0.9228593707084656, 0.3881321847438812, 0.9746328592300415]  ‚Üí  acq = -0.8376349102835847
X = [0.6234831213951111, 0.8127129673957825, 0.36968737840652466, 0.5844079256057739, 0.7788641452789307, 0.7181087136268616, 0.7788925766944885, 0.06511753797531128, 0.6828930377960205, 0.05651962757110596, 0.573238730430603, 0.6327170729637146, 0.18307894468307495, 0.8158033490180969, 0.06521856784820557, 0.7243998050689697, 0.8793015480041504, 0.4533410370349884, 0.018241524696350098]  ‚Üí  acq = -0.8339320859795583
X = [0.24746304750442505, 0.25033313035964966, 0.9069421291351318, 0.3778378367424011, 0.9698758125305176, 0.17303234338760376, 0.10521763563156128, 0.19993489980697632, 0.9870834350585938, 0.3404318690299988, 0.21306800842285156, 0.6377829909324646, 0.8913337588310242, 0.044623494148254395, 0.7074243426322937, 0.4051145911216736, 0.3545602560043335, 0.24171993136405945, 0.7204521298408508]  ‚Üí  acq = -0.8334330418832981
X = [0.6241765022277832, 0.8897442817687988, 0.17849880456924438, 0.716151237487793, 0.6833332777023315, 0.020620882511138916, 0.5157556533813477, 0.9479966163635254, 0.84186190366745, 0.14147183299064636, 0.0617673397064209, 0.21349221467971802, 0.9864579439163208, 0.22172331809997559, 0.2996382713317871, 0.03686213493347168, 0.6170431971549988, 0.31663525104522705, 0.971221923828125]  ‚Üí  acq = -0.833492551516585
X = [0.22086328268051147, 0.282958984375, 0.15647387504577637, 0.7640331387519836, 0.1157483458518982, 0.6380847692489624, 0.8118444085121155, 0.9104241132736206, 0.7222160696983337, 0.07315658777952194, 0.8714834451675415, 0.1990312933921814, 0.9923198819160461, 0.8284327983856201, 0.14262902736663818, 0.3115057945251465, 0.8077713251113892, 0.628324031829834, 0.3487977981567383]  ‚Üí  acq = -0.8334675230329605
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3254, dtype=torch.float64), tensor(0.0880, dtype=torch.float64), 0, 0, 0, tensor(0.0135, dtype=torch.float64), tensor(0.0574, dtype=torch.float64), tensor(0.0560, dtype=torch.float64), tensor(0.4597, dtype=torch.float64), 31, 0, 1, 1, 1, 0, 78, 0.05440247231379241, 27.52955732320367, 0]
normalized proposed parameters for next round by BO: [tensor(0.3254, dtype=torch.float64), tensor(0.0880, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(2.1485e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0135, dtype=torch.float64), tensor(0.0574, dtype=torch.float64), tensor(0.0560, dtype=torch.float64), tensor(0.4597, dtype=torch.float64), tensor(0.9681, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6065, dtype=torch.float64), tensor(0.5440, dtype=torch.float64), tensor(0.5735, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  4  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.325
  gsm8k: 0.088
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0.014
  wikitext: 0.057
  mmlu: 0.056
  arc_challenge: 0.46

LoRA Parameters:
  lora_r: (78,)
  lora_dropout: (0.05440247231379241,)
  num_layers_to_apply: (31,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (27.52955732320367,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  31
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  78
lora dropout:  0.05440247231379241
lora alpha:  27.52955732320367
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 101,517,312 || all params: 8,131,778,560 || trainable%: 1.2484
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7364, 'grad_norm': 0.8850420713424683, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.284900426864624, 'eval_runtime': 5.338, 'eval_samples_per_second': 187.337, 'eval_steps_per_second': 11.802, 'epoch': 0.04}
{'loss': 1.1566, 'grad_norm': 0.5037345886230469, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.000347375869751, 'eval_runtime': 5.3058, 'eval_samples_per_second': 188.472, 'eval_steps_per_second': 11.874, 'epoch': 0.08}
{'loss': 0.9464, 'grad_norm': 0.29711824655532837, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9367073774337769, 'eval_runtime': 5.3005, 'eval_samples_per_second': 188.662, 'eval_steps_per_second': 11.886, 'epoch': 0.12}
{'loss': 1.0088, 'grad_norm': 0.24295875430107117, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9254806041717529, 'eval_runtime': 5.2959, 'eval_samples_per_second': 188.825, 'eval_steps_per_second': 11.896, 'epoch': 0.16}
{'loss': 0.9078, 'grad_norm': 0.21348917484283447, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9142524600028992, 'eval_runtime': 5.3047, 'eval_samples_per_second': 188.511, 'eval_steps_per_second': 11.876, 'epoch': 0.2}
{'loss': 0.9056, 'grad_norm': 0.2555384039878845, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9066334366798401, 'eval_runtime': 5.3111, 'eval_samples_per_second': 188.286, 'eval_steps_per_second': 11.862, 'epoch': 0.24}
{'loss': 0.8938, 'grad_norm': 0.21606971323490143, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9028176069259644, 'eval_runtime': 5.331, 'eval_samples_per_second': 187.583, 'eval_steps_per_second': 11.818, 'epoch': 0.28}
{'loss': 0.8827, 'grad_norm': 0.2554794251918793, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8958427309989929, 'eval_runtime': 5.3161, 'eval_samples_per_second': 188.108, 'eval_steps_per_second': 11.851, 'epoch': 0.32}
{'loss': 0.8459, 'grad_norm': 0.31137725710868835, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8897599577903748, 'eval_runtime': 5.3273, 'eval_samples_per_second': 187.714, 'eval_steps_per_second': 11.826, 'epoch': 0.36}
{'loss': 0.8678, 'grad_norm': 0.23855988681316376, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8910698294639587, 'eval_runtime': 5.3148, 'eval_samples_per_second': 188.153, 'eval_steps_per_second': 11.854, 'epoch': 0.4}
{'loss': 0.8281, 'grad_norm': 0.32785364985466003, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8869171142578125, 'eval_runtime': 5.3144, 'eval_samples_per_second': 188.169, 'eval_steps_per_second': 11.855, 'epoch': 0.44}
{'loss': 0.7857, 'grad_norm': 0.24724066257476807, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8851006031036377, 'eval_runtime': 5.3533, 'eval_samples_per_second': 186.8, 'eval_steps_per_second': 11.768, 'epoch': 0.48}
{'loss': 0.7627, 'grad_norm': 0.28915610909461975, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8777197003364563, 'eval_runtime': 5.3346, 'eval_samples_per_second': 187.454, 'eval_steps_per_second': 11.81, 'epoch': 0.52}
{'loss': 0.8246, 'grad_norm': 0.3193437159061432, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.874406635761261, 'eval_runtime': 5.3683, 'eval_samples_per_second': 186.278, 'eval_steps_per_second': 11.736, 'epoch': 0.56}
{'loss': 0.7464, 'grad_norm': 0.3208256661891937, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8745805621147156, 'eval_runtime': 5.363, 'eval_samples_per_second': 186.463, 'eval_steps_per_second': 11.747, 'epoch': 0.6}
{'loss': 0.7216, 'grad_norm': 0.5435256958007812, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8717247247695923, 'eval_runtime': 5.351, 'eval_samples_per_second': 186.881, 'eval_steps_per_second': 11.774, 'epoch': 0.64}
{'loss': 0.7882, 'grad_norm': 0.49530157446861267, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.87096107006073, 'eval_runtime': 5.3375, 'eval_samples_per_second': 187.354, 'eval_steps_per_second': 11.803, 'epoch': 0.68}
{'loss': 0.7571, 'grad_norm': 0.28653058409690857, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8640590310096741, 'eval_runtime': 5.333, 'eval_samples_per_second': 187.512, 'eval_steps_per_second': 11.813, 'epoch': 0.72}
{'loss': 0.6976, 'grad_norm': 0.36970850825309753, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8659824132919312, 'eval_runtime': 5.3256, 'eval_samples_per_second': 187.773, 'eval_steps_per_second': 11.83, 'epoch': 0.76}
{'loss': 0.7172, 'grad_norm': 0.372391939163208, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8618758320808411, 'eval_runtime': 5.3078, 'eval_samples_per_second': 188.403, 'eval_steps_per_second': 11.869, 'epoch': 0.8}
{'loss': 0.712, 'grad_norm': 0.2822100520133972, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8598489761352539, 'eval_runtime': 5.3053, 'eval_samples_per_second': 188.492, 'eval_steps_per_second': 11.875, 'epoch': 0.84}
{'loss': 0.7252, 'grad_norm': 0.2706023156642914, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8612133860588074, 'eval_runtime': 5.3187, 'eval_samples_per_second': 188.014, 'eval_steps_per_second': 11.845, 'epoch': 0.88}
{'loss': 0.6667, 'grad_norm': 0.34505558013916016, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8595026135444641, 'eval_runtime': 5.3172, 'eval_samples_per_second': 188.07, 'eval_steps_per_second': 11.848, 'epoch': 0.92}
{'loss': 0.6879, 'grad_norm': 0.3591095805168152, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8587468266487122, 'eval_runtime': 5.3277, 'eval_samples_per_second': 187.697, 'eval_steps_per_second': 11.825, 'epoch': 0.96}
{'loss': 0.6986, 'grad_norm': 0.4013478457927704, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8583806753158569, 'eval_runtime': 5.3931, 'eval_samples_per_second': 185.422, 'eval_steps_per_second': 11.682, 'epoch': 1.0}
{'train_runtime': 342.0874, 'train_samples_per_second': 29.221, 'train_steps_per_second': 1.827, 'train_loss': 0.8908603973388672, 'epoch': 1.0}
train_results:  {'eval_loss': [1.284900426864624, 1.000347375869751, 0.9367073774337769, 0.9254806041717529, 0.9142524600028992, 0.9066334366798401, 0.9028176069259644, 0.8958427309989929, 0.8897599577903748, 0.8910698294639587, 0.8869171142578125, 0.8851006031036377, 0.8777197003364563, 0.874406635761261, 0.8745805621147156, 0.8717247247695923, 0.87096107006073, 0.8640590310096741, 0.8659824132919312, 0.8618758320808411, 0.8598489761352539, 0.8612133860588074, 0.8595026135444641, 0.8587468266487122, 0.8583806753158569], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.284900426864624, 1.000347375869751, 0.9367073774337769, 0.9254806041717529, 0.9142524600028992, 0.9066334366798401, 0.9028176069259644, 0.8958427309989929, 0.8897599577903748, 0.8910698294639587, 0.8869171142578125, 0.8851006031036377, 0.8777197003364563, 0.874406635761261, 0.8745805621147156, 0.8717247247695923, 0.87096107006073, 0.8640590310096741, 0.8659824132919312, 0.8618758320808411, 0.8598489761352539, 0.8612133860588074, 0.8595026135444641, 0.8587468266487122, 0.8583806753158569]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0452039241790771
current iteration best possible eval_loss (full train run):  -0.8583806753158569
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.6182 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.06433850526809692, 0.50473552942276, 0.4210546016693115, 0.6836512088775635, 0.4850150942802429, 0.10502982139587402, 0.4997008442878723, 0.19547182321548462, 0.6389873623847961, 0.881937563419342, 0.28656548261642456, 0.22471177577972412, 0.38344573974609375, 0.4024169445037842, 0.5377413034439087, 0.5426982641220093, 0.6661252975463867, 0.3365659713745117, 0.2939772605895996]  ‚Üí  acq = -0.8282025645515998
X = [0.9308610558509827, 0.7850350737571716, 0.7465183734893799, 0.16657328605651855, 0.8350784182548523, 0.974524199962616, 0.3051397204399109, 0.023647725582122803, 0.6660334467887878, 0.28388267755508423, 0.6862972974777222, 0.3400799632072449, 0.9397324919700623, 0.35767048597335815, 0.5324565768241882, 0.42407336831092834, 0.24413615465164185, 0.6884256601333618, 0.8478764891624451]  ‚Üí  acq = -0.82791048567572
X = [0.06694936752319336, 0.5175358653068542, 0.8238212466239929, 0.6605879068374634, 0.020123779773712158, 0.4720451235771179, 0.722555935382843, 0.6574421525001526, 0.0001609325408935547, 0.42611822485923767, 0.18076545000076294, 0.2772452235221863, 0.49140775203704834, 0.9487783312797546, 0.7664200663566589, 0.1952262967824936, 0.764849066734314, 0.2548362612724304, 0.6169077157974243]  ‚Üí  acq = -0.82791048567572
X = [0.07988590002059937, 0.5490165948867798, 0.3071357011795044, 0.9823702573776245, 0.13642889261245728, 0.25173115730285645, 0.7703142762184143, 0.306768000125885, 0.6516563892364502, 0.951154887676239, 0.09277522563934326, 0.04332327842712402, 0.4911101460456848, 0.5605348944664001, 0.7479527592658997, 0.7227839231491089, 0.12401306629180908, 0.9223390817642212, 0.3799903988838196]  ‚Üí  acq = -0.8279661125249804
X = [0.8099525570869446, 0.18380647897720337, 0.6421754360198975, 0.8084840774536133, 0.8015547394752502, 0.1620665192604065, 0.18879306316375732, 0.54170823097229, 0.6152615547180176, 0.8923534750938416, 0.5019571185112, 0.9914546608924866, 0.2618297338485718, 0.7198339700698853, 0.8123634457588196, 0.8559361696243286, 0.20193207263946533, 0.5331242084503174, 0.6938096284866333]  ‚Üí  acq = -0.82791048567572
proposed candidate layer mask is:  tensor([1., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3804, dtype=torch.float64), 0, tensor(0.0614, dtype=torch.float64), 0, tensor(0.0326, dtype=torch.float64), 0, 0, tensor(0.0732, dtype=torch.float64), tensor(0.4524, dtype=torch.float64), 32, 1, 1, 1, 1, 0, 128, 3.0357660829594127e-19, 48.0, 0]
normalized proposed parameters for next round by BO: [tensor(0.3804, dtype=torch.float64), tensor(1.5245e-18, dtype=torch.float64), tensor(0.0614, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0326, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0732, dtype=torch.float64), tensor(0.4524, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(3.0358e-18, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  5  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.38
  gsm8k: 0
  rowan_hellaswag: 0.061
  sciq: 0
  triviaqa: 0.033
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.073
  arc_challenge: 0.452

LoRA Parameters:
  lora_r: (128,)
  lora_dropout: (3.0357660829594127e-19,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 1, 1, 0],)
  lora_alpha: (48.0,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 1, 0]
lora rank:  128
lora dropout:  3.0357660829594127e-19
lora alpha:  48.0
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 205,520,896 || all params: 8,235,782,144 || trainable%: 2.4955
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5989, 'grad_norm': 0.43708643317222595, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.1692472696304321, 'eval_runtime': 5.357, 'eval_samples_per_second': 186.671, 'eval_steps_per_second': 11.76, 'epoch': 0.04}
{'loss': 1.1137, 'grad_norm': 0.427778959274292, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9755159020423889, 'eval_runtime': 5.3624, 'eval_samples_per_second': 186.483, 'eval_steps_per_second': 11.748, 'epoch': 0.08}
{'loss': 1.0666, 'grad_norm': 0.25526171922683716, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9182935953140259, 'eval_runtime': 5.3623, 'eval_samples_per_second': 186.486, 'eval_steps_per_second': 11.749, 'epoch': 0.12}
{'loss': 0.983, 'grad_norm': 0.24371418356895447, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9071477055549622, 'eval_runtime': 5.3771, 'eval_samples_per_second': 185.973, 'eval_steps_per_second': 11.716, 'epoch': 0.16}
{'loss': 1.023, 'grad_norm': 0.21805429458618164, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8976031541824341, 'eval_runtime': 5.4513, 'eval_samples_per_second': 183.441, 'eval_steps_per_second': 11.557, 'epoch': 0.2}
{'loss': 0.9688, 'grad_norm': 0.26532816886901855, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9043182730674744, 'eval_runtime': 5.4294, 'eval_samples_per_second': 184.183, 'eval_steps_per_second': 11.604, 'epoch': 0.24}
{'loss': 0.9052, 'grad_norm': 0.3879116177558899, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8923488855361938, 'eval_runtime': 5.4338, 'eval_samples_per_second': 184.032, 'eval_steps_per_second': 11.594, 'epoch': 0.28}
{'loss': 0.9492, 'grad_norm': 0.24756662547588348, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8814241290092468, 'eval_runtime': 5.3981, 'eval_samples_per_second': 185.25, 'eval_steps_per_second': 11.671, 'epoch': 0.32}
{'loss': 0.8975, 'grad_norm': 0.2753027081489563, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8806849718093872, 'eval_runtime': 5.381, 'eval_samples_per_second': 185.84, 'eval_steps_per_second': 11.708, 'epoch': 0.36}
{'loss': 0.8333, 'grad_norm': 0.24107636511325836, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8711467981338501, 'eval_runtime': 5.3847, 'eval_samples_per_second': 185.711, 'eval_steps_per_second': 11.7, 'epoch': 0.4}
{'loss': 0.8395, 'grad_norm': 0.35563480854034424, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8705285787582397, 'eval_runtime': 5.3775, 'eval_samples_per_second': 185.961, 'eval_steps_per_second': 11.716, 'epoch': 0.44}
{'loss': 0.8585, 'grad_norm': 0.2590468227863312, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8662398457527161, 'eval_runtime': 5.3781, 'eval_samples_per_second': 185.938, 'eval_steps_per_second': 11.714, 'epoch': 0.48}
{'loss': 0.8329, 'grad_norm': 0.31422656774520874, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.865098237991333, 'eval_runtime': 5.3825, 'eval_samples_per_second': 185.786, 'eval_steps_per_second': 11.704, 'epoch': 0.52}
{'loss': 0.7508, 'grad_norm': 0.2704101800918579, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8624300360679626, 'eval_runtime': 5.3711, 'eval_samples_per_second': 186.181, 'eval_steps_per_second': 11.729, 'epoch': 0.56}
{'loss': 0.7846, 'grad_norm': 0.4311530590057373, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8604238629341125, 'eval_runtime': 5.363, 'eval_samples_per_second': 186.463, 'eval_steps_per_second': 11.747, 'epoch': 0.6}
{'loss': 0.8339, 'grad_norm': 0.3080667555332184, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8579403162002563, 'eval_runtime': 5.3618, 'eval_samples_per_second': 186.504, 'eval_steps_per_second': 11.75, 'epoch': 0.64}
{'loss': 0.7214, 'grad_norm': 0.2486458718776703, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8557298183441162, 'eval_runtime': 5.36, 'eval_samples_per_second': 186.567, 'eval_steps_per_second': 11.754, 'epoch': 0.68}
{'loss': 0.7173, 'grad_norm': 0.28249818086624146, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8519776463508606, 'eval_runtime': 5.3606, 'eval_samples_per_second': 186.546, 'eval_steps_per_second': 11.752, 'epoch': 0.72}
{'loss': 0.7916, 'grad_norm': 0.3138424754142761, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8513963222503662, 'eval_runtime': 5.3634, 'eval_samples_per_second': 186.45, 'eval_steps_per_second': 11.746, 'epoch': 0.76}
{'loss': 0.7296, 'grad_norm': 0.23290520906448364, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8480345606803894, 'eval_runtime': 5.3662, 'eval_samples_per_second': 186.353, 'eval_steps_per_second': 11.74, 'epoch': 0.8}
{'loss': 0.6606, 'grad_norm': 0.25415438413619995, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.847728967666626, 'eval_runtime': 5.3628, 'eval_samples_per_second': 186.468, 'eval_steps_per_second': 11.747, 'epoch': 0.84}
{'loss': 0.6962, 'grad_norm': 0.3279361128807068, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8456248044967651, 'eval_runtime': 5.3636, 'eval_samples_per_second': 186.442, 'eval_steps_per_second': 11.746, 'epoch': 0.88}
{'loss': 0.6687, 'grad_norm': 0.22042684257030487, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8453581929206848, 'eval_runtime': 5.3595, 'eval_samples_per_second': 186.584, 'eval_steps_per_second': 11.755, 'epoch': 0.92}
{'loss': 0.6707, 'grad_norm': 0.2340310961008072, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8438323736190796, 'eval_runtime': 5.3622, 'eval_samples_per_second': 186.492, 'eval_steps_per_second': 11.749, 'epoch': 0.96}
{'loss': 0.6915, 'grad_norm': 0.32730215787887573, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8432464003562927, 'eval_runtime': 5.3602, 'eval_samples_per_second': 186.559, 'eval_steps_per_second': 11.753, 'epoch': 1.0}
{'train_runtime': 351.7417, 'train_samples_per_second': 28.424, 'train_steps_per_second': 1.777, 'train_loss': 0.9034878021240235, 'epoch': 1.0}
train_results:  {'eval_loss': [1.1692472696304321, 0.9755159020423889, 0.9182935953140259, 0.9071477055549622, 0.8976031541824341, 0.9043182730674744, 0.8923488855361938, 0.8814241290092468, 0.8806849718093872, 0.8711467981338501, 0.8705285787582397, 0.8662398457527161, 0.865098237991333, 0.8624300360679626, 0.8604238629341125, 0.8579403162002563, 0.8557298183441162, 0.8519776463508606, 0.8513963222503662, 0.8480345606803894, 0.847728967666626, 0.8456248044967651, 0.8453581929206848, 0.8438323736190796, 0.8432464003562927], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.1692472696304321, 0.9755159020423889, 0.9182935953140259, 0.9071477055549622, 0.8976031541824341, 0.9043182730674744, 0.8923488855361938, 0.8814241290092468, 0.8806849718093872, 0.8711467981338501, 0.8705285787582397, 0.8662398457527161, 0.865098237991333, 0.8624300360679626, 0.8604238629341125, 0.8579403162002563, 0.8557298183441162, 0.8519776463508606, 0.8513963222503662, 0.8480345606803894, 0.847728967666626, 0.8456248044967651, 0.8453581929206848, 0.8438323736190796, 0.8432464003562927]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.2228953838348389
current iteration best possible eval_loss (full train run):  -0.8432464003562927
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.6150 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6346412897109985, 0.9979836344718933, 0.7175027132034302, 0.09794968366622925, 0.007579445838928223, 0.7134420275688171, 0.7704801559448242, 0.8697661757469177, 0.10287010669708252, 0.19419144093990326, 0.9070438742637634, 0.9054400324821472, 0.5220442414283752, 0.007521688938140869, 0.987917423248291, 0.750337541103363, 0.3050987720489502, 0.39818140864372253, 0.49315524101257324]  ‚Üí  acq = -0.8635953947100267
X = [0.9335173964500427, 0.5189917683601379, 0.819793164730072, 0.7302754521369934, 0.2581833004951477, 0.0015076398849487305, 0.8209108114242554, 0.21831399202346802, 0.6847650408744812, 0.8008294701576233, 0.3685798645019531, 0.18799203634262085, 0.9806296229362488, 0.22527247667312622, 0.9114632606506348, 0.9030665159225464, 0.9609596729278564, 0.9652138948440552, 0.4331236481666565]  ‚Üí  acq = -0.8632896333154145
X = [0.8817262649536133, 0.05581390857696533, 0.5420476794242859, 0.15767186880111694, 0.12707173824310303, 0.7648663520812988, 0.24276012182235718, 0.39057302474975586, 0.0375140905380249, 0.27019092440605164, 0.6721958518028259, 0.9521002173423767, 0.6285829544067383, 0.4609801173210144, 0.22714883089065552, 0.19768813252449036, 0.21395939588546753, 0.8834457397460938, 0.2856326103210449]  ‚Üí  acq = -0.9046248820009581
X = [0.6812859177589417, 0.6982809901237488, 0.19342195987701416, 0.2312648892402649, 0.3635638952255249, 0.15587210655212402, 0.995784342288971, 0.2507968544960022, 0.0661017894744873, 0.198833167552948, 0.5038816332817078, 0.9680747985839844, 0.05920696258544922, 0.5522938966751099, 0.5082452893257141, 0.4922761917114258, 0.5792016983032227, 0.33047816157341003, 0.6994286775588989]  ‚Üí  acq = -0.8638935971385268
X = [0.6101909875869751, 0.929810106754303, 0.8966465592384338, 0.5881779789924622, 0.20913714170455933, 0.5008677840232849, 0.11800360679626465, 0.6872270107269287, 0.6122615933418274, 0.5168914198875427, 0.029352962970733643, 0.20413661003112793, 0.752475380897522, 0.8746907711029053, 0.1870233416557312, 0.1833476424217224, 0.6010990738868713, 0.7691606283187866, 0.3282063603401184]  ‚Üí  acq = -0.8639852644430707
proposed candidate layer mask is:  tensor([0., 1., 1., 0., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.4300, dtype=torch.float64), tensor(0.1258, dtype=torch.float64), 0, tensor(0.0294, dtype=torch.float64), tensor(0.0961, dtype=torch.float64), tensor(0.0719, dtype=torch.float64), 0, 0, tensor(0.2468, dtype=torch.float64), 21, 0, 1, 1, 0, 0, 63, 0.004572883543628167, 23.826856873198565, 0]
normalized proposed parameters for next round by BO: [tensor(0.4300, dtype=torch.float64), tensor(0.1258, dtype=torch.float64), tensor(6.2602e-19, dtype=torch.float64), tensor(0.0294, dtype=torch.float64), tensor(0.0961, dtype=torch.float64), tensor(0.0719, dtype=torch.float64), tensor(5.5656e-17, dtype=torch.float64), tensor(1.0814e-17, dtype=torch.float64), tensor(0.2468, dtype=torch.float64), tensor(0.6714, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4906, dtype=torch.float64), tensor(0.0457, dtype=torch.float64), tensor(0.4964, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  6  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.43
  gsm8k: 0.126
  rowan_hellaswag: 0
  sciq: 0.029
  triviaqa: 0.096
  truthfulqa_gen: 0.072
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.247

LoRA Parameters:
  lora_r: (63,)
  lora_dropout: (0.004572883543628167,)
  num_layers_to_apply: (21,)
  five_dim_vector: ([0, 1, 1, 0, 0],)
  lora_alpha: (23.826856873198565,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  21
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 0, 0]
lora rank:  63
lora dropout:  0.004572883543628167
lora alpha:  23.826856873198565
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 31,159,296 || all params: 8,061,420,544 || trainable%: 0.3865
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.212, 'grad_norm': 0.6818772554397583, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9484038352966309, 'eval_runtime': 4.7287, 'eval_samples_per_second': 211.474, 'eval_steps_per_second': 13.323, 'epoch': 0.04}
{'loss': 1.4137, 'grad_norm': 0.2717418968677521, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.196634292602539, 'eval_runtime': 4.7489, 'eval_samples_per_second': 210.577, 'eval_steps_per_second': 13.266, 'epoch': 0.08}
{'loss': 1.1537, 'grad_norm': 0.2223244160413742, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1108920574188232, 'eval_runtime': 4.7532, 'eval_samples_per_second': 210.383, 'eval_steps_per_second': 13.254, 'epoch': 0.12}
{'loss': 1.0752, 'grad_norm': 0.24600443243980408, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0454307794570923, 'eval_runtime': 4.7461, 'eval_samples_per_second': 210.701, 'eval_steps_per_second': 13.274, 'epoch': 0.16}
{'loss': 0.9927, 'grad_norm': 0.26477861404418945, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0019747018814087, 'eval_runtime': 4.7519, 'eval_samples_per_second': 210.441, 'eval_steps_per_second': 13.258, 'epoch': 0.2}
{'loss': 0.9762, 'grad_norm': 0.22609806060791016, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9788780808448792, 'eval_runtime': 4.7583, 'eval_samples_per_second': 210.159, 'eval_steps_per_second': 13.24, 'epoch': 0.24}
{'loss': 0.9655, 'grad_norm': 0.26844534277915955, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9466663599014282, 'eval_runtime': 4.7775, 'eval_samples_per_second': 209.314, 'eval_steps_per_second': 13.187, 'epoch': 0.28}
{'loss': 0.9064, 'grad_norm': 0.25624123215675354, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9158382415771484, 'eval_runtime': 4.7778, 'eval_samples_per_second': 209.3, 'eval_steps_per_second': 13.186, 'epoch': 0.32}
{'loss': 0.8806, 'grad_norm': 0.2387755811214447, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9073022603988647, 'eval_runtime': 4.7795, 'eval_samples_per_second': 209.225, 'eval_steps_per_second': 13.181, 'epoch': 0.36}
{'loss': 0.8874, 'grad_norm': 0.2178516834974289, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8999580144882202, 'eval_runtime': 4.7898, 'eval_samples_per_second': 208.778, 'eval_steps_per_second': 13.153, 'epoch': 0.4}
{'loss': 0.8758, 'grad_norm': 0.23355676233768463, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8998122811317444, 'eval_runtime': 4.8016, 'eval_samples_per_second': 208.264, 'eval_steps_per_second': 13.121, 'epoch': 0.44}
{'loss': 0.9075, 'grad_norm': 0.2353941947221756, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8935296535491943, 'eval_runtime': 4.82, 'eval_samples_per_second': 207.47, 'eval_steps_per_second': 13.071, 'epoch': 0.48}
{'loss': 0.8767, 'grad_norm': 0.23829996585845947, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8907343745231628, 'eval_runtime': 4.803, 'eval_samples_per_second': 208.204, 'eval_steps_per_second': 13.117, 'epoch': 0.52}
{'loss': 0.8824, 'grad_norm': 0.25988900661468506, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8936986327171326, 'eval_runtime': 4.8081, 'eval_samples_per_second': 207.983, 'eval_steps_per_second': 13.103, 'epoch': 0.56}
{'loss': 0.8494, 'grad_norm': 0.22900351881980896, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.888177752494812, 'eval_runtime': 4.7759, 'eval_samples_per_second': 209.385, 'eval_steps_per_second': 13.191, 'epoch': 0.6}
{'loss': 0.8612, 'grad_norm': 0.24408771097660065, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8831030130386353, 'eval_runtime': 4.7757, 'eval_samples_per_second': 209.393, 'eval_steps_per_second': 13.192, 'epoch': 0.64}
{'loss': 0.8596, 'grad_norm': 0.2133883684873581, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8820104002952576, 'eval_runtime': 4.776, 'eval_samples_per_second': 209.38, 'eval_steps_per_second': 13.191, 'epoch': 0.68}
{'loss': 0.8419, 'grad_norm': 0.25266677141189575, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8824238181114197, 'eval_runtime': 4.7778, 'eval_samples_per_second': 209.303, 'eval_steps_per_second': 13.186, 'epoch': 0.72}
{'loss': 0.8464, 'grad_norm': 0.23794299364089966, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8780876398086548, 'eval_runtime': 4.8341, 'eval_samples_per_second': 206.862, 'eval_steps_per_second': 13.032, 'epoch': 0.76}
{'loss': 0.8179, 'grad_norm': 0.2584851086139679, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8761677742004395, 'eval_runtime': 4.7972, 'eval_samples_per_second': 208.455, 'eval_steps_per_second': 13.133, 'epoch': 0.8}
{'loss': 0.833, 'grad_norm': 0.2450478971004486, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8762570023536682, 'eval_runtime': 4.8219, 'eval_samples_per_second': 207.386, 'eval_steps_per_second': 13.065, 'epoch': 0.84}
{'loss': 0.8475, 'grad_norm': 0.27652814984321594, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8737958669662476, 'eval_runtime': 4.7803, 'eval_samples_per_second': 209.192, 'eval_steps_per_second': 13.179, 'epoch': 0.88}
{'loss': 0.8323, 'grad_norm': 0.26972097158432007, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8736433386802673, 'eval_runtime': 4.7833, 'eval_samples_per_second': 209.059, 'eval_steps_per_second': 13.171, 'epoch': 0.92}
{'loss': 0.8251, 'grad_norm': 0.2455061674118042, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8723816871643066, 'eval_runtime': 4.7865, 'eval_samples_per_second': 208.92, 'eval_steps_per_second': 13.162, 'epoch': 0.96}
{'loss': 0.832, 'grad_norm': 0.3032216727733612, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8721480965614319, 'eval_runtime': 4.8932, 'eval_samples_per_second': 204.365, 'eval_steps_per_second': 12.875, 'epoch': 1.0}
{'train_runtime': 272.7829, 'train_samples_per_second': 36.652, 'train_steps_per_second': 2.291, 'train_loss': 1.0100877258300782, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9484038352966309, 1.196634292602539, 1.1108920574188232, 1.0454307794570923, 1.0019747018814087, 0.9788780808448792, 0.9466663599014282, 0.9158382415771484, 0.9073022603988647, 0.8999580144882202, 0.8998122811317444, 0.8935296535491943, 0.8907343745231628, 0.8936986327171326, 0.888177752494812, 0.8831030130386353, 0.8820104002952576, 0.8824238181114197, 0.8780876398086548, 0.8761677742004395, 0.8762570023536682, 0.8737958669662476, 0.8736433386802673, 0.8723816871643066, 0.8721480965614319], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9484038352966309, 1.196634292602539, 1.1108920574188232, 1.0454307794570923, 1.0019747018814087, 0.9788780808448792, 0.9466663599014282, 0.9158382415771484, 0.9073022603988647, 0.8999580144882202, 0.8998122811317444, 0.8935296535491943, 0.8907343745231628, 0.8936986327171326, 0.888177752494812, 0.8831030130386353, 0.8820104002952576, 0.8824238181114197, 0.8780876398086548, 0.8761677742004395, 0.8762570023536682, 0.8737958669662476, 0.8736433386802673, 0.8723816871643066, 0.8721480965614319]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0731794834136963
current iteration best possible eval_loss (full train run):  -0.8721480965614319
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.7998 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.0017865896224975586, 0.8776335120201111, 0.18018925189971924, 0.9346457719802856, 0.880981981754303, 0.581531286239624, 0.2723011374473572, 0.49695104360580444, 0.24106496572494507, 0.10738632082939148, 0.15032744407653809, 0.3497846722602844, 0.572274923324585, 0.8607686758041382, 0.4703384041786194, 0.46085256338119507, 0.3034648299217224, 0.8845210075378418, 0.5330603718757629]  ‚Üí  acq = -0.9183273800163062
X = [0.0752406120300293, 0.07475310564041138, 0.9701084494590759, 0.6050729751586914, 0.9701277613639832, 0.47759610414505005, 0.6473668217658997, 0.024429142475128174, 0.14988404512405396, 0.9748101234436035, 0.1852504014968872, 0.235798180103302, 0.5180254578590393, 0.472089946269989, 0.03980189561843872, 0.8289780616760254, 0.0066245198249816895, 0.2876109480857849, 0.9490264058113098]  ‚Üí  acq = -0.9019702729097313
X = [0.8149895071983337, 0.6321797370910645, 0.8430664539337158, 0.14903193712234497, 0.9722407460212708, 0.5365822911262512, 0.6482887864112854, 0.7565659880638123, 0.9232513308525085, 0.14723242819309235, 0.9281252026557922, 0.2729107737541199, 0.46538442373275757, 0.6995220184326172, 0.8974609375, 0.8168087005615234, 0.9298104643821716, 0.3693460524082184, 0.9340886473655701]  ‚Üí  acq = -0.9017665709833413
X = [0.5788182616233826, 0.6719420552253723, 0.7755480408668518, 0.565514862537384, 0.7982152104377747, 0.3033444285392761, 0.012481331825256348, 0.786230206489563, 0.9106844663619995, 0.8485005497932434, 0.4454132318496704, 0.31198328733444214, 0.29781317710876465, 0.7958215475082397, 0.8544618487358093, 0.5140908360481262, 0.9426186680793762, 0.8339533805847168, 0.7412276268005371]  ‚Üí  acq = -0.9018535054957438
X = [0.05690562725067139, 0.9120071530342102, 0.6444032788276672, 0.15294921398162842, 0.6173548698425293, 0.49594181776046753, 0.17610323429107666, 0.8946483135223389, 0.1521429419517517, 0.3593105375766754, 0.13383805751800537, 0.7427614331245422, 0.1425524353981018, 0.3262947201728821, 0.8489412665367126, 0.4628297984600067, 0.4256795048713684, 0.22413276135921478, 0.7072871923446655]  ‚Üí  acq = -0.9027674411194695
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2376, dtype=torch.float64), tensor(0.1120, dtype=torch.float64), tensor(0.0241, dtype=torch.float64), 0, tensor(0.1950, dtype=torch.float64), tensor(0.1234, dtype=torch.float64), tensor(0.0172, dtype=torch.float64), tensor(0.0311, dtype=torch.float64), tensor(0.2595, dtype=torch.float64), 30, 0, 1, 1, 1, 0, 45, 0.0065405152620106825, 30.517398635615372, 0]
normalized proposed parameters for next round by BO: [tensor(0.2376, dtype=torch.float64), tensor(0.1120, dtype=torch.float64), tensor(0.0241, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1950, dtype=torch.float64), tensor(0.1234, dtype=torch.float64), tensor(0.0172, dtype=torch.float64), tensor(0.0311, dtype=torch.float64), tensor(0.2595, dtype=torch.float64), tensor(0.9385, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3552, dtype=torch.float64), tensor(0.0654, dtype=torch.float64), tensor(0.6358, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  7  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.238
  gsm8k: 0.112
  rowan_hellaswag: 0.024
  sciq: 0
  triviaqa: 0.195
  truthfulqa_gen: 0.123
  wikitext: 0.017
  mmlu: 0.031
  arc_challenge: 0.26

LoRA Parameters:
  lora_r: (45,)
  lora_dropout: (0.0065405152620106825,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (30.517398635615372,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  45
lora dropout:  0.0065405152620106825
lora alpha:  30.517398635615372
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 56,678,400 || all params: 8,086,939,648 || trainable%: 0.7009
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9046, 'grad_norm': 1.2986253499984741, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3262815475463867, 'eval_runtime': 5.4855, 'eval_samples_per_second': 182.299, 'eval_steps_per_second': 11.485, 'epoch': 0.04}
{'loss': 1.135, 'grad_norm': 0.4862372875213623, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9933058023452759, 'eval_runtime': 5.4358, 'eval_samples_per_second': 183.965, 'eval_steps_per_second': 11.59, 'epoch': 0.08}
{'loss': 0.9984, 'grad_norm': 0.33983203768730164, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9437479376792908, 'eval_runtime': 5.3993, 'eval_samples_per_second': 185.208, 'eval_steps_per_second': 11.668, 'epoch': 0.12}
{'loss': 0.983, 'grad_norm': 0.3500598967075348, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9351691603660583, 'eval_runtime': 5.3821, 'eval_samples_per_second': 185.801, 'eval_steps_per_second': 11.705, 'epoch': 0.16}
{'loss': 0.974, 'grad_norm': 0.26293039321899414, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9126916527748108, 'eval_runtime': 5.373, 'eval_samples_per_second': 186.115, 'eval_steps_per_second': 11.725, 'epoch': 0.2}
{'loss': 1.0528, 'grad_norm': 0.2450728863477707, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.920764684677124, 'eval_runtime': 5.4014, 'eval_samples_per_second': 185.136, 'eval_steps_per_second': 11.664, 'epoch': 0.24}
{'loss': 0.9446, 'grad_norm': 0.29467320442199707, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9155380725860596, 'eval_runtime': 5.3991, 'eval_samples_per_second': 185.216, 'eval_steps_per_second': 11.669, 'epoch': 0.28}
{'loss': 0.9344, 'grad_norm': 0.28237682580947876, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9032731652259827, 'eval_runtime': 5.4267, 'eval_samples_per_second': 184.275, 'eval_steps_per_second': 11.609, 'epoch': 0.32}
{'loss': 0.9621, 'grad_norm': 0.3018990159034729, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.8988183736801147, 'eval_runtime': 5.4131, 'eval_samples_per_second': 184.738, 'eval_steps_per_second': 11.639, 'epoch': 0.36}
{'loss': 0.9689, 'grad_norm': 0.33939987421035767, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8970702886581421, 'eval_runtime': 5.4025, 'eval_samples_per_second': 185.099, 'eval_steps_per_second': 11.661, 'epoch': 0.4}
{'loss': 0.9103, 'grad_norm': 0.3661569058895111, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9004207253456116, 'eval_runtime': 5.4262, 'eval_samples_per_second': 184.292, 'eval_steps_per_second': 11.61, 'epoch': 0.44}
{'loss': 0.9576, 'grad_norm': 0.36066266894340515, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8921520709991455, 'eval_runtime': 5.4441, 'eval_samples_per_second': 183.686, 'eval_steps_per_second': 11.572, 'epoch': 0.48}
{'loss': 0.9068, 'grad_norm': 0.35605520009994507, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8853507041931152, 'eval_runtime': 5.636, 'eval_samples_per_second': 177.43, 'eval_steps_per_second': 11.178, 'epoch': 0.52}
{'loss': 0.8859, 'grad_norm': 0.37829217314720154, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8853941559791565, 'eval_runtime': 5.5703, 'eval_samples_per_second': 179.522, 'eval_steps_per_second': 11.31, 'epoch': 0.56}
{'loss': 0.8849, 'grad_norm': 0.40255120396614075, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8830104470252991, 'eval_runtime': 5.6047, 'eval_samples_per_second': 178.423, 'eval_steps_per_second': 11.241, 'epoch': 0.6}
{'loss': 0.8673, 'grad_norm': 0.3570622503757477, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8828690648078918, 'eval_runtime': 5.5238, 'eval_samples_per_second': 181.034, 'eval_steps_per_second': 11.405, 'epoch': 0.64}
{'loss': 0.8605, 'grad_norm': 0.4454319179058075, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.87962406873703, 'eval_runtime': 5.5363, 'eval_samples_per_second': 180.627, 'eval_steps_per_second': 11.38, 'epoch': 0.68}
{'loss': 0.8673, 'grad_norm': 0.3142937123775482, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8744540214538574, 'eval_runtime': 5.5288, 'eval_samples_per_second': 180.87, 'eval_steps_per_second': 11.395, 'epoch': 0.72}
{'loss': 0.8664, 'grad_norm': 0.4009252190589905, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8767260313034058, 'eval_runtime': 5.5271, 'eval_samples_per_second': 180.926, 'eval_steps_per_second': 11.398, 'epoch': 0.76}
{'loss': 0.8013, 'grad_norm': 0.4979237914085388, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.87470543384552, 'eval_runtime': 5.5321, 'eval_samples_per_second': 180.765, 'eval_steps_per_second': 11.388, 'epoch': 0.8}
{'loss': 0.8497, 'grad_norm': 0.35169175267219543, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8743298053741455, 'eval_runtime': 5.5178, 'eval_samples_per_second': 181.231, 'eval_steps_per_second': 11.418, 'epoch': 0.84}
{'loss': 0.8196, 'grad_norm': 0.3068198561668396, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8718430995941162, 'eval_runtime': 5.3963, 'eval_samples_per_second': 185.313, 'eval_steps_per_second': 11.675, 'epoch': 0.88}
{'loss': 0.856, 'grad_norm': 0.505614161491394, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8710232973098755, 'eval_runtime': 5.392, 'eval_samples_per_second': 185.458, 'eval_steps_per_second': 11.684, 'epoch': 0.92}
{'loss': 0.8196, 'grad_norm': 0.3913947343826294, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8693442344665527, 'eval_runtime': 5.3864, 'eval_samples_per_second': 185.653, 'eval_steps_per_second': 11.696, 'epoch': 0.96}
{'loss': 0.8891, 'grad_norm': 0.4429103136062622, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.868810772895813, 'eval_runtime': 5.3951, 'eval_samples_per_second': 185.355, 'eval_steps_per_second': 11.677, 'epoch': 1.0}
{'train_runtime': 355.2258, 'train_samples_per_second': 28.14, 'train_steps_per_second': 1.759, 'train_loss': 0.9959997741699219, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3262815475463867, 0.9933058023452759, 0.9437479376792908, 0.9351691603660583, 0.9126916527748108, 0.920764684677124, 0.9155380725860596, 0.9032731652259827, 0.8988183736801147, 0.8970702886581421, 0.9004207253456116, 0.8921520709991455, 0.8853507041931152, 0.8853941559791565, 0.8830104470252991, 0.8828690648078918, 0.87962406873703, 0.8744540214538574, 0.8767260313034058, 0.87470543384552, 0.8743298053741455, 0.8718430995941162, 0.8710232973098755, 0.8693442344665527, 0.868810772895813], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3262815475463867, 0.9933058023452759, 0.9437479376792908, 0.9351691603660583, 0.9126916527748108, 0.920764684677124, 0.9155380725860596, 0.9032731652259827, 0.8988183736801147, 0.8970702886581421, 0.9004207253456116, 0.8921520709991455, 0.8853507041931152, 0.8853941559791565, 0.8830104470252991, 0.8828690648078918, 0.87962406873703, 0.8744540214538574, 0.8767260313034058, 0.87470543384552, 0.8743298053741455, 0.8718430995941162, 0.8710232973098755, 0.8693442344665527, 0.868810772895813]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0186767578125
current iteration best possible eval_loss (full train run):  -0.868810772895813
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5280 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7717249989509583, 0.3931189775466919, 0.9207555055618286, 0.6752821803092957, 0.8252210021018982, 0.5749474763870239, 0.19482320547103882, 0.3749505877494812, 0.8963236808776855, 0.5773240327835083, 0.31038546562194824, 0.7448617815971375, 0.24277514219284058, 0.4127753973007202, 0.003319978713989258, 0.6625216603279114, 0.6627236008644104, 0.6259675025939941, 0.7597888708114624]  ‚Üí  acq = -0.8625612630241477
X = [0.9179736375808716, 0.8076769113540649, 0.5971965789794922, 0.6392064094543457, 0.07758927345275879, 0.04760700464248657, 0.7743387222290039, 0.531842827796936, 0.10114860534667969, 0.4827705919742584, 0.8483054041862488, 0.9347643852233887, 0.5640563368797302, 0.6046855449676514, 0.06090492010116577, 0.2806549370288849, 0.544792890548706, 0.43411964178085327, 0.6534545421600342]  ‚Üí  acq = -0.8625801314638226
X = [0.9846633076667786, 0.6882033944129944, 0.6483343839645386, 0.5092257261276245, 0.9673016667366028, 0.16204100847244263, 0.889657199382782, 0.9086887836456299, 0.5554662346839905, 0.13470706343650818, 0.3620854616165161, 0.9840407967567444, 0.6774638891220093, 0.9396688938140869, 0.9420399069786072, 0.7139959931373596, 0.5669505000114441, 0.1414482146501541, 0.7658460736274719]  ‚Üí  acq = -0.8625218911979242
X = [0.4091559648513794, 0.5145012736320496, 0.6770163178443909, 0.6386376619338989, 0.7971786260604858, 0.7271236777305603, 0.46384894847869873, 0.17084431648254395, 0.40315020084381104, 0.9105441570281982, 0.35536110401153564, 0.6271535754203796, 0.161312997341156, 0.7081161141395569, 0.3376033306121826, 0.949032723903656, 0.04203289747238159, 0.7722232341766357, 0.17325276136398315]  ‚Üí  acq = -0.862705264359434
X = [0.5620851516723633, 0.8297916054725647, 0.6557984352111816, 0.6903063654899597, 0.9957290291786194, 0.5265406370162964, 0.6590622663497925, 0.7576286196708679, 0.04699522256851196, 0.9328292012214661, 0.19118666648864746, 0.26380205154418945, 0.4248855710029602, 0.009187877178192139, 0.7503892779350281, 0.19457247853279114, 0.7006362080574036, 0.5936758518218994, 0.6974127292633057]  ‚Üí  acq = -0.8625243888586449
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0815, dtype=torch.float64), 0, 0, 0, tensor(0.2687, dtype=torch.float64), tensor(0.1310, dtype=torch.float64), tensor(0.0288, dtype=torch.float64), 0, tensor(0.4900, dtype=torch.float64), 26, 0, 1, 1, 1, 0, 18, 0.03084186213832493, 33.04038074131623, 0]
normalized proposed parameters for next round by BO: [tensor(0.0815, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(3.3327e-17, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2687, dtype=torch.float64), tensor(0.1310, dtype=torch.float64), tensor(0.0288, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4900, dtype=torch.float64), tensor(0.8230, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1440, dtype=torch.float64), tensor(0.3084, dtype=torch.float64), tensor(0.6883, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  8  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.082
  gsm8k: 0
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.269
  truthfulqa_gen: 0.131
  wikitext: 0.029
  mmlu: 0
  arc_challenge: 0.49

LoRA Parameters:
  lora_r: (18,)
  lora_dropout: (0.03084186213832493,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (33.04038074131623,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  18
lora dropout:  0.03084186213832493
lora alpha:  33.04038074131623
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 19,648,512 || all params: 8,049,909,760 || trainable%: 0.2441
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9884, 'grad_norm': 2.074248790740967, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4353688955307007, 'eval_runtime': 5.0735, 'eval_samples_per_second': 197.102, 'eval_steps_per_second': 12.417, 'epoch': 0.04}
{'loss': 1.0551, 'grad_norm': 0.7880260348320007, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.067638635635376, 'eval_runtime': 5.0911, 'eval_samples_per_second': 196.422, 'eval_steps_per_second': 12.375, 'epoch': 0.08}
{'loss': 0.9536, 'grad_norm': 0.6944264769554138, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0063984394073486, 'eval_runtime': 5.0806, 'eval_samples_per_second': 196.827, 'eval_steps_per_second': 12.4, 'epoch': 0.12}
{'loss': 0.8929, 'grad_norm': 0.5459507703781128, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0033079385757446, 'eval_runtime': 5.062, 'eval_samples_per_second': 197.551, 'eval_steps_per_second': 12.446, 'epoch': 0.16}
{'loss': 0.8759, 'grad_norm': 0.5737996101379395, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9670990705490112, 'eval_runtime': 5.0694, 'eval_samples_per_second': 197.262, 'eval_steps_per_second': 12.428, 'epoch': 0.2}
{'loss': 0.8152, 'grad_norm': 0.6387635469436646, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.977307140827179, 'eval_runtime': 5.0639, 'eval_samples_per_second': 197.476, 'eval_steps_per_second': 12.441, 'epoch': 0.24}
{'loss': 0.813, 'grad_norm': 0.777640163898468, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9602646231651306, 'eval_runtime': 5.072, 'eval_samples_per_second': 197.162, 'eval_steps_per_second': 12.421, 'epoch': 0.28}
{'loss': 0.7666, 'grad_norm': 0.6200555562973022, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9572293162345886, 'eval_runtime': 5.0664, 'eval_samples_per_second': 197.381, 'eval_steps_per_second': 12.435, 'epoch': 0.32}
{'loss': 0.7849, 'grad_norm': 0.8713808059692383, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9448193311691284, 'eval_runtime': 5.0702, 'eval_samples_per_second': 197.232, 'eval_steps_per_second': 12.426, 'epoch': 0.36}
{'loss': 0.749, 'grad_norm': 0.7270954847335815, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.946935772895813, 'eval_runtime': 5.0805, 'eval_samples_per_second': 196.83, 'eval_steps_per_second': 12.4, 'epoch': 0.4}
{'loss': 0.6788, 'grad_norm': 0.7394434809684753, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9556344151496887, 'eval_runtime': 5.07, 'eval_samples_per_second': 197.238, 'eval_steps_per_second': 12.426, 'epoch': 0.44}
{'loss': 0.6702, 'grad_norm': 0.7328177690505981, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9419610500335693, 'eval_runtime': 5.054, 'eval_samples_per_second': 197.862, 'eval_steps_per_second': 12.465, 'epoch': 0.48}
{'loss': 0.6582, 'grad_norm': 0.792923629283905, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9327526092529297, 'eval_runtime': 5.0567, 'eval_samples_per_second': 197.759, 'eval_steps_per_second': 12.459, 'epoch': 0.52}
{'loss': 0.6546, 'grad_norm': 0.7499526739120483, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9344908595085144, 'eval_runtime': 5.066, 'eval_samples_per_second': 197.395, 'eval_steps_per_second': 12.436, 'epoch': 0.56}
{'loss': 0.6348, 'grad_norm': 0.6915155649185181, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9319071173667908, 'eval_runtime': 5.0751, 'eval_samples_per_second': 197.041, 'eval_steps_per_second': 12.414, 'epoch': 0.6}
{'loss': 0.6261, 'grad_norm': 0.7638373970985413, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9210630655288696, 'eval_runtime': 5.0736, 'eval_samples_per_second': 197.098, 'eval_steps_per_second': 12.417, 'epoch': 0.64}
{'loss': 0.5621, 'grad_norm': 0.705547034740448, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9200851917266846, 'eval_runtime': 5.0906, 'eval_samples_per_second': 196.439, 'eval_steps_per_second': 12.376, 'epoch': 0.68}
{'loss': 0.5289, 'grad_norm': 0.8608941435813904, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9222403764724731, 'eval_runtime': 5.0965, 'eval_samples_per_second': 196.213, 'eval_steps_per_second': 12.361, 'epoch': 0.72}
{'loss': 0.5563, 'grad_norm': 0.5775110721588135, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9180947542190552, 'eval_runtime': 5.0782, 'eval_samples_per_second': 196.92, 'eval_steps_per_second': 12.406, 'epoch': 0.76}
{'loss': 0.5354, 'grad_norm': 0.7998472452163696, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9157899618148804, 'eval_runtime': 5.0862, 'eval_samples_per_second': 196.609, 'eval_steps_per_second': 12.386, 'epoch': 0.8}
{'loss': 0.5273, 'grad_norm': 0.8946398496627808, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9162296652793884, 'eval_runtime': 5.0505, 'eval_samples_per_second': 197.999, 'eval_steps_per_second': 12.474, 'epoch': 0.84}
{'loss': 0.4926, 'grad_norm': 0.7157688736915588, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9151921272277832, 'eval_runtime': 5.057, 'eval_samples_per_second': 197.744, 'eval_steps_per_second': 12.458, 'epoch': 0.88}
{'loss': 0.5036, 'grad_norm': 0.8631858825683594, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9140197038650513, 'eval_runtime': 5.0502, 'eval_samples_per_second': 198.014, 'eval_steps_per_second': 12.475, 'epoch': 0.92}
{'loss': 0.5222, 'grad_norm': 0.9762375354766846, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9136046767234802, 'eval_runtime': 5.0554, 'eval_samples_per_second': 197.809, 'eval_steps_per_second': 12.462, 'epoch': 0.96}
{'loss': 0.4713, 'grad_norm': 0.7373855113983154, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9140738248825073, 'eval_runtime': 5.0563, 'eval_samples_per_second': 197.772, 'eval_steps_per_second': 12.46, 'epoch': 1.0}
{'train_runtime': 278.3385, 'train_samples_per_second': 35.92, 'train_steps_per_second': 2.245, 'train_loss': 0.7726747711181641, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4353688955307007, 1.067638635635376, 1.0063984394073486, 1.0033079385757446, 0.9670990705490112, 0.977307140827179, 0.9602646231651306, 0.9572293162345886, 0.9448193311691284, 0.946935772895813, 0.9556344151496887, 0.9419610500335693, 0.9327526092529297, 0.9344908595085144, 0.9319071173667908, 0.9210630655288696, 0.9200851917266846, 0.9222403764724731, 0.9180947542190552, 0.9157899618148804, 0.9162296652793884, 0.9151921272277832, 0.9140197038650513, 0.9136046767234802, 0.9140738248825073], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.4353688955307007, 1.067638635635376, 1.0063984394073486, 1.0033079385757446, 0.9670990705490112, 0.977307140827179, 0.9602646231651306, 0.9572293162345886, 0.9448193311691284, 0.946935772895813, 0.9556344151496887, 0.9419610500335693, 0.9327526092529297, 0.9344908595085144, 0.9319071173667908, 0.9210630655288696, 0.9200851917266846, 0.9222403764724731, 0.9180947542190552, 0.9157899618148804, 0.9162296652793884, 0.9151921272277832, 0.9140197038650513, 0.9136046767234802, 0.9140738248825073]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.146902084350586
current iteration best possible eval_loss (full train run):  -0.9140738248825073
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.5520 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9227593541145325, 0.09270203113555908, 0.12950563430786133, 0.5234801173210144, 0.9463421702384949, 0.9387034773826599, 0.9346001148223877, 0.8004587888717651, 0.40152454376220703, 0.7741458415985107, 0.7339673638343811, 0.8599051237106323, 0.239396870136261, 0.09876358509063721, 0.6888900399208069, 0.6687252521514893, 0.032737672328948975, 0.27277064323425293, 0.8990642428398132]  ‚Üí  acq = -0.8855202073745576
X = [0.5903847813606262, 0.7491182684898376, 0.16013598442077637, 0.4153406023979187, 0.5735043287277222, 0.059216201305389404, 0.010030269622802734, 0.8829187750816345, 0.9734378457069397, 0.9890723824501038, 0.06079226732254028, 0.4769786596298218, 0.45913833379745483, 0.32676321268081665, 0.028142869472503662, 0.03405582159757614, 0.12386679649353027, 0.8159302473068237, 0.591465413570404]  ‚Üí  acq = -0.8855179770518904
X = [0.8348991870880127, 0.7790366411209106, 0.0899885892868042, 0.8509238362312317, 0.8812801837921143, 0.06203502416610718, 0.8282999992370605, 0.42648839950561523, 0.044465720653533936, 0.7046675682067871, 0.7035248875617981, 0.9822481870651245, 0.8110877871513367, 0.329359233379364, 0.471097469329834, 0.6797796487808228, 0.8633646368980408, 0.5784467458724976, 0.14758515357971191]  ‚Üí  acq = -0.8855202073745583
X = [0.9936292171478271, 0.5789740681648254, 0.741007924079895, 0.6693617701530457, 0.8430807590484619, 0.5848448276519775, 0.0019243955612182617, 0.8098867535591125, 0.8848571181297302, 0.38326355814933777, 0.635259211063385, 0.020447075366973877, 0.698662281036377, 0.582832932472229, 0.3791559934616089, 0.776610791683197, 0.572867214679718, 0.9192330837249756, 0.9858017563819885]  ‚Üí  acq = -0.8855202073792826
X = [0.2613598108291626, 0.7777879238128662, 0.397970974445343, 0.6408610343933105, 0.267985463142395, 0.8416429162025452, 0.10219216346740723, 0.9882810711860657, 0.9247068166732788, 0.6257792115211487, 0.15530431270599365, 0.597465455532074, 0.33775657415390015, 0.9299086332321167, 0.6287887096405029, 0.3323131799697876, 0.1318853497505188, 0.4583084285259247, 0.3500043749809265]  ‚Üí  acq = -0.8855213671685784
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1762, dtype=torch.float64), tensor(0.1604, dtype=torch.float64), tensor(0.0566, dtype=torch.float64), tensor(0.0340, dtype=torch.float64), tensor(0.1128, dtype=torch.float64), 0, tensor(0.0279, dtype=torch.float64), 0, tensor(0.4321, dtype=torch.float64), 26, 0, 1, 1, 1, 1, 125, 0.05415440840859075, 30.56758872637405, 1]
normalized proposed parameters for next round by BO: [tensor(0.1762, dtype=torch.float64), tensor(0.1604, dtype=torch.float64), tensor(0.0566, dtype=torch.float64), tensor(0.0340, dtype=torch.float64), tensor(0.1128, dtype=torch.float64), tensor(3.5837e-18, dtype=torch.float64), tensor(0.0279, dtype=torch.float64), tensor(8.6543e-19, dtype=torch.float64), tensor(0.4321, dtype=torch.float64), tensor(0.8073, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.9780, dtype=torch.float64), tensor(0.5415, dtype=torch.float64), tensor(0.6368, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  9  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.176
  gsm8k: 0.16
  rowan_hellaswag: 0.057
  sciq: 0.034
  triviaqa: 0.113
  truthfulqa_gen: 0
  wikitext: 0.028
  mmlu: 0
  arc_challenge: 0.432

LoRA Parameters:
  lora_r: (125,)
  lora_dropout: (0.05415440840859075,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 1, 1],)
  lora_alpha: (30.56758872637405,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 1]
lora rank:  125
lora dropout:  0.05415440840859075
lora alpha:  30.56758872637405
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 196,352,000 || all params: 8,226,613,248 || trainable%: 2.3868
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6935, 'grad_norm': 0.8026516437530518, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3471901416778564, 'eval_runtime': 5.774, 'eval_samples_per_second': 173.19, 'eval_steps_per_second': 10.911, 'epoch': 0.04}
{'loss': 1.2694, 'grad_norm': 0.34048226475715637, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0729008913040161, 'eval_runtime': 5.7647, 'eval_samples_per_second': 173.471, 'eval_steps_per_second': 10.929, 'epoch': 0.08}
{'loss': 1.0975, 'grad_norm': 0.23298795521259308, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0295872688293457, 'eval_runtime': 5.7653, 'eval_samples_per_second': 173.452, 'eval_steps_per_second': 10.927, 'epoch': 0.12}
{'loss': 1.0533, 'grad_norm': 0.26332515478134155, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9766937494277954, 'eval_runtime': 5.7793, 'eval_samples_per_second': 173.032, 'eval_steps_per_second': 10.901, 'epoch': 0.16}
{'loss': 1.0023, 'grad_norm': 0.22653844952583313, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9416662454605103, 'eval_runtime': 5.7891, 'eval_samples_per_second': 172.739, 'eval_steps_per_second': 10.883, 'epoch': 0.2}
{'loss': 0.9975, 'grad_norm': 0.24260585010051727, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9301930665969849, 'eval_runtime': 5.7868, 'eval_samples_per_second': 172.806, 'eval_steps_per_second': 10.887, 'epoch': 0.24}
{'loss': 0.9649, 'grad_norm': 0.19531461596488953, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9192519783973694, 'eval_runtime': 5.7901, 'eval_samples_per_second': 172.709, 'eval_steps_per_second': 10.881, 'epoch': 0.28}
{'loss': 0.9008, 'grad_norm': 0.2143755704164505, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9187187552452087, 'eval_runtime': 5.8034, 'eval_samples_per_second': 172.312, 'eval_steps_per_second': 10.856, 'epoch': 0.32}
{'loss': 0.9138, 'grad_norm': 0.25749075412750244, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9117960929870605, 'eval_runtime': 5.7893, 'eval_samples_per_second': 172.732, 'eval_steps_per_second': 10.882, 'epoch': 0.36}
{'loss': 0.8877, 'grad_norm': 0.2851191461086273, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9102031588554382, 'eval_runtime': 5.7924, 'eval_samples_per_second': 172.639, 'eval_steps_per_second': 10.876, 'epoch': 0.4}
{'loss': 0.8615, 'grad_norm': 0.2638736367225647, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9108569622039795, 'eval_runtime': 5.7899, 'eval_samples_per_second': 172.714, 'eval_steps_per_second': 10.881, 'epoch': 0.44}
{'loss': 0.8333, 'grad_norm': 0.22928544878959656, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8997749090194702, 'eval_runtime': 5.7939, 'eval_samples_per_second': 172.596, 'eval_steps_per_second': 10.874, 'epoch': 0.48}
{'loss': 0.8214, 'grad_norm': 0.24121400713920593, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8932564854621887, 'eval_runtime': 5.8252, 'eval_samples_per_second': 171.667, 'eval_steps_per_second': 10.815, 'epoch': 0.52}
{'loss': 0.838, 'grad_norm': 0.2691882848739624, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8975347280502319, 'eval_runtime': 5.8097, 'eval_samples_per_second': 172.126, 'eval_steps_per_second': 10.844, 'epoch': 0.56}
{'loss': 0.8322, 'grad_norm': 0.24297158420085907, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8921637535095215, 'eval_runtime': 5.8392, 'eval_samples_per_second': 171.257, 'eval_steps_per_second': 10.789, 'epoch': 0.6}
{'loss': 0.8077, 'grad_norm': 0.22809040546417236, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8903835415840149, 'eval_runtime': 5.8315, 'eval_samples_per_second': 171.484, 'eval_steps_per_second': 10.803, 'epoch': 0.64}
{'loss': 0.8022, 'grad_norm': 0.2291562706232071, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.889162003993988, 'eval_runtime': 5.8249, 'eval_samples_per_second': 171.676, 'eval_steps_per_second': 10.816, 'epoch': 0.68}
{'loss': 0.8521, 'grad_norm': 0.2938670516014099, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8882248997688293, 'eval_runtime': 5.8365, 'eval_samples_per_second': 171.334, 'eval_steps_per_second': 10.794, 'epoch': 0.72}
{'loss': 0.8365, 'grad_norm': 0.3103666603565216, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8855187892913818, 'eval_runtime': 5.828, 'eval_samples_per_second': 171.585, 'eval_steps_per_second': 10.81, 'epoch': 0.76}
{'loss': 0.7601, 'grad_norm': 0.35040977597236633, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8868126273155212, 'eval_runtime': 5.8676, 'eval_samples_per_second': 170.427, 'eval_steps_per_second': 10.737, 'epoch': 0.8}
{'loss': 0.7361, 'grad_norm': 0.3645446002483368, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8824416995048523, 'eval_runtime': 5.8843, 'eval_samples_per_second': 169.945, 'eval_steps_per_second': 10.707, 'epoch': 0.84}
{'loss': 0.7202, 'grad_norm': 0.33347052335739136, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8806982636451721, 'eval_runtime': 5.8777, 'eval_samples_per_second': 170.135, 'eval_steps_per_second': 10.719, 'epoch': 0.88}
{'loss': 0.7239, 'grad_norm': 0.19578976929187775, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8813567161560059, 'eval_runtime': 5.8343, 'eval_samples_per_second': 171.401, 'eval_steps_per_second': 10.798, 'epoch': 0.92}
{'loss': 0.7607, 'grad_norm': 0.20783096551895142, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8810295462608337, 'eval_runtime': 5.8282, 'eval_samples_per_second': 171.578, 'eval_steps_per_second': 10.809, 'epoch': 0.96}
{'loss': 0.7635, 'grad_norm': 0.23944422602653503, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8810084462165833, 'eval_runtime': 5.8288, 'eval_samples_per_second': 171.562, 'eval_steps_per_second': 10.808, 'epoch': 1.0}
{'train_runtime': 400.3619, 'train_samples_per_second': 24.97, 'train_steps_per_second': 1.561, 'train_loss': 0.9492057434082031, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3471901416778564, 1.0729008913040161, 1.0295872688293457, 0.9766937494277954, 0.9416662454605103, 0.9301930665969849, 0.9192519783973694, 0.9187187552452087, 0.9117960929870605, 0.9102031588554382, 0.9108569622039795, 0.8997749090194702, 0.8932564854621887, 0.8975347280502319, 0.8921637535095215, 0.8903835415840149, 0.889162003993988, 0.8882248997688293, 0.8855187892913818, 0.8868126273155212, 0.8824416995048523, 0.8806982636451721, 0.8813567161560059, 0.8810295462608337, 0.8810084462165833], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3471901416778564, 1.0729008913040161, 1.0295872688293457, 0.9766937494277954, 0.9416662454605103, 0.9301930665969849, 0.9192519783973694, 0.9187187552452087, 0.9117960929870605, 0.9102031588554382, 0.9108569622039795, 0.8997749090194702, 0.8932564854621887, 0.8975347280502319, 0.8921637535095215, 0.8903835415840149, 0.889162003993988, 0.8882248997688293, 0.8855187892913818, 0.8868126273155212, 0.8824416995048523, 0.8806982636451721, 0.8813567161560059, 0.8810295462608337, 0.8810084462165833]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.158286452293396
current iteration best possible eval_loss (full train run):  -0.8810084462165833
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.2508 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7182386517524719, 0.31047528982162476, 0.8264759182929993, 0.941551148891449, 0.6777949333190918, 0.020546019077301025, 0.42309367656707764, 0.23488205671310425, 0.2574614882469177, 0.10306958109140396, 0.6260443925857544, 0.17470037937164307, 0.6499568223953247, 0.2913281321525574, 0.8692778944969177, 0.16640162467956543, 0.919781506061554, 0.9117460250854492, 0.2508368492126465]  ‚Üí  acq = -0.911198774717044
X = [0.9905890226364136, 0.0551074743270874, 0.6507843732833862, 0.2365320324897766, 0.9754101037979126, 0.5206316709518433, 0.07653564214706421, 0.6301301717758179, 0.29114675521850586, 0.22970221936702728, 0.800187885761261, 0.8969747424125671, 0.9849119782447815, 0.6199882626533508, 0.9949815273284912, 0.7660770416259766, 0.9943764209747314, 0.8549709320068359, 0.5030623078346252]  ‚Üí  acq = -0.9111987729178626
X = [0.7485067248344421, 0.7228544354438782, 0.7270810008049011, 0.46116071939468384, 0.1628429889678955, 0.9557974934577942, 0.05652296543121338, 0.1538066864013672, 0.41532599925994873, 0.38161376118659973, 0.8665747046470642, 0.5554915070533752, 0.12801343202590942, 0.8708495497703552, 0.6550924777984619, 0.034474872052669525, 0.9721140265464783, 0.07354127615690231, 0.3236018419265747]  ‚Üí  acq = -0.9112022725486227
X = [0.9728158116340637, 0.7064208984375, 0.7911195755004883, 0.363947331905365, 0.02992570400238037, 0.3345460295677185, 0.7500701546669006, 0.8437953591346741, 0.7014566659927368, 0.3562088906764984, 0.7769957780838013, 0.893889844417572, 0.04227316379547119, 0.3215111494064331, 0.12362712621688843, 0.0846899002790451, 0.7159355282783508, 0.9012787342071533, 0.41329818964004517]  ‚Üí  acq = -0.9111987729116025
X = [0.37084996700286865, 0.4860697388648987, 0.06683415174484253, 0.8602600693702698, 0.45287615060806274, 0.8010461330413818, 0.9572079181671143, 0.8384402990341187, 0.6754447817802429, 0.41918280720710754, 0.34060734510421753, 0.9966937899589539, 0.4164462089538574, 0.9604843258857727, 0.1054425835609436, 0.052955590188503265, 0.9501203298568726, 0.7730306386947632, 0.04848372936248779]  ‚Üí  acq = -0.911198772911652
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.3078, dtype=torch.float64), 0, tensor(0.0556, dtype=torch.float64), 0, tensor(0.0764, dtype=torch.float64), tensor(0.0976, dtype=torch.float64), 0, tensor(0.1023, dtype=torch.float64), tensor(0.3603, dtype=torch.float64), 26, 0, 1, 1, 1, 0, 54, 0.027334173050309458, 21.135216047584723, 0]
normalized proposed parameters for next round by BO: [tensor(0.3078, dtype=torch.float64), tensor(1.6565e-18, dtype=torch.float64), tensor(0.0556, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0764, dtype=torch.float64), tensor(0.0976, dtype=torch.float64), tensor(1.0113e-17, dtype=torch.float64), tensor(0.1023, dtype=torch.float64), tensor(0.3603, dtype=torch.float64), tensor(0.8163, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4233, dtype=torch.float64), tensor(0.2733, dtype=torch.float64), tensor(0.4403, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  10  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.308
  gsm8k: 0
  rowan_hellaswag: 0.056
  sciq: 0
  triviaqa: 0.076
  truthfulqa_gen: 0.098
  wikitext: 0
  mmlu: 0.102
  arc_challenge: 0.36

LoRA Parameters:
  lora_r: (54,)
  lora_dropout: (0.027334173050309458,)
  num_layers_to_apply: (26,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (21.135216047584723,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  26
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  54
lora dropout:  0.027334173050309458
lora alpha:  21.135216047584723
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 58,945,536 || all params: 8,089,206,784 || trainable%: 0.7287
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2382, 'grad_norm': 0.9455018043518066, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.593682050704956, 'eval_runtime': 5.0761, 'eval_samples_per_second': 197.0, 'eval_steps_per_second': 12.411, 'epoch': 0.04}
{'loss': 1.3227, 'grad_norm': 0.4915756285190582, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9934614300727844, 'eval_runtime': 5.0719, 'eval_samples_per_second': 197.166, 'eval_steps_per_second': 12.421, 'epoch': 0.08}
{'loss': 1.0707, 'grad_norm': 0.21065813302993774, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.946753740310669, 'eval_runtime': 5.0605, 'eval_samples_per_second': 197.611, 'eval_steps_per_second': 12.449, 'epoch': 0.12}
{'loss': 1.0599, 'grad_norm': 0.22822324931621552, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9288874268531799, 'eval_runtime': 5.0716, 'eval_samples_per_second': 197.177, 'eval_steps_per_second': 12.422, 'epoch': 0.16}
{'loss': 0.9955, 'grad_norm': 0.26324284076690674, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9207925200462341, 'eval_runtime': 5.0673, 'eval_samples_per_second': 197.342, 'eval_steps_per_second': 12.433, 'epoch': 0.2}
{'loss': 1.0045, 'grad_norm': 0.23878438770771027, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9110457897186279, 'eval_runtime': 5.0791, 'eval_samples_per_second': 196.885, 'eval_steps_per_second': 12.404, 'epoch': 0.24}
{'loss': 1.0229, 'grad_norm': 0.24447773396968842, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9110022783279419, 'eval_runtime': 5.0763, 'eval_samples_per_second': 196.995, 'eval_steps_per_second': 12.411, 'epoch': 0.28}
{'loss': 0.9663, 'grad_norm': 0.26023542881011963, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9064885377883911, 'eval_runtime': 5.0809, 'eval_samples_per_second': 196.817, 'eval_steps_per_second': 12.399, 'epoch': 0.32}
{'loss': 0.9832, 'grad_norm': 0.2530503273010254, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9024801850318909, 'eval_runtime': 5.0827, 'eval_samples_per_second': 196.745, 'eval_steps_per_second': 12.395, 'epoch': 0.36}
{'loss': 0.9952, 'grad_norm': 0.25438785552978516, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9031318426132202, 'eval_runtime': 5.0849, 'eval_samples_per_second': 196.66, 'eval_steps_per_second': 12.39, 'epoch': 0.4}
{'loss': 0.9621, 'grad_norm': 0.25494468212127686, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.901412844657898, 'eval_runtime': 5.0889, 'eval_samples_per_second': 196.505, 'eval_steps_per_second': 12.38, 'epoch': 0.44}
{'loss': 0.9217, 'grad_norm': 0.24464495480060577, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8919979333877563, 'eval_runtime': 5.0943, 'eval_samples_per_second': 196.297, 'eval_steps_per_second': 12.367, 'epoch': 0.48}
{'loss': 0.9185, 'grad_norm': 0.27863410115242004, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8893652558326721, 'eval_runtime': 5.0968, 'eval_samples_per_second': 196.202, 'eval_steps_per_second': 12.361, 'epoch': 0.52}
{'loss': 0.9437, 'grad_norm': 0.29267483949661255, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8922009468078613, 'eval_runtime': 5.1081, 'eval_samples_per_second': 195.767, 'eval_steps_per_second': 12.333, 'epoch': 0.56}
{'loss': 0.9133, 'grad_norm': 0.26516249775886536, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8838556408882141, 'eval_runtime': 5.1158, 'eval_samples_per_second': 195.473, 'eval_steps_per_second': 12.315, 'epoch': 0.6}
{'loss': 0.9023, 'grad_norm': 0.29936516284942627, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8816000819206238, 'eval_runtime': 5.1073, 'eval_samples_per_second': 195.799, 'eval_steps_per_second': 12.335, 'epoch': 0.64}
{'loss': 0.9205, 'grad_norm': 0.2883848547935486, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8845804929733276, 'eval_runtime': 5.1136, 'eval_samples_per_second': 195.558, 'eval_steps_per_second': 12.32, 'epoch': 0.68}
{'loss': 0.9683, 'grad_norm': 0.37284013628959656, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8785534501075745, 'eval_runtime': 5.1013, 'eval_samples_per_second': 196.029, 'eval_steps_per_second': 12.35, 'epoch': 0.72}
{'loss': 0.9541, 'grad_norm': 0.2884608209133148, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8767956495285034, 'eval_runtime': 5.0941, 'eval_samples_per_second': 196.306, 'eval_steps_per_second': 12.367, 'epoch': 0.76}
{'loss': 0.9158, 'grad_norm': 0.30931368470191956, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8761216998100281, 'eval_runtime': 5.0925, 'eval_samples_per_second': 196.366, 'eval_steps_per_second': 12.371, 'epoch': 0.8}
{'loss': 0.8608, 'grad_norm': 0.38791927695274353, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8720051646232605, 'eval_runtime': 5.0923, 'eval_samples_per_second': 196.376, 'eval_steps_per_second': 12.372, 'epoch': 0.84}
{'loss': 0.881, 'grad_norm': 0.36739081144332886, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8728980422019958, 'eval_runtime': 5.0909, 'eval_samples_per_second': 196.429, 'eval_steps_per_second': 12.375, 'epoch': 0.88}
{'loss': 0.8816, 'grad_norm': 0.3858261704444885, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8708373308181763, 'eval_runtime': 5.0959, 'eval_samples_per_second': 196.234, 'eval_steps_per_second': 12.363, 'epoch': 0.92}
{'loss': 0.9152, 'grad_norm': 0.3317411243915558, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8708077669143677, 'eval_runtime': 5.0893, 'eval_samples_per_second': 196.489, 'eval_steps_per_second': 12.379, 'epoch': 0.96}
{'loss': 0.8756, 'grad_norm': 0.40077340602874756, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8707413077354431, 'eval_runtime': 5.096, 'eval_samples_per_second': 196.233, 'eval_steps_per_second': 12.363, 'epoch': 1.0}
{'train_runtime': 316.4402, 'train_samples_per_second': 31.592, 'train_steps_per_second': 1.975, 'train_loss': 1.0557384887695314, 'epoch': 1.0}
train_results:  {'eval_loss': [1.593682050704956, 0.9934614300727844, 0.946753740310669, 0.9288874268531799, 0.9207925200462341, 0.9110457897186279, 0.9110022783279419, 0.9064885377883911, 0.9024801850318909, 0.9031318426132202, 0.901412844657898, 0.8919979333877563, 0.8893652558326721, 0.8922009468078613, 0.8838556408882141, 0.8816000819206238, 0.8845804929733276, 0.8785534501075745, 0.8767956495285034, 0.8761216998100281, 0.8720051646232605, 0.8728980422019958, 0.8708373308181763, 0.8708077669143677, 0.8707413077354431], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.593682050704956, 0.9934614300727844, 0.946753740310669, 0.9288874268531799, 0.9207925200462341, 0.9110457897186279, 0.9110022783279419, 0.9064885377883911, 0.9024801850318909, 0.9031318426132202, 0.901412844657898, 0.8919979333877563, 0.8893652558326721, 0.8922009468078613, 0.8838556408882141, 0.8816000819206238, 0.8845804929733276, 0.8785534501075745, 0.8767956495285034, 0.8761216998100281, 0.8720051646232605, 0.8728980422019958, 0.8708373308181763, 0.8708077669143677, 0.8707413077354431]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.009720802307129
current iteration best possible eval_loss (full train run):  -0.8707413077354431
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9622 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7742140293121338, 0.45275312662124634, 0.8311971426010132, 0.9466180205345154, 0.5241358876228333, 0.8108326196670532, 0.1247032880783081, 0.2205706238746643, 0.2958891987800598, 0.807266116142273, 0.20962661504745483, 0.6430747509002686, 0.6093101501464844, 0.4138326048851013, 0.12062281370162964, 0.7881916761398315, 0.5773974061012268, 0.35845625400543213, 0.07152366638183594]  ‚Üí  acq = -0.9161438167935823
X = [0.5636504292488098, 0.4796243906021118, 0.4268649220466614, 0.4849214553833008, 0.015171229839324951, 0.4103096127510071, 0.7042558789253235, 0.4842594265937805, 0.6193363070487976, 0.5605196952819824, 0.5303605198860168, 0.9504585862159729, 0.5603010654449463, 0.9274217486381531, 0.6309018731117249, 0.6315646767616272, 0.2499348521232605, 0.6944359540939331, 0.7650787830352783]  ‚Üí  acq = -0.9161463781054988
X = [0.9024564027786255, 0.6652516722679138, 0.12141412496566772, 0.8221784234046936, 0.9579598307609558, 0.8169945478439331, 0.48157328367233276, 0.5467605590820312, 0.984917938709259, 0.911651074886322, 0.055326223373413086, 0.45030295848846436, 0.12008339166641235, 0.4670383334159851, 0.10925418138504028, 0.483948290348053, 0.022005975246429443, 0.1799357682466507, 0.49969446659088135]  ‚Üí  acq = -0.916152008253591
X = [0.7527661323547363, 0.41271156072616577, 0.314677357673645, 0.8815593123435974, 0.5601663589477539, 0.24608474969863892, 0.3004351854324341, 0.7857574820518494, 0.5358787775039673, 0.5074102282524109, 0.5506842136383057, 0.33297544717788696, 0.42730605602264404, 0.9252958297729492, 0.8326297998428345, 0.6332313418388367, 0.630294680595398, 0.4930584132671356, 0.06750988960266113]  ‚Üí  acq = -0.9161438159460553
X = [0.203538179397583, 0.6768487095832825, 0.6658650040626526, 0.5713086128234863, 0.2150021195411682, 0.7517347931861877, 0.9438826441764832, 0.6268109083175659, 0.6458637714385986, 0.9806448817253113, 0.5306293368339539, 0.2511675953865051, 0.041422903537750244, 0.2728269696235657, 0.47408175468444824, 0.7798543572425842, 0.2598608732223511, 0.6594997644424438, 0.7008309364318848]  ‚Üí  acq = -0.9161438142128391
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0641, dtype=torch.float64), 0, tensor(0.0700, dtype=torch.float64), tensor(0.0111, dtype=torch.float64), tensor(0.1384, dtype=torch.float64), tensor(0.2631, dtype=torch.float64), 0, tensor(0.0186, dtype=torch.float64), tensor(0.4347, dtype=torch.float64), 22, 0, 1, 1, 1, 0, 90, 0.009696145966038586, 19.30509234853406, 0]
normalized proposed parameters for next round by BO: [tensor(0.0641, dtype=torch.float64), tensor(2.1263e-18, dtype=torch.float64), tensor(0.0700, dtype=torch.float64), tensor(0.0111, dtype=torch.float64), tensor(0.1384, dtype=torch.float64), tensor(0.2631, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.0186, dtype=torch.float64), tensor(0.4347, dtype=torch.float64), tensor(0.6994, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7009, dtype=torch.float64), tensor(0.0970, dtype=torch.float64), tensor(0.4022, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  11  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.064
  gsm8k: 0
  rowan_hellaswag: 0.07
  sciq: 0.011
  triviaqa: 0.138
  truthfulqa_gen: 0.263
  wikitext: 0
  mmlu: 0.019
  arc_challenge: 0.435

LoRA Parameters:
  lora_r: (90,)
  lora_dropout: (0.009696145966038586,)
  num_layers_to_apply: (22,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (19.30509234853406,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  22
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  90
lora dropout:  0.009696145966038586
lora alpha:  19.30509234853406
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 83,128,320 || all params: 8,113,389,568 || trainable%: 1.0246
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3601, 'grad_norm': 0.75116366147995, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.9618464708328247, 'eval_runtime': 5.0206, 'eval_samples_per_second': 199.177, 'eval_steps_per_second': 12.548, 'epoch': 0.04}
{'loss': 1.3434, 'grad_norm': 0.4452892243862152, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0879106521606445, 'eval_runtime': 5.037, 'eval_samples_per_second': 198.533, 'eval_steps_per_second': 12.508, 'epoch': 0.08}
{'loss': 1.0898, 'grad_norm': 0.20933599770069122, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0434703826904297, 'eval_runtime': 5.0475, 'eval_samples_per_second': 198.119, 'eval_steps_per_second': 12.482, 'epoch': 0.12}
{'loss': 1.0165, 'grad_norm': 0.204381063580513, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0385102033615112, 'eval_runtime': 5.0391, 'eval_samples_per_second': 198.448, 'eval_steps_per_second': 12.502, 'epoch': 0.16}
{'loss': 1.0539, 'grad_norm': 0.20480862259864807, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0219206809997559, 'eval_runtime': 5.0379, 'eval_samples_per_second': 198.494, 'eval_steps_per_second': 12.505, 'epoch': 0.2}
{'loss': 1.0714, 'grad_norm': 0.18036788702011108, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9967374801635742, 'eval_runtime': 5.0345, 'eval_samples_per_second': 198.629, 'eval_steps_per_second': 12.514, 'epoch': 0.24}
{'loss': 0.9452, 'grad_norm': 0.20325499773025513, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9957051873207092, 'eval_runtime': 5.0581, 'eval_samples_per_second': 197.703, 'eval_steps_per_second': 12.455, 'epoch': 0.28}
{'loss': 0.9834, 'grad_norm': 0.23392370343208313, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9918323755264282, 'eval_runtime': 5.0693, 'eval_samples_per_second': 197.264, 'eval_steps_per_second': 12.428, 'epoch': 0.32}
{'loss': 0.9666, 'grad_norm': 0.22009895741939545, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.988523542881012, 'eval_runtime': 5.0718, 'eval_samples_per_second': 197.17, 'eval_steps_per_second': 12.422, 'epoch': 0.36}
{'loss': 0.9716, 'grad_norm': 0.1998276263475418, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9814754128456116, 'eval_runtime': 5.0651, 'eval_samples_per_second': 197.428, 'eval_steps_per_second': 12.438, 'epoch': 0.4}
{'loss': 0.8966, 'grad_norm': 0.2542823851108551, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9791703820228577, 'eval_runtime': 5.0704, 'eval_samples_per_second': 197.222, 'eval_steps_per_second': 12.425, 'epoch': 0.44}
{'loss': 0.9686, 'grad_norm': 0.24068035185337067, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9774964451789856, 'eval_runtime': 5.0169, 'eval_samples_per_second': 199.324, 'eval_steps_per_second': 12.557, 'epoch': 0.48}
{'loss': 0.8683, 'grad_norm': 0.28336653113365173, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9626855254173279, 'eval_runtime': 5.0164, 'eval_samples_per_second': 199.348, 'eval_steps_per_second': 12.559, 'epoch': 0.52}
{'loss': 0.9015, 'grad_norm': 0.2650803327560425, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9652198553085327, 'eval_runtime': 5.0154, 'eval_samples_per_second': 199.386, 'eval_steps_per_second': 12.561, 'epoch': 0.56}
{'loss': 0.9006, 'grad_norm': 0.33177822828292847, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9652437567710876, 'eval_runtime': 5.0186, 'eval_samples_per_second': 199.258, 'eval_steps_per_second': 12.553, 'epoch': 0.6}
{'loss': 0.8737, 'grad_norm': 0.2268700748682022, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9537138938903809, 'eval_runtime': 5.0137, 'eval_samples_per_second': 199.452, 'eval_steps_per_second': 12.565, 'epoch': 0.64}
{'loss': 0.8397, 'grad_norm': 0.3428378403186798, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.960715115070343, 'eval_runtime': 5.0215, 'eval_samples_per_second': 199.142, 'eval_steps_per_second': 12.546, 'epoch': 0.68}
{'loss': 0.8381, 'grad_norm': 0.2659412920475006, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9570251703262329, 'eval_runtime': 5.0273, 'eval_samples_per_second': 198.913, 'eval_steps_per_second': 12.532, 'epoch': 0.72}
{'loss': 0.8385, 'grad_norm': 0.42365387082099915, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9602792859077454, 'eval_runtime': 5.0559, 'eval_samples_per_second': 197.791, 'eval_steps_per_second': 12.461, 'epoch': 0.76}
{'loss': 0.8188, 'grad_norm': 0.300311416387558, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9530412554740906, 'eval_runtime': 5.0509, 'eval_samples_per_second': 197.986, 'eval_steps_per_second': 12.473, 'epoch': 0.8}
{'loss': 0.7954, 'grad_norm': 0.387560099363327, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9534637928009033, 'eval_runtime': 5.0493, 'eval_samples_per_second': 198.046, 'eval_steps_per_second': 12.477, 'epoch': 0.84}
{'loss': 0.8293, 'grad_norm': 0.4289410710334778, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9483171105384827, 'eval_runtime': 5.0353, 'eval_samples_per_second': 198.596, 'eval_steps_per_second': 12.512, 'epoch': 0.88}
{'loss': 0.7525, 'grad_norm': 0.3177701234817505, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9467462301254272, 'eval_runtime': 5.0407, 'eval_samples_per_second': 198.387, 'eval_steps_per_second': 12.498, 'epoch': 0.92}
{'loss': 0.7611, 'grad_norm': 0.35474246740341187, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.949027955532074, 'eval_runtime': 5.039, 'eval_samples_per_second': 198.451, 'eval_steps_per_second': 12.502, 'epoch': 0.96}
{'loss': 0.7555, 'grad_norm': 0.3343603312969208, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9481663107872009, 'eval_runtime': 5.0427, 'eval_samples_per_second': 198.306, 'eval_steps_per_second': 12.493, 'epoch': 1.0}
{'train_runtime': 295.877, 'train_samples_per_second': 33.788, 'train_steps_per_second': 2.112, 'train_loss': 1.017597930908203, 'epoch': 1.0}
train_results:  {'eval_loss': [1.9618464708328247, 1.0879106521606445, 1.0434703826904297, 1.0385102033615112, 1.0219206809997559, 0.9967374801635742, 0.9957051873207092, 0.9918323755264282, 0.988523542881012, 0.9814754128456116, 0.9791703820228577, 0.9774964451789856, 0.9626855254173279, 0.9652198553085327, 0.9652437567710876, 0.9537138938903809, 0.960715115070343, 0.9570251703262329, 0.9602792859077454, 0.9530412554740906, 0.9534637928009033, 0.9483171105384827, 0.9467462301254272, 0.949027955532074, 0.9481663107872009], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.9618464708328247, 1.0879106521606445, 1.0434703826904297, 1.0385102033615112, 1.0219206809997559, 0.9967374801635742, 0.9957051873207092, 0.9918323755264282, 0.988523542881012, 0.9814754128456116, 0.9791703820228577, 0.9774964451789856, 0.9626855254173279, 0.9652198553085327, 0.9652437567710876, 0.9537138938903809, 0.960715115070343, 0.9570251703262329, 0.9602792859077454, 0.9530412554740906, 0.9534637928009033, 0.9483171105384827, 0.9467462301254272, 0.949027955532074, 0.9481663107872009]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0834424495697021
current iteration best possible eval_loss (full train run):  -0.9481663107872009
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.3510 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.784311056137085, 0.4783253073692322, 0.7043008804321289, 0.725765585899353, 0.4861075282096863, 0.2787923216819763, 0.29957282543182373, 0.5923912525177002, 0.8282220363616943, 0.8412190675735474, 0.4107237458229065, 0.7874518036842346, 0.12362807989120483, 0.21245026588439941, 0.034817516803741455, 0.22668592631816864, 0.8301550149917603, 0.3823719024658203, 0.4275026321411133]  ‚Üí  acq = -0.892618857217587
X = [0.5584064722061157, 0.44919145107269287, 0.32144320011138916, 0.7054430842399597, 0.7268563508987427, 0.5880426168441772, 0.4248616099357605, 0.17556768655776978, 0.29544466733932495, 0.4818800985813141, 0.810372531414032, 0.7529270648956299, 0.9706915616989136, 0.7960174679756165, 0.16252559423446655, 0.2255697399377823, 0.46233898401260376, 0.3697861135005951, 0.9076562523841858]  ‚Üí  acq = -0.8950125398113491
X = [0.20838326215744019, 0.58420729637146, 0.5558130741119385, 0.8941408395767212, 0.5463425517082214, 0.539378821849823, 0.6101441383361816, 0.42813771963119507, 0.6221595406532288, 0.06164299324154854, 0.6520464420318604, 0.6492470502853394, 0.019079983234405518, 0.9904217720031738, 0.6705264449119568, 0.5228995680809021, 0.4145408272743225, 0.8589955568313599, 0.7290428876876831]  ‚Üí  acq = -0.8924677547933363
X = [0.6510567665100098, 0.8864595293998718, 0.5675499439239502, 0.34420323371887207, 0.2640973925590515, 0.5927631258964539, 0.4000641107559204, 0.15476346015930176, 0.13708847761154175, 0.2613682746887207, 0.1723441481590271, 0.5438426733016968, 0.2560986280441284, 0.41083842515945435, 0.9889391660690308, 0.26987963914871216, 0.647262454032898, 0.2808990776538849, 0.15090978145599365]  ‚Üí  acq = -0.8966123851568529
X = [0.9737928509712219, 0.11804687976837158, 0.48535317182540894, 0.7090001702308655, 0.7036911249160767, 0.670799970626831, 0.8314781785011292, 0.12475329637527466, 0.13615381717681885, 0.3458307683467865, 0.868510901927948, 0.0368269681930542, 0.06938421726226807, 0.7864115238189697, 0.012897372245788574, 0.5269995927810669, 0.154333233833313, 0.7413930892944336, 0.007459759712219238]  ‚Üí  acq = -0.8923646486098029
proposed candidate layer mask is:  tensor([1., 0., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.5059, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.0188, dtype=torch.float64), tensor(0.0722, dtype=torch.float64), tensor(0.1285, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0.0277, dtype=torch.float64), tensor(0.0987, dtype=torch.float64), tensor(0.0489, dtype=torch.float64), 29, 1, 0, 1, 0, 1, 25, 0.06979459890740512, 39.057867234943785, 0]
normalized proposed parameters for next round by BO: [tensor(0.5059, dtype=torch.float64), tensor(0.0291, dtype=torch.float64), tensor(0.0188, dtype=torch.float64), tensor(0.0722, dtype=torch.float64), tensor(0.1285, dtype=torch.float64), tensor(0.0702, dtype=torch.float64), tensor(0.0277, dtype=torch.float64), tensor(0.0987, dtype=torch.float64), tensor(0.0489, dtype=torch.float64), tensor(0.8955, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1923, dtype=torch.float64), tensor(0.6979, dtype=torch.float64), tensor(0.8137, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  12  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.506
  gsm8k: 0.029
  rowan_hellaswag: 0.019
  sciq: 0.072
  triviaqa: 0.129
  truthfulqa_gen: 0.07
  wikitext: 0.028
  mmlu: 0.099
  arc_challenge: 0.049

LoRA Parameters:
  lora_r: (25,)
  lora_dropout: (0.06979459890740512,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 0, 1, 0, 1],)
  lora_alpha: (39.057867234943785,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 1, 0, 1]
lora rank:  25
lora dropout:  0.06979459890740512
lora alpha:  39.057867234943785
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 32,665,600 || all params: 8,062,926,848 || trainable%: 0.4051
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1206, 'grad_norm': 1.1488780975341797, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.3769242763519287, 'eval_runtime': 5.202, 'eval_samples_per_second': 192.233, 'eval_steps_per_second': 12.111, 'epoch': 0.04}
{'loss': 1.372, 'grad_norm': 0.46006476879119873, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.100218415260315, 'eval_runtime': 5.1814, 'eval_samples_per_second': 192.999, 'eval_steps_per_second': 12.159, 'epoch': 0.08}
{'loss': 1.2356, 'grad_norm': 0.40657979249954224, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9767743349075317, 'eval_runtime': 5.2066, 'eval_samples_per_second': 192.062, 'eval_steps_per_second': 12.1, 'epoch': 0.12}
{'loss': 1.0798, 'grad_norm': 0.42878779768943787, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9074068665504456, 'eval_runtime': 5.2155, 'eval_samples_per_second': 191.734, 'eval_steps_per_second': 12.079, 'epoch': 0.16}
{'loss': 1.0675, 'grad_norm': 0.3453323245048523, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.8925586342811584, 'eval_runtime': 5.215, 'eval_samples_per_second': 191.754, 'eval_steps_per_second': 12.08, 'epoch': 0.2}
{'loss': 1.022, 'grad_norm': 0.4153675138950348, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.8787455558776855, 'eval_runtime': 5.2134, 'eval_samples_per_second': 191.812, 'eval_steps_per_second': 12.084, 'epoch': 0.24}
{'loss': 1.0155, 'grad_norm': 0.37850454449653625, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.8788491487503052, 'eval_runtime': 5.2118, 'eval_samples_per_second': 191.874, 'eval_steps_per_second': 12.088, 'epoch': 0.28}
{'loss': 0.9693, 'grad_norm': 0.3564077317714691, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.8727749586105347, 'eval_runtime': 5.2179, 'eval_samples_per_second': 191.647, 'eval_steps_per_second': 12.074, 'epoch': 0.32}
{'loss': 1.0103, 'grad_norm': 0.41932213306427, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.872849702835083, 'eval_runtime': 5.2323, 'eval_samples_per_second': 191.121, 'eval_steps_per_second': 12.041, 'epoch': 0.36}
{'loss': 0.9678, 'grad_norm': 0.37314847111701965, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.8653590083122253, 'eval_runtime': 5.2231, 'eval_samples_per_second': 191.456, 'eval_steps_per_second': 12.062, 'epoch': 0.4}
{'loss': 0.9781, 'grad_norm': 0.386457234621048, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.8668362498283386, 'eval_runtime': 5.2209, 'eval_samples_per_second': 191.539, 'eval_steps_per_second': 12.067, 'epoch': 0.44}
{'loss': 0.9746, 'grad_norm': 0.357455313205719, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8601475954055786, 'eval_runtime': 5.2192, 'eval_samples_per_second': 191.602, 'eval_steps_per_second': 12.071, 'epoch': 0.48}
{'loss': 0.9311, 'grad_norm': 0.41053932905197144, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8549342751502991, 'eval_runtime': 5.219, 'eval_samples_per_second': 191.607, 'eval_steps_per_second': 12.071, 'epoch': 0.52}
{'loss': 0.9187, 'grad_norm': 0.3684215247631073, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8532487750053406, 'eval_runtime': 5.221, 'eval_samples_per_second': 191.536, 'eval_steps_per_second': 12.067, 'epoch': 0.56}
{'loss': 0.956, 'grad_norm': 0.43320420384407043, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8521368503570557, 'eval_runtime': 5.2178, 'eval_samples_per_second': 191.651, 'eval_steps_per_second': 12.074, 'epoch': 0.6}
{'loss': 0.957, 'grad_norm': 0.36338433623313904, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8490168452262878, 'eval_runtime': 5.2152, 'eval_samples_per_second': 191.746, 'eval_steps_per_second': 12.08, 'epoch': 0.64}
{'loss': 0.9549, 'grad_norm': 0.3702962100505829, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.845362663269043, 'eval_runtime': 5.227, 'eval_samples_per_second': 191.315, 'eval_steps_per_second': 12.053, 'epoch': 0.68}
{'loss': 0.923, 'grad_norm': 0.42738813161849976, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.843522846698761, 'eval_runtime': 5.235, 'eval_samples_per_second': 191.022, 'eval_steps_per_second': 12.034, 'epoch': 0.72}
{'loss': 0.9081, 'grad_norm': 0.4137401878833771, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8387150168418884, 'eval_runtime': 5.2418, 'eval_samples_per_second': 190.776, 'eval_steps_per_second': 12.019, 'epoch': 0.76}
{'loss': 0.9766, 'grad_norm': 0.3796618580818176, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8364936709403992, 'eval_runtime': 5.3072, 'eval_samples_per_second': 188.422, 'eval_steps_per_second': 11.871, 'epoch': 0.8}
{'loss': 0.9174, 'grad_norm': 0.42259305715560913, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.837229311466217, 'eval_runtime': 5.3017, 'eval_samples_per_second': 188.62, 'eval_steps_per_second': 11.883, 'epoch': 0.84}
{'loss': 0.9208, 'grad_norm': 0.36952564120292664, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8348265290260315, 'eval_runtime': 5.2911, 'eval_samples_per_second': 188.997, 'eval_steps_per_second': 11.907, 'epoch': 0.88}
{'loss': 0.9536, 'grad_norm': 0.40247756242752075, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8328009247779846, 'eval_runtime': 5.2999, 'eval_samples_per_second': 188.683, 'eval_steps_per_second': 11.887, 'epoch': 0.92}
{'loss': 0.9374, 'grad_norm': 0.4479045271873474, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8334901928901672, 'eval_runtime': 5.3013, 'eval_samples_per_second': 188.634, 'eval_steps_per_second': 11.884, 'epoch': 0.96}
{'loss': 0.9261, 'grad_norm': 0.3914404809474945, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.83283531665802, 'eval_runtime': 5.3097, 'eval_samples_per_second': 188.334, 'eval_steps_per_second': 11.865, 'epoch': 1.0}
{'train_runtime': 313.6481, 'train_samples_per_second': 31.873, 'train_steps_per_second': 1.993, 'train_loss': 1.0797531494140624, 'epoch': 1.0}
train_results:  {'eval_loss': [1.3769242763519287, 1.100218415260315, 0.9767743349075317, 0.9074068665504456, 0.8925586342811584, 0.8787455558776855, 0.8788491487503052, 0.8727749586105347, 0.872849702835083, 0.8653590083122253, 0.8668362498283386, 0.8601475954055786, 0.8549342751502991, 0.8532487750053406, 0.8521368503570557, 0.8490168452262878, 0.845362663269043, 0.843522846698761, 0.8387150168418884, 0.8364936709403992, 0.837229311466217, 0.8348265290260315, 0.8328009247779846, 0.8334901928901672, 0.83283531665802], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.3769242763519287, 1.100218415260315, 0.9767743349075317, 0.9074068665504456, 0.8925586342811584, 0.8787455558776855, 0.8788491487503052, 0.8727749586105347, 0.872849702835083, 0.8653590083122253, 0.8668362498283386, 0.8601475954055786, 0.8549342751502991, 0.8532487750053406, 0.8521368503570557, 0.8490168452262878, 0.845362663269043, 0.843522846698761, 0.8387150168418884, 0.8364936709403992, 0.837229311466217, 0.8348265290260315, 0.8328009247779846, 0.8334901928901672, 0.83283531665802]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1549623012542725
current iteration best possible eval_loss (full train run):  -0.83283531665802
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 21.2304 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6974703669548035, 0.4607950448989868, 0.8264415860176086, 0.04410123825073242, 0.5994921922683716, 0.3795362114906311, 0.302287220954895, 0.6134722232818604, 0.5043690800666809, 0.15795986354351044, 0.6499233245849609, 0.7763527631759644, 0.688374936580658, 0.4434259533882141, 0.9395368695259094, 0.37341463565826416, 0.5809779763221741, 0.27533066272735596, 0.03358107805252075]  ‚Üí  acq = -0.8750128356727285
X = [0.3518112897872925, 0.14945441484451294, 0.41263043880462646, 0.731982409954071, 0.7148564457893372, 0.4512711763381958, 0.2851555347442627, 0.76151442527771, 0.4265725612640381, 0.14288733899593353, 0.2102144956588745, 0.05346560478210449, 0.1188850998878479, 0.34684574604034424, 0.16592669486999512, 0.6620250940322876, 0.5913098454475403, 0.04972274228930473, 0.33564668893814087]  ‚Üí  acq = -0.8763174248970211
X = [0.8099195957183838, 0.02526146173477173, 0.06062203645706177, 0.8779995441436768, 0.6028299331665039, 0.5516091585159302, 0.9053958654403687, 0.2136979103088379, 0.783804178237915, 0.0964193344116211, 0.7257537245750427, 0.3840676546096802, 0.23924213647842407, 0.6780440807342529, 0.5803513526916504, 0.09483984112739563, 0.5482228994369507, 0.10996528714895248, 0.510372519493103]  ‚Üí  acq = -0.8741459714987609
X = [0.9753549695014954, 0.8854005336761475, 0.03140681982040405, 0.10194069147109985, 0.14696693420410156, 0.752841591835022, 0.33589285612106323, 0.3141000270843506, 0.566781759262085, 0.5800305008888245, 0.9905827045440674, 0.9659500122070312, 0.42219460010528564, 0.9536076188087463, 0.7185770869255066, 0.4780624210834503, 0.03939622640609741, 0.9219683408737183, 0.7918861508369446]  ‚Üí  acq = -0.8686654080016841
X = [0.5622198581695557, 0.6252537965774536, 0.22417795658111572, 0.26098769903182983, 0.4574270248413086, 0.5959587097167969, 0.516653835773468, 0.5227282643318176, 0.4093313217163086, 0.7100425362586975, 0.04777747392654419, 0.43128204345703125, 0.0678098201751709, 0.974764347076416, 0.7470415830612183, 0.26881667971611023, 0.9093899726867676, 0.30449700355529785, 0.8694303035736084]  ‚Üí  acq = -0.8754366788617989
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1019, dtype=torch.float64), tensor(0.4335, dtype=torch.float64), 0, 0, tensor(0.1164, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), 0, 0, tensor(0.2910, dtype=torch.float64), 28, 0, 1, 1, 1, 0, 80, 0.046776489419539775, 5.242818810543914, 0]
normalized proposed parameters for next round by BO: [tensor(0.1019, dtype=torch.float64), tensor(0.4335, dtype=torch.float64), tensor(7.6894e-19, dtype=torch.float64), tensor(0.0021, dtype=torch.float64), tensor(0.1164, dtype=torch.float64), tensor(0.0549, dtype=torch.float64), tensor(4.5411e-22, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.2910, dtype=torch.float64), tensor(0.8850, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6220, dtype=torch.float64), tensor(0.4678, dtype=torch.float64), tensor(0.1092, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  13  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.102
  gsm8k: 0.434
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.116
  truthfulqa_gen: 0.055
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.291

LoRA Parameters:
  lora_r: (80,)
  lora_dropout: (0.046776489419539775,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (5.242818810543914,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  80
lora dropout:  0.046776489419539775
lora alpha:  5.242818810543914
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 94,044,160 || all params: 8,124,305,408 || trainable%: 1.1576
length of training data:  9977
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.9565, 'grad_norm': 0.35062122344970703, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.5976645946502686, 'eval_runtime': 5.1173, 'eval_samples_per_second': 195.415, 'eval_steps_per_second': 12.311, 'epoch': 0.04}
{'loss': 1.3919, 'grad_norm': 0.13408800959587097, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1935700178146362, 'eval_runtime': 5.1438, 'eval_samples_per_second': 194.409, 'eval_steps_per_second': 12.248, 'epoch': 0.08}
{'loss': 0.9716, 'grad_norm': 0.11096813529729843, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 1.0486093759536743, 'eval_runtime': 5.1104, 'eval_samples_per_second': 195.679, 'eval_steps_per_second': 12.328, 'epoch': 0.12}
{'loss': 0.9385, 'grad_norm': 0.0816187709569931, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 1.0127894878387451, 'eval_runtime': 5.1456, 'eval_samples_per_second': 194.34, 'eval_steps_per_second': 12.243, 'epoch': 0.16}
{'loss': 0.8731, 'grad_norm': 0.08814092725515366, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 1.001353144645691, 'eval_runtime': 5.1222, 'eval_samples_per_second': 195.229, 'eval_steps_per_second': 12.299, 'epoch': 0.2}
{'loss': 0.8928, 'grad_norm': 0.10439605265855789, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9937573075294495, 'eval_runtime': 5.1379, 'eval_samples_per_second': 194.63, 'eval_steps_per_second': 12.262, 'epoch': 0.24}
{'loss': 0.8834, 'grad_norm': 0.0935000404715538, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.9768927097320557, 'eval_runtime': 5.1468, 'eval_samples_per_second': 194.297, 'eval_steps_per_second': 12.241, 'epoch': 0.28}
{'loss': 0.8696, 'grad_norm': 0.09540421515703201, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.976047694683075, 'eval_runtime': 5.1458, 'eval_samples_per_second': 194.332, 'eval_steps_per_second': 12.243, 'epoch': 0.32}
{'loss': 0.8866, 'grad_norm': 0.09866229444742203, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.9625421762466431, 'eval_runtime': 5.1396, 'eval_samples_per_second': 194.569, 'eval_steps_per_second': 12.258, 'epoch': 0.36}
{'loss': 0.8702, 'grad_norm': 0.10262413322925568, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.9669651985168457, 'eval_runtime': 5.1553, 'eval_samples_per_second': 193.976, 'eval_steps_per_second': 12.22, 'epoch': 0.4}
{'loss': 0.8631, 'grad_norm': 0.09017011523246765, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.95307457447052, 'eval_runtime': 5.1032, 'eval_samples_per_second': 195.955, 'eval_steps_per_second': 12.345, 'epoch': 0.44}
{'loss': 0.8636, 'grad_norm': 0.09644054621458054, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.950861930847168, 'eval_runtime': 5.0702, 'eval_samples_per_second': 197.232, 'eval_steps_per_second': 12.426, 'epoch': 0.48}
{'loss': 0.8459, 'grad_norm': 0.09175652265548706, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.9476746320724487, 'eval_runtime': 5.0798, 'eval_samples_per_second': 196.857, 'eval_steps_per_second': 12.402, 'epoch': 0.52}
{'loss': 0.8284, 'grad_norm': 0.09399297833442688, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.9499374032020569, 'eval_runtime': 5.077, 'eval_samples_per_second': 196.968, 'eval_steps_per_second': 12.409, 'epoch': 0.56}
{'loss': 0.8384, 'grad_norm': 0.09773944318294525, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.9408966302871704, 'eval_runtime': 5.0718, 'eval_samples_per_second': 197.169, 'eval_steps_per_second': 12.422, 'epoch': 0.6}
{'loss': 0.8363, 'grad_norm': 0.09184790402650833, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.9364435076713562, 'eval_runtime': 5.0759, 'eval_samples_per_second': 197.011, 'eval_steps_per_second': 12.412, 'epoch': 0.64}
{'loss': 0.8488, 'grad_norm': 0.11061230301856995, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.9364878535270691, 'eval_runtime': 5.0759, 'eval_samples_per_second': 197.011, 'eval_steps_per_second': 12.412, 'epoch': 0.68}
{'loss': 0.8371, 'grad_norm': 0.11792005598545074, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.9374012351036072, 'eval_runtime': 5.1141, 'eval_samples_per_second': 195.537, 'eval_steps_per_second': 12.319, 'epoch': 0.72}
{'loss': 0.8104, 'grad_norm': 0.10535915195941925, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.9337327480316162, 'eval_runtime': 5.1384, 'eval_samples_per_second': 194.615, 'eval_steps_per_second': 12.261, 'epoch': 0.76}
{'loss': 0.8391, 'grad_norm': 0.11444342136383057, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.9321599006652832, 'eval_runtime': 5.1369, 'eval_samples_per_second': 194.67, 'eval_steps_per_second': 12.264, 'epoch': 0.8}
{'loss': 0.822, 'grad_norm': 0.10493019968271255, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.9273449778556824, 'eval_runtime': 5.0922, 'eval_samples_per_second': 196.38, 'eval_steps_per_second': 12.372, 'epoch': 0.84}
{'loss': 0.8226, 'grad_norm': 0.10653340816497803, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.925158679485321, 'eval_runtime': 5.096, 'eval_samples_per_second': 196.231, 'eval_steps_per_second': 12.363, 'epoch': 0.88}
{'loss': 0.8212, 'grad_norm': 0.12015645951032639, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.9259915947914124, 'eval_runtime': 5.0746, 'eval_samples_per_second': 197.059, 'eval_steps_per_second': 12.415, 'epoch': 0.92}
{'loss': 0.816, 'grad_norm': 0.11501694470643997, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.9257520437240601, 'eval_runtime': 5.0836, 'eval_samples_per_second': 196.712, 'eval_steps_per_second': 12.393, 'epoch': 0.96}
{'train_runtime': 338.6187, 'train_samples_per_second': 29.464, 'train_steps_per_second': 1.843, 'train_loss': 0.9624591974111704, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5976645946502686, 1.1935700178146362, 1.0486093759536743, 1.0127894878387451, 1.001353144645691, 0.9937573075294495, 0.9768927097320557, 0.976047694683075, 0.9625421762466431, 0.9669651985168457, 0.95307457447052, 0.950861930847168, 0.9476746320724487, 0.9499374032020569, 0.9408966302871704, 0.9364435076713562, 0.9364878535270691, 0.9374012351036072, 0.9337327480316162, 0.9321599006652832, 0.9273449778556824, 0.925158679485321, 0.9259915947914124, 0.9257520437240601], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.5976645946502686, 1.1935700178146362, 1.0486093759536743, 1.0127894878387451, 1.001353144645691, 0.9937573075294495, 0.9768927097320557, 0.976047694683075, 0.9625421762466431, 0.9669651985168457, 0.95307457447052, 0.950861930847168, 0.9476746320724487, 0.9499374032020569, 0.9408966302871704, 0.9364435076713562, 0.9364878535270691, 0.9374012351036072, 0.9337327480316162, 0.9321599006652832, 0.9273449778556824, 0.925158679485321, 0.9259915947914124, 0.9257520437240601]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9855165481567383
current iteration best possible eval_loss (full train run):  -0.9257520437240601
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.2777 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.5840210318565369, 0.5403404831886292, 0.8951655626296997, 0.996795654296875, 0.6967943906784058, 0.400291383266449, 0.7584985494613647, 0.6661302447319031, 0.059392571449279785, 0.49047714471817017, 0.8636659979820251, 0.10396885871887207, 0.23290777206420898, 0.25407153367996216, 0.8228784799575806, 0.09389074146747589, 0.5328817963600159, 0.1704922765493393, 0.8951139450073242]  ‚Üí  acq = -0.9265072415945692
X = [0.20226186513900757, 0.6831576228141785, 0.21675461530685425, 0.30253392457962036, 0.5191553235054016, 0.4726647734642029, 0.18306398391723633, 0.06165790557861328, 0.173406720161438, 0.24446256458759308, 0.7839422821998596, 0.6810406446456909, 0.17775046825408936, 0.2680701017379761, 0.8789662718772888, 0.06929440051317215, 0.9363643527030945, 0.4914133846759796, 0.07482516765594482]  ‚Üí  acq = -0.9500705981652517
X = [0.7401218414306641, 0.8363668322563171, 0.7301251888275146, 0.36155009269714355, 0.3025091290473938, 0.5445978045463562, 0.1628202199935913, 0.5834011435508728, 0.1753699779510498, 0.5837264060974121, 0.37286609411239624, 0.8461750149726868, 0.012964069843292236, 0.17281311750411987, 0.7752206921577454, 0.32410305738449097, 0.8762340545654297, 0.1423635184764862, 0.6519256234169006]  ‚Üí  acq = -0.9265190197815562
X = [0.5809492468833923, 0.9983730316162109, 0.8246482610702515, 0.7318549156188965, 0.4389488101005554, 0.6096560955047607, 0.8080414533615112, 0.13101553916931152, 0.02456521987915039, 0.5152989625930786, 0.0025587081909179688, 0.5287423133850098, 0.5921137928962708, 0.8601848483085632, 0.8594462275505066, 0.04051867127418518, 0.9936825633049011, 0.13591282069683075, 0.053788065910339355]  ‚Üí  acq = -0.9265076635353815
X = [0.3528999090194702, 0.5949426293373108, 0.3813283443450928, 0.4279024004936218, 0.4661039710044861, 0.3905106782913208, 0.3274908661842346, 0.7039413452148438, 0.7107980847358704, 0.40121349692344666, 0.7189667820930481, 0.7950335144996643, 0.43342822790145874, 0.4151228666305542, 0.8805846571922302, 0.4900456964969635, 0.8887658715248108, 0.9805734157562256, 0.013015925884246826]  ‚Üí  acq = -0.9266903526290595
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0811, dtype=torch.float64), tensor(0.3604, dtype=torch.float64), tensor(0.0235, dtype=torch.float64), 0, tensor(0.1196, dtype=torch.float64), tensor(0.0585, dtype=torch.float64), tensor(0.0165, dtype=torch.float64), tensor(0.1156, dtype=torch.float64), tensor(0.2154, dtype=torch.float64), 28, 1, 0, 0, 1, 0, 46, 0.03934633542388128, 27.6177717216438, 0]
normalized proposed parameters for next round by BO: [tensor(0.0811, dtype=torch.float64), tensor(0.3604, dtype=torch.float64), tensor(0.0235, dtype=torch.float64), tensor(0.0094, dtype=torch.float64), tensor(0.1196, dtype=torch.float64), tensor(0.0585, dtype=torch.float64), tensor(0.0165, dtype=torch.float64), tensor(0.1156, dtype=torch.float64), tensor(0.2154, dtype=torch.float64), tensor(0.8685, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.3588, dtype=torch.float64), tensor(0.3935, dtype=torch.float64), tensor(0.5754, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  14  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.081
  gsm8k: 0.36
  rowan_hellaswag: 0.024
  sciq: 0
  triviaqa: 0.12
  truthfulqa_gen: 0.059
  wikitext: 0.017
  mmlu: 0.116
  arc_challenge: 0.215

LoRA Parameters:
  lora_r: (46,)
  lora_dropout: (0.03934633542388128,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 0, 0, 1, 0],)
  lora_alpha: (27.6177717216438,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 0]
lora rank:  46
lora dropout:  0.03934633542388128
lora alpha:  27.6177717216438
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 34,291,712 || all params: 8,064,552,960 || trainable%: 0.4252
length of training data:  9903
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7264, 'grad_norm': 1.028602123260498, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.202167510986328, 'eval_runtime': 4.8313, 'eval_samples_per_second': 206.985, 'eval_steps_per_second': 13.04, 'epoch': 0.04}
{'loss': 1.3158, 'grad_norm': 0.44702041149139404, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0958889722824097, 'eval_runtime': 4.8292, 'eval_samples_per_second': 207.075, 'eval_steps_per_second': 13.046, 'epoch': 0.08}
{'loss': 1.0774, 'grad_norm': 0.27935975790023804, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 1.0459038019180298, 'eval_runtime': 4.8516, 'eval_samples_per_second': 206.119, 'eval_steps_per_second': 12.985, 'epoch': 0.12}
{'loss': 1.0398, 'grad_norm': 0.2989652156829834, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 1.0170987844467163, 'eval_runtime': 4.8548, 'eval_samples_per_second': 205.982, 'eval_steps_per_second': 12.977, 'epoch': 0.16}
{'loss': 1.0598, 'grad_norm': 0.22802814841270447, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 0.9945195317268372, 'eval_runtime': 4.8585, 'eval_samples_per_second': 205.827, 'eval_steps_per_second': 12.967, 'epoch': 0.2}
{'loss': 0.9616, 'grad_norm': 0.2201923429965973, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 0.9852465391159058, 'eval_runtime': 4.8673, 'eval_samples_per_second': 205.451, 'eval_steps_per_second': 12.943, 'epoch': 0.24}
{'loss': 0.9999, 'grad_norm': 0.31034737825393677, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 0.9677117466926575, 'eval_runtime': 4.8839, 'eval_samples_per_second': 204.754, 'eval_steps_per_second': 12.9, 'epoch': 0.28}
{'loss': 0.9751, 'grad_norm': 0.25450199842453003, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 0.9602361917495728, 'eval_runtime': 4.9176, 'eval_samples_per_second': 203.35, 'eval_steps_per_second': 12.811, 'epoch': 0.32}
{'loss': 0.9796, 'grad_norm': 0.2051907330751419, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 0.9559757113456726, 'eval_runtime': 4.9155, 'eval_samples_per_second': 203.44, 'eval_steps_per_second': 12.817, 'epoch': 0.36}
{'loss': 0.9653, 'grad_norm': 0.24327784776687622, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.9519181251525879, 'eval_runtime': 4.8908, 'eval_samples_per_second': 204.466, 'eval_steps_per_second': 12.881, 'epoch': 0.4}
{'loss': 0.9423, 'grad_norm': 0.2656206786632538, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.9473393559455872, 'eval_runtime': 4.9026, 'eval_samples_per_second': 203.972, 'eval_steps_per_second': 12.85, 'epoch': 0.44}
{'loss': 0.9783, 'grad_norm': 0.2502547800540924, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.9439805746078491, 'eval_runtime': 4.8947, 'eval_samples_per_second': 204.305, 'eval_steps_per_second': 12.871, 'epoch': 0.48}
{'loss': 0.9765, 'grad_norm': 0.23712122440338135, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.9516666531562805, 'eval_runtime': 4.9139, 'eval_samples_per_second': 203.504, 'eval_steps_per_second': 12.821, 'epoch': 0.53}
{'loss': 0.958, 'grad_norm': 0.239122673869133, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.9440010190010071, 'eval_runtime': 4.8936, 'eval_samples_per_second': 204.35, 'eval_steps_per_second': 12.874, 'epoch': 0.57}
{'loss': 0.9382, 'grad_norm': 0.2557317614555359, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.9395690560340881, 'eval_runtime': 4.9065, 'eval_samples_per_second': 203.81, 'eval_steps_per_second': 12.84, 'epoch': 0.61}
{'loss': 0.9292, 'grad_norm': 0.2349243462085724, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.9375656843185425, 'eval_runtime': 4.8966, 'eval_samples_per_second': 204.222, 'eval_steps_per_second': 12.866, 'epoch': 0.65}
{'loss': 0.9325, 'grad_norm': 0.26321282982826233, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.9296706318855286, 'eval_runtime': 4.9173, 'eval_samples_per_second': 203.365, 'eval_steps_per_second': 12.812, 'epoch': 0.69}
{'loss': 0.9564, 'grad_norm': 0.24737448990345, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.9289818406105042, 'eval_runtime': 4.9274, 'eval_samples_per_second': 202.948, 'eval_steps_per_second': 12.786, 'epoch': 0.73}
{'loss': 0.9379, 'grad_norm': 0.29457616806030273, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.9356836080551147, 'eval_runtime': 4.9261, 'eval_samples_per_second': 203.002, 'eval_steps_per_second': 12.789, 'epoch': 0.77}
{'loss': 0.926, 'grad_norm': 0.20756849646568298, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.9325249195098877, 'eval_runtime': 4.9324, 'eval_samples_per_second': 202.742, 'eval_steps_per_second': 12.773, 'epoch': 0.81}
{'loss': 0.9449, 'grad_norm': 0.2675439119338989, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.9296455979347229, 'eval_runtime': 4.9381, 'eval_samples_per_second': 202.507, 'eval_steps_per_second': 12.758, 'epoch': 0.85}
{'loss': 0.9349, 'grad_norm': 0.2543027698993683, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.9286465048789978, 'eval_runtime': 4.9467, 'eval_samples_per_second': 202.154, 'eval_steps_per_second': 12.736, 'epoch': 0.89}
{'loss': 0.9326, 'grad_norm': 0.23508423566818237, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.9309943914413452, 'eval_runtime': 4.9631, 'eval_samples_per_second': 201.489, 'eval_steps_per_second': 12.694, 'epoch': 0.93}
{'loss': 0.9366, 'grad_norm': 0.2564326524734497, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.9291384816169739, 'eval_runtime': 4.954, 'eval_samples_per_second': 201.858, 'eval_steps_per_second': 12.717, 'epoch': 0.97}
{'train_runtime': 324.1252, 'train_samples_per_second': 30.553, 'train_steps_per_second': 1.91, 'train_loss': 1.0519690875668133, 'epoch': 1.0}
train_results:  {'eval_loss': [2.202167510986328, 1.0958889722824097, 1.0459038019180298, 1.0170987844467163, 0.9945195317268372, 0.9852465391159058, 0.9677117466926575, 0.9602361917495728, 0.9559757113456726, 0.9519181251525879, 0.9473393559455872, 0.9439805746078491, 0.9516666531562805, 0.9440010190010071, 0.9395690560340881, 0.9375656843185425, 0.9296706318855286, 0.9289818406105042, 0.9356836080551147, 0.9325249195098877, 0.9296455979347229, 0.9286465048789978, 0.9309943914413452, 0.9291384816169739], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.202167510986328, 1.0958889722824097, 1.0459038019180298, 1.0170987844467163, 0.9945195317268372, 0.9852465391159058, 0.9677117466926575, 0.9602361917495728, 0.9559757113456726, 0.9519181251525879, 0.9473393559455872, 0.9439805746078491, 0.9516666531562805, 0.9440010190010071, 0.9395690560340881, 0.9375656843185425, 0.9296706318855286, 0.9289818406105042, 0.9356836080551147, 0.9325249195098877, 0.9296455979347229, 0.9286465048789978, 0.9309943914413452, 0.9291384816169739]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0253865718841553
current iteration best possible eval_loss (full train run):  -0.9291384816169739
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.4174 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4017324447631836, 0.8098095655441284, 0.318198025226593, 0.714120626449585, 0.6876267790794373, 0.012319743633270264, 0.4178018569946289, 0.540200412273407, 0.36777955293655396, 0.9023115634918213, 0.3290713429450989, 0.8067486882209778, 0.6726211905479431, 0.15280961990356445, 0.03174775838851929, 0.7156268954277039, 0.5243701934814453, 0.35216549038887024, 0.006528973579406738]  ‚Üí  acq = -0.9407730633783615
X = [0.083668053150177, 0.21730327606201172, 0.38655704259872437, 0.9606111645698547, 0.07689505815505981, 0.3735589385032654, 0.8073387145996094, 0.15076309442520142, 0.8720141053199768, 0.4629462659358978, 0.49664074182510376, 0.07627946138381958, 0.0051836371421813965, 0.5091192722320557, 0.2833280563354492, 0.09527727216482162, 0.25031810998916626, 0.8219367265701294, 0.7197057604789734]  ‚Üí  acq = -0.9520296678009286
X = [0.2491104006767273, 0.20842111110687256, 0.0142403244972229, 0.38917386531829834, 0.8244441747665405, 0.5437911748886108, 0.8293101787567139, 0.3755408525466919, 0.7399113774299622, 0.4023188650608063, 0.07552939653396606, 0.27186697721481323, 0.03176403045654297, 0.7652087211608887, 0.46531397104263306, 0.9325355887413025, 0.1334356665611267, 0.16606317460536957, 0.5709797143936157]  ‚Üí  acq = -0.9469259850790125
X = [0.6106637716293335, 0.9664523601531982, 0.3077254891395569, 0.5043574571609497, 0.8137807250022888, 3.6776065826416016e-05, 0.44411540031433105, 0.023098528385162354, 0.0688665509223938, 0.13759151101112366, 0.28943711519241333, 0.03323030471801758, 0.9961235523223877, 0.1205984354019165, 0.9392281770706177, 0.8348419070243835, 0.620255172252655, 0.1275952160358429, 0.9232467412948608]  ‚Üí  acq = -0.9467372696094883
X = [0.332327663898468, 0.6604408025741577, 0.7938937544822693, 0.9335769414901733, 0.08874589204788208, 0.5772082209587097, 0.8431816101074219, 0.7458587884902954, 0.48169541358947754, 0.6904752850532532, 0.5186182260513306, 0.2655754089355469, 0.9871400594711304, 0.36942172050476074, 0.33066344261169434, 0.19332276284694672, 0.0007928609848022461, 0.8442171812057495, 0.3439427614212036]  ‚Üí  acq = -0.9500300286894279
proposed candidate layer mask is:  tensor([1., 0., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2589, dtype=torch.float64), tensor(0.0168, dtype=torch.float64), 0, tensor(0.0451, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), tensor(0.1133, dtype=torch.float64), 0, 0, tensor(0.4814, dtype=torch.float64), 20, 1, 0, 0, 1, 1, 82, 0.04543252200055806, 20.218405490277977, 1]
normalized proposed parameters for next round by BO: [tensor(0.2589, dtype=torch.float64), tensor(0.0168, dtype=torch.float64), tensor(1.5981e-19, dtype=torch.float64), tensor(0.0451, dtype=torch.float64), tensor(0.0812, dtype=torch.float64), tensor(0.1133, dtype=torch.float64), tensor(0.0033, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4814, dtype=torch.float64), tensor(0.6121, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.6389, dtype=torch.float64), tensor(0.4543, dtype=torch.float64), tensor(0.4212, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  15  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.259
  gsm8k: 0.017
  rowan_hellaswag: 0
  sciq: 0.045
  triviaqa: 0.081
  truthfulqa_gen: 0.113
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.481

LoRA Parameters:
  lora_r: (82,)
  lora_dropout: (0.04543252200055806,)
  num_layers_to_apply: (20,)
  five_dim_vector: ([1, 0, 0, 1, 1],)
  lora_alpha: (20.218405490277977,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  20
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 0, 0, 1, 1]
lora rank:  82
lora dropout:  0.04543252200055806
lora alpha:  20.218405490277977
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 73,891,840 || all params: 8,104,153,088 || trainable%: 0.9118
length of training data:  9963
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.3844, 'grad_norm': 1.486817479133606, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.0189735889434814, 'eval_runtime': 5.0036, 'eval_samples_per_second': 199.858, 'eval_steps_per_second': 12.591, 'epoch': 0.04}
{'loss': 1.2944, 'grad_norm': 0.6389399170875549, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.159928560256958, 'eval_runtime': 5.0146, 'eval_samples_per_second': 199.418, 'eval_steps_per_second': 12.563, 'epoch': 0.08}
{'loss': 1.0376, 'grad_norm': 0.2014245241880417, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.0510436296463013, 'eval_runtime': 5.0368, 'eval_samples_per_second': 198.54, 'eval_steps_per_second': 12.508, 'epoch': 0.12}
{'loss': 0.9826, 'grad_norm': 0.35894694924354553, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.9896235466003418, 'eval_runtime': 5.02, 'eval_samples_per_second': 199.203, 'eval_steps_per_second': 12.55, 'epoch': 0.16}
{'loss': 0.9007, 'grad_norm': 0.3844453990459442, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.9598732590675354, 'eval_runtime': 5.0256, 'eval_samples_per_second': 198.982, 'eval_steps_per_second': 12.536, 'epoch': 0.2}
{'loss': 0.8738, 'grad_norm': 0.21809297800064087, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.942667543888092, 'eval_runtime': 5.0289, 'eval_samples_per_second': 198.85, 'eval_steps_per_second': 12.528, 'epoch': 0.24}
{'loss': 0.8395, 'grad_norm': 0.23931783437728882, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.9228248000144958, 'eval_runtime': 5.0056, 'eval_samples_per_second': 199.777, 'eval_steps_per_second': 12.586, 'epoch': 0.28}
{'loss': 0.8295, 'grad_norm': 0.24333246052265167, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.9208006858825684, 'eval_runtime': 4.9856, 'eval_samples_per_second': 200.578, 'eval_steps_per_second': 12.636, 'epoch': 0.32}
{'loss': 0.8163, 'grad_norm': 0.20673753321170807, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.9117670059204102, 'eval_runtime': 4.9925, 'eval_samples_per_second': 200.3, 'eval_steps_per_second': 12.619, 'epoch': 0.36}
{'loss': 0.8211, 'grad_norm': 0.2532777786254883, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.9130075573921204, 'eval_runtime': 4.9805, 'eval_samples_per_second': 200.783, 'eval_steps_per_second': 12.649, 'epoch': 0.4}
{'loss': 0.7717, 'grad_norm': 0.28194746375083923, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.9080072641372681, 'eval_runtime': 4.9836, 'eval_samples_per_second': 200.656, 'eval_steps_per_second': 12.641, 'epoch': 0.44}
{'loss': 0.7983, 'grad_norm': 0.3152003586292267, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.9020920395851135, 'eval_runtime': 5.0011, 'eval_samples_per_second': 199.954, 'eval_steps_per_second': 12.597, 'epoch': 0.48}
{'loss': 0.782, 'grad_norm': 0.3117917478084564, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.9009451866149902, 'eval_runtime': 4.9939, 'eval_samples_per_second': 200.243, 'eval_steps_per_second': 12.615, 'epoch': 0.52}
{'loss': 0.7262, 'grad_norm': 0.4756833612918854, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.9038372039794922, 'eval_runtime': 5.0086, 'eval_samples_per_second': 199.658, 'eval_steps_per_second': 12.578, 'epoch': 0.56}
{'loss': 0.7359, 'grad_norm': 0.30026915669441223, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.9015275239944458, 'eval_runtime': 4.9998, 'eval_samples_per_second': 200.01, 'eval_steps_per_second': 12.601, 'epoch': 0.6}
{'loss': 0.7371, 'grad_norm': 0.4424610137939453, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.9002700448036194, 'eval_runtime': 5.0158, 'eval_samples_per_second': 199.37, 'eval_steps_per_second': 12.56, 'epoch': 0.64}
{'loss': 0.698, 'grad_norm': 0.39199090003967285, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.8979436755180359, 'eval_runtime': 5.0244, 'eval_samples_per_second': 199.029, 'eval_steps_per_second': 12.539, 'epoch': 0.68}
{'loss': 0.7044, 'grad_norm': 0.4538818597793579, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.893311083316803, 'eval_runtime': 5.0473, 'eval_samples_per_second': 198.124, 'eval_steps_per_second': 12.482, 'epoch': 0.72}
{'loss': 0.6815, 'grad_norm': 0.3947039246559143, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.894761860370636, 'eval_runtime': 5.028, 'eval_samples_per_second': 198.885, 'eval_steps_per_second': 12.53, 'epoch': 0.76}
{'loss': 0.6918, 'grad_norm': 0.38602033257484436, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.8891955614089966, 'eval_runtime': 5.016, 'eval_samples_per_second': 199.361, 'eval_steps_per_second': 12.56, 'epoch': 0.8}
{'loss': 0.6361, 'grad_norm': 0.5040374994277954, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.8937383890151978, 'eval_runtime': 5.035, 'eval_samples_per_second': 198.611, 'eval_steps_per_second': 12.513, 'epoch': 0.84}
{'loss': 0.6679, 'grad_norm': 0.4985521733760834, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.888847291469574, 'eval_runtime': 5.0053, 'eval_samples_per_second': 199.788, 'eval_steps_per_second': 12.587, 'epoch': 0.88}
{'loss': 0.6486, 'grad_norm': 0.4695281982421875, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.8893932700157166, 'eval_runtime': 5.0235, 'eval_samples_per_second': 199.065, 'eval_steps_per_second': 12.541, 'epoch': 0.92}
{'loss': 0.6148, 'grad_norm': 0.4239637851715088, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.8885197639465332, 'eval_runtime': 5.017, 'eval_samples_per_second': 199.323, 'eval_steps_per_second': 12.557, 'epoch': 0.96}
{'train_runtime': 281.2156, 'train_samples_per_second': 35.428, 'train_steps_per_second': 2.215, 'train_loss': 0.8934640562936161, 'epoch': 1.0}
train_results:  {'eval_loss': [2.0189735889434814, 1.159928560256958, 1.0510436296463013, 0.9896235466003418, 0.9598732590675354, 0.942667543888092, 0.9228248000144958, 0.9208006858825684, 0.9117670059204102, 0.9130075573921204, 0.9080072641372681, 0.9020920395851135, 0.9009451866149902, 0.9038372039794922, 0.9015275239944458, 0.9002700448036194, 0.8979436755180359, 0.893311083316803, 0.894761860370636, 0.8891955614089966, 0.8937383890151978, 0.888847291469574, 0.8893932700157166, 0.8885197639465332], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.0189735889434814, 1.159928560256958, 1.0510436296463013, 0.9896235466003418, 0.9598732590675354, 0.942667543888092, 0.9228248000144958, 0.9208006858825684, 0.9117670059204102, 0.9130075573921204, 0.9080072641372681, 0.9020920395851135, 0.9009451866149902, 0.9038372039794922, 0.9015275239944458, 0.9002700448036194, 0.8979436755180359, 0.893311083316803, 0.894761860370636, 0.8891955614089966, 0.8937383890151978, 0.888847291469574, 0.8893932700157166, 0.8885197639465332]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0859076976776123
current iteration best possible eval_loss (full train run):  -0.8885197639465332
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.5025 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.007365286350250244, 0.05544036626815796, 0.15365111827850342, 0.7281826734542847, 0.8755306601524353, 0.36490166187286377, 0.4565690755844116, 0.8796051144599915, 0.23900067806243896, 0.9870830178260803, 0.754863977432251, 0.9104870557785034, 0.5426647663116455, 0.7492760419845581, 0.9776453375816345, 0.8787160515785217, 0.16884422302246094, 0.48048388957977295, 0.9369502067565918]  ‚Üí  acq = -0.9337753490613024
X = [0.06936377286911011, 0.632781982421875, 0.092129647731781, 0.7595736980438232, 0.2624407410621643, 0.37410789728164673, 0.8005918264389038, 0.3958597779273987, 0.1338949203491211, 0.7510114908218384, 0.5261915326118469, 0.9568371772766113, 0.028494656085968018, 0.1428215503692627, 0.7683084607124329, 0.13144083321094513, 0.18786925077438354, 0.9205564260482788, 0.21238505840301514]  ‚Üí  acq = -0.9337632539263352
X = [0.1816890835762024, 0.5671297311782837, 0.6558997631072998, 0.32971757650375366, 0.6777505874633789, 0.4247628450393677, 0.4855687618255615, 0.09471511840820312, 0.47373509407043457, 0.3449631333351135, 0.8766421675682068, 0.5072073936462402, 0.49650073051452637, 0.48039573431015015, 0.5661623477935791, 0.30366525053977966, 0.8914388418197632, 0.9394388198852539, 0.7902622818946838]  ‚Üí  acq = -0.9341568274150582
X = [0.4756810665130615, 0.8949254751205444, 0.420803964138031, 0.1660451889038086, 0.24296170473098755, 0.7732431888580322, 0.3857458829879761, 0.9024378657341003, 0.37086379528045654, 0.4715011417865753, 0.27662307024002075, 0.1057593822479248, 0.8969522714614868, 0.7619646191596985, 0.40543514490127563, 0.19461123645305634, 0.8357580900192261, 0.8589680194854736, 0.6040682792663574]  ‚Üí  acq = -0.9337508273212334
X = [0.14415353536605835, 0.1752747893333435, 0.926478385925293, 0.16231077909469604, 0.5967749357223511, 0.8759607672691345, 0.8920565247535706, 0.6357200145721436, 0.32187438011169434, 0.6041354537010193, 0.18114352226257324, 0.5152944922447205, 0.2512813210487366, 0.9533259868621826, 0.09903591871261597, 0.49458131194114685, 0.7254683971405029, 0.4729866683483124, 0.9238991737365723]  ‚Üí  acq = -0.9337499198708998
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2221, dtype=torch.float64), tensor(0.0207, dtype=torch.float64), 0, tensor(0.0115, dtype=torch.float64), tensor(0.1429, dtype=torch.float64), tensor(0.0411, dtype=torch.float64), tensor(0.0207, dtype=torch.float64), tensor(0.1097, dtype=torch.float64), tensor(0.4216, dtype=torch.float64), 25, 0, 1, 1, 1, 0, 91, 0.02562866465287938, 26.839051350240034, 0]
normalized proposed parameters for next round by BO: [tensor(0.2221, dtype=torch.float64), tensor(0.0207, dtype=torch.float64), tensor(0.0097, dtype=torch.float64), tensor(0.0115, dtype=torch.float64), tensor(0.1429, dtype=torch.float64), tensor(0.0411, dtype=torch.float64), tensor(0.0207, dtype=torch.float64), tensor(0.1097, dtype=torch.float64), tensor(0.4216, dtype=torch.float64), tensor(0.7665, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.7083, dtype=torch.float64), tensor(0.2563, dtype=torch.float64), tensor(0.5591, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  16  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.222
  gsm8k: 0.021
  rowan_hellaswag: 0
  sciq: 0.011
  triviaqa: 0.143
  truthfulqa_gen: 0.041
  wikitext: 0.021
  mmlu: 0.11
  arc_challenge: 0.422

LoRA Parameters:
  lora_r: (91,)
  lora_dropout: (0.02562866465287938,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (26.839051350240034,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  91
lora dropout:  0.02562866465287938
lora alpha:  26.839051350240034
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 95,513,600 || all params: 8,125,774,848 || trainable%: 1.1754
length of training data:  9898
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0043, 'grad_norm': 0.8758056163787842, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.4797638654708862, 'eval_runtime': 5.2381, 'eval_samples_per_second': 190.91, 'eval_steps_per_second': 12.027, 'epoch': 0.04}
{'loss': 1.226, 'grad_norm': 0.388315349817276, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.02232825756073, 'eval_runtime': 5.2499, 'eval_samples_per_second': 190.48, 'eval_steps_per_second': 12.0, 'epoch': 0.08}
{'loss': 1.0746, 'grad_norm': 0.21687175333499908, 'learning_rate': 0.0002873462214411247, 'epoch': 0.12}
{'eval_loss': 0.9607735276222229, 'eval_runtime': 5.2572, 'eval_samples_per_second': 190.215, 'eval_steps_per_second': 11.984, 'epoch': 0.12}
{'loss': 0.9917, 'grad_norm': 0.20011666417121887, 'learning_rate': 0.00027416520210896307, 'epoch': 0.16}
{'eval_loss': 0.9426363110542297, 'eval_runtime': 5.2628, 'eval_samples_per_second': 190.013, 'eval_steps_per_second': 11.971, 'epoch': 0.16}
{'loss': 1.0035, 'grad_norm': 0.20417994260787964, 'learning_rate': 0.0002609841827768014, 'epoch': 0.2}
{'eval_loss': 0.9271015524864197, 'eval_runtime': 5.2816, 'eval_samples_per_second': 189.335, 'eval_steps_per_second': 11.928, 'epoch': 0.2}
{'loss': 0.9513, 'grad_norm': 0.17755776643753052, 'learning_rate': 0.0002478031634446397, 'epoch': 0.24}
{'eval_loss': 0.9308379888534546, 'eval_runtime': 5.271, 'eval_samples_per_second': 189.718, 'eval_steps_per_second': 11.952, 'epoch': 0.24}
{'loss': 0.9098, 'grad_norm': 0.2134065181016922, 'learning_rate': 0.000234622144112478, 'epoch': 0.28}
{'eval_loss': 0.9242365956306458, 'eval_runtime': 5.2744, 'eval_samples_per_second': 189.595, 'eval_steps_per_second': 11.944, 'epoch': 0.28}
{'loss': 0.8932, 'grad_norm': 0.25550323724746704, 'learning_rate': 0.00022144112478031632, 'epoch': 0.32}
{'eval_loss': 0.9142048358917236, 'eval_runtime': 5.278, 'eval_samples_per_second': 189.467, 'eval_steps_per_second': 11.936, 'epoch': 0.32}
{'loss': 0.9362, 'grad_norm': 0.273593932390213, 'learning_rate': 0.00020826010544815464, 'epoch': 0.36}
{'eval_loss': 0.9188563227653503, 'eval_runtime': 5.2725, 'eval_samples_per_second': 189.662, 'eval_steps_per_second': 11.949, 'epoch': 0.36}
{'loss': 0.9373, 'grad_norm': 0.24581731855869293, 'learning_rate': 0.00019507908611599294, 'epoch': 0.4}
{'eval_loss': 0.9127854108810425, 'eval_runtime': 5.2867, 'eval_samples_per_second': 189.153, 'eval_steps_per_second': 11.917, 'epoch': 0.4}
{'loss': 0.8496, 'grad_norm': 0.22977308928966522, 'learning_rate': 0.00018189806678383126, 'epoch': 0.44}
{'eval_loss': 0.9073829054832458, 'eval_runtime': 5.2787, 'eval_samples_per_second': 189.44, 'eval_steps_per_second': 11.935, 'epoch': 0.44}
{'loss': 0.8148, 'grad_norm': 0.24661192297935486, 'learning_rate': 0.0001687170474516696, 'epoch': 0.48}
{'eval_loss': 0.8993216753005981, 'eval_runtime': 5.2685, 'eval_samples_per_second': 189.807, 'eval_steps_per_second': 11.958, 'epoch': 0.48}
{'loss': 0.8378, 'grad_norm': 0.24958764016628265, 'learning_rate': 0.0001555360281195079, 'epoch': 0.53}
{'eval_loss': 0.8997750282287598, 'eval_runtime': 5.2735, 'eval_samples_per_second': 189.627, 'eval_steps_per_second': 11.946, 'epoch': 0.53}
{'loss': 0.8042, 'grad_norm': 0.2686673700809479, 'learning_rate': 0.00014235500878734622, 'epoch': 0.57}
{'eval_loss': 0.8987115025520325, 'eval_runtime': 5.2683, 'eval_samples_per_second': 189.814, 'eval_steps_per_second': 11.958, 'epoch': 0.57}
{'loss': 0.8504, 'grad_norm': 0.38027167320251465, 'learning_rate': 0.0001291739894551845, 'epoch': 0.61}
{'eval_loss': 0.8964601755142212, 'eval_runtime': 5.2716, 'eval_samples_per_second': 189.696, 'eval_steps_per_second': 11.951, 'epoch': 0.61}
{'loss': 0.7708, 'grad_norm': 0.34307801723480225, 'learning_rate': 0.00011599297012302283, 'epoch': 0.65}
{'eval_loss': 0.8941575884819031, 'eval_runtime': 5.2719, 'eval_samples_per_second': 189.686, 'eval_steps_per_second': 11.95, 'epoch': 0.65}
{'loss': 0.8258, 'grad_norm': 0.31821805238723755, 'learning_rate': 0.00010281195079086115, 'epoch': 0.69}
{'eval_loss': 0.8906535506248474, 'eval_runtime': 5.2715, 'eval_samples_per_second': 189.7, 'eval_steps_per_second': 11.951, 'epoch': 0.69}
{'loss': 0.8378, 'grad_norm': 0.2339145690202713, 'learning_rate': 8.963093145869947e-05, 'epoch': 0.73}
{'eval_loss': 0.8897532820701599, 'eval_runtime': 5.2718, 'eval_samples_per_second': 189.688, 'eval_steps_per_second': 11.95, 'epoch': 0.73}
{'loss': 0.7376, 'grad_norm': 0.3201267123222351, 'learning_rate': 7.644991212653778e-05, 'epoch': 0.77}
{'eval_loss': 0.8913001418113708, 'eval_runtime': 5.2717, 'eval_samples_per_second': 189.692, 'eval_steps_per_second': 11.951, 'epoch': 0.77}
{'loss': 0.7768, 'grad_norm': 0.3433087170124054, 'learning_rate': 6.32688927943761e-05, 'epoch': 0.81}
{'eval_loss': 0.8837469816207886, 'eval_runtime': 5.2718, 'eval_samples_per_second': 189.687, 'eval_steps_per_second': 11.95, 'epoch': 0.81}
{'loss': 0.7327, 'grad_norm': 0.331012099981308, 'learning_rate': 5.0087873462214405e-05, 'epoch': 0.85}
{'eval_loss': 0.8850578665733337, 'eval_runtime': 5.2719, 'eval_samples_per_second': 189.683, 'eval_steps_per_second': 11.95, 'epoch': 0.85}
{'loss': 0.7631, 'grad_norm': 0.5120016932487488, 'learning_rate': 3.690685413005272e-05, 'epoch': 0.89}
{'eval_loss': 0.8838188648223877, 'eval_runtime': 5.3222, 'eval_samples_per_second': 187.892, 'eval_steps_per_second': 11.837, 'epoch': 0.89}
{'loss': 0.7732, 'grad_norm': 0.3554037809371948, 'learning_rate': 2.3725834797891035e-05, 'epoch': 0.93}
{'eval_loss': 0.8828895092010498, 'eval_runtime': 5.2993, 'eval_samples_per_second': 188.705, 'eval_steps_per_second': 11.888, 'epoch': 0.93}
{'loss': 0.7243, 'grad_norm': 0.3239743709564209, 'learning_rate': 1.054481546572935e-05, 'epoch': 0.97}
{'eval_loss': 0.8824106454849243, 'eval_runtime': 5.3011, 'eval_samples_per_second': 188.64, 'eval_steps_per_second': 11.884, 'epoch': 0.97}
{'train_runtime': 303.0366, 'train_samples_per_second': 32.663, 'train_steps_per_second': 2.043, 'train_loss': 0.9549011775865693, 'epoch': 1.0}
train_results:  {'eval_loss': [1.4797638654708862, 1.02232825756073, 0.9607735276222229, 0.9426363110542297, 0.9271015524864197, 0.9308379888534546, 0.9242365956306458, 0.9142048358917236, 0.9188563227653503, 0.9127854108810425, 0.9073829054832458, 0.8993216753005981, 0.8997750282287598, 0.8987115025520325, 0.8964601755142212, 0.8941575884819031, 0.8906535506248474, 0.8897532820701599, 0.8913001418113708, 0.8837469816207886, 0.8850578665733337, 0.8838188648223877, 0.8828895092010498, 0.8824106454849243], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.4797638654708862, 1.02232825756073, 0.9607735276222229, 0.9426363110542297, 0.9271015524864197, 0.9308379888534546, 0.9242365956306458, 0.9142048358917236, 0.9188563227653503, 0.9127854108810425, 0.9073829054832458, 0.8993216753005981, 0.8997750282287598, 0.8987115025520325, 0.8964601755142212, 0.8941575884819031, 0.8906535506248474, 0.8897532820701599, 0.8913001418113708, 0.8837469816207886, 0.8850578665733337, 0.8838188648223877, 0.8828895092010498, 0.8824106454849243]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0956453084945679
current iteration best possible eval_loss (full train run):  -0.8824106454849243
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.4352 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.35491812229156494, 0.0927686095237732, 0.5218586325645447, 0.9991548657417297, 0.8260970711708069, 0.17205184698104858, 0.03345644474029541, 0.26095283031463623, 0.2436653971672058, 0.7262229919433594, 0.8165992498397827, 0.8520699143409729, 0.988444447517395, 0.5198254585266113, 0.031100332736968994, 0.9559857845306396, 0.3273009657859802, 0.9828505516052246, 0.5352497100830078]  ‚Üí  acq = -0.9574221595496873
X = [0.8189860582351685, 0.7869956493377686, 0.683575451374054, 0.5418528914451599, 0.8440313339233398, 0.7836773991584778, 0.2994930148124695, 0.37160301208496094, 0.4038698673248291, 0.8973821401596069, 0.7314273715019226, 0.9184417128562927, 0.6111648678779602, 0.9333778619766235, 0.29845958948135376, 0.9420246481895447, 0.9331071972846985, 0.31236356496810913, 0.36077266931533813]  ‚Üí  acq = -0.9562464831648857
X = [0.7738390564918518, 0.5021045207977295, 0.15234124660491943, 0.5285479426383972, 0.835478663444519, 0.30028289556503296, 0.9548570513725281, 0.32182931900024414, 0.4971200227737427, 0.5741828680038452, 0.5517953038215637, 0.7892404794692993, 0.5463884472846985, 0.7489384412765503, 0.08544248342514038, 0.9668935537338257, 0.5031551122665405, 0.7251023054122925, 0.2269490361213684]  ‚Üí  acq = -0.9562404936839352
X = [0.20103693008422852, 0.27278316020965576, 0.921504020690918, 0.632042646408081, 0.3334336280822754, 0.6413266658782959, 0.7466988563537598, 0.7273094058036804, 0.4721810817718506, 0.14547844231128693, 0.0291365385055542, 0.3460528254508972, 0.9624823927879333, 0.8216774463653564, 0.783478856086731, 0.32679685950279236, 0.4884251356124878, 0.9665257930755615, 0.2274898886680603]  ‚Üí  acq = -0.9562401512215122
X = [0.6374526023864746, 0.36883002519607544, 0.838202953338623, 0.39927512407302856, 0.984289288520813, 0.7759299874305725, 0.45928966999053955, 0.24248510599136353, 0.4136202335357666, 0.3681538701057434, 0.6981962323188782, 0.7622081637382507, 0.9070555567741394, 0.8970206379890442, 0.477536141872406, 0.5123441219329834, 0.3907546401023865, 0.21269100904464722, 0.09688013792037964]  ‚Üí  acq = -0.9562408808194094
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1974, dtype=torch.float64), tensor(0.2036, dtype=torch.float64), tensor(0.0775, dtype=torch.float64), tensor(0.0711, dtype=torch.float64), tensor(0.1284, dtype=torch.float64), tensor(0.0421, dtype=torch.float64), tensor(0.0224, dtype=torch.float64), 0, tensor(0.2574, dtype=torch.float64), 28, 0, 1, 1, 1, 0, 78, 0.04691643249230856, 26.3742571270078, 0]
normalized proposed parameters for next round by BO: [tensor(0.1974, dtype=torch.float64), tensor(0.2036, dtype=torch.float64), tensor(0.0775, dtype=torch.float64), tensor(0.0711, dtype=torch.float64), tensor(0.1284, dtype=torch.float64), tensor(0.0421, dtype=torch.float64), tensor(0.0224, dtype=torch.float64), tensor(5.4675e-19, dtype=torch.float64), tensor(0.2574, dtype=torch.float64), tensor(0.8636, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6093, dtype=torch.float64), tensor(0.4692, dtype=torch.float64), tensor(0.5495, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  17  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.197
  gsm8k: 0.204
  rowan_hellaswag: 0.078
  sciq: 0.071
  triviaqa: 0.128
  truthfulqa_gen: 0.042
  wikitext: 0.022
  mmlu: 0
  arc_challenge: 0.257

LoRA Parameters:
  lora_r: (78,)
  lora_dropout: (0.04691643249230856,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (26.3742571270078,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  78
lora dropout:  0.04691643249230856
lora alpha:  26.3742571270078
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 91,693,056 || all params: 8,121,954,304 || trainable%: 1.1290
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.8422, 'grad_norm': 0.6887769103050232, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.5548241138458252, 'eval_runtime': 5.2311, 'eval_samples_per_second': 191.164, 'eval_steps_per_second': 12.043, 'epoch': 0.04}
{'loss': 1.3786, 'grad_norm': 0.2998037338256836, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 0.9947389960289001, 'eval_runtime': 5.2647, 'eval_samples_per_second': 189.944, 'eval_steps_per_second': 11.966, 'epoch': 0.08}
{'loss': 1.0528, 'grad_norm': 0.23595532774925232, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9640756845474243, 'eval_runtime': 5.262, 'eval_samples_per_second': 190.042, 'eval_steps_per_second': 11.973, 'epoch': 0.12}
{'loss': 1.0574, 'grad_norm': 0.21785025298595428, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9469245672225952, 'eval_runtime': 5.2476, 'eval_samples_per_second': 190.565, 'eval_steps_per_second': 12.006, 'epoch': 0.16}
{'loss': 1.0251, 'grad_norm': 0.2126958966255188, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9289773106575012, 'eval_runtime': 5.2426, 'eval_samples_per_second': 190.746, 'eval_steps_per_second': 12.017, 'epoch': 0.2}
{'loss': 1.0365, 'grad_norm': 0.22523151338100433, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9293110966682434, 'eval_runtime': 5.2513, 'eval_samples_per_second': 190.428, 'eval_steps_per_second': 11.997, 'epoch': 0.24}
{'loss': 1.0336, 'grad_norm': 0.17523780465126038, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.919819176197052, 'eval_runtime': 5.2544, 'eval_samples_per_second': 190.318, 'eval_steps_per_second': 11.99, 'epoch': 0.28}
{'loss': 1.0094, 'grad_norm': 0.21713028848171234, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9237369894981384, 'eval_runtime': 5.259, 'eval_samples_per_second': 190.152, 'eval_steps_per_second': 11.98, 'epoch': 0.32}
{'loss': 1.0356, 'grad_norm': 0.20488926768302917, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9110580682754517, 'eval_runtime': 5.2681, 'eval_samples_per_second': 189.823, 'eval_steps_per_second': 11.959, 'epoch': 0.36}
{'loss': 0.9943, 'grad_norm': 0.21963800489902496, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.912544310092926, 'eval_runtime': 5.2689, 'eval_samples_per_second': 189.792, 'eval_steps_per_second': 11.957, 'epoch': 0.4}
{'loss': 0.9865, 'grad_norm': 0.22202089428901672, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.911635160446167, 'eval_runtime': 5.2569, 'eval_samples_per_second': 190.227, 'eval_steps_per_second': 11.984, 'epoch': 0.44}
{'loss': 0.9871, 'grad_norm': 0.21789748966693878, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9008749127388, 'eval_runtime': 5.2879, 'eval_samples_per_second': 189.112, 'eval_steps_per_second': 11.914, 'epoch': 0.48}
{'loss': 1.017, 'grad_norm': 0.23424670100212097, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8980395793914795, 'eval_runtime': 5.28, 'eval_samples_per_second': 189.395, 'eval_steps_per_second': 11.932, 'epoch': 0.52}
{'loss': 1.0013, 'grad_norm': 0.20759150385856628, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9015350341796875, 'eval_runtime': 5.2523, 'eval_samples_per_second': 190.391, 'eval_steps_per_second': 11.995, 'epoch': 0.56}
{'loss': 0.9794, 'grad_norm': 0.23398858308792114, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.899182915687561, 'eval_runtime': 5.2622, 'eval_samples_per_second': 190.034, 'eval_steps_per_second': 11.972, 'epoch': 0.6}
{'loss': 0.9865, 'grad_norm': 0.20332562923431396, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8976448774337769, 'eval_runtime': 5.2724, 'eval_samples_per_second': 189.665, 'eval_steps_per_second': 11.949, 'epoch': 0.64}
{'loss': 0.9526, 'grad_norm': 0.23910574615001678, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.894596517086029, 'eval_runtime': 5.2465, 'eval_samples_per_second': 190.602, 'eval_steps_per_second': 12.008, 'epoch': 0.68}
{'loss': 0.9354, 'grad_norm': 0.2751082479953766, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8895806670188904, 'eval_runtime': 5.2633, 'eval_samples_per_second': 189.995, 'eval_steps_per_second': 11.97, 'epoch': 0.72}
{'loss': 0.9305, 'grad_norm': 0.2596120834350586, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8882364630699158, 'eval_runtime': 5.273, 'eval_samples_per_second': 189.645, 'eval_steps_per_second': 11.948, 'epoch': 0.76}
{'loss': 0.8959, 'grad_norm': 0.2539232075214386, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8881654143333435, 'eval_runtime': 5.2921, 'eval_samples_per_second': 188.961, 'eval_steps_per_second': 11.905, 'epoch': 0.8}
{'loss': 0.9143, 'grad_norm': 0.26367583870887756, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8840369582176208, 'eval_runtime': 5.2693, 'eval_samples_per_second': 189.777, 'eval_steps_per_second': 11.956, 'epoch': 0.84}
{'loss': 0.9484, 'grad_norm': 0.26850882172584534, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8822474479675293, 'eval_runtime': 5.2654, 'eval_samples_per_second': 189.918, 'eval_steps_per_second': 11.965, 'epoch': 0.88}
{'loss': 0.9843, 'grad_norm': 0.25114908814430237, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.883142352104187, 'eval_runtime': 5.2763, 'eval_samples_per_second': 189.526, 'eval_steps_per_second': 11.94, 'epoch': 0.92}
{'loss': 0.9143, 'grad_norm': 0.2217687964439392, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8840848803520203, 'eval_runtime': 5.2874, 'eval_samples_per_second': 189.127, 'eval_steps_per_second': 11.915, 'epoch': 0.96}
{'loss': 0.9306, 'grad_norm': 0.26631850004196167, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8834204077720642, 'eval_runtime': 5.2922, 'eval_samples_per_second': 188.958, 'eval_steps_per_second': 11.904, 'epoch': 1.0}
{'train_runtime': 353.3042, 'train_samples_per_second': 28.296, 'train_steps_per_second': 1.769, 'train_loss': 1.0731816131591796, 'epoch': 1.0}
train_results:  {'eval_loss': [1.5548241138458252, 0.9947389960289001, 0.9640756845474243, 0.9469245672225952, 0.9289773106575012, 0.9293110966682434, 0.919819176197052, 0.9237369894981384, 0.9110580682754517, 0.912544310092926, 0.911635160446167, 0.9008749127388, 0.8980395793914795, 0.9015350341796875, 0.899182915687561, 0.8976448774337769, 0.894596517086029, 0.8895806670188904, 0.8882364630699158, 0.8881654143333435, 0.8840369582176208, 0.8822474479675293, 0.883142352104187, 0.8840848803520203, 0.8834204077720642], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.5548241138458252, 0.9947389960289001, 0.9640756845474243, 0.9469245672225952, 0.9289773106575012, 0.9293110966682434, 0.919819176197052, 0.9237369894981384, 0.9110580682754517, 0.912544310092926, 0.911635160446167, 0.9008749127388, 0.8980395793914795, 0.9015350341796875, 0.899182915687561, 0.8976448774337769, 0.894596517086029, 0.8895806670188904, 0.8882364630699158, 0.8881654143333435, 0.8840369582176208, 0.8822474479675293, 0.883142352104187, 0.8840848803520203, 0.8834204077720642]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0593405961990356
current iteration best possible eval_loss (full train run):  -0.8834204077720642
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.2048 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6528939604759216, 0.45803213119506836, 0.6395756602287292, 0.41395068168640137, 0.13181352615356445, 0.16059410572052002, 0.3969634771347046, 0.5405004024505615, 0.1537771224975586, 0.8252032995223999, 0.14166873693466187, 0.409503698348999, 0.014202237129211426, 0.12962335348129272, 0.883480966091156, 0.9162870645523071, 0.8848122358322144, 0.9529834985733032, 0.9132158160209656]  ‚Üí  acq = -0.9621314451291723
X = [0.8627103567123413, 0.08600771427154541, 0.1993621587753296, 0.30744802951812744, 0.06755012273788452, 0.33472657203674316, 0.02848505973815918, 0.3924814462661743, 0.28531861305236816, 0.5600935816764832, 0.5932743549346924, 0.9966981410980225, 0.7297819256782532, 0.21619582176208496, 0.9975385665893555, 0.20695267617702484, 0.13808739185333252, 0.41705504059791565, 0.8170029520988464]  ‚Üí  acq = -0.9660455914807864
X = [0.2950940728187561, 0.5771868228912354, 0.10922980308532715, 0.027972638607025146, 0.6282843947410583, 0.6379469633102417, 0.8366923332214355, 0.5536704063415527, 0.12135124206542969, 0.09011299163103104, 0.0024709701538085938, 0.9674053192138672, 0.391141414642334, 0.8502101898193359, 0.15156877040863037, 0.14680489897727966, 0.8321003913879395, 0.3116019666194916, 0.6408820152282715]  ‚Üí  acq = -0.9616602920899052
X = [0.9438592791557312, 0.2882199287414551, 0.743429958820343, 0.43473517894744873, 0.29208463430404663, 0.5645989775657654, 0.3958442211151123, 0.7323349118232727, 0.9123662114143372, 0.84892737865448, 0.31978273391723633, 0.6559408903121948, 0.9790109395980835, 0.5334115028381348, 0.7911179065704346, 0.9900975227355957, 0.020802080631256104, 0.5676398277282715, 0.17085957527160645]  ‚Üí  acq = -0.960836363249769
X = [0.6622090339660645, 0.79157555103302, 0.1524304747581482, 0.3094300627708435, 0.873835563659668, 0.33339792490005493, 0.7636342644691467, 0.6787083148956299, 0.1586219072341919, 0.252647340297699, 0.19427955150604248, 0.42576682567596436, 0.1545339822769165, 0.44936603307724, 0.6860421299934387, 0.8695747256278992, 0.5007997155189514, 0.8213040828704834, 0.5900448560714722]  ‚Üí  acq = -0.9588911017452564
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2591, dtype=torch.float64), tensor(0.1445, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0.0346, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.0604, dtype=torch.float64), 0, 0, tensor(0.3719, dtype=torch.float64), 24, 0, 1, 1, 1, 0, 77, 0.05892758114716593, 14.580875458317513, 0]
normalized proposed parameters for next round by BO: [tensor(0.2591, dtype=torch.float64), tensor(0.1445, dtype=torch.float64), tensor(0.0575, dtype=torch.float64), tensor(0.0346, dtype=torch.float64), tensor(0.0721, dtype=torch.float64), tensor(0.0604, dtype=torch.float64), tensor(4.4628e-22, dtype=torch.float64), tensor(2.3458e-18, dtype=torch.float64), tensor(0.3719, dtype=torch.float64), tensor(0.7607, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5998, dtype=torch.float64), tensor(0.5893, dtype=torch.float64), tensor(0.3038, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  18  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.259
  gsm8k: 0.144
  rowan_hellaswag: 0.057
  sciq: 0.035
  triviaqa: 0.072
  truthfulqa_gen: 0.06
  wikitext: 0
  mmlu: 0
  arc_challenge: 0.372

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (0.05892758114716593,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (14.580875458317513,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  77
lora dropout:  0.05892758114716593
lora alpha:  14.580875458317513
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 77,586,432 || all params: 8,107,847,680 || trainable%: 0.9569
length of training data:  9995
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1011, 'grad_norm': 0.726196825504303, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.915191650390625, 'eval_runtime': 5.1756, 'eval_samples_per_second': 193.215, 'eval_steps_per_second': 12.173, 'epoch': 0.04}
{'loss': 1.3635, 'grad_norm': 0.21657325327396393, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0536555051803589, 'eval_runtime': 5.1917, 'eval_samples_per_second': 192.617, 'eval_steps_per_second': 12.135, 'epoch': 0.08}
{'loss': 1.0869, 'grad_norm': 0.15451839566230774, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9701218605041504, 'eval_runtime': 5.1911, 'eval_samples_per_second': 192.637, 'eval_steps_per_second': 12.136, 'epoch': 0.12}
{'loss': 0.9837, 'grad_norm': 0.1596735268831253, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9481974244117737, 'eval_runtime': 5.1922, 'eval_samples_per_second': 192.595, 'eval_steps_per_second': 12.133, 'epoch': 0.16}
{'loss': 1.0206, 'grad_norm': 0.331794798374176, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.937757134437561, 'eval_runtime': 5.2097, 'eval_samples_per_second': 191.949, 'eval_steps_per_second': 12.093, 'epoch': 0.2}
{'loss': 0.9804, 'grad_norm': 0.17663493752479553, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9298160672187805, 'eval_runtime': 5.2129, 'eval_samples_per_second': 191.832, 'eval_steps_per_second': 12.085, 'epoch': 0.24}
{'loss': 0.9626, 'grad_norm': 0.16485823690891266, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9278331398963928, 'eval_runtime': 5.2103, 'eval_samples_per_second': 191.929, 'eval_steps_per_second': 12.092, 'epoch': 0.28}
{'loss': 0.9662, 'grad_norm': 0.17071938514709473, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9156607985496521, 'eval_runtime': 5.2164, 'eval_samples_per_second': 191.705, 'eval_steps_per_second': 12.077, 'epoch': 0.32}
{'loss': 0.9428, 'grad_norm': 0.16305570304393768, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9139249920845032, 'eval_runtime': 5.2101, 'eval_samples_per_second': 191.933, 'eval_steps_per_second': 12.092, 'epoch': 0.36}
{'loss': 0.9663, 'grad_norm': 0.18810898065567017, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9101752042770386, 'eval_runtime': 5.2116, 'eval_samples_per_second': 191.879, 'eval_steps_per_second': 12.088, 'epoch': 0.4}
{'loss': 0.9264, 'grad_norm': 0.1641654372215271, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9079510569572449, 'eval_runtime': 5.2213, 'eval_samples_per_second': 191.524, 'eval_steps_per_second': 12.066, 'epoch': 0.44}
{'loss': 0.9543, 'grad_norm': 0.173190176486969, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9022242426872253, 'eval_runtime': 5.2174, 'eval_samples_per_second': 191.668, 'eval_steps_per_second': 12.075, 'epoch': 0.48}
{'loss': 0.8984, 'grad_norm': 0.1569685935974121, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9109539985656738, 'eval_runtime': 5.2167, 'eval_samples_per_second': 191.694, 'eval_steps_per_second': 12.077, 'epoch': 0.52}
{'loss': 0.9239, 'grad_norm': 0.21464067697525024, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9011045098304749, 'eval_runtime': 5.2214, 'eval_samples_per_second': 191.52, 'eval_steps_per_second': 12.066, 'epoch': 0.56}
{'loss': 0.8959, 'grad_norm': 0.20440348982810974, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8961749076843262, 'eval_runtime': 5.2164, 'eval_samples_per_second': 191.702, 'eval_steps_per_second': 12.077, 'epoch': 0.6}
{'loss': 0.904, 'grad_norm': 0.19147004187107086, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8994942903518677, 'eval_runtime': 5.2211, 'eval_samples_per_second': 191.53, 'eval_steps_per_second': 12.066, 'epoch': 0.64}
{'loss': 0.9406, 'grad_norm': 0.21761898696422577, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8962776064872742, 'eval_runtime': 5.2202, 'eval_samples_per_second': 191.565, 'eval_steps_per_second': 12.069, 'epoch': 0.68}
{'loss': 0.9639, 'grad_norm': 0.19867315888404846, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8921588659286499, 'eval_runtime': 5.2218, 'eval_samples_per_second': 191.505, 'eval_steps_per_second': 12.065, 'epoch': 0.72}
{'loss': 0.8767, 'grad_norm': 0.289424866437912, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8957504630088806, 'eval_runtime': 5.2139, 'eval_samples_per_second': 191.796, 'eval_steps_per_second': 12.083, 'epoch': 0.76}
{'loss': 0.9201, 'grad_norm': 0.19399738311767578, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8912461400032043, 'eval_runtime': 5.215, 'eval_samples_per_second': 191.754, 'eval_steps_per_second': 12.081, 'epoch': 0.8}
{'loss': 0.8556, 'grad_norm': 0.2555600106716156, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8906412124633789, 'eval_runtime': 5.2166, 'eval_samples_per_second': 191.695, 'eval_steps_per_second': 12.077, 'epoch': 0.84}
{'loss': 0.8591, 'grad_norm': 0.29808518290519714, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.890074610710144, 'eval_runtime': 5.2168, 'eval_samples_per_second': 191.688, 'eval_steps_per_second': 12.076, 'epoch': 0.88}
{'loss': 0.89, 'grad_norm': 0.2704865634441376, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8882978558540344, 'eval_runtime': 5.2197, 'eval_samples_per_second': 191.583, 'eval_steps_per_second': 12.07, 'epoch': 0.92}
{'loss': 0.8539, 'grad_norm': 0.2861979007720947, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8868545889854431, 'eval_runtime': 5.2138, 'eval_samples_per_second': 191.797, 'eval_steps_per_second': 12.083, 'epoch': 0.96}
{'loss': 0.8413, 'grad_norm': 0.30534034967422485, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8870855569839478, 'eval_runtime': 5.2208, 'eval_samples_per_second': 191.542, 'eval_steps_per_second': 12.067, 'epoch': 1.0}
{'train_runtime': 329.3986, 'train_samples_per_second': 30.343, 'train_steps_per_second': 1.897, 'train_loss': 1.0351194854736327, 'epoch': 1.0}
train_results:  {'eval_loss': [1.915191650390625, 1.0536555051803589, 0.9701218605041504, 0.9481974244117737, 0.937757134437561, 0.9298160672187805, 0.9278331398963928, 0.9156607985496521, 0.9139249920845032, 0.9101752042770386, 0.9079510569572449, 0.9022242426872253, 0.9109539985656738, 0.9011045098304749, 0.8961749076843262, 0.8994942903518677, 0.8962776064872742, 0.8921588659286499, 0.8957504630088806, 0.8912461400032043, 0.8906412124633789, 0.890074610710144, 0.8882978558540344, 0.8868545889854431, 0.8870855569839478], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.915191650390625, 1.0536555051803589, 0.9701218605041504, 0.9481974244117737, 0.937757134437561, 0.9298160672187805, 0.9278331398963928, 0.9156607985496521, 0.9139249920845032, 0.9101752042770386, 0.9079510569572449, 0.9022242426872253, 0.9109539985656738, 0.9011045098304749, 0.8961749076843262, 0.8994942903518677, 0.8962776064872742, 0.8921588659286499, 0.8957504630088806, 0.8912461400032043, 0.8906412124633789, 0.890074610710144, 0.8882978558540344, 0.8868545889854431, 0.8870855569839478]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0353463888168335
current iteration best possible eval_loss (full train run):  -0.8870855569839478
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.9479 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.2962341904640198, 0.82075434923172, 0.2647177577018738, 0.42845386266708374, 0.15730291604995728, 0.15299081802368164, 0.17763495445251465, 0.8658952116966248, 0.9379879832267761, 0.9138310551643372, 0.28278684616088867, 0.7373226881027222, 0.47738534212112427, 0.4243614077568054, 0.05005854368209839, 0.08625077456235886, 0.9400144815444946, 0.9540963172912598, 0.8012327551841736]  ‚Üí  acq = -0.964964044448626
X = [0.33454614877700806, 0.5452781915664673, 0.34265732765197754, 0.876674473285675, 0.7544479966163635, 0.20097434520721436, 0.47008955478668213, 0.5161110162734985, 0.12353461980819702, 0.37957140803337097, 0.48378652334213257, 0.38838088512420654, 0.19706475734710693, 0.5216847658157349, 0.31215590238571167, 0.6112278699874878, 0.9380749464035034, 0.3674313724040985, 0.06246674060821533]  ‚Üí  acq = -0.9656888364132562
X = [0.5894963145256042, 0.6319558620452881, 0.37408900260925293, 0.7515134215354919, 0.1657804250717163, 0.9286734461784363, 0.2053823471069336, 0.3099049925804138, 0.12947320938110352, 0.5297918915748596, 0.16111403703689575, 0.3974562883377075, 0.647453248500824, 0.4768308401107788, 0.43715083599090576, 0.6067101955413818, 0.47161221504211426, 0.46995872259140015, 0.0678371787071228]  ‚Üí  acq = -0.968372357886119
X = [0.4231249690055847, 0.6987919807434082, 0.06285983324050903, 0.6670610308647156, 0.9282882213592529, 0.30631834268569946, 0.8289872407913208, 0.7324812412261963, 0.12291663885116577, 0.4462727904319763, 0.02472454309463501, 0.03433138132095337, 0.934790849685669, 0.22012978792190552, 0.37334394454956055, 0.5405794382095337, 0.06654441356658936, 0.2114507555961609, 0.6823987364768982]  ‚Üí  acq = -0.9648607883430663
X = [0.15775883197784424, 0.4864782691001892, 0.05650252103805542, 0.30110472440719604, 0.5412899851799011, 0.8217805624008179, 0.5733664035797119, 0.9863817095756531, 0.8804963827133179, 0.941512405872345, 0.3807917833328247, 0.6845978498458862, 0.20292234420776367, 0.32528477907180786, 0.484236478805542, 0.4367160201072693, 0.7705504298210144, 0.5857620239257812, 0.22822386026382446]  ‚Üí  acq = -0.9648604930482987
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2744, dtype=torch.float64), tensor(0.1883, dtype=torch.float64), tensor(0.0452, dtype=torch.float64), tensor(0.0817, dtype=torch.float64), tensor(0.0984, dtype=torch.float64), tensor(0.1098, dtype=torch.float64), tensor(0.0125, dtype=torch.float64), 0, tensor(0.1882, dtype=torch.float64), 25, 0, 1, 1, 1, 0, 77, 0.025950364994338754, 24.962792049159418, 0]
normalized proposed parameters for next round by BO: [tensor(0.2744, dtype=torch.float64), tensor(0.1883, dtype=torch.float64), tensor(0.0452, dtype=torch.float64), tensor(0.0817, dtype=torch.float64), tensor(0.0984, dtype=torch.float64), tensor(0.1098, dtype=torch.float64), tensor(0.0125, dtype=torch.float64), tensor(0.0015, dtype=torch.float64), tensor(0.1882, dtype=torch.float64), tensor(0.7711, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.6005, dtype=torch.float64), tensor(0.2595, dtype=torch.float64), tensor(0.5201, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  19  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.274
  gsm8k: 0.188
  rowan_hellaswag: 0.045
  sciq: 0.082
  triviaqa: 0.098
  truthfulqa_gen: 0.11
  wikitext: 0.012
  mmlu: 0
  arc_challenge: 0.188

LoRA Parameters:
  lora_r: (77,)
  lora_dropout: (0.025950364994338754,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (24.962792049159418,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  77
lora dropout:  0.025950364994338754
lora alpha:  24.962792049159418
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 80,819,200 || all params: 8,111,080,448 || trainable%: 0.9964
length of training data:  9980
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0461, 'grad_norm': 1.067122459411621, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.568231225013733, 'eval_runtime': 5.3145, 'eval_samples_per_second': 188.165, 'eval_steps_per_second': 11.854, 'epoch': 0.04}
{'loss': 1.2326, 'grad_norm': 0.3774420917034149, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0031909942626953, 'eval_runtime': 5.3306, 'eval_samples_per_second': 187.595, 'eval_steps_per_second': 11.818, 'epoch': 0.08}
{'loss': 1.0362, 'grad_norm': 0.19036653637886047, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.9534088373184204, 'eval_runtime': 5.3135, 'eval_samples_per_second': 188.2, 'eval_steps_per_second': 11.857, 'epoch': 0.12}
{'loss': 0.9681, 'grad_norm': 0.18917115032672882, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.9251202344894409, 'eval_runtime': 5.3332, 'eval_samples_per_second': 187.505, 'eval_steps_per_second': 11.813, 'epoch': 0.16}
{'loss': 0.9887, 'grad_norm': 0.2036193311214447, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.9265254139900208, 'eval_runtime': 5.3273, 'eval_samples_per_second': 187.711, 'eval_steps_per_second': 11.826, 'epoch': 0.2}
{'loss': 0.9731, 'grad_norm': 0.20770953595638275, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9216606020927429, 'eval_runtime': 5.3429, 'eval_samples_per_second': 187.164, 'eval_steps_per_second': 11.791, 'epoch': 0.24}
{'loss': 1.015, 'grad_norm': 0.1954633742570877, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.9152045845985413, 'eval_runtime': 5.3362, 'eval_samples_per_second': 187.398, 'eval_steps_per_second': 11.806, 'epoch': 0.28}
{'loss': 0.9934, 'grad_norm': 0.19357061386108398, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.9192997813224792, 'eval_runtime': 5.3417, 'eval_samples_per_second': 187.208, 'eval_steps_per_second': 11.794, 'epoch': 0.32}
{'loss': 0.9556, 'grad_norm': 0.1850973665714264, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.9110630750656128, 'eval_runtime': 5.3262, 'eval_samples_per_second': 187.751, 'eval_steps_per_second': 11.828, 'epoch': 0.36}
{'loss': 0.9483, 'grad_norm': 0.21311244368553162, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.8989359140396118, 'eval_runtime': 5.3585, 'eval_samples_per_second': 186.621, 'eval_steps_per_second': 11.757, 'epoch': 0.4}
{'loss': 0.9589, 'grad_norm': 0.20970018208026886, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.8969776630401611, 'eval_runtime': 5.3333, 'eval_samples_per_second': 187.501, 'eval_steps_per_second': 11.813, 'epoch': 0.44}
{'loss': 0.9181, 'grad_norm': 0.20180442929267883, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.8989918231964111, 'eval_runtime': 5.3375, 'eval_samples_per_second': 187.354, 'eval_steps_per_second': 11.803, 'epoch': 0.48}
{'loss': 0.9591, 'grad_norm': 0.24480797350406647, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.8958572149276733, 'eval_runtime': 5.3426, 'eval_samples_per_second': 187.176, 'eval_steps_per_second': 11.792, 'epoch': 0.52}
{'loss': 0.9532, 'grad_norm': 0.21728989481925964, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.897889256477356, 'eval_runtime': 5.3494, 'eval_samples_per_second': 186.936, 'eval_steps_per_second': 11.777, 'epoch': 0.56}
{'loss': 0.8891, 'grad_norm': 0.22274847328662872, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.8923506140708923, 'eval_runtime': 5.3304, 'eval_samples_per_second': 187.603, 'eval_steps_per_second': 11.819, 'epoch': 0.6}
{'loss': 0.9839, 'grad_norm': 0.20766301453113556, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.8873215317726135, 'eval_runtime': 5.305, 'eval_samples_per_second': 188.501, 'eval_steps_per_second': 11.876, 'epoch': 0.64}
{'loss': 0.9409, 'grad_norm': 0.19965022802352905, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.8870283365249634, 'eval_runtime': 5.3579, 'eval_samples_per_second': 186.642, 'eval_steps_per_second': 11.758, 'epoch': 0.68}
{'loss': 0.9181, 'grad_norm': 0.2156645804643631, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.8832265734672546, 'eval_runtime': 5.3311, 'eval_samples_per_second': 187.58, 'eval_steps_per_second': 11.818, 'epoch': 0.72}
{'loss': 0.9572, 'grad_norm': 0.1993369311094284, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.8818618655204773, 'eval_runtime': 5.3051, 'eval_samples_per_second': 188.498, 'eval_steps_per_second': 11.875, 'epoch': 0.76}
{'loss': 0.9208, 'grad_norm': 0.19828641414642334, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.8821015954017639, 'eval_runtime': 5.3044, 'eval_samples_per_second': 188.522, 'eval_steps_per_second': 11.877, 'epoch': 0.8}
{'loss': 0.9218, 'grad_norm': 0.22658568620681763, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.8789792656898499, 'eval_runtime': 5.3198, 'eval_samples_per_second': 187.978, 'eval_steps_per_second': 11.843, 'epoch': 0.84}
{'loss': 0.926, 'grad_norm': 0.21843354403972626, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.8781659603118896, 'eval_runtime': 5.288, 'eval_samples_per_second': 189.106, 'eval_steps_per_second': 11.914, 'epoch': 0.88}
{'loss': 0.9317, 'grad_norm': 0.26639461517333984, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.8756406307220459, 'eval_runtime': 5.2881, 'eval_samples_per_second': 189.104, 'eval_steps_per_second': 11.914, 'epoch': 0.92}
{'loss': 0.9318, 'grad_norm': 0.2330126166343689, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.875321626663208, 'eval_runtime': 5.2745, 'eval_samples_per_second': 189.592, 'eval_steps_per_second': 11.944, 'epoch': 0.96}
{'train_runtime': 342.3394, 'train_samples_per_second': 29.152, 'train_steps_per_second': 1.823, 'train_loss': 1.0477770016743586, 'epoch': 1.0}
train_results:  {'eval_loss': [1.568231225013733, 1.0031909942626953, 0.9534088373184204, 0.9251202344894409, 0.9265254139900208, 0.9216606020927429, 0.9152045845985413, 0.9192997813224792, 0.9110630750656128, 0.8989359140396118, 0.8969776630401611, 0.8989918231964111, 0.8958572149276733, 0.897889256477356, 0.8923506140708923, 0.8873215317726135, 0.8870283365249634, 0.8832265734672546, 0.8818618655204773, 0.8821015954017639, 0.8789792656898499, 0.8781659603118896, 0.8756406307220459, 0.875321626663208], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.568231225013733, 1.0031909942626953, 0.9534088373184204, 0.9251202344894409, 0.9265254139900208, 0.9216606020927429, 0.9152045845985413, 0.9192997813224792, 0.9110630750656128, 0.8989359140396118, 0.8969776630401611, 0.8989918231964111, 0.8958572149276733, 0.897889256477356, 0.8923506140708923, 0.8873215317726135, 0.8870283365249634, 0.8832265734672546, 0.8818618655204773, 0.8821015954017639, 0.8789792656898499, 0.8781659603118896, 0.8756406307220459, 0.875321626663208]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.066068410873413
current iteration best possible eval_loss (full train run):  -0.875321626663208
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 9.2937 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7657198309898376, 0.9410857558250427, 0.03986847400665283, 0.6952877044677734, 0.14549404382705688, 0.2639145851135254, 0.1955254077911377, 0.6364889144897461, 0.20661407709121704, 0.32434797286987305, 0.1894705891609192, 0.7398009896278381, 0.6692944765090942, 0.05867350101470947, 0.3082960844039917, 0.25163692235946655, 0.6103376746177673, 0.06646472215652466, 0.1532604694366455]  ‚Üí  acq = -0.938058076878857
X = [0.9276310801506042, 0.01835566759109497, 0.655583918094635, 0.5734493136405945, 0.6499941945075989, 0.4649926424026489, 0.9130101799964905, 0.7905814051628113, 0.9651851654052734, 0.4864007830619812, 0.852687656879425, 0.4021565914154053, 0.8258103728294373, 0.2814340591430664, 0.9827962517738342, 0.7941768169403076, 0.03410828113555908, 0.9558417797088623, 0.6570152640342712]  ‚Üí  acq = -0.9372105947203216
X = [0.3898862600326538, 0.1672157645225525, 0.4256635308265686, 0.3451581597328186, 0.165164053440094, 0.771423876285553, 0.4699157476425171, 0.1562402844429016, 0.004905879497528076, 0.8220118284225464, 0.09181463718414307, 0.7922564744949341, 0.40889763832092285, 0.7658670544624329, 0.35354381799697876, 0.8501660823822021, 0.8251820206642151, 0.8375023603439331, 0.5925764441490173]  ‚Üí  acq = -0.9373263080091617
X = [0.7604454159736633, 0.9365105628967285, 0.11750274896621704, 0.4367682933807373, 0.89451003074646, 0.07668685913085938, 0.43763238191604614, 0.49595075845718384, 0.08068454265594482, 0.14734964072704315, 0.7609574198722839, 0.3722277879714966, 0.11241912841796875, 0.9222967028617859, 0.7591463327407837, 0.9713605642318726, 0.19831812381744385, 0.38370370864868164, 0.6161256432533264]  ‚Üí  acq = -0.9372106027058358
X = [0.0232393741607666, 0.5331325531005859, 0.4393623471260071, 0.7613267302513123, 0.25029700994491577, 0.2948909401893616, 0.8885897994041443, 0.28309160470962524, 0.2914825677871704, 0.8101420402526855, 0.707656979560852, 0.6279451251029968, 0.7150348424911499, 0.3896148204803467, 0.1548752784729004, 0.42141979932785034, 0.38733386993408203, 0.770684003829956, 0.287418007850647]  ‚Üí  acq = -0.9372105954780939
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1421, dtype=torch.float64), tensor(0.0440, dtype=torch.float64), 0, tensor(0.0207, dtype=torch.float64), tensor(0.0959, dtype=torch.float64), tensor(0.0892, dtype=torch.float64), 0, tensor(0.1120, dtype=torch.float64), tensor(0.4923, dtype=torch.float64), 24, 0, 1, 1, 1, 0, 56, 0.036037062711555316, 13.794392719019536, 0]
normalized proposed parameters for next round by BO: [tensor(0.1421, dtype=torch.float64), tensor(0.0440, dtype=torch.float64), tensor(0.0039, dtype=torch.float64), tensor(0.0207, dtype=torch.float64), tensor(0.0959, dtype=torch.float64), tensor(0.0892, dtype=torch.float64), tensor(6.1435e-18, dtype=torch.float64), tensor(0.1120, dtype=torch.float64), tensor(0.4923, dtype=torch.float64), tensor(0.7467, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.4358, dtype=torch.float64), tensor(0.3604, dtype=torch.float64), tensor(0.2874, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  20  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.142
  gsm8k: 0.044
  rowan_hellaswag: 0
  sciq: 0.021
  triviaqa: 0.096
  truthfulqa_gen: 0.089
  wikitext: 0
  mmlu: 0.112
  arc_challenge: 0.492

LoRA Parameters:
  lora_r: (56,)
  lora_dropout: (0.036037062711555316,)
  num_layers_to_apply: (24,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (13.794392719019536,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  24
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  56
lora dropout:  0.036037062711555316
lora alpha:  13.794392719019536
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 56,426,496 || all params: 8,086,687,744 || trainable%: 0.6978
length of training data:  9956
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1809, 'grad_norm': 0.8459959030151367, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.980659008026123, 'eval_runtime': 4.9255, 'eval_samples_per_second': 203.025, 'eval_steps_per_second': 12.791, 'epoch': 0.04}
{'loss': 1.2832, 'grad_norm': 0.258657842874527, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1131176948547363, 'eval_runtime': 4.9315, 'eval_samples_per_second': 202.779, 'eval_steps_per_second': 12.775, 'epoch': 0.08}
{'loss': 0.987, 'grad_norm': 0.17431287467479706, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.026562213897705, 'eval_runtime': 4.9211, 'eval_samples_per_second': 203.207, 'eval_steps_per_second': 12.802, 'epoch': 0.12}
{'loss': 0.9639, 'grad_norm': 0.20275448262691498, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 0.9889032244682312, 'eval_runtime': 4.9325, 'eval_samples_per_second': 202.738, 'eval_steps_per_second': 12.773, 'epoch': 0.16}
{'loss': 0.9375, 'grad_norm': 0.17548932135105133, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.9754209518432617, 'eval_runtime': 4.9502, 'eval_samples_per_second': 202.012, 'eval_steps_per_second': 12.727, 'epoch': 0.2}
{'loss': 0.963, 'grad_norm': 0.22861772775650024, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.9564045667648315, 'eval_runtime': 4.9601, 'eval_samples_per_second': 201.61, 'eval_steps_per_second': 12.701, 'epoch': 0.24}
{'loss': 0.9118, 'grad_norm': 0.20963750779628754, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.961769700050354, 'eval_runtime': 4.9593, 'eval_samples_per_second': 201.642, 'eval_steps_per_second': 12.703, 'epoch': 0.28}
{'loss': 0.9328, 'grad_norm': 0.1906539499759674, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.9419030547142029, 'eval_runtime': 4.9642, 'eval_samples_per_second': 201.441, 'eval_steps_per_second': 12.691, 'epoch': 0.32}
{'loss': 0.9339, 'grad_norm': 0.21380753815174103, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.9477463960647583, 'eval_runtime': 4.9695, 'eval_samples_per_second': 201.226, 'eval_steps_per_second': 12.677, 'epoch': 0.36}
{'loss': 0.8826, 'grad_norm': 0.20969490706920624, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.9373621940612793, 'eval_runtime': 4.9681, 'eval_samples_per_second': 201.283, 'eval_steps_per_second': 12.681, 'epoch': 0.4}
{'loss': 0.8677, 'grad_norm': 0.25771617889404297, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.9366341829299927, 'eval_runtime': 4.9628, 'eval_samples_per_second': 201.499, 'eval_steps_per_second': 12.694, 'epoch': 0.44}
{'loss': 0.8789, 'grad_norm': 0.27980613708496094, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.927949845790863, 'eval_runtime': 4.9738, 'eval_samples_per_second': 201.052, 'eval_steps_per_second': 12.666, 'epoch': 0.48}
{'loss': 0.8251, 'grad_norm': 0.2342224419116974, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.9307330846786499, 'eval_runtime': 4.9661, 'eval_samples_per_second': 201.366, 'eval_steps_per_second': 12.686, 'epoch': 0.52}
{'loss': 0.8424, 'grad_norm': 0.34110215306282043, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.9371756315231323, 'eval_runtime': 4.964, 'eval_samples_per_second': 201.451, 'eval_steps_per_second': 12.691, 'epoch': 0.56}
{'loss': 0.8087, 'grad_norm': 0.3413013815879822, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.9265502095222473, 'eval_runtime': 4.9654, 'eval_samples_per_second': 201.394, 'eval_steps_per_second': 12.688, 'epoch': 0.6}
{'loss': 0.791, 'grad_norm': 0.2943178713321686, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.9228950142860413, 'eval_runtime': 4.9652, 'eval_samples_per_second': 201.403, 'eval_steps_per_second': 12.688, 'epoch': 0.64}
{'loss': 0.7608, 'grad_norm': 0.34759756922721863, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.920120120048523, 'eval_runtime': 4.9675, 'eval_samples_per_second': 201.307, 'eval_steps_per_second': 12.682, 'epoch': 0.68}
{'loss': 0.7675, 'grad_norm': 0.3823833465576172, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.9271485805511475, 'eval_runtime': 4.9626, 'eval_samples_per_second': 201.508, 'eval_steps_per_second': 12.695, 'epoch': 0.72}
{'loss': 0.7677, 'grad_norm': 0.37744903564453125, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.9159653782844543, 'eval_runtime': 4.9685, 'eval_samples_per_second': 201.267, 'eval_steps_per_second': 12.68, 'epoch': 0.76}
{'loss': 0.7671, 'grad_norm': 0.3358517587184906, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.9159796237945557, 'eval_runtime': 5.0102, 'eval_samples_per_second': 199.595, 'eval_steps_per_second': 12.574, 'epoch': 0.8}
{'loss': 0.7293, 'grad_norm': 0.379805326461792, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.9167402386665344, 'eval_runtime': 4.9763, 'eval_samples_per_second': 200.954, 'eval_steps_per_second': 12.66, 'epoch': 0.84}
{'loss': 0.7469, 'grad_norm': 0.4847189784049988, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.9156579375267029, 'eval_runtime': 4.9883, 'eval_samples_per_second': 200.469, 'eval_steps_per_second': 12.63, 'epoch': 0.88}
{'loss': 0.7635, 'grad_norm': 0.3216090500354767, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.9146235585212708, 'eval_runtime': 4.9835, 'eval_samples_per_second': 200.661, 'eval_steps_per_second': 12.642, 'epoch': 0.92}
{'loss': 0.707, 'grad_norm': 0.4764028489589691, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.913427472114563, 'eval_runtime': 4.9618, 'eval_samples_per_second': 201.539, 'eval_steps_per_second': 12.697, 'epoch': 0.96}
{'train_runtime': 284.6284, 'train_samples_per_second': 34.979, 'train_steps_per_second': 2.189, 'train_loss': 0.9484088386425429, 'epoch': 1.0}
train_results:  {'eval_loss': [1.980659008026123, 1.1131176948547363, 1.026562213897705, 0.9889032244682312, 0.9754209518432617, 0.9564045667648315, 0.961769700050354, 0.9419030547142029, 0.9477463960647583, 0.9373621940612793, 0.9366341829299927, 0.927949845790863, 0.9307330846786499, 0.9371756315231323, 0.9265502095222473, 0.9228950142860413, 0.920120120048523, 0.9271485805511475, 0.9159653782844543, 0.9159796237945557, 0.9167402386665344, 0.9156579375267029, 0.9146235585212708, 0.913427472114563], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.980659008026123, 1.1131176948547363, 1.026562213897705, 0.9889032244682312, 0.9754209518432617, 0.9564045667648315, 0.961769700050354, 0.9419030547142029, 0.9477463960647583, 0.9373621940612793, 0.9366341829299927, 0.927949845790863, 0.9307330846786499, 0.9371756315231323, 0.9265502095222473, 0.9228950142860413, 0.920120120048523, 0.9271485805511475, 0.9159653782844543, 0.9159796237945557, 0.9167402386665344, 0.9156579375267029, 0.9146235585212708, 0.913427472114563]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9974642992019653
current iteration best possible eval_loss (full train run):  -0.913427472114563
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 24.4744 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.6823287010192871, 0.1862812042236328, 0.8970202207565308, 0.9420492053031921, 0.08797204494476318, 0.5372844934463501, 0.5283955335617065, 0.8666674494743347, 0.4410834312438965, 0.8836188316345215, 0.9964625239372253, 0.7934750914573669, 0.09433919191360474, 0.2745521068572998, 0.2743326425552368, 0.33229848742485046, 0.9432199001312256, 0.3315914273262024, 0.7924636006355286]  ‚Üí  acq = -0.979342115345757
X = [0.6971794366836548, 0.0029845833778381348, 0.26995527744293213, 0.4964855909347534, 0.23586487770080566, 0.7555009126663208, 0.21712642908096313, 0.027570784091949463, 0.8386974334716797, 0.9298456907272339, 0.9158058762550354, 0.6235672831535339, 0.6400220394134521, 0.9358646273612976, 0.35674506425857544, 0.5776915550231934, 0.40536046028137207, 0.8601958751678467, 0.07649117708206177]  ‚Üí  acq = -0.976350619843948
X = [0.7746965289115906, 0.45509761571884155, 0.19304150342941284, 0.8645700216293335, 0.6138942837715149, 0.34241217374801636, 0.8082748055458069, 0.28664201498031616, 0.4680793285369873, 0.08178042620420456, 0.07738137245178223, 0.8753153681755066, 0.40089279413223267, 0.10053563117980957, 0.7970610857009888, 0.7854319214820862, 0.9972329139709473, 0.8323233127593994, 0.5278875827789307]  ‚Üí  acq = -0.9793427572278044
X = [0.6284062266349792, 0.25792115926742554, 0.6750133037567139, 0.037352144718170166, 0.9293985962867737, 0.8550317287445068, 0.21394050121307373, 0.461556613445282, 0.03737133741378784, 0.653922438621521, 0.4825098514556885, 0.12023776769638062, 0.2823812961578369, 0.27488136291503906, 0.9535494446754456, 0.750758707523346, 0.8871928453445435, 0.8711596727371216, 0.8900622725486755]  ‚Üí  acq = -0.9793587588526558
X = [0.7428956627845764, 0.7939770817756653, 0.004957675933837891, 0.2842784523963928, 0.41364288330078125, 0.3952515721321106, 0.3819403648376465, 0.872658908367157, 0.3920016884803772, 0.10134205967187881, 0.695570707321167, 0.8957255482673645, 0.6388514637947083, 0.9526784420013428, 0.17277437448501587, 0.2861731946468353, 0.8315107822418213, 0.7387340068817139, 0.4935872554779053]  ‚Üí  acq = -0.9793670489335049
proposed candidate layer mask is:  tensor([0., 1., 1., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2278, dtype=torch.float64), 0, tensor(0.0855, dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0379, dtype=torch.float64), tensor(0.0484, dtype=torch.float64), 0, tensor(0.0918, dtype=torch.float64), tensor(0.4893, dtype=torch.float64), 28, 0, 1, 1, 1, 0, 74, 0.04176684502004917, 18.207778788621034, 0]
normalized proposed parameters for next round by BO: [tensor(0.2278, dtype=torch.float64), tensor(1.9480e-18, dtype=torch.float64), tensor(0.0855, dtype=torch.float64), tensor(0.0178, dtype=torch.float64), tensor(0.0379, dtype=torch.float64), tensor(0.0484, dtype=torch.float64), tensor(0.0016, dtype=torch.float64), tensor(0.0918, dtype=torch.float64), tensor(0.4893, dtype=torch.float64), tensor(0.8746, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.5791, dtype=torch.float64), tensor(0.4177, dtype=torch.float64), tensor(0.3793, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  21  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.228
  gsm8k: 0
  rowan_hellaswag: 0.085
  sciq: 0.018
  triviaqa: 0.038
  truthfulqa_gen: 0.048
  wikitext: 0
  mmlu: 0.092
  arc_challenge: 0.489

LoRA Parameters:
  lora_r: (74,)
  lora_dropout: (0.04176684502004917,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([0, 1, 1, 1, 0],)
  lora_alpha: (18.207778788621034,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 1, 1, 0]
lora rank:  74
lora dropout:  0.04176684502004917
lora alpha:  18.207778788621034
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 86,990,848 || all params: 8,117,252,096 || trainable%: 1.0717
length of training data:  9980
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.2058, 'grad_norm': 0.6958187222480774, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.6684017181396484, 'eval_runtime': 5.2368, 'eval_samples_per_second': 190.958, 'eval_steps_per_second': 12.03, 'epoch': 0.04}
{'loss': 1.3782, 'grad_norm': 0.29008787870407104, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.044132947921753, 'eval_runtime': 5.2503, 'eval_samples_per_second': 190.464, 'eval_steps_per_second': 11.999, 'epoch': 0.08}
{'loss': 1.1196, 'grad_norm': 0.21963530778884888, 'learning_rate': 0.00028745644599303136, 'epoch': 0.12}
{'eval_loss': 0.9765679836273193, 'eval_runtime': 5.2355, 'eval_samples_per_second': 191.004, 'eval_steps_per_second': 12.033, 'epoch': 0.12}
{'loss': 1.0718, 'grad_norm': 0.2504587173461914, 'learning_rate': 0.000274390243902439, 'epoch': 0.16}
{'eval_loss': 0.9418671131134033, 'eval_runtime': 5.2789, 'eval_samples_per_second': 189.434, 'eval_steps_per_second': 11.934, 'epoch': 0.16}
{'loss': 1.0735, 'grad_norm': 0.20975923538208008, 'learning_rate': 0.00026132404181184664, 'epoch': 0.2}
{'eval_loss': 0.931627094745636, 'eval_runtime': 5.2422, 'eval_samples_per_second': 190.76, 'eval_steps_per_second': 12.018, 'epoch': 0.2}
{'loss': 0.9723, 'grad_norm': 0.19318154454231262, 'learning_rate': 0.00024825783972125433, 'epoch': 0.24}
{'eval_loss': 0.9404561519622803, 'eval_runtime': 5.2557, 'eval_samples_per_second': 190.27, 'eval_steps_per_second': 11.987, 'epoch': 0.24}
{'loss': 1.0429, 'grad_norm': 0.21209263801574707, 'learning_rate': 0.000235191637630662, 'epoch': 0.28}
{'eval_loss': 0.9253835082054138, 'eval_runtime': 5.2646, 'eval_samples_per_second': 189.948, 'eval_steps_per_second': 11.967, 'epoch': 0.28}
{'loss': 1.0252, 'grad_norm': 0.2042703926563263, 'learning_rate': 0.00022212543554006969, 'epoch': 0.32}
{'eval_loss': 0.9231237173080444, 'eval_runtime': 5.2529, 'eval_samples_per_second': 190.371, 'eval_steps_per_second': 11.993, 'epoch': 0.32}
{'loss': 0.9878, 'grad_norm': 0.25485560297966003, 'learning_rate': 0.00020905923344947735, 'epoch': 0.36}
{'eval_loss': 0.9198591113090515, 'eval_runtime': 5.2387, 'eval_samples_per_second': 190.886, 'eval_steps_per_second': 12.026, 'epoch': 0.36}
{'loss': 0.9976, 'grad_norm': 0.20231100916862488, 'learning_rate': 0.000195993031358885, 'epoch': 0.4}
{'eval_loss': 0.9161978960037231, 'eval_runtime': 5.2415, 'eval_samples_per_second': 190.785, 'eval_steps_per_second': 12.019, 'epoch': 0.4}
{'loss': 0.9146, 'grad_norm': 0.24804140627384186, 'learning_rate': 0.00018292682926829266, 'epoch': 0.44}
{'eval_loss': 0.9094967246055603, 'eval_runtime': 5.2378, 'eval_samples_per_second': 190.92, 'eval_steps_per_second': 12.028, 'epoch': 0.44}
{'loss': 1.0154, 'grad_norm': 0.19846810400485992, 'learning_rate': 0.00016986062717770035, 'epoch': 0.48}
{'eval_loss': 0.9075552821159363, 'eval_runtime': 5.2389, 'eval_samples_per_second': 190.881, 'eval_steps_per_second': 12.025, 'epoch': 0.48}
{'loss': 1.0067, 'grad_norm': 0.27536001801490784, 'learning_rate': 0.00015679442508710801, 'epoch': 0.52}
{'eval_loss': 0.9012126326560974, 'eval_runtime': 5.2768, 'eval_samples_per_second': 189.509, 'eval_steps_per_second': 11.939, 'epoch': 0.52}
{'loss': 0.935, 'grad_norm': 0.2607545852661133, 'learning_rate': 0.00014372822299651568, 'epoch': 0.56}
{'eval_loss': 0.9066945910453796, 'eval_runtime': 5.2628, 'eval_samples_per_second': 190.011, 'eval_steps_per_second': 11.971, 'epoch': 0.56}
{'loss': 0.861, 'grad_norm': 0.29828399419784546, 'learning_rate': 0.00013066202090592332, 'epoch': 0.6}
{'eval_loss': 0.9026739597320557, 'eval_runtime': 5.258, 'eval_samples_per_second': 190.187, 'eval_steps_per_second': 11.982, 'epoch': 0.6}
{'loss': 0.9175, 'grad_norm': 0.29185473918914795, 'learning_rate': 0.000117595818815331, 'epoch': 0.64}
{'eval_loss': 0.896541953086853, 'eval_runtime': 5.2725, 'eval_samples_per_second': 189.663, 'eval_steps_per_second': 11.949, 'epoch': 0.64}
{'loss': 0.8813, 'grad_norm': 0.38286080956459045, 'learning_rate': 0.00010452961672473868, 'epoch': 0.68}
{'eval_loss': 0.8970053791999817, 'eval_runtime': 5.2752, 'eval_samples_per_second': 189.565, 'eval_steps_per_second': 11.943, 'epoch': 0.68}
{'loss': 0.8984, 'grad_norm': 0.27465617656707764, 'learning_rate': 9.146341463414633e-05, 'epoch': 0.72}
{'eval_loss': 0.892244815826416, 'eval_runtime': 5.228, 'eval_samples_per_second': 191.278, 'eval_steps_per_second': 12.051, 'epoch': 0.72}
{'loss': 0.8504, 'grad_norm': 0.27388155460357666, 'learning_rate': 7.839721254355401e-05, 'epoch': 0.76}
{'eval_loss': 0.8898670673370361, 'eval_runtime': 5.2374, 'eval_samples_per_second': 190.935, 'eval_steps_per_second': 12.029, 'epoch': 0.76}
{'loss': 0.8238, 'grad_norm': 0.2900361716747284, 'learning_rate': 6.533101045296166e-05, 'epoch': 0.8}
{'eval_loss': 0.8899164795875549, 'eval_runtime': 5.2313, 'eval_samples_per_second': 191.157, 'eval_steps_per_second': 12.043, 'epoch': 0.8}
{'loss': 0.871, 'grad_norm': 0.3382933437824249, 'learning_rate': 5.226480836236934e-05, 'epoch': 0.84}
{'eval_loss': 0.8880334496498108, 'eval_runtime': 5.2209, 'eval_samples_per_second': 191.536, 'eval_steps_per_second': 12.067, 'epoch': 0.84}
{'loss': 0.8759, 'grad_norm': 0.35831084847450256, 'learning_rate': 3.9198606271777003e-05, 'epoch': 0.88}
{'eval_loss': 0.8886817693710327, 'eval_runtime': 5.2195, 'eval_samples_per_second': 191.591, 'eval_steps_per_second': 12.07, 'epoch': 0.88}
{'loss': 0.887, 'grad_norm': 0.3375278413295746, 'learning_rate': 2.613240418118467e-05, 'epoch': 0.92}
{'eval_loss': 0.8856602907180786, 'eval_runtime': 5.2166, 'eval_samples_per_second': 191.695, 'eval_steps_per_second': 12.077, 'epoch': 0.92}
{'loss': 0.8083, 'grad_norm': 0.36375969648361206, 'learning_rate': 1.3066202090592334e-05, 'epoch': 0.96}
{'eval_loss': 0.8854158520698547, 'eval_runtime': 5.2221, 'eval_samples_per_second': 191.492, 'eval_steps_per_second': 12.064, 'epoch': 0.96}
{'train_runtime': 338.9153, 'train_samples_per_second': 29.447, 'train_steps_per_second': 1.841, 'train_loss': 1.0497728127699633, 'epoch': 1.0}
train_results:  {'eval_loss': [1.6684017181396484, 1.044132947921753, 0.9765679836273193, 0.9418671131134033, 0.931627094745636, 0.9404561519622803, 0.9253835082054138, 0.9231237173080444, 0.9198591113090515, 0.9161978960037231, 0.9094967246055603, 0.9075552821159363, 0.9012126326560974, 0.9066945910453796, 0.9026739597320557, 0.896541953086853, 0.8970053791999817, 0.892244815826416, 0.8898670673370361, 0.8899164795875549, 0.8880334496498108, 0.8886817693710327, 0.8856602907180786, 0.8854158520698547], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.6684017181396484, 1.044132947921753, 0.9765679836273193, 0.9418671131134033, 0.931627094745636, 0.9404561519622803, 0.9253835082054138, 0.9231237173080444, 0.9198591113090515, 0.9161978960037231, 0.9094967246055603, 0.9075552821159363, 0.9012126326560974, 0.9066945910453796, 0.9026739597320557, 0.896541953086853, 0.8970053791999817, 0.892244815826416, 0.8898670673370361, 0.8899164795875549, 0.8880334496498108, 0.8886817693710327, 0.8856602907180786, 0.8854158520698547]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.023471713066101
current iteration best possible eval_loss (full train run):  -0.8854158520698547
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.7608 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.1793595552444458, 0.3586342930793762, 0.11602413654327393, 0.4066738486289978, 0.8684375882148743, 0.5997217893600464, 0.9453309774398804, 0.5592350959777832, 0.9776346683502197, 0.34960928559303284, 0.057854533195495605, 0.7710180282592773, 0.38107073307037354, 0.8118119835853577, 0.8704851865768433, 0.6546881198883057, 0.3577408194541931, 0.8840495347976685, 0.5400984883308411]  ‚Üí  acq = -0.99566922345309
X = [0.7988576889038086, 0.1653779149055481, 0.5303308367729187, 0.5103733539581299, 0.23054015636444092, 0.2252374291419983, 0.6409772634506226, 0.9417331218719482, 0.8386891484260559, 0.9513463973999023, 0.8898367881774902, 0.2988312840461731, 0.7724373936653137, 0.05733346939086914, 0.3388344645500183, 0.24033895134925842, 0.2050917148590088, 0.051226988434791565, 0.8115118741989136]  ‚Üí  acq = -0.9956684855265987
X = [0.22445803880691528, 0.07812052965164185, 0.32493531703948975, 0.918027400970459, 0.40309834480285645, 0.5952427387237549, 0.7784673571586609, 0.5494266152381897, 0.16604727506637573, 0.9895481467247009, 0.6373879313468933, 0.9490465521812439, 0.7621972560882568, 0.333041250705719, 0.705083429813385, 0.6339337825775146, 0.44919973611831665, 0.9787859916687012, 0.45290279388427734]  ‚Üí  acq = -0.9956722698487802
X = [0.3654218316078186, 0.6524207592010498, 0.45629966259002686, 0.9936108589172363, 0.9902206659317017, 0.7348255515098572, 0.9084284901618958, 0.5383283495903015, 0.9914198517799377, 0.8971459269523621, 0.9170647859573364, 0.6597838997840881, 0.16925257444381714, 0.6921932697296143, 0.6407592296600342, 0.8601893186569214, 0.164650559425354, 0.14350587129592896, 0.7966952323913574]  ‚Üí  acq = -0.9956685026110433
X = [0.4475772976875305, 0.6951091885566711, 0.4215993285179138, 0.726239025592804, 0.2475719451904297, 0.18795019388198853, 0.22851938009262085, 0.4415127635002136, 0.046321094036102295, 0.06307335197925568, 0.7526708841323853, 0.20946532487869263, 0.12849992513656616, 0.11788642406463623, 0.6525771021842957, 0.10499551892280579, 0.07692551612854004, 0.719855546951294, 0.04362046718597412]  ‚Üí  acq = -0.9977710042504573
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2335, dtype=torch.float64), tensor(0.0443, dtype=torch.float64), tensor(0.0374, dtype=torch.float64), tensor(0.0758, dtype=torch.float64), tensor(0.1535, dtype=torch.float64), tensor(0.0859, dtype=torch.float64), 0, tensor(0.1266, dtype=torch.float64), tensor(0.2430, dtype=torch.float64), 30, 1, 1, 1, 0, 1, 48, 0.038470029502677694, 20.56859309327719, 0]
normalized proposed parameters for next round by BO: [tensor(0.2335, dtype=torch.float64), tensor(0.0443, dtype=torch.float64), tensor(0.0374, dtype=torch.float64), tensor(0.0758, dtype=torch.float64), tensor(0.1535, dtype=torch.float64), tensor(0.0859, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1266, dtype=torch.float64), tensor(0.2430, dtype=torch.float64), tensor(0.9278, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3774, dtype=torch.float64), tensor(0.3847, dtype=torch.float64), tensor(0.4285, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  22  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.234
  gsm8k: 0.044
  rowan_hellaswag: 0.037
  sciq: 0.076
  triviaqa: 0.154
  truthfulqa_gen: 0.086
  wikitext: 0
  mmlu: 0.127
  arc_challenge: 0.243

LoRA Parameters:
  lora_r: (48,)
  lora_dropout: (0.038470029502677694,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (20.56859309327719,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  48
lora dropout:  0.038470029502677694
lora alpha:  20.56859309327719
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 72,253,440 || all params: 8,102,514,688 || trainable%: 0.8917
length of training data:  9996
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1921, 'grad_norm': 0.8463996648788452, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.632445216178894, 'eval_runtime': 5.1677, 'eval_samples_per_second': 193.508, 'eval_steps_per_second': 12.191, 'epoch': 0.04}
{'loss': 1.4462, 'grad_norm': 0.24484509229660034, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.171840786933899, 'eval_runtime': 5.1704, 'eval_samples_per_second': 193.41, 'eval_steps_per_second': 12.185, 'epoch': 0.08}
{'loss': 1.2709, 'grad_norm': 0.27060091495513916, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0847948789596558, 'eval_runtime': 5.175, 'eval_samples_per_second': 193.238, 'eval_steps_per_second': 12.174, 'epoch': 0.12}
{'loss': 1.1754, 'grad_norm': 0.2596818804740906, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.0161770582199097, 'eval_runtime': 5.1805, 'eval_samples_per_second': 193.031, 'eval_steps_per_second': 12.161, 'epoch': 0.16}
{'loss': 1.0703, 'grad_norm': 0.24210312962532043, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9359525442123413, 'eval_runtime': 5.1767, 'eval_samples_per_second': 193.172, 'eval_steps_per_second': 12.17, 'epoch': 0.2}
{'loss': 1.0077, 'grad_norm': 0.2420460283756256, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9240407943725586, 'eval_runtime': 5.1807, 'eval_samples_per_second': 193.023, 'eval_steps_per_second': 12.16, 'epoch': 0.24}
{'loss': 1.0412, 'grad_norm': 0.2356407642364502, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9160324931144714, 'eval_runtime': 5.2159, 'eval_samples_per_second': 191.721, 'eval_steps_per_second': 12.078, 'epoch': 0.28}
{'loss': 1.0281, 'grad_norm': 0.25338223576545715, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9100480675697327, 'eval_runtime': 5.2108, 'eval_samples_per_second': 191.908, 'eval_steps_per_second': 12.09, 'epoch': 0.32}
{'loss': 0.9752, 'grad_norm': 0.22568871080875397, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9079482555389404, 'eval_runtime': 5.1989, 'eval_samples_per_second': 192.349, 'eval_steps_per_second': 12.118, 'epoch': 0.36}
{'loss': 0.9438, 'grad_norm': 0.23195548355579376, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.908816397190094, 'eval_runtime': 5.212, 'eval_samples_per_second': 191.867, 'eval_steps_per_second': 12.088, 'epoch': 0.4}
{'loss': 0.9971, 'grad_norm': 0.26718154549598694, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9011220932006836, 'eval_runtime': 5.2322, 'eval_samples_per_second': 191.123, 'eval_steps_per_second': 12.041, 'epoch': 0.44}
{'loss': 0.9467, 'grad_norm': 0.2689654231071472, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8958160281181335, 'eval_runtime': 5.217, 'eval_samples_per_second': 191.682, 'eval_steps_per_second': 12.076, 'epoch': 0.48}
{'loss': 0.9448, 'grad_norm': 0.3608211576938629, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8904399871826172, 'eval_runtime': 5.2142, 'eval_samples_per_second': 191.782, 'eval_steps_per_second': 12.082, 'epoch': 0.52}
{'loss': 1.0001, 'grad_norm': 0.33615854382514954, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8904445767402649, 'eval_runtime': 5.1995, 'eval_samples_per_second': 192.325, 'eval_steps_per_second': 12.117, 'epoch': 0.56}
{'loss': 0.9112, 'grad_norm': 0.24835799634456635, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.88685542345047, 'eval_runtime': 5.1969, 'eval_samples_per_second': 192.421, 'eval_steps_per_second': 12.123, 'epoch': 0.6}
{'loss': 0.9602, 'grad_norm': 0.31142300367355347, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.887729287147522, 'eval_runtime': 5.1954, 'eval_samples_per_second': 192.477, 'eval_steps_per_second': 12.126, 'epoch': 0.64}
{'loss': 0.9259, 'grad_norm': 0.30936866998672485, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8883029818534851, 'eval_runtime': 5.1958, 'eval_samples_per_second': 192.465, 'eval_steps_per_second': 12.125, 'epoch': 0.68}
{'loss': 0.9351, 'grad_norm': 0.23822017014026642, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8818157911300659, 'eval_runtime': 5.2494, 'eval_samples_per_second': 190.497, 'eval_steps_per_second': 12.001, 'epoch': 0.72}
{'loss': 0.9325, 'grad_norm': 0.2720775008201599, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8817847371101379, 'eval_runtime': 5.2224, 'eval_samples_per_second': 191.484, 'eval_steps_per_second': 12.064, 'epoch': 0.76}
{'loss': 0.9482, 'grad_norm': 0.4071411192417145, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8790788650512695, 'eval_runtime': 5.2348, 'eval_samples_per_second': 191.029, 'eval_steps_per_second': 12.035, 'epoch': 0.8}
{'loss': 0.9251, 'grad_norm': 0.2704695761203766, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8793366551399231, 'eval_runtime': 5.2383, 'eval_samples_per_second': 190.9, 'eval_steps_per_second': 12.027, 'epoch': 0.84}
{'loss': 0.8854, 'grad_norm': 0.25130951404571533, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8760411739349365, 'eval_runtime': 5.2434, 'eval_samples_per_second': 190.715, 'eval_steps_per_second': 12.015, 'epoch': 0.88}
{'loss': 0.9258, 'grad_norm': 0.393412709236145, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8751517534255981, 'eval_runtime': 5.2419, 'eval_samples_per_second': 190.77, 'eval_steps_per_second': 12.019, 'epoch': 0.92}
{'loss': 0.9432, 'grad_norm': 0.3550257086753845, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8746026158332825, 'eval_runtime': 5.2575, 'eval_samples_per_second': 190.203, 'eval_steps_per_second': 11.983, 'epoch': 0.96}
{'loss': 0.9316, 'grad_norm': 0.442507266998291, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8742722272872925, 'eval_runtime': 5.2637, 'eval_samples_per_second': 189.98, 'eval_steps_per_second': 11.969, 'epoch': 1.0}
{'train_runtime': 331.204, 'train_samples_per_second': 30.181, 'train_steps_per_second': 1.887, 'train_loss': 1.0905578826904296, 'epoch': 1.0}
train_results:  {'eval_loss': [1.632445216178894, 1.171840786933899, 1.0847948789596558, 1.0161770582199097, 0.9359525442123413, 0.9240407943725586, 0.9160324931144714, 0.9100480675697327, 0.9079482555389404, 0.908816397190094, 0.9011220932006836, 0.8958160281181335, 0.8904399871826172, 0.8904445767402649, 0.88685542345047, 0.887729287147522, 0.8883029818534851, 0.8818157911300659, 0.8817847371101379, 0.8790788650512695, 0.8793366551399231, 0.8760411739349365, 0.8751517534255981, 0.8746026158332825, 0.8742722272872925], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.632445216178894, 1.171840786933899, 1.0847948789596558, 1.0161770582199097, 0.9359525442123413, 0.9240407943725586, 0.9160324931144714, 0.9100480675697327, 0.9079482555389404, 0.908816397190094, 0.9011220932006836, 0.8958160281181335, 0.8904399871826172, 0.8904445767402649, 0.88685542345047, 0.887729287147522, 0.8883029818534851, 0.8818157911300659, 0.8817847371101379, 0.8790788650512695, 0.8793366551399231, 0.8760411739349365, 0.8751517534255981, 0.8746026158332825, 0.8742722272872925]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9886037707328796
current iteration best possible eval_loss (full train run):  -0.8742722272872925
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 5.1401 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.4753988981246948, 0.8961065411567688, 0.19753950834274292, 0.671665370464325, 0.06938451528549194, 0.19600969552993774, 0.8524817228317261, 0.21737313270568848, 0.4866626262664795, 0.6609281897544861, 0.847377359867096, 0.8555901646614075, 0.7310363054275513, 0.61064213514328, 0.9655588865280151, 0.2983042299747467, 0.14850598573684692, 0.6075927019119263, 0.6337894201278687]  ‚Üí  acq = -0.9947232319771485
X = [0.8848512172698975, 0.781201183795929, 0.15058434009552002, 0.9274570345878601, 0.6459645628929138, 0.3017991781234741, 0.9778422713279724, 0.10921823978424072, 0.1414751410484314, 0.1795206367969513, 0.175001323223114, 0.23856991529464722, 0.004647314548492432, 0.2729220986366272, 0.8522514700889587, 0.9269974231719971, 0.5002951622009277, 0.5946985483169556, 0.9915117621421814]  ‚Üí  acq = -0.9946377011346356
X = [0.7680455446243286, 0.23242336511611938, 0.005153834819793701, 0.6329408288002014, 0.6618794798851013, 0.7114154696464539, 0.9347782731056213, 0.08449238538742065, 0.8182916045188904, 0.27332258224487305, 0.11163574457168579, 0.5557024478912354, 0.330188512802124, 0.6703822016716003, 0.9445821046829224, 0.27072423696517944, 0.026326775550842285, 0.39488446712493896, 0.817596435546875]  ‚Üí  acq = -0.9946307791303949
X = [0.18772703409194946, 0.5698724985122681, 0.49812501668930054, 0.3492876887321472, 0.04210507869720459, 0.006526827812194824, 0.2068500518798828, 0.9515436887741089, 0.6659542918205261, 0.45626744627952576, 0.13718295097351074, 0.7426033616065979, 0.8587663769721985, 0.6703038811683655, 0.7598890066146851, 0.8735454082489014, 0.10180890560150146, 0.1626366823911667, 0.6423792243003845]  ‚Üí  acq = -0.9946438298113822
X = [0.8900066614151001, 0.32442641258239746, 0.28420430421829224, 0.9018345475196838, 0.5268966555595398, 0.9674438238143921, 0.2684450149536133, 0.9440930485725403, 0.9866945743560791, 0.9079306721687317, 0.8527958989143372, 0.9788607954978943, 0.9893125891685486, 0.4561086893081665, 0.231636643409729, 0.8462718725204468, 0.5441073775291443, 0.5085300207138062, 0.7157379388809204]  ‚Üí  acq = -0.9946391591032049
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.2476, dtype=torch.float64), tensor(0.0619, dtype=torch.float64), tensor(0.0233, dtype=torch.float64), 0, tensor(0.0772, dtype=torch.float64), tensor(0.1805, dtype=torch.float64), 0, tensor(0.1473, dtype=torch.float64), tensor(0.2622, dtype=torch.float64), 29, 1, 1, 1, 0, 1, 46, 0.08978898264472278, 18.08998952854078, 0]
normalized proposed parameters for next round by BO: [tensor(0.2476, dtype=torch.float64), tensor(0.0619, dtype=torch.float64), tensor(0.0233, dtype=torch.float64), tensor(1.0727e-18, dtype=torch.float64), tensor(0.0772, dtype=torch.float64), tensor(0.1805, dtype=torch.float64), tensor(1.4597e-18, dtype=torch.float64), tensor(0.1473, dtype=torch.float64), tensor(0.2622, dtype=torch.float64), tensor(0.9062, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.3607, dtype=torch.float64), tensor(0.8979, dtype=torch.float64), tensor(0.3769, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  23  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.248
  gsm8k: 0.062
  rowan_hellaswag: 0.023
  sciq: 0
  triviaqa: 0.077
  truthfulqa_gen: 0.181
  wikitext: 0
  mmlu: 0.147
  arc_challenge: 0.262

LoRA Parameters:
  lora_r: (46,)
  lora_dropout: (0.08978898264472278,)
  num_layers_to_apply: (29,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (18.08998952854078,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  29
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  46
lora dropout:  0.08978898264472278
lora alpha:  18.08998952854078
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 66,934,784 || all params: 8,097,196,032 || trainable%: 0.8266
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.1973, 'grad_norm': 0.6381586194038391, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.8241550922393799, 'eval_runtime': 5.2852, 'eval_samples_per_second': 189.209, 'eval_steps_per_second': 11.92, 'epoch': 0.04}
{'loss': 1.4383, 'grad_norm': 0.3364085257053375, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1739736795425415, 'eval_runtime': 5.2654, 'eval_samples_per_second': 189.92, 'eval_steps_per_second': 11.965, 'epoch': 0.08}
{'loss': 1.2144, 'grad_norm': 0.262483149766922, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.0926909446716309, 'eval_runtime': 5.279, 'eval_samples_per_second': 189.431, 'eval_steps_per_second': 11.934, 'epoch': 0.12}
{'loss': 1.1296, 'grad_norm': 0.2394784539937973, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.010329246520996, 'eval_runtime': 5.2786, 'eval_samples_per_second': 189.446, 'eval_steps_per_second': 11.935, 'epoch': 0.16}
{'loss': 1.0492, 'grad_norm': 0.28544172644615173, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9584622979164124, 'eval_runtime': 5.2791, 'eval_samples_per_second': 189.426, 'eval_steps_per_second': 11.934, 'epoch': 0.2}
{'loss': 0.9783, 'grad_norm': 0.2158758044242859, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.920353353023529, 'eval_runtime': 5.2806, 'eval_samples_per_second': 189.371, 'eval_steps_per_second': 11.93, 'epoch': 0.24}
{'loss': 0.9847, 'grad_norm': 0.22805248200893402, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9163299798965454, 'eval_runtime': 5.2886, 'eval_samples_per_second': 189.086, 'eval_steps_per_second': 11.912, 'epoch': 0.28}
{'loss': 1.0005, 'grad_norm': 0.20995476841926575, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9183738827705383, 'eval_runtime': 5.285, 'eval_samples_per_second': 189.213, 'eval_steps_per_second': 11.92, 'epoch': 0.32}
{'loss': 0.9588, 'grad_norm': 0.2363310605287552, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9073423743247986, 'eval_runtime': 5.2876, 'eval_samples_per_second': 189.123, 'eval_steps_per_second': 11.915, 'epoch': 0.36}
{'loss': 0.9426, 'grad_norm': 0.32758215069770813, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9094691276550293, 'eval_runtime': 5.2996, 'eval_samples_per_second': 188.693, 'eval_steps_per_second': 11.888, 'epoch': 0.4}
{'loss': 0.9333, 'grad_norm': 0.27329927682876587, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.905518651008606, 'eval_runtime': 5.3023, 'eval_samples_per_second': 188.597, 'eval_steps_per_second': 11.882, 'epoch': 0.44}
{'loss': 0.8885, 'grad_norm': 0.2933664321899414, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.8938735127449036, 'eval_runtime': 5.3212, 'eval_samples_per_second': 187.927, 'eval_steps_per_second': 11.839, 'epoch': 0.48}
{'loss': 0.9647, 'grad_norm': 0.282835990190506, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.8903512954711914, 'eval_runtime': 5.2891, 'eval_samples_per_second': 189.068, 'eval_steps_per_second': 11.911, 'epoch': 0.52}
{'loss': 0.959, 'grad_norm': 0.32647502422332764, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.8895890712738037, 'eval_runtime': 5.2847, 'eval_samples_per_second': 189.225, 'eval_steps_per_second': 11.921, 'epoch': 0.56}
{'loss': 0.877, 'grad_norm': 0.2871084213256836, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.8882574439048767, 'eval_runtime': 5.284, 'eval_samples_per_second': 189.251, 'eval_steps_per_second': 11.923, 'epoch': 0.6}
{'loss': 0.8968, 'grad_norm': 0.31925153732299805, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8865402936935425, 'eval_runtime': 5.2828, 'eval_samples_per_second': 189.293, 'eval_steps_per_second': 11.925, 'epoch': 0.64}
{'loss': 0.9449, 'grad_norm': 0.26931363344192505, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8845438361167908, 'eval_runtime': 5.2865, 'eval_samples_per_second': 189.162, 'eval_steps_per_second': 11.917, 'epoch': 0.68}
{'loss': 0.8905, 'grad_norm': 0.31949952244758606, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8828515410423279, 'eval_runtime': 5.2913, 'eval_samples_per_second': 188.99, 'eval_steps_per_second': 11.906, 'epoch': 0.72}
{'loss': 0.9035, 'grad_norm': 0.33145174384117126, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8802461624145508, 'eval_runtime': 5.29, 'eval_samples_per_second': 189.035, 'eval_steps_per_second': 11.909, 'epoch': 0.76}
{'loss': 0.8946, 'grad_norm': 0.3233647346496582, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8796575665473938, 'eval_runtime': 5.2904, 'eval_samples_per_second': 189.023, 'eval_steps_per_second': 11.908, 'epoch': 0.8}
{'loss': 0.8823, 'grad_norm': 0.28177279233932495, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8788841366767883, 'eval_runtime': 5.2988, 'eval_samples_per_second': 188.724, 'eval_steps_per_second': 11.89, 'epoch': 0.84}
{'loss': 0.8371, 'grad_norm': 0.3484324812889099, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8748776912689209, 'eval_runtime': 5.2803, 'eval_samples_per_second': 189.383, 'eval_steps_per_second': 11.931, 'epoch': 0.88}
{'loss': 0.8895, 'grad_norm': 0.2756989300251007, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8757134079933167, 'eval_runtime': 5.2814, 'eval_samples_per_second': 189.344, 'eval_steps_per_second': 11.929, 'epoch': 0.92}
{'loss': 0.8578, 'grad_norm': 0.27206265926361084, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8765400648117065, 'eval_runtime': 5.2818, 'eval_samples_per_second': 189.328, 'eval_steps_per_second': 11.928, 'epoch': 0.96}
{'loss': 0.8542, 'grad_norm': 0.4385165572166443, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.875694215297699, 'eval_runtime': 5.2914, 'eval_samples_per_second': 188.985, 'eval_steps_per_second': 11.906, 'epoch': 1.0}
{'train_runtime': 335.9747, 'train_samples_per_second': 29.755, 'train_steps_per_second': 1.86, 'train_loss': 1.0546921813964845, 'epoch': 1.0}
train_results:  {'eval_loss': [1.8241550922393799, 1.1739736795425415, 1.0926909446716309, 1.010329246520996, 0.9584622979164124, 0.920353353023529, 0.9163299798965454, 0.9183738827705383, 0.9073423743247986, 0.9094691276550293, 0.905518651008606, 0.8938735127449036, 0.8903512954711914, 0.8895890712738037, 0.8882574439048767, 0.8865402936935425, 0.8845438361167908, 0.8828515410423279, 0.8802461624145508, 0.8796575665473938, 0.8788841366767883, 0.8748776912689209, 0.8757134079933167, 0.8765400648117065, 0.875694215297699], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.8241550922393799, 1.1739736795425415, 1.0926909446716309, 1.010329246520996, 0.9584622979164124, 0.920353353023529, 0.9163299798965454, 0.9183738827705383, 0.9073423743247986, 0.9094691276550293, 0.905518651008606, 0.8938735127449036, 0.8903512954711914, 0.8895890712738037, 0.8882574439048767, 0.8865402936935425, 0.8845438361167908, 0.8828515410423279, 0.8802461624145508, 0.8796575665473938, 0.8788841366767883, 0.8748776912689209, 0.8757134079933167, 0.8765400648117065, 0.875694215297699]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9815524220466614
current iteration best possible eval_loss (full train run):  -0.875694215297699
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796, -0.9815524220466614]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 6.8181 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8550962209701538, 0.9847139120101929, 0.4367312788963318, 0.05751532316207886, 0.24003762006759644, 0.4202191233634949, 0.2928928732872009, 0.816238522529602, 0.9171960949897766, 0.8883829712867737, 0.6117203235626221, 0.26643162965774536, 0.19661879539489746, 0.44116103649139404, 0.2139490246772766, 0.900454580783844, 0.11642980575561523, 0.13730639219284058, 0.20953714847564697]  ‚Üí  acq = -0.9931702159439906
X = [0.17066329717636108, 0.19292670488357544, 0.6832596659660339, 0.2928600311279297, 0.1800115704536438, 0.8647443652153015, 0.29678642749786377, 0.9472507238388062, 0.8764203786849976, 0.931416928768158, 0.05130273103713989, 0.8308997750282288, 0.1693008542060852, 0.6540895104408264, 0.18004006147384644, 0.9249464869499207, 0.9654239416122437, 0.1982581913471222, 0.6849347949028015]  ‚Üí  acq = -0.9931306205426228
X = [0.25909268856048584, 0.441548228263855, 0.954920768737793, 0.9094239473342896, 0.07574361562728882, 0.7251740097999573, 0.20533788204193115, 0.28760480880737305, 0.9090394377708435, 0.29381248354911804, 0.651909351348877, 0.08008641004562378, 0.12545561790466309, 0.017686307430267334, 0.6907520294189453, 0.10822603851556778, 0.8972193002700806, 0.6298680305480957, 0.16254359483718872]  ‚Üí  acq = -0.9931480380570579
X = [0.9556276798248291, 0.5240601301193237, 0.3295055627822876, 0.8211559057235718, 0.18214941024780273, 0.19318467378616333, 0.28041762113571167, 0.4059585928916931, 0.8458959460258484, 0.5235821008682251, 0.19148892164230347, 0.09057146310806274, 0.4766210913658142, 0.710713267326355, 0.002808213233947754, 0.6680919528007507, 0.5655561685562134, 0.34631481766700745, 0.7355300188064575]  ‚Üí  acq = -0.9951064035456382
X = [0.26506900787353516, 0.26607489585876465, 0.28361159563064575, 0.6710712909698486, 0.9180149435997009, 0.51537024974823, 0.6614297032356262, 0.7947990298271179, 0.7881297469139099, 0.14556428790092468, 0.3178449273109436, 0.6809708476066589, 0.9541901350021362, 0.05764073133468628, 0.0027261972427368164, 0.3900866210460663, 0.7018656134605408, 0.5995568037033081, 0.45656460523605347]  ‚Üí  acq = -0.993149989462096
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.0895, dtype=torch.float64), tensor(0.3795, dtype=torch.float64), 0, 0, tensor(0.0295, dtype=torch.float64), tensor(0.1208, dtype=torch.float64), 0, tensor(0.1402, dtype=torch.float64), tensor(0.2405, dtype=torch.float64), 28, 1, 1, 1, 0, 1, 19, 0.060295594692041335, 11.556082610737949, 0]
normalized proposed parameters for next round by BO: [tensor(0.0895, dtype=torch.float64), tensor(0.3795, dtype=torch.float64), tensor(1.0918e-17, dtype=torch.float64), tensor(3.7626e-18, dtype=torch.float64), tensor(0.0295, dtype=torch.float64), tensor(0.1208, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1402, dtype=torch.float64), tensor(0.2405, dtype=torch.float64), tensor(0.8794, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1495, dtype=torch.float64), tensor(0.6030, dtype=torch.float64), tensor(0.2408, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  24  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.09
  gsm8k: 0.379
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.03
  truthfulqa_gen: 0.121
  wikitext: 0
  mmlu: 0.14
  arc_challenge: 0.241

LoRA Parameters:
  lora_r: (19,)
  lora_dropout: (0.060295594692041335,)
  num_layers_to_apply: (28,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (11.556082610737949,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  28
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  19
lora dropout:  0.060295594692041335
lora alpha:  11.556082610737949
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 26,693,632 || all params: 8,056,954,880 || trainable%: 0.3313
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.6919, 'grad_norm': 0.471140593290329, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.4061102867126465, 'eval_runtime': 5.336, 'eval_samples_per_second': 187.406, 'eval_steps_per_second': 11.807, 'epoch': 0.04}
{'loss': 1.3451, 'grad_norm': 0.2739131450653076, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.2717770338058472, 'eval_runtime': 5.3434, 'eval_samples_per_second': 187.147, 'eval_steps_per_second': 11.79, 'epoch': 0.08}
{'loss': 1.1036, 'grad_norm': 0.2391650229692459, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1843860149383545, 'eval_runtime': 5.3434, 'eval_samples_per_second': 187.147, 'eval_steps_per_second': 11.79, 'epoch': 0.12}
{'loss': 1.0719, 'grad_norm': 0.2774822413921356, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1172834634780884, 'eval_runtime': 5.3536, 'eval_samples_per_second': 186.789, 'eval_steps_per_second': 11.768, 'epoch': 0.16}
{'loss': 1.0222, 'grad_norm': 0.24282778799533844, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.058706521987915, 'eval_runtime': 5.3546, 'eval_samples_per_second': 186.757, 'eval_steps_per_second': 11.766, 'epoch': 0.2}
{'loss': 1.01, 'grad_norm': 0.3057821989059448, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.009186863899231, 'eval_runtime': 5.3537, 'eval_samples_per_second': 186.787, 'eval_steps_per_second': 11.768, 'epoch': 0.24}
{'loss': 0.9462, 'grad_norm': 0.25819069147109985, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.961059033870697, 'eval_runtime': 5.349, 'eval_samples_per_second': 186.95, 'eval_steps_per_second': 11.778, 'epoch': 0.28}
{'loss': 0.936, 'grad_norm': 0.2239253968000412, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9779045581817627, 'eval_runtime': 5.3393, 'eval_samples_per_second': 187.29, 'eval_steps_per_second': 11.799, 'epoch': 0.32}
{'loss': 0.8934, 'grad_norm': 0.32781314849853516, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9578214287757874, 'eval_runtime': 5.3411, 'eval_samples_per_second': 187.226, 'eval_steps_per_second': 11.795, 'epoch': 0.36}
{'loss': 0.9294, 'grad_norm': 0.2449328452348709, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9479452967643738, 'eval_runtime': 5.3521, 'eval_samples_per_second': 186.841, 'eval_steps_per_second': 11.771, 'epoch': 0.4}
{'loss': 0.8888, 'grad_norm': 0.24740564823150635, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9508452415466309, 'eval_runtime': 5.3383, 'eval_samples_per_second': 187.327, 'eval_steps_per_second': 11.802, 'epoch': 0.44}
{'loss': 0.9089, 'grad_norm': 0.22991366684436798, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9458659887313843, 'eval_runtime': 5.3472, 'eval_samples_per_second': 187.014, 'eval_steps_per_second': 11.782, 'epoch': 0.48}
{'loss': 0.8891, 'grad_norm': 0.24606357514858246, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9344035387039185, 'eval_runtime': 5.3425, 'eval_samples_per_second': 187.179, 'eval_steps_per_second': 11.792, 'epoch': 0.52}
{'loss': 0.8951, 'grad_norm': 0.2942383289337158, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9399411082267761, 'eval_runtime': 5.3549, 'eval_samples_per_second': 186.744, 'eval_steps_per_second': 11.765, 'epoch': 0.56}
{'loss': 0.849, 'grad_norm': 0.2894769012928009, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9325809478759766, 'eval_runtime': 5.3388, 'eval_samples_per_second': 187.31, 'eval_steps_per_second': 11.801, 'epoch': 0.6}
{'loss': 0.8938, 'grad_norm': 0.2790719270706177, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9304092526435852, 'eval_runtime': 5.3642, 'eval_samples_per_second': 186.423, 'eval_steps_per_second': 11.745, 'epoch': 0.64}
{'loss': 0.8747, 'grad_norm': 0.240408793091774, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9292314648628235, 'eval_runtime': 5.368, 'eval_samples_per_second': 186.289, 'eval_steps_per_second': 11.736, 'epoch': 0.68}
{'loss': 0.8968, 'grad_norm': 0.29580363631248474, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9276557564735413, 'eval_runtime': 5.3661, 'eval_samples_per_second': 186.357, 'eval_steps_per_second': 11.74, 'epoch': 0.72}
{'loss': 0.8457, 'grad_norm': 0.29122811555862427, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9252990484237671, 'eval_runtime': 5.3672, 'eval_samples_per_second': 186.316, 'eval_steps_per_second': 11.738, 'epoch': 0.76}
{'loss': 0.8721, 'grad_norm': 0.28726062178611755, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9308506846427917, 'eval_runtime': 5.3619, 'eval_samples_per_second': 186.501, 'eval_steps_per_second': 11.75, 'epoch': 0.8}
{'loss': 0.8368, 'grad_norm': 0.2921818792819977, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9220225811004639, 'eval_runtime': 5.3593, 'eval_samples_per_second': 186.593, 'eval_steps_per_second': 11.755, 'epoch': 0.84}
{'loss': 0.8429, 'grad_norm': 0.2972123622894287, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9208625555038452, 'eval_runtime': 5.3596, 'eval_samples_per_second': 186.581, 'eval_steps_per_second': 11.755, 'epoch': 0.88}
{'loss': 0.8207, 'grad_norm': 0.28709447383880615, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9207383990287781, 'eval_runtime': 5.3597, 'eval_samples_per_second': 186.577, 'eval_steps_per_second': 11.754, 'epoch': 0.92}
{'loss': 0.8378, 'grad_norm': 0.29602447152137756, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9215894341468811, 'eval_runtime': 5.3731, 'eval_samples_per_second': 186.113, 'eval_steps_per_second': 11.725, 'epoch': 0.96}
{'loss': 0.8265, 'grad_norm': 0.347833514213562, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.9216276407241821, 'eval_runtime': 5.4343, 'eval_samples_per_second': 184.018, 'eval_steps_per_second': 11.593, 'epoch': 1.0}
{'train_runtime': 367.9142, 'train_samples_per_second': 27.172, 'train_steps_per_second': 1.699, 'train_loss': 0.9971402496337891, 'epoch': 1.0}
train_results:  {'eval_loss': [2.4061102867126465, 1.2717770338058472, 1.1843860149383545, 1.1172834634780884, 1.058706521987915, 1.009186863899231, 0.961059033870697, 0.9779045581817627, 0.9578214287757874, 0.9479452967643738, 0.9508452415466309, 0.9458659887313843, 0.9344035387039185, 0.9399411082267761, 0.9325809478759766, 0.9304092526435852, 0.9292314648628235, 0.9276557564735413, 0.9252990484237671, 0.9308506846427917, 0.9220225811004639, 0.9208625555038452, 0.9207383990287781, 0.9215894341468811, 0.9216276407241821], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.4061102867126465, 1.2717770338058472, 1.1843860149383545, 1.1172834634780884, 1.058706521987915, 1.009186863899231, 0.961059033870697, 0.9779045581817627, 0.9578214287757874, 0.9479452967643738, 0.9508452415466309, 0.9458659887313843, 0.9344035387039185, 0.9399411082267761, 0.9325809478759766, 0.9304092526435852, 0.9292314648628235, 0.9276557564735413, 0.9252990484237671, 0.9308506846427917, 0.9220225811004639, 0.9208625555038452, 0.9207383990287781, 0.9215894341468811, 0.9216276407241821]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9423834085464478
current iteration best possible eval_loss (full train run):  -0.9216276407241821
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796, -0.9815524220466614, -0.9423834085464478]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 13.7793 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8698185682296753, 0.5119956731796265, 0.602379560470581, 0.47692549228668213, 0.19846570491790771, 0.9742437601089478, 0.9742191433906555, 0.3347463607788086, 0.6549691557884216, 0.824859082698822, 0.7135453224182129, 0.8653855323791504, 0.743358314037323, 0.3226756453514099, 0.355441689491272, 0.42920219898223877, 0.45111382007598877, 0.3757714629173279, 0.098782479763031]  ‚Üí  acq = -1.0038664684690135
X = [0.9932886362075806, 0.7287918329238892, 0.45022910833358765, 0.24317121505737305, 0.5785284042358398, 0.15285080671310425, 0.3036497235298157, 0.3416205644607544, 0.8672244548797607, 0.5292837023735046, 0.7168238759040833, 0.5323895215988159, 0.5231084227561951, 0.9802610278129578, 0.5262150168418884, 0.6125524044036865, 0.313107967376709, 0.25588956475257874, 0.03715479373931885]  ‚Üí  acq = -1.0046913889229123
X = [0.5659990310668945, 0.2688130736351013, 0.24113255739212036, 0.16683202981948853, 0.48615360260009766, 0.7162089347839355, 0.0010988116264343262, 0.3778548836708069, 0.9447525143623352, 0.42882978916168213, 0.17042183876037598, 0.08974480628967285, 0.23191064596176147, 0.9482576847076416, 0.21524584293365479, 0.2189868837594986, 0.15074169635772705, 0.5687676668167114, 0.7731989026069641]  ‚Üí  acq = -1.0052837740884075
X = [0.8106231689453125, 0.6845783591270447, 0.7733466625213623, 0.026730716228485107, 0.43211013078689575, 0.07046419382095337, 0.7029524445533752, 0.6063298583030701, 0.3806919455528259, 0.9444905519485474, 0.17238521575927734, 0.324030339717865, 0.5392807722091675, 0.7099223732948303, 0.5437468886375427, 0.6186452507972717, 0.9093855619430542, 0.7461531162261963, 0.9890440702438354]  ‚Üí  acq = -1.0038661315761672
X = [0.23856782913208008, 0.6098546981811523, 0.957812488079071, 0.10195112228393555, 0.9931964874267578, 0.37048768997192383, 0.46233826875686646, 0.401641309261322, 0.9630946516990662, 0.6271727085113525, 0.014934837818145752, 0.9937937259674072, 0.3518297076225281, 0.8314701914787292, 0.3034810423851013, 0.020147601142525673, 0.012760698795318604, 0.9845035076141357, 0.9811075925827026]  ‚Üí  acq = -1.0038664498309273
proposed candidate layer mask is:  tensor([0., 0., 1., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1466, dtype=torch.float64), tensor(0.2760, dtype=torch.float64), 0, tensor(0.0579, dtype=torch.float64), tensor(0.1289, dtype=torch.float64), tensor(0.0626, dtype=torch.float64), tensor(0.0222, dtype=torch.float64), tensor(0.0828, dtype=torch.float64), tensor(0.2186, dtype=torch.float64), 25, 0, 0, 1, 1, 1, 69, 0.05578677306194388, 23.755155306654864, 1]
normalized proposed parameters for next round by BO: [tensor(0.1466, dtype=torch.float64), tensor(0.2760, dtype=torch.float64), tensor(0.0043, dtype=torch.float64), tensor(0.0579, dtype=torch.float64), tensor(0.1289, dtype=torch.float64), tensor(0.0626, dtype=torch.float64), tensor(0.0222, dtype=torch.float64), tensor(0.0828, dtype=torch.float64), tensor(0.2186, dtype=torch.float64), tensor(0.7946, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.5403, dtype=torch.float64), tensor(0.5579, dtype=torch.float64), tensor(0.4949, dtype=torch.float64), tensor(1., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  25  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.147
  gsm8k: 0.276
  rowan_hellaswag: 0
  sciq: 0.058
  triviaqa: 0.129
  truthfulqa_gen: 0.063
  wikitext: 0.022
  mmlu: 0.083
  arc_challenge: 0.219

LoRA Parameters:
  lora_r: (69,)
  lora_dropout: (0.05578677306194388,)
  num_layers_to_apply: (25,)
  five_dim_vector: ([0, 0, 1, 1, 1],)
  lora_alpha: (23.755155306654864,)
  lora_reverse: (1,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  25
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 0, 1, 1, 1]
lora rank:  69
lora dropout:  0.05578677306194388
lora alpha:  23.755155306654864
lora reverse (apply to rear layers if False, else front layers):  1
trainable params: 95,385,600 || all params: 8,125,646,848 || trainable%: 1.1739
length of training data:  9953
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7051, 'grad_norm': 1.324347734451294, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7378787994384766, 'eval_runtime': 5.4793, 'eval_samples_per_second': 182.505, 'eval_steps_per_second': 11.498, 'epoch': 0.04}
{'loss': 1.2164, 'grad_norm': 0.5579282641410828, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1474153995513916, 'eval_runtime': 5.4995, 'eval_samples_per_second': 181.834, 'eval_steps_per_second': 11.456, 'epoch': 0.08}
{'loss': 1.1122, 'grad_norm': 0.1799037754535675, 'learning_rate': 0.00028743455497382193, 'epoch': 0.12}
{'eval_loss': 1.0495952367782593, 'eval_runtime': 5.483, 'eval_samples_per_second': 182.381, 'eval_steps_per_second': 11.49, 'epoch': 0.12}
{'loss': 0.9975, 'grad_norm': 0.20803755521774292, 'learning_rate': 0.0002743455497382199, 'epoch': 0.16}
{'eval_loss': 1.0039154291152954, 'eval_runtime': 5.4871, 'eval_samples_per_second': 182.245, 'eval_steps_per_second': 11.481, 'epoch': 0.16}
{'loss': 0.9892, 'grad_norm': 0.22120843827724457, 'learning_rate': 0.00026125654450261777, 'epoch': 0.2}
{'eval_loss': 0.953519880771637, 'eval_runtime': 5.4892, 'eval_samples_per_second': 182.175, 'eval_steps_per_second': 11.477, 'epoch': 0.2}
{'loss': 0.9846, 'grad_norm': 0.3284337520599365, 'learning_rate': 0.0002481675392670157, 'epoch': 0.24}
{'eval_loss': 0.9387263059616089, 'eval_runtime': 5.5209, 'eval_samples_per_second': 181.128, 'eval_steps_per_second': 11.411, 'epoch': 0.24}
{'loss': 0.9238, 'grad_norm': 0.3004903495311737, 'learning_rate': 0.00023507853403141357, 'epoch': 0.28}
{'eval_loss': 0.9413551092147827, 'eval_runtime': 5.4701, 'eval_samples_per_second': 182.811, 'eval_steps_per_second': 11.517, 'epoch': 0.28}
{'loss': 0.9267, 'grad_norm': 0.24927501380443573, 'learning_rate': 0.00022198952879581152, 'epoch': 0.32}
{'eval_loss': 0.9320472478866577, 'eval_runtime': 5.4673, 'eval_samples_per_second': 182.905, 'eval_steps_per_second': 11.523, 'epoch': 0.32}
{'loss': 0.9376, 'grad_norm': 0.21146750450134277, 'learning_rate': 0.0002089005235602094, 'epoch': 0.36}
{'eval_loss': 0.9278488755226135, 'eval_runtime': 5.4644, 'eval_samples_per_second': 183.004, 'eval_steps_per_second': 11.529, 'epoch': 0.36}
{'loss': 0.9403, 'grad_norm': 0.22406470775604248, 'learning_rate': 0.0001958115183246073, 'epoch': 0.4}
{'eval_loss': 0.9147511124610901, 'eval_runtime': 5.4533, 'eval_samples_per_second': 183.374, 'eval_steps_per_second': 11.553, 'epoch': 0.4}
{'loss': 0.9443, 'grad_norm': 0.18956059217453003, 'learning_rate': 0.00018272251308900522, 'epoch': 0.44}
{'eval_loss': 0.914375364780426, 'eval_runtime': 5.4927, 'eval_samples_per_second': 182.061, 'eval_steps_per_second': 11.47, 'epoch': 0.44}
{'loss': 0.8898, 'grad_norm': 0.17918133735656738, 'learning_rate': 0.00016963350785340314, 'epoch': 0.48}
{'eval_loss': 0.9141027927398682, 'eval_runtime': 5.4836, 'eval_samples_per_second': 182.362, 'eval_steps_per_second': 11.489, 'epoch': 0.48}
{'loss': 0.9217, 'grad_norm': 0.17222295701503754, 'learning_rate': 0.00015654450261780103, 'epoch': 0.52}
{'eval_loss': 0.9094973206520081, 'eval_runtime': 5.4756, 'eval_samples_per_second': 182.628, 'eval_steps_per_second': 11.506, 'epoch': 0.52}
{'loss': 0.898, 'grad_norm': 0.23923154175281525, 'learning_rate': 0.00014345549738219895, 'epoch': 0.56}
{'eval_loss': 0.9065086841583252, 'eval_runtime': 5.497, 'eval_samples_per_second': 181.917, 'eval_steps_per_second': 11.461, 'epoch': 0.56}
{'loss': 0.8801, 'grad_norm': 0.20348377525806427, 'learning_rate': 0.00013036649214659686, 'epoch': 0.6}
{'eval_loss': 0.9073584079742432, 'eval_runtime': 5.4724, 'eval_samples_per_second': 182.736, 'eval_steps_per_second': 11.512, 'epoch': 0.6}
{'loss': 0.8651, 'grad_norm': 0.21626730263233185, 'learning_rate': 0.00011727748691099475, 'epoch': 0.64}
{'eval_loss': 0.9067128896713257, 'eval_runtime': 5.4703, 'eval_samples_per_second': 182.805, 'eval_steps_per_second': 11.517, 'epoch': 0.64}
{'loss': 0.8763, 'grad_norm': 0.23415939509868622, 'learning_rate': 0.00010418848167539266, 'epoch': 0.68}
{'eval_loss': 0.8991929888725281, 'eval_runtime': 5.4562, 'eval_samples_per_second': 183.278, 'eval_steps_per_second': 11.547, 'epoch': 0.68}
{'loss': 0.8839, 'grad_norm': 0.19548703730106354, 'learning_rate': 9.109947643979056e-05, 'epoch': 0.72}
{'eval_loss': 0.901425302028656, 'eval_runtime': 5.4405, 'eval_samples_per_second': 183.806, 'eval_steps_per_second': 11.58, 'epoch': 0.72}
{'loss': 0.8791, 'grad_norm': 0.21930789947509766, 'learning_rate': 7.801047120418848e-05, 'epoch': 0.76}
{'eval_loss': 0.9016359448432922, 'eval_runtime': 5.4426, 'eval_samples_per_second': 183.736, 'eval_steps_per_second': 11.575, 'epoch': 0.76}
{'loss': 0.8843, 'grad_norm': 0.23463349044322968, 'learning_rate': 6.492146596858637e-05, 'epoch': 0.8}
{'eval_loss': 0.8979175686836243, 'eval_runtime': 5.4388, 'eval_samples_per_second': 183.865, 'eval_steps_per_second': 11.583, 'epoch': 0.8}
{'loss': 0.8841, 'grad_norm': 0.27073973417282104, 'learning_rate': 5.1832460732984284e-05, 'epoch': 0.84}
{'eval_loss': 0.8956593871116638, 'eval_runtime': 5.439, 'eval_samples_per_second': 183.857, 'eval_steps_per_second': 11.583, 'epoch': 0.84}
{'loss': 0.8919, 'grad_norm': 0.3439942002296448, 'learning_rate': 3.8743455497382195e-05, 'epoch': 0.88}
{'eval_loss': 0.8947622179985046, 'eval_runtime': 5.4448, 'eval_samples_per_second': 183.661, 'eval_steps_per_second': 11.571, 'epoch': 0.88}
{'loss': 0.8731, 'grad_norm': 0.2622697949409485, 'learning_rate': 2.5654450261780103e-05, 'epoch': 0.92}
{'eval_loss': 0.8927967548370361, 'eval_runtime': 5.4524, 'eval_samples_per_second': 183.407, 'eval_steps_per_second': 11.555, 'epoch': 0.92}
{'loss': 0.8285, 'grad_norm': 0.21634945273399353, 'learning_rate': 1.256544502617801e-05, 'epoch': 0.96}
{'eval_loss': 0.8933585286140442, 'eval_runtime': 5.4407, 'eval_samples_per_second': 183.8, 'eval_steps_per_second': 11.579, 'epoch': 0.96}
{'train_runtime': 370.6406, 'train_samples_per_second': 26.854, 'train_steps_per_second': 1.681, 'train_loss': 0.9998142554710229, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7378787994384766, 1.1474153995513916, 1.0495952367782593, 1.0039154291152954, 0.953519880771637, 0.9387263059616089, 0.9413551092147827, 0.9320472478866577, 0.9278488755226135, 0.9147511124610901, 0.914375364780426, 0.9141027927398682, 0.9094973206520081, 0.9065086841583252, 0.9073584079742432, 0.9067128896713257, 0.8991929888725281, 0.901425302028656, 0.9016359448432922, 0.8979175686836243, 0.8956593871116638, 0.8947622179985046, 0.8927967548370361, 0.8933585286140442], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [1.7378787994384766, 1.1474153995513916, 1.0495952367782593, 1.0039154291152954, 0.953519880771637, 0.9387263059616089, 0.9413551092147827, 0.9320472478866577, 0.9278488755226135, 0.9147511124610901, 0.914375364780426, 0.9141027927398682, 0.9094973206520081, 0.9065086841583252, 0.9073584079742432, 0.9067128896713257, 0.8991929888725281, 0.901425302028656, 0.9016359448432922, 0.8979175686836243, 0.8956593871116638, 0.8947622179985046, 0.8927967548370361, 0.8933585286140442]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.0522664785385132
current iteration best possible eval_loss (full train run):  -0.8933585286140442
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796, -0.9815524220466614, -0.9423834085464478, -1.0522664785385132]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9560 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.038794755935668945, 0.5530740022659302, 0.9659146070480347, 0.9924764633178711, 0.5337935090065002, 0.8522648215293884, 0.2538560628890991, 0.03790736198425293, 0.12087494134902954, 0.32418566942214966, 0.03446310758590698, 0.8077076077461243, 0.5240886211395264, 0.26980793476104736, 0.19171595573425293, 0.5978026986122131, 0.24681228399276733, 0.3197188079357147, 0.43591630458831787]  ‚Üí  acq = -1.0668251983864314
X = [0.06298112869262695, 0.4111078381538391, 0.990811824798584, 0.8854760527610779, 0.5448755025863647, 0.12630784511566162, 0.6236385703086853, 0.21763485670089722, 0.0575137734413147, 0.2800779640674591, 0.1305062174797058, 0.7458388209342957, 0.0784522294998169, 0.4153570532798767, 0.5576200485229492, 0.2502773702144623, 0.26675117015838623, 0.721631646156311, 0.1087694764137268]  ‚Üí  acq = -1.0668234632406621
X = [0.7721713781356812, 0.36252284049987793, 0.10557281970977783, 0.4819630980491638, 0.40283071994781494, 0.5137096047401428, 0.3549082279205322, 0.57498699426651, 0.6754608750343323, 0.8363065719604492, 0.2568463683128357, 0.6493399143218994, 0.8961844444274902, 0.4917372465133667, 0.469235897064209, 0.7879849672317505, 0.044145405292510986, 0.17305481433868408, 0.9866487383842468]  ‚Üí  acq = -1.0672913172380145
X = [0.5755511522293091, 0.49270403385162354, 0.01341170072555542, 0.1392582654953003, 0.5176948308944702, 0.38125747442245483, 0.713839590549469, 0.11993622779846191, 0.6937973499298096, 0.2550579309463501, 0.3171401023864746, 0.8667199611663818, 0.6704480648040771, 0.16916072368621826, 0.742415189743042, 0.6549527049064636, 0.34602898359298706, 0.037224020808935165, 0.7405623197555542]  ‚Üí  acq = -1.0650673817065255
X = [0.7848239541053772, 0.7650286555290222, 0.6905014514923096, 0.5621405243873596, 0.0999937653541565, 0.15589159727096558, 0.42336052656173706, 0.6475326418876648, 0.474023699760437, 0.2828661799430847, 0.7089584469795227, 0.5923287272453308, 0.13956528902053833, 0.8958969116210938, 0.7650451064109802, 0.9000691771507263, 0.3606852889060974, 0.7490195035934448, 0.591159462928772]  ‚Üí  acq = -1.0668234739498892
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1098, dtype=torch.float64), tensor(0.2029, dtype=torch.float64), 0, 0, tensor(0.0751, dtype=torch.float64), tensor(0.3790, dtype=torch.float64), 0, tensor(0.1306, dtype=torch.float64), tensor(0.1026, dtype=torch.float64), 30, 0, 1, 0, 1, 1, 28, 0.07556832638185083, 36.54846065229333, 0]
normalized proposed parameters for next round by BO: [tensor(0.1098, dtype=torch.float64), tensor(0.2029, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.1901e-19, dtype=torch.float64), tensor(0.0751, dtype=torch.float64), tensor(0.3790, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1306, dtype=torch.float64), tensor(0.1026, dtype=torch.float64), tensor(0.9335, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.2183, dtype=torch.float64), tensor(0.7557, dtype=torch.float64), tensor(0.7614, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  26  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.11
  gsm8k: 0.203
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.075
  truthfulqa_gen: 0.379
  wikitext: 0
  mmlu: 0.131
  arc_challenge: 0.103

LoRA Parameters:
  lora_r: (28,)
  lora_dropout: (0.07556832638185083,)
  num_layers_to_apply: (30,)
  five_dim_vector: ([0, 1, 0, 1, 1],)
  lora_alpha: (36.54846065229333,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  30
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [0, 1, 0, 1, 1]
lora rank:  28
lora dropout:  0.07556832638185083
lora alpha:  36.54846065229333
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 35,266,560 || all params: 8,065,527,808 || trainable%: 0.4373
length of training data:  9998
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7154, 'grad_norm': 1.5073187351226807, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.547485113143921, 'eval_runtime': 5.1511, 'eval_samples_per_second': 194.132, 'eval_steps_per_second': 12.23, 'epoch': 0.04}
{'loss': 1.0748, 'grad_norm': 0.4918559789657593, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0438036918640137, 'eval_runtime': 5.146, 'eval_samples_per_second': 194.324, 'eval_steps_per_second': 12.242, 'epoch': 0.08}
{'loss': 0.9363, 'grad_norm': 0.4338906407356262, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.984849750995636, 'eval_runtime': 5.1515, 'eval_samples_per_second': 194.118, 'eval_steps_per_second': 12.229, 'epoch': 0.12}
{'loss': 0.9641, 'grad_norm': 0.36893585324287415, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.9815953969955444, 'eval_runtime': 5.1663, 'eval_samples_per_second': 193.563, 'eval_steps_per_second': 12.194, 'epoch': 0.16}
{'loss': 0.9245, 'grad_norm': 0.41928258538246155, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.9566425681114197, 'eval_runtime': 5.1562, 'eval_samples_per_second': 193.943, 'eval_steps_per_second': 12.218, 'epoch': 0.2}
{'loss': 0.8964, 'grad_norm': 0.36717692017555237, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9585072994232178, 'eval_runtime': 5.1657, 'eval_samples_per_second': 193.584, 'eval_steps_per_second': 12.196, 'epoch': 0.24}
{'loss': 0.8584, 'grad_norm': 0.4287188947200775, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9395673871040344, 'eval_runtime': 5.1684, 'eval_samples_per_second': 193.483, 'eval_steps_per_second': 12.189, 'epoch': 0.28}
{'loss': 0.8584, 'grad_norm': 0.3719782531261444, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9367689490318298, 'eval_runtime': 5.1719, 'eval_samples_per_second': 193.354, 'eval_steps_per_second': 12.181, 'epoch': 0.32}
{'loss': 0.8237, 'grad_norm': 0.3910108506679535, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9233020544052124, 'eval_runtime': 5.2117, 'eval_samples_per_second': 191.875, 'eval_steps_per_second': 12.088, 'epoch': 0.36}
{'loss': 0.8376, 'grad_norm': 0.34862038493156433, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9276303648948669, 'eval_runtime': 5.2378, 'eval_samples_per_second': 190.92, 'eval_steps_per_second': 12.028, 'epoch': 0.4}
{'loss': 0.8357, 'grad_norm': 0.38965749740600586, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9246494770050049, 'eval_runtime': 5.2384, 'eval_samples_per_second': 190.898, 'eval_steps_per_second': 12.027, 'epoch': 0.44}
{'loss': 0.8035, 'grad_norm': 0.3645731508731842, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9143252968788147, 'eval_runtime': 5.2215, 'eval_samples_per_second': 191.516, 'eval_steps_per_second': 12.065, 'epoch': 0.48}
{'loss': 0.8197, 'grad_norm': 0.460326224565506, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9125708937644958, 'eval_runtime': 5.2443, 'eval_samples_per_second': 190.682, 'eval_steps_per_second': 12.013, 'epoch': 0.52}
{'loss': 0.8082, 'grad_norm': 0.6271556615829468, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9197661280632019, 'eval_runtime': 5.2423, 'eval_samples_per_second': 190.756, 'eval_steps_per_second': 12.018, 'epoch': 0.56}
{'loss': 0.8391, 'grad_norm': 0.40471315383911133, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.912495493888855, 'eval_runtime': 5.2624, 'eval_samples_per_second': 190.027, 'eval_steps_per_second': 11.972, 'epoch': 0.6}
{'loss': 0.8049, 'grad_norm': 0.7191171050071716, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9026227593421936, 'eval_runtime': 5.2265, 'eval_samples_per_second': 191.331, 'eval_steps_per_second': 12.054, 'epoch': 0.64}
{'loss': 0.7537, 'grad_norm': 0.388942152261734, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9070454239845276, 'eval_runtime': 5.2177, 'eval_samples_per_second': 191.656, 'eval_steps_per_second': 12.074, 'epoch': 0.68}
{'loss': 0.7724, 'grad_norm': 0.35150179266929626, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9041776061058044, 'eval_runtime': 5.213, 'eval_samples_per_second': 191.829, 'eval_steps_per_second': 12.085, 'epoch': 0.72}
{'loss': 0.7586, 'grad_norm': 0.4048979580402374, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9020785093307495, 'eval_runtime': 5.2134, 'eval_samples_per_second': 191.813, 'eval_steps_per_second': 12.084, 'epoch': 0.76}
{'loss': 0.7348, 'grad_norm': 0.35807958245277405, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8992260694503784, 'eval_runtime': 5.2111, 'eval_samples_per_second': 191.896, 'eval_steps_per_second': 12.089, 'epoch': 0.8}
{'loss': 0.7542, 'grad_norm': 0.4376470148563385, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8975939750671387, 'eval_runtime': 5.1817, 'eval_samples_per_second': 192.988, 'eval_steps_per_second': 12.158, 'epoch': 0.84}
{'loss': 0.7583, 'grad_norm': 0.45023366808891296, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.896207869052887, 'eval_runtime': 5.2001, 'eval_samples_per_second': 192.305, 'eval_steps_per_second': 12.115, 'epoch': 0.88}
{'loss': 0.767, 'grad_norm': 0.4876619279384613, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8946987390518188, 'eval_runtime': 5.2045, 'eval_samples_per_second': 192.141, 'eval_steps_per_second': 12.105, 'epoch': 0.92}
{'loss': 0.7623, 'grad_norm': 0.46675577759742737, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.894856870174408, 'eval_runtime': 5.1955, 'eval_samples_per_second': 192.474, 'eval_steps_per_second': 12.126, 'epoch': 0.96}
{'loss': 0.8012, 'grad_norm': 0.42186856269836426, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.8949460387229919, 'eval_runtime': 5.1973, 'eval_samples_per_second': 192.408, 'eval_steps_per_second': 12.122, 'epoch': 1.0}
{'train_runtime': 347.6241, 'train_samples_per_second': 28.761, 'train_steps_per_second': 1.798, 'train_loss': 0.9065291534423828, 'epoch': 1.0}
train_results:  {'eval_loss': [1.547485113143921, 1.0438036918640137, 0.984849750995636, 0.9815953969955444, 0.9566425681114197, 0.9585072994232178, 0.9395673871040344, 0.9367689490318298, 0.9233020544052124, 0.9276303648948669, 0.9246494770050049, 0.9143252968788147, 0.9125708937644958, 0.9197661280632019, 0.912495493888855, 0.9026227593421936, 0.9070454239845276, 0.9041776061058044, 0.9020785093307495, 0.8992260694503784, 0.8975939750671387, 0.896207869052887, 0.8946987390518188, 0.894856870174408, 0.8949460387229919], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.547485113143921, 1.0438036918640137, 0.984849750995636, 0.9815953969955444, 0.9566425681114197, 0.9585072994232178, 0.9395673871040344, 0.9367689490318298, 0.9233020544052124, 0.9276303648948669, 0.9246494770050049, 0.9143252968788147, 0.9125708937644958, 0.9197661280632019, 0.912495493888855, 0.9026227593421936, 0.9070454239845276, 0.9041776061058044, 0.9020785093307495, 0.8992260694503784, 0.8975939750671387, 0.896207869052887, 0.8946987390518188, 0.894856870174408, 0.8949460387229919]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.1252762079238892
current iteration best possible eval_loss (full train run):  -0.8949460387229919
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796, -0.9815524220466614, -0.9423834085464478, -1.0522664785385132, -1.1252762079238892]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 2.9946 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.7903437614440918, 0.8047296404838562, 0.13228046894073486, 0.41512149572372437, 0.0015775561332702637, 0.7393354177474976, 0.39104926586151123, 0.628807783126831, 0.6647308468818665, 0.9456225037574768, 0.45014268159866333, 0.35209107398986816, 0.9718325138092041, 0.46064722537994385, 0.5915921330451965, 0.5574356913566589, 0.236403226852417, 0.7948049306869507, 0.0865660309791565]  ‚Üí  acq = -1.076176081374687
X = [0.7500212788581848, 0.7434861660003662, 0.8264944553375244, 0.1466771364212036, 0.8510677218437195, 0.9619389772415161, 0.49168217182159424, 0.026700198650360107, 0.476356565952301, 0.5180422067642212, 0.0625949501991272, 0.46902430057525635, 0.33481699228286743, 0.04672032594680786, 0.8247895836830139, 0.6381722688674927, 0.2869639992713928, 0.9552024602890015, 0.5254051685333252]  ‚Üí  acq = -1.0760915999692129
X = [0.5276162624359131, 0.5615600347518921, 0.0869060754776001, 0.2889663577079773, 0.5673976540565491, 0.3151257634162903, 0.7601602077484131, 0.5132786631584167, 0.6058185696601868, 0.9846616387367249, 0.28551214933395386, 0.43264758586883545, 0.20677942037582397, 0.062458932399749756, 0.6141131520271301, 0.858392596244812, 0.6692355275154114, 0.13454866409301758, 0.8236895203590393]  ‚Üí  acq = -1.0761093501261616
X = [0.04342454671859741, 0.14995735883712769, 0.9550195336341858, 0.2705121636390686, 0.23881769180297852, 0.2921637296676636, 0.2600720524787903, 0.6402718424797058, 0.23645484447479248, 0.04851953685283661, 0.34876418113708496, 0.5511668920516968, 0.5321722626686096, 0.5048695206642151, 0.0011958479881286621, 0.3705838620662689, 0.21825557947158813, 0.3988022804260254, 0.21945631504058838]  ‚Üí  acq = -1.0760893360533008
X = [0.484433650970459, 0.40925705432891846, 0.04244208335876465, 0.7230846285820007, 0.6559848785400391, 0.6305665969848633, 0.6887122988700867, 0.4752557873725891, 0.5960436463356018, 0.052417635917663574, 0.8234619498252869, 0.894453763961792, 0.3016206622123718, 0.9169406294822693, 0.3473445177078247, 0.7071468234062195, 0.30779868364334106, 0.24430783092975616, 0.24161124229431152]  ‚Üí  acq = -1.076159673609954
proposed candidate layer mask is:  tensor([1., 1., 1., 0., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1132, dtype=torch.float64), tensor(0.4534, dtype=torch.float64), 0, 0, tensor(0.1664, dtype=torch.float64), tensor(0.1069, dtype=torch.float64), tensor(0.0436, dtype=torch.float64), tensor(0.1006, dtype=torch.float64), tensor(0.0159, dtype=torch.float64), 19, 1, 1, 1, 0, 1, 61, 0.02710204532086204, 14.98103139747407, 0]
normalized proposed parameters for next round by BO: [tensor(0.1132, dtype=torch.float64), tensor(0.4534, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1664, dtype=torch.float64), tensor(0.1069, dtype=torch.float64), tensor(0.0436, dtype=torch.float64), tensor(0.1006, dtype=torch.float64), tensor(0.0159, dtype=torch.float64), tensor(0.5942, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.4727, dtype=torch.float64), tensor(0.2710, dtype=torch.float64), tensor(0.3121, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  27  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.113
  gsm8k: 0.453
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.166
  truthfulqa_gen: 0.107
  wikitext: 0.044
  mmlu: 0.101
  arc_challenge: 0.016

LoRA Parameters:
  lora_r: (61,)
  lora_dropout: (0.02710204532086204,)
  num_layers_to_apply: (19,)
  five_dim_vector: ([1, 1, 1, 0, 1],)
  lora_alpha: (14.98103139747407,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  19
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 1, 0, 1]
lora rank:  61
lora dropout:  0.02710204532086204
lora alpha:  14.98103139747407
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 58,153,984 || all params: 8,088,415,232 || trainable%: 0.7190
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.7563, 'grad_norm': 0.29179733991622925, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.5444881916046143, 'eval_runtime': 5.1021, 'eval_samples_per_second': 195.999, 'eval_steps_per_second': 12.348, 'epoch': 0.04}
{'loss': 1.4151, 'grad_norm': 0.17112664878368378, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.3208582401275635, 'eval_runtime': 5.114, 'eval_samples_per_second': 195.541, 'eval_steps_per_second': 12.319, 'epoch': 0.08}
{'loss': 1.233, 'grad_norm': 0.1546851247549057, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 1.1925907135009766, 'eval_runtime': 5.122, 'eval_samples_per_second': 195.236, 'eval_steps_per_second': 12.3, 'epoch': 0.12}
{'loss': 1.1296, 'grad_norm': 0.1627856194972992, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 1.1310786008834839, 'eval_runtime': 5.1262, 'eval_samples_per_second': 195.076, 'eval_steps_per_second': 12.29, 'epoch': 0.16}
{'loss': 1.0989, 'grad_norm': 0.150750532746315, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 1.0761792659759521, 'eval_runtime': 5.1074, 'eval_samples_per_second': 195.795, 'eval_steps_per_second': 12.335, 'epoch': 0.2}
{'loss': 1.0318, 'grad_norm': 0.18892042338848114, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 1.0326884984970093, 'eval_runtime': 5.1168, 'eval_samples_per_second': 195.436, 'eval_steps_per_second': 12.312, 'epoch': 0.24}
{'loss': 1.0018, 'grad_norm': 0.13514982163906097, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9879614114761353, 'eval_runtime': 5.1165, 'eval_samples_per_second': 195.446, 'eval_steps_per_second': 12.313, 'epoch': 0.28}
{'loss': 0.9528, 'grad_norm': 0.1396818310022354, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.983309805393219, 'eval_runtime': 5.1053, 'eval_samples_per_second': 195.877, 'eval_steps_per_second': 12.34, 'epoch': 0.32}
{'loss': 1.0054, 'grad_norm': 0.14056873321533203, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9694678783416748, 'eval_runtime': 5.1176, 'eval_samples_per_second': 195.405, 'eval_steps_per_second': 12.311, 'epoch': 0.36}
{'loss': 1.0088, 'grad_norm': 0.1381644457578659, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9601439237594604, 'eval_runtime': 5.1182, 'eval_samples_per_second': 195.38, 'eval_steps_per_second': 12.309, 'epoch': 0.4}
{'loss': 0.9713, 'grad_norm': 0.12718720734119415, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9720334410667419, 'eval_runtime': 5.1183, 'eval_samples_per_second': 195.378, 'eval_steps_per_second': 12.309, 'epoch': 0.44}
{'loss': 0.9728, 'grad_norm': 0.13645830750465393, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9569198489189148, 'eval_runtime': 5.142, 'eval_samples_per_second': 194.475, 'eval_steps_per_second': 12.252, 'epoch': 0.48}
{'loss': 0.9519, 'grad_norm': 0.16615547239780426, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9463342428207397, 'eval_runtime': 5.1171, 'eval_samples_per_second': 195.425, 'eval_steps_per_second': 12.312, 'epoch': 0.52}
{'loss': 0.917, 'grad_norm': 0.1415030062198639, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9464060068130493, 'eval_runtime': 5.1192, 'eval_samples_per_second': 195.342, 'eval_steps_per_second': 12.307, 'epoch': 0.56}
{'loss': 0.9512, 'grad_norm': 0.1547841727733612, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9409326910972595, 'eval_runtime': 5.1224, 'eval_samples_per_second': 195.222, 'eval_steps_per_second': 12.299, 'epoch': 0.6}
{'loss': 0.9603, 'grad_norm': 0.14536426961421967, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.9384435415267944, 'eval_runtime': 5.1243, 'eval_samples_per_second': 195.149, 'eval_steps_per_second': 12.294, 'epoch': 0.64}
{'loss': 0.9431, 'grad_norm': 0.1698988974094391, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.9386788606643677, 'eval_runtime': 5.1405, 'eval_samples_per_second': 194.535, 'eval_steps_per_second': 12.256, 'epoch': 0.68}
{'loss': 0.9558, 'grad_norm': 0.1540437787771225, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.9371427893638611, 'eval_runtime': 5.1778, 'eval_samples_per_second': 193.132, 'eval_steps_per_second': 12.167, 'epoch': 0.72}
{'loss': 0.9374, 'grad_norm': 0.17321547865867615, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.9360556602478027, 'eval_runtime': 5.1391, 'eval_samples_per_second': 194.587, 'eval_steps_per_second': 12.259, 'epoch': 0.76}
{'loss': 0.9442, 'grad_norm': 0.136514350771904, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.9374467134475708, 'eval_runtime': 5.1402, 'eval_samples_per_second': 194.546, 'eval_steps_per_second': 12.256, 'epoch': 0.8}
{'loss': 0.9318, 'grad_norm': 0.1700127273797989, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.9309388399124146, 'eval_runtime': 5.1381, 'eval_samples_per_second': 194.625, 'eval_steps_per_second': 12.261, 'epoch': 0.84}
{'loss': 0.9189, 'grad_norm': 0.14792701601982117, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.9272116422653198, 'eval_runtime': 5.1538, 'eval_samples_per_second': 194.033, 'eval_steps_per_second': 12.224, 'epoch': 0.88}
{'loss': 0.9369, 'grad_norm': 0.1712743639945984, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.9283448457717896, 'eval_runtime': 5.1418, 'eval_samples_per_second': 194.486, 'eval_steps_per_second': 12.253, 'epoch': 0.92}
{'loss': 0.9023, 'grad_norm': 0.14891023933887482, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.9281803965568542, 'eval_runtime': 5.1417, 'eval_samples_per_second': 194.488, 'eval_steps_per_second': 12.253, 'epoch': 0.96}
{'loss': 0.9234, 'grad_norm': 0.17856146395206451, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.928623378276825, 'eval_runtime': 5.1683, 'eval_samples_per_second': 193.488, 'eval_steps_per_second': 12.19, 'epoch': 1.0}
{'train_runtime': 318.1395, 'train_samples_per_second': 31.423, 'train_steps_per_second': 1.965, 'train_loss': 1.0700541076660157, 'epoch': 1.0}
train_results:  {'eval_loss': [2.5444881916046143, 1.3208582401275635, 1.1925907135009766, 1.1310786008834839, 1.0761792659759521, 1.0326884984970093, 0.9879614114761353, 0.983309805393219, 0.9694678783416748, 0.9601439237594604, 0.9720334410667419, 0.9569198489189148, 0.9463342428207397, 0.9464060068130493, 0.9409326910972595, 0.9384435415267944, 0.9386788606643677, 0.9371427893638611, 0.9360556602478027, 0.9374467134475708, 0.9309388399124146, 0.9272116422653198, 0.9283448457717896, 0.9281803965568542, 0.928623378276825], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [2.5444881916046143, 1.3208582401275635, 1.1925907135009766, 1.1310786008834839, 1.0761792659759521, 1.0326884984970093, 0.9879614114761353, 0.983309805393219, 0.9694678783416748, 0.9601439237594604, 0.9720334410667419, 0.9569198489189148, 0.9463342428207397, 0.9464060068130493, 0.9409326910972595, 0.9384435415267944, 0.9386788606643677, 0.9371427893638611, 0.9360556602478027, 0.9374467134475708, 0.9309388399124146, 0.9272116422653198, 0.9283448457717896, 0.9281803965568542, 0.928623378276825]
current iteration observed (possibly low-fid or predicted) eval_loss:  -1.045215368270874
current iteration best possible eval_loss (full train run):  -0.928623378276825
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796, -0.9815524220466614, -0.9423834085464478, -1.0522664785385132, -1.1252762079238892, -1.045215368270874]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.5876 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.9852668046951294, 0.7275074124336243, 0.5811176896095276, 0.9981693029403687, 0.35632556676864624, 0.8268628716468811, 0.30742716789245605, 0.8634069561958313, 0.7770436406135559, 0.4230007231235504, 0.04633253812789917, 0.8669166564941406, 0.003984928131103516, 0.5223613977432251, 0.46824127435684204, 0.0993184894323349, 0.5334663987159729, 0.1635502576828003, 0.3389108180999756]  ‚Üí  acq = -1.079691027316324
X = [0.9803091287612915, 0.56136155128479, 0.693087637424469, 0.21477162837982178, 0.7210942506790161, 0.9523599743843079, 0.48714154958724976, 0.2692612409591675, 0.7294906973838806, 0.7016791105270386, 0.016992270946502686, 0.2703777551651001, 0.3962728977203369, 0.23176586627960205, 0.027868032455444336, 0.5473737716674805, 0.30727219581604004, 0.5976512432098389, 0.3444458246231079]  ‚Üí  acq = -1.0796963872868734
X = [0.8974170088768005, 0.46087926626205444, 0.6173551082611084, 0.10800439119338989, 0.5072851777076721, 0.5860732793807983, 0.3739680051803589, 0.9474056959152222, 0.8749518990516663, 0.5320674180984497, 0.2113267183303833, 0.7842222452163696, 0.17673414945602417, 0.9297425746917725, 0.7199150323867798, 0.06697317212820053, 0.6311293244361877, 0.5729125738143921, 0.008942186832427979]  ‚Üí  acq = -1.0796910179344246
X = [0.041183412075042725, 0.13811087608337402, 0.5779871940612793, 0.16824162006378174, 0.9846409559249878, 0.8281779289245605, 0.927112340927124, 0.7004026174545288, 0.6300967335700989, 0.4970613121986389, 0.488383948802948, 0.21784842014312744, 0.7982703447341919, 0.7192627191543579, 0.3136094808578491, 0.807643711566925, 0.6237255334854126, 0.840650200843811, 0.3124581575393677]  ‚Üí  acq = -1.0796910212712703
X = [0.1885993480682373, 0.8312870264053345, 0.5665619969367981, 0.10100406408309937, 0.553330659866333, 0.45840704441070557, 0.44444334506988525, 0.37204623222351074, 0.06517422199249268, 0.30719199776649475, 0.3092554807662964, 0.6049742102622986, 0.2883668541908264, 0.7875701189041138, 0.0963255763053894, 0.2865552604198456, 0.6288021206855774, 0.8627077341079712, 0.19194185733795166]  ‚Üí  acq = -1.080850960537037
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1473, dtype=torch.float64), tensor(0.2437, dtype=torch.float64), 0, 0, tensor(0.1834, dtype=torch.float64), 0, 0, tensor(0.1356, dtype=torch.float64), tensor(0.2810, dtype=torch.float64), 32, 1, 1, 0, 1, 1, 18, 0.08408381007967397, 4.189410363253511, 0]
normalized proposed parameters for next round by BO: [tensor(0.1473, dtype=torch.float64), tensor(0.2437, dtype=torch.float64), tensor(0.0092, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1834, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(4.9319e-18, dtype=torch.float64), tensor(0.1356, dtype=torch.float64), tensor(0.2810, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.1430, dtype=torch.float64), tensor(0.8408, dtype=torch.float64), tensor(0.0873, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  28  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.147
  gsm8k: 0.244
  rowan_hellaswag: 0
  sciq: 0
  triviaqa: 0.183
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.136
  arc_challenge: 0.281

LoRA Parameters:
  lora_r: (18,)
  lora_dropout: (0.08408381007967397,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (4.189410363253511,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  18
lora dropout:  0.08408381007967397
lora alpha:  4.189410363253511
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 28,901,376 || all params: 8,059,162,624 || trainable%: 0.3586
length of training data:  9905
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 3.0604, 'grad_norm': 0.9200863242149353, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 2.720247507095337, 'eval_runtime': 5.4733, 'eval_samples_per_second': 182.706, 'eval_steps_per_second': 11.51, 'epoch': 0.04}
{'loss': 1.5429, 'grad_norm': 0.36979350447654724, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.1643080711364746, 'eval_runtime': 5.4392, 'eval_samples_per_second': 183.849, 'eval_steps_per_second': 11.583, 'epoch': 0.08}
{'loss': 1.0845, 'grad_norm': 0.47909989953041077, 'learning_rate': 0.00028736842105263154, 'epoch': 0.12}
{'eval_loss': 1.0192170143127441, 'eval_runtime': 5.4131, 'eval_samples_per_second': 184.736, 'eval_steps_per_second': 11.638, 'epoch': 0.12}
{'loss': 0.988, 'grad_norm': 0.22878533601760864, 'learning_rate': 0.00027421052631578945, 'epoch': 0.16}
{'eval_loss': 0.9769535064697266, 'eval_runtime': 5.4393, 'eval_samples_per_second': 183.848, 'eval_steps_per_second': 11.582, 'epoch': 0.16}
{'loss': 0.9823, 'grad_norm': 0.16435490548610687, 'learning_rate': 0.00026105263157894735, 'epoch': 0.2}
{'eval_loss': 0.9613297581672668, 'eval_runtime': 5.4542, 'eval_samples_per_second': 183.346, 'eval_steps_per_second': 11.551, 'epoch': 0.2}
{'loss': 0.9924, 'grad_norm': 0.1702052354812622, 'learning_rate': 0.00024789473684210526, 'epoch': 0.24}
{'eval_loss': 0.9563090801239014, 'eval_runtime': 5.45, 'eval_samples_per_second': 183.487, 'eval_steps_per_second': 11.56, 'epoch': 0.24}
{'loss': 0.9463, 'grad_norm': 0.18287885189056396, 'learning_rate': 0.00023473684210526314, 'epoch': 0.28}
{'eval_loss': 0.9472239017486572, 'eval_runtime': 5.4452, 'eval_samples_per_second': 183.647, 'eval_steps_per_second': 11.57, 'epoch': 0.28}
{'loss': 0.9424, 'grad_norm': 0.1785736232995987, 'learning_rate': 0.00022157894736842101, 'epoch': 0.32}
{'eval_loss': 0.93970787525177, 'eval_runtime': 5.441, 'eval_samples_per_second': 183.791, 'eval_steps_per_second': 11.579, 'epoch': 0.32}
{'loss': 0.9494, 'grad_norm': 0.17298524081707, 'learning_rate': 0.00020842105263157895, 'epoch': 0.36}
{'eval_loss': 0.9324349761009216, 'eval_runtime': 5.432, 'eval_samples_per_second': 184.095, 'eval_steps_per_second': 11.598, 'epoch': 0.36}
{'loss': 0.9118, 'grad_norm': 0.1667548269033432, 'learning_rate': 0.00019526315789473683, 'epoch': 0.4}
{'eval_loss': 0.9377012848854065, 'eval_runtime': 5.4258, 'eval_samples_per_second': 184.305, 'eval_steps_per_second': 11.611, 'epoch': 0.4}
{'loss': 0.965, 'grad_norm': 0.19113565981388092, 'learning_rate': 0.00018210526315789473, 'epoch': 0.44}
{'eval_loss': 0.9356897473335266, 'eval_runtime': 5.4208, 'eval_samples_per_second': 184.476, 'eval_steps_per_second': 11.622, 'epoch': 0.44}
{'loss': 0.9388, 'grad_norm': 0.19733743369579315, 'learning_rate': 0.0001689473684210526, 'epoch': 0.48}
{'eval_loss': 0.9254942536354065, 'eval_runtime': 5.4254, 'eval_samples_per_second': 184.318, 'eval_steps_per_second': 11.612, 'epoch': 0.48}
{'loss': 0.9024, 'grad_norm': 0.24002966284751892, 'learning_rate': 0.0001557894736842105, 'epoch': 0.52}
{'eval_loss': 0.9221988320350647, 'eval_runtime': 5.4262, 'eval_samples_per_second': 184.29, 'eval_steps_per_second': 11.61, 'epoch': 0.52}
{'loss': 0.9216, 'grad_norm': 0.20588655769824982, 'learning_rate': 0.0001426315789473684, 'epoch': 0.56}
{'eval_loss': 0.9236681461334229, 'eval_runtime': 5.4306, 'eval_samples_per_second': 184.143, 'eval_steps_per_second': 11.601, 'epoch': 0.56}
{'loss': 0.9401, 'grad_norm': 0.23320387303829193, 'learning_rate': 0.0001294736842105263, 'epoch': 0.6}
{'eval_loss': 0.9193633198738098, 'eval_runtime': 5.4264, 'eval_samples_per_second': 184.285, 'eval_steps_per_second': 11.61, 'epoch': 0.6}
{'loss': 0.9364, 'grad_norm': 0.20950131118297577, 'learning_rate': 0.0001163157894736842, 'epoch': 0.65}
{'eval_loss': 0.9221593141555786, 'eval_runtime': 5.4241, 'eval_samples_per_second': 184.362, 'eval_steps_per_second': 11.615, 'epoch': 0.65}
{'loss': 0.9267, 'grad_norm': 0.20910589396953583, 'learning_rate': 0.0001031578947368421, 'epoch': 0.69}
{'eval_loss': 0.914128303527832, 'eval_runtime': 5.4227, 'eval_samples_per_second': 184.411, 'eval_steps_per_second': 11.618, 'epoch': 0.69}
{'loss': 0.8621, 'grad_norm': 0.20289883017539978, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.73}
{'eval_loss': 0.914039134979248, 'eval_runtime': 5.4161, 'eval_samples_per_second': 184.634, 'eval_steps_per_second': 11.632, 'epoch': 0.73}
{'loss': 0.9063, 'grad_norm': 0.24166035652160645, 'learning_rate': 7.68421052631579e-05, 'epoch': 0.77}
{'eval_loss': 0.9083967208862305, 'eval_runtime': 5.4191, 'eval_samples_per_second': 184.534, 'eval_steps_per_second': 11.626, 'epoch': 0.77}
{'loss': 0.8982, 'grad_norm': 0.21064358949661255, 'learning_rate': 6.368421052631578e-05, 'epoch': 0.81}
{'eval_loss': 0.9118890166282654, 'eval_runtime': 5.4241, 'eval_samples_per_second': 184.361, 'eval_steps_per_second': 11.615, 'epoch': 0.81}
{'loss': 0.891, 'grad_norm': 0.22352761030197144, 'learning_rate': 5.0526315789473676e-05, 'epoch': 0.85}
{'eval_loss': 0.9123787879943848, 'eval_runtime': 5.4163, 'eval_samples_per_second': 184.629, 'eval_steps_per_second': 11.632, 'epoch': 0.85}
{'loss': 0.8725, 'grad_norm': 0.20547367632389069, 'learning_rate': 3.7368421052631575e-05, 'epoch': 0.89}
{'eval_loss': 0.9070873856544495, 'eval_runtime': 5.4176, 'eval_samples_per_second': 184.585, 'eval_steps_per_second': 11.629, 'epoch': 0.89}
{'loss': 0.9113, 'grad_norm': 0.20142510533332825, 'learning_rate': 2.421052631578947e-05, 'epoch': 0.93}
{'eval_loss': 0.9079223275184631, 'eval_runtime': 5.4122, 'eval_samples_per_second': 184.769, 'eval_steps_per_second': 11.64, 'epoch': 0.93}
{'loss': 0.9105, 'grad_norm': 0.18810820579528809, 'learning_rate': 1.1052631578947366e-05, 'epoch': 0.97}
{'eval_loss': 0.9070004820823669, 'eval_runtime': 5.4211, 'eval_samples_per_second': 184.466, 'eval_steps_per_second': 11.621, 'epoch': 0.97}
{'train_runtime': 371.3183, 'train_samples_per_second': 26.675, 'train_steps_per_second': 1.67, 'train_loss': 1.044281408863683, 'epoch': 1.0}
train_results:  {'eval_loss': [2.720247507095337, 1.1643080711364746, 1.0192170143127441, 0.9769535064697266, 0.9613297581672668, 0.9563090801239014, 0.9472239017486572, 0.93970787525177, 0.9324349761009216, 0.9377012848854065, 0.9356897473335266, 0.9254942536354065, 0.9221988320350647, 0.9236681461334229, 0.9193633198738098, 0.9221593141555786, 0.914128303527832, 0.914039134979248, 0.9083967208862305, 0.9118890166282654, 0.9123787879943848, 0.9070873856544495, 0.9079223275184631, 0.9070004820823669], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  24
eval_loss logs:  [2.720247507095337, 1.1643080711364746, 1.0192170143127441, 0.9769535064697266, 0.9613297581672668, 0.9563090801239014, 0.9472239017486572, 0.93970787525177, 0.9324349761009216, 0.9377012848854065, 0.9356897473335266, 0.9254942536354065, 0.9221988320350647, 0.9236681461334229, 0.9193633198738098, 0.9221593141555786, 0.914128303527832, 0.914039134979248, 0.9083967208862305, 0.9118890166282654, 0.9123787879943848, 0.9070873856544495, 0.9079223275184631, 0.9070004820823669]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.9327304363250732
current iteration best possible eval_loss (full train run):  -0.9070004820823669
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796, -0.9815524220466614, -0.9423834085464478, -1.0522664785385132, -1.1252762079238892, -1.045215368270874, -0.9327304363250732]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 3.0087 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.01858508586883545, 0.27087509632110596, 0.28604018688201904, 0.9655972719192505, 0.41934478282928467, 0.7529501914978027, 0.8967930674552917, 0.2059735655784607, 0.40415942668914795, 0.3516203463077545, 0.6742079257965088, 0.8425379991531372, 0.1471734642982483, 0.16597437858581543, 0.49485671520233154, 0.3618133068084717, 0.971827507019043, 0.9077439308166504, 0.298198401927948]  ‚Üí  acq = -1.0777346152467695
X = [0.8427011370658875, 0.498738169670105, 0.8024415969848633, 0.4285522699356079, 0.8464704751968384, 0.4606736898422241, 0.9603627920150757, 0.35994040966033936, 0.8239242434501648, 0.949406087398529, 0.2025597095489502, 0.1727500557899475, 0.5345157384872437, 0.2376563549041748, 0.5827286839485168, 0.6172329187393188, 0.7515861392021179, 0.5611653327941895, 0.20585447549819946]  ‚Üí  acq = -1.0776805019632894
X = [0.2902684807777405, 0.8506918549537659, 0.9467999339103699, 0.7306930422782898, 0.36220765113830566, 0.7957260012626648, 0.20566540956497192, 0.8878775835037231, 0.38044893741607666, 0.44211840629577637, 0.9807340502738953, 0.05173856019973755, 0.6718758940696716, 0.3596378564834595, 0.5948803424835205, 0.5744302272796631, 0.3193025588989258, 0.46983620524406433, 0.4887549877166748]  ‚Üí  acq = -1.077680500102775
X = [0.4835529327392578, 0.9963791966438293, 0.4985392093658447, 0.2583252191543579, 0.6950103640556335, 0.17230606079101562, 0.27995556592941284, 0.5112245678901672, 0.7412656545639038, 0.5757967829704285, 0.605756938457489, 0.38045990467071533, 0.9548166394233704, 0.11543363332748413, 0.13198411464691162, 0.12970638275146484, 0.7689643502235413, 0.6867401599884033, 0.6047636270523071]  ‚Üí  acq = -1.0778785011961416
X = [0.500048816204071, 0.24933886528015137, 0.07242149114608765, 0.8406274318695068, 0.8167679905891418, 0.7659611701965332, 0.3473196029663086, 0.08422183990478516, 0.34111177921295166, 0.07476793229579926, 0.3871777653694153, 0.952457845211029, 0.5624963045120239, 0.9972479939460754, 0.3902493715286255, 0.24435395002365112, 0.10478633642196655, 0.9867004156112671, 0.0338512659072876]  ‚Üí  acq = -1.0710924538437936
proposed candidate layer mask is:  tensor([1., 1., 0., 1., 1.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1517, dtype=torch.float64), tensor(0.3811, dtype=torch.float64), tensor(0.0151, dtype=torch.float64), 0, 0, 0, 0, tensor(0.1341, dtype=torch.float64), tensor(0.3180, dtype=torch.float64), 32, 1, 1, 0, 1, 1, 12, 0.08355638090181036, 17.846375297193827, 0]
normalized proposed parameters for next round by BO: [tensor(0.1517, dtype=torch.float64), tensor(0.3811, dtype=torch.float64), tensor(0.0151, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(7.5513e-18, dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1341, dtype=torch.float64), tensor(0.3180, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0.0904, dtype=torch.float64), tensor(0.8356, dtype=torch.float64), tensor(0.3718, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None




======== BO iteration:  29  ==========
=== Candidate ===

Data Domains and Mixing Ratios:
  commonsense_qa: 0.152
  gsm8k: 0.381
  rowan_hellaswag: 0.015
  sciq: 0
  triviaqa: 0
  truthfulqa_gen: 0
  wikitext: 0
  mmlu: 0.134
  arc_challenge: 0.318

LoRA Parameters:
  lora_r: (12,)
  lora_dropout: (0.08355638090181036,)
  num_layers_to_apply: (32,)
  five_dim_vector: ([1, 1, 0, 1, 1],)
  lora_alpha: (17.846375297193827,)
  lora_reverse: (0,)
=========================

creating lora config with parameters, to be trained: 
number of layers to apply lora:  32
which modules to apply lora (q_proj, v_proj, up_proj, down_proj, gate_proj):  [1, 1, 0, 1, 1]
lora rank:  12
lora dropout:  0.08355638090181036
lora alpha:  17.846375297193827
lora reverse (apply to rear layers if False, else front layers):  0
trainable params: 19,267,584 || all params: 8,049,528,832 || trainable%: 0.2394
length of training data:  9997
length of validation data:  1000
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.
{'loss': 2.5145, 'grad_norm': 1.1857695579528809, 'learning_rate': 0.00014399999999999998, 'epoch': 0.04}
{'eval_loss': 1.7617071866989136, 'eval_runtime': 5.391, 'eval_samples_per_second': 185.494, 'eval_steps_per_second': 11.686, 'epoch': 0.04}
{'loss': 1.1546, 'grad_norm': 0.6403271555900574, 'learning_rate': 0.000294, 'epoch': 0.08}
{'eval_loss': 1.0457793474197388, 'eval_runtime': 5.393, 'eval_samples_per_second': 185.426, 'eval_steps_per_second': 11.682, 'epoch': 0.08}
{'loss': 1.0288, 'grad_norm': 0.4608544111251831, 'learning_rate': 0.0002874782608695652, 'epoch': 0.12}
{'eval_loss': 0.9853023290634155, 'eval_runtime': 5.4002, 'eval_samples_per_second': 185.178, 'eval_steps_per_second': 11.666, 'epoch': 0.12}
{'loss': 0.9505, 'grad_norm': 0.4350929856300354, 'learning_rate': 0.0002744347826086956, 'epoch': 0.16}
{'eval_loss': 0.957528293132782, 'eval_runtime': 5.4025, 'eval_samples_per_second': 185.099, 'eval_steps_per_second': 11.661, 'epoch': 0.16}
{'loss': 0.9301, 'grad_norm': 0.34061628580093384, 'learning_rate': 0.0002613913043478261, 'epoch': 0.2}
{'eval_loss': 0.944485068321228, 'eval_runtime': 5.4002, 'eval_samples_per_second': 185.179, 'eval_steps_per_second': 11.666, 'epoch': 0.2}
{'loss': 0.9023, 'grad_norm': 0.3978928327560425, 'learning_rate': 0.0002483478260869565, 'epoch': 0.24}
{'eval_loss': 0.9293338656425476, 'eval_runtime': 5.4155, 'eval_samples_per_second': 184.656, 'eval_steps_per_second': 11.633, 'epoch': 0.24}
{'loss': 0.9156, 'grad_norm': 0.33970025181770325, 'learning_rate': 0.00023530434782608693, 'epoch': 0.28}
{'eval_loss': 0.9273716807365417, 'eval_runtime': 5.4076, 'eval_samples_per_second': 184.924, 'eval_steps_per_second': 11.65, 'epoch': 0.28}
{'loss': 0.9253, 'grad_norm': 0.3607349097728729, 'learning_rate': 0.0002222608695652174, 'epoch': 0.32}
{'eval_loss': 0.9200037121772766, 'eval_runtime': 5.4437, 'eval_samples_per_second': 183.7, 'eval_steps_per_second': 11.573, 'epoch': 0.32}
{'loss': 0.9139, 'grad_norm': 0.39407962560653687, 'learning_rate': 0.0002092173913043478, 'epoch': 0.36}
{'eval_loss': 0.9168000221252441, 'eval_runtime': 5.4383, 'eval_samples_per_second': 183.88, 'eval_steps_per_second': 11.584, 'epoch': 0.36}
{'loss': 0.8811, 'grad_norm': 0.3810266852378845, 'learning_rate': 0.00019617391304347823, 'epoch': 0.4}
{'eval_loss': 0.9158207774162292, 'eval_runtime': 5.4129, 'eval_samples_per_second': 184.742, 'eval_steps_per_second': 11.639, 'epoch': 0.4}
{'loss': 0.9006, 'grad_norm': 0.3864028751850128, 'learning_rate': 0.00018313043478260867, 'epoch': 0.44}
{'eval_loss': 0.9157038331031799, 'eval_runtime': 5.4272, 'eval_samples_per_second': 184.257, 'eval_steps_per_second': 11.608, 'epoch': 0.44}
{'loss': 0.8989, 'grad_norm': 0.40697982907295227, 'learning_rate': 0.00017008695652173913, 'epoch': 0.48}
{'eval_loss': 0.9070060849189758, 'eval_runtime': 5.4326, 'eval_samples_per_second': 184.075, 'eval_steps_per_second': 11.597, 'epoch': 0.48}
{'loss': 0.8447, 'grad_norm': 0.4779805541038513, 'learning_rate': 0.00015704347826086954, 'epoch': 0.52}
{'eval_loss': 0.9049053192138672, 'eval_runtime': 5.4176, 'eval_samples_per_second': 184.584, 'eval_steps_per_second': 11.629, 'epoch': 0.52}
{'loss': 0.8554, 'grad_norm': 0.42270633578300476, 'learning_rate': 0.00014399999999999998, 'epoch': 0.56}
{'eval_loss': 0.9049052596092224, 'eval_runtime': 5.4108, 'eval_samples_per_second': 184.816, 'eval_steps_per_second': 11.643, 'epoch': 0.56}
{'loss': 0.8175, 'grad_norm': 0.4480243921279907, 'learning_rate': 0.00013095652173913044, 'epoch': 0.6}
{'eval_loss': 0.9029157757759094, 'eval_runtime': 5.4062, 'eval_samples_per_second': 184.972, 'eval_steps_per_second': 11.653, 'epoch': 0.6}
{'loss': 0.8436, 'grad_norm': 0.3762960433959961, 'learning_rate': 0.00011791304347826085, 'epoch': 0.64}
{'eval_loss': 0.8998978137969971, 'eval_runtime': 5.4028, 'eval_samples_per_second': 185.089, 'eval_steps_per_second': 11.661, 'epoch': 0.64}
{'loss': 0.8449, 'grad_norm': 0.4613169729709625, 'learning_rate': 0.0001048695652173913, 'epoch': 0.68}
{'eval_loss': 0.8983923196792603, 'eval_runtime': 5.4072, 'eval_samples_per_second': 184.937, 'eval_steps_per_second': 11.651, 'epoch': 0.68}
{'loss': 0.8239, 'grad_norm': 0.5360555052757263, 'learning_rate': 9.182608695652173e-05, 'epoch': 0.72}
{'eval_loss': 0.8980850577354431, 'eval_runtime': 5.41, 'eval_samples_per_second': 184.841, 'eval_steps_per_second': 11.645, 'epoch': 0.72}
{'loss': 0.848, 'grad_norm': 0.394561231136322, 'learning_rate': 7.878260869565217e-05, 'epoch': 0.76}
{'eval_loss': 0.8958765864372253, 'eval_runtime': 5.4223, 'eval_samples_per_second': 184.423, 'eval_steps_per_second': 11.619, 'epoch': 0.76}
{'loss': 0.8509, 'grad_norm': 0.48586204648017883, 'learning_rate': 6.57391304347826e-05, 'epoch': 0.8}
{'eval_loss': 0.8956838250160217, 'eval_runtime': 5.4098, 'eval_samples_per_second': 184.85, 'eval_steps_per_second': 11.646, 'epoch': 0.8}
{'loss': 0.8331, 'grad_norm': 0.4872343838214874, 'learning_rate': 5.2695652173913035e-05, 'epoch': 0.84}
{'eval_loss': 0.8912221193313599, 'eval_runtime': 5.4083, 'eval_samples_per_second': 184.902, 'eval_steps_per_second': 11.649, 'epoch': 0.84}
{'loss': 0.8025, 'grad_norm': 0.4488627016544342, 'learning_rate': 3.965217391304347e-05, 'epoch': 0.88}
{'eval_loss': 0.8890557885169983, 'eval_runtime': 5.4128, 'eval_samples_per_second': 184.748, 'eval_steps_per_second': 11.639, 'epoch': 0.88}
{'loss': 0.7966, 'grad_norm': 0.7096526622772217, 'learning_rate': 2.660869565217391e-05, 'epoch': 0.92}
{'eval_loss': 0.8908789157867432, 'eval_runtime': 5.413, 'eval_samples_per_second': 184.741, 'eval_steps_per_second': 11.639, 'epoch': 0.92}
{'loss': 0.8015, 'grad_norm': 0.45984983444213867, 'learning_rate': 1.3565217391304348e-05, 'epoch': 0.96}
{'eval_loss': 0.8899173736572266, 'eval_runtime': 5.4096, 'eval_samples_per_second': 184.858, 'eval_steps_per_second': 11.646, 'epoch': 0.96}
{'loss': 0.784, 'grad_norm': 0.6719172596931458, 'learning_rate': 5.217391304347826e-07, 'epoch': 1.0}
{'eval_loss': 0.889920711517334, 'eval_runtime': 5.4137, 'eval_samples_per_second': 184.715, 'eval_steps_per_second': 11.637, 'epoch': 1.0}
{'train_runtime': 384.5557, 'train_samples_per_second': 25.996, 'train_steps_per_second': 1.625, 'train_loss': 0.946508364868164, 'epoch': 1.0}
train_results:  {'eval_loss': [1.7617071866989136, 1.0457793474197388, 0.9853023290634155, 0.957528293132782, 0.944485068321228, 0.9293338656425476, 0.9273716807365417, 0.9200037121772766, 0.9168000221252441, 0.9158207774162292, 0.9157038331031799, 0.9070060849189758, 0.9049053192138672, 0.9049052596092224, 0.9029157757759094, 0.8998978137969971, 0.8983923196792603, 0.8980850577354431, 0.8958765864372253, 0.8956838250160217, 0.8912221193313599, 0.8890557885169983, 0.8908789157867432, 0.8899173736572266, 0.889920711517334], 'performance': None}
Applying JoBS: Extracted eval_loss logs for performance prediction.
Length of eval_loss logs:  25
eval_loss logs:  [1.7617071866989136, 1.0457793474197388, 0.9853023290634155, 0.957528293132782, 0.944485068321228, 0.9293338656425476, 0.9273716807365417, 0.9200037121772766, 0.9168000221252441, 0.9158207774162292, 0.9157038331031799, 0.9070060849189758, 0.9049053192138672, 0.9049052596092224, 0.9029157757759094, 0.8998978137969971, 0.8983923196792603, 0.8980850577354431, 0.8958765864372253, 0.8956838250160217, 0.8912221193313599, 0.8890557885169983, 0.8908789157867432, 0.8899173736572266, 0.889920711517334]
current iteration observed (possibly low-fid or predicted) eval_loss:  -0.949668288230896
current iteration best possible eval_loss (full train run):  -0.889920711517334
max eval_loss so far:  -0.8283027410507202
BO observations:  [-1.2132879495620728, -1.5139060020446777, -1.257798433303833, -1.1118427515029907, -1.0452039241790771, -1.2228953838348389, -1.0731794834136963, -1.0186767578125, -1.146902084350586, -1.158286452293396, -1.009720802307129, -1.0834424495697021, -1.1549623012542725, -0.9855165481567383, -1.0253865718841553, -1.0859076976776123, -1.0956453084945679, -1.0593405961990356, -1.0353463888168335, -1.066068410873413, -0.9974642992019653, -1.023471713066101, -0.9886037707328796, -0.9815524220466614, -0.9423834085464478, -1.0522664785385132, -1.1252762079238892, -1.045215368270874, -0.9327304363250732, -0.949668288230896]
creating a GP: MixedSingleTaskGP
fitting GP to data since optimize_method is not random
acq optimization method is mixed (alternating discrete and continuous)
Time taken to perform acquisition optimization: 4.0540 seconds
/home/alfred/Data-Mixing/BO.py:1322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X_sampled = torch.tensor(X_sampled, dtype=torch.double)
Sampled points and acquisition values for multi fidelity:
X = [0.8035042881965637, 0.017681360244750977, 0.37396925687789917, 0.15887385606765747, 0.5996578931808472, 0.8025267720222473, 0.6625286936759949, 0.7461644411087036, 0.44088155031204224, 0.2482948750257492, 0.414609432220459, 0.9056562781333923, 0.692918598651886, 0.7245609164237976, 0.9934723973274231, 0.19214506447315216, 0.606965959072113, 0.753315806388855, 0.4424137473106384]  ‚Üí  acq = -1.0778985705286677
X = [0.42251503467559814, 0.8128004670143127, 0.5967663526535034, 0.028729796409606934, 0.1361721158027649, 0.6117905974388123, 0.26617634296417236, 0.8648793697357178, 0.009343624114990234, 0.4992852210998535, 0.5469313859939575, 0.08031833171844482, 0.17627757787704468, 0.7262287735939026, 0.7334456443786621, 0.33248594403266907, 0.4159647822380066, 0.6191318035125732, 0.050814270973205566]  ‚Üí  acq = -1.0778980409293895
X = [0.7123335003852844, 0.7468824982643127, 0.3395087718963623, 0.43934136629104614, 0.19132208824157715, 0.9396559596061707, 0.47767341136932373, 0.022542357444763184, 0.38677525520324707, 0.3839244544506073, 0.15088504552841187, 0.751442015171051, 0.17621082067489624, 0.5161756277084351, 0.7738797068595886, 0.6942612528800964, 0.9456675052642822, 0.08089366555213928, 0.43891578912734985]  ‚Üí  acq = -1.0789939904991512
X = [0.2543426752090454, 0.7384370565414429, 0.061468660831451416, 0.8893586993217468, 0.5562097430229187, 0.2619137167930603, 0.281788170337677, 0.3158698081970215, 0.6168457269668579, 0.27002662420272827, 0.8897009491920471, 0.934760332107544, 0.4247353672981262, 0.49001544713974, 0.5673343539237976, 0.33494624495506287, 0.6652377843856812, 0.08096535503864288, 0.2110685110092163]  ‚Üí  acq = -1.0809014263689383
X = [0.4057742953300476, 0.6099357008934021, 0.38725948333740234, 0.23149025440216064, 0.07062345743179321, 0.7792069911956787, 0.9907643795013428, 0.3170267939567566, 0.5801001787185669, 0.7922449707984924, 0.09272158145904541, 0.9690634608268738, 0.6228570938110352, 0.9109976291656494, 0.5967274308204651, 0.9399470686912537, 0.06500035524368286, 0.5090670585632324, 0.2519305348396301]  ‚Üí  acq = -1.077899420233396
proposed candidate layer mask is:  tensor([0., 1., 0., 1., 0.], dtype=torch.float64)
proposed parameters for next round by BO: [tensor(0.1439, dtype=torch.float64), tensor(0.1139, dtype=torch.float64), 0, 0, tensor(0.3172, dtype=torch.float64), 0, 0, tensor(0.1382, dtype=torch.float64), tensor(0.2788, dtype=torch.float64), 32, 0, 1, 0, 1, 0, 19, 0.08393818457115504, 9.805590610190098, 0]
normalized proposed parameters for next round by BO: [tensor(0.1439, dtype=torch.float64), tensor(0.1139, dtype=torch.float64), tensor(0.0080, dtype=torch.float64), tensor(4.2098e-18, dtype=torch.float64), tensor(0.3172, dtype=torch.float64), tensor(5.7898e-18, dtype=torch.float64), tensor(3.6142e-18, dtype=torch.float64), tensor(0.1382, dtype=torch.float64), tensor(0.2788, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(0., dtype=torch.float64), tensor(0.1519, dtype=torch.float64), tensor(0.8394, dtype=torch.float64), tensor(0.2043, dtype=torch.float64), tensor(0., dtype=torch.float64)]
fidelity for next round by BO: None
count of count_high_fidelity:  30
count of count_low_fidelity:  0
Best at every step: [-0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202]
final results:  {'command line args': {'iterations': 50, 'num_data': 10000, 'epochs': '1', 'trials': '3', 'evaluation_cuda': 0, 'eval_tasks': 'commonsense_qa', 'eval_method': 'eval_loss', 'experiments_setting': 'in_dist', 'time_limit': '1000', 'lora_rank': '128', 'model': 'llama-8b', 'JoBS': True, 'acq_function': 'ucb', 'ucb_beta': '20', 'optimize_method': 'mixed', 'save_name': 'llama-8b_ucb_commonsense_qa_eval_loss_in_dist.json', 'seed': 13549, 'limit': '100', 'run_BO_on': 'general', 'training_batch': 16, 'evaluation_batch': 16, 'dkl_feature_dim': 32, 'dkl_hidden': 64, 'dkl_freeze_nn': False, 'local_rank': 0, 'eval_random_config': False, 'num_random_configs': 100, 'eval_specific_config': False, 'specific_data_config': None, 'specific_model_config': None}, 'training domain': ['commonsense_qa', 'gsm8k', 'rowan_hellaswag', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext', 'mmlu', 'arc_challenge'], 'evaluation domain': ['commonsense_qa'], 'weight': [1.0], 'random': [[-0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008, -0.8220756649971008], [-0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328, -0.81962651014328], [-0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202, -0.8283027410507202]], 'random_full_inputs': [[[0.7400222366933097, 0, 0, 0, 0, 0, 0, 0, 0.25997776330669015, 32, 0, 1, 0, 1, 0, 128, 4.85722573273506e-18, 48.0, 0], [0.28880353455621866, 0, 0, 0, 0, 0, 0, 0, 0.7111964654435196, 1, 1, 0, 1, 0, 0, 2, 0.09999999999999598, 48.0, 0], [0.21289126247233128, 0, 0, 0, 0, 0, 0, 0.4172748164205979, 0.36983392110707103, 29, 0, 0, 1, 1, 1, 128, 1.0408340855860844e-18, 48.0, 0], [0.31195720251438575, 0, 0, 0, 0.16042850524827995, 0.22779023018301459, 0, 0, 0.2998240620543198, 32, 0, 0, 0, 1, 1, 128, 0.0, 22.5376784297763, 0], [0.3276648568500299, 0.09027516463856082, 0, 0, 0, 0.01112683576852688, 0.05842718953385466, 0.05620427586551357, 0.4563016773435142, 31, 0, 1, 1, 1, 0, 79, 0.05456455613909951, 27.34771376568814, 0], [0.38399998700891597, 0, 0.06139702309171832, 0, 0.03155550403332063, 0, 0, 0.07313265822121033, 0.4499148276448347, 32, 1, 1, 1, 1, 0, 128, 2.6266350919432143e-18, 48.0, 0], [0.4418971429946049, 0.12552112779621585, 0, 0.028683321250292694, 0.09622375196555519, 0.0711918633033692, 0, 0, 0.23648279268996192, 22, 0, 1, 1, 0, 0, 63, 0.004942536284200288, 23.81322528526384, 0], [0.17718210037704918, 0.2010790828102019, 0.06196590138932034, 0.023983500119960307, 0.12132825101795036, 0, 0.01662351041549974, 0, 0.39783765387001807, 25, 0, 1, 1, 1, 0, 87, 0.059330546894060844, 30.622254077718864, 1], [0.0777971044360938, 0, 0, 0, 0.2459930241025418, 0.14866080581440347, 0.027032062390675016, 0, 0.5005170032562859, 26, 0, 1, 1, 1, 0, 5, 0.031094385890600353, 31.63428654780393, 0], [0.19199176517550579, 0, 0.03528859630903565, 0, 0.11705045630364046, 0.15415320438304317, 0, 0.09656916384881468, 0.40494681397996024, 28, 0, 1, 1, 1, 0, 67, 0.02712092141236019, 15.91471119582505, 0], [0.07408617849535035, 0, 0, 0.023046481940744504, 0.2573436180261443, 0.2276292684816796, 0.051990041589786, 0, 0.35947348762279413, 18, 0, 1, 1, 1, 0, 91, 0.01201202743568238, 27.728622443450995, 0], [0.47787332474417915, 0.09765632925101941, 0.03317028407076774, 0.07792013365694514, 0.15310140180200346, 0.05590320699129475, 0, 0.1027405786456987, 0, 32, 1, 1, 1, 1, 1, 76, 0.07291596506131386, 28.9231859549192, 0], [0.2487259148638311, 0.11077922942920572, 0.07480028228266662, 0, 0.15812529766579292, 0.049793978584393946, 0.0258877279019865, 0.0669515006817858, 0.26493606859033747, 32, 0, 0, 0, 1, 0, 20, 0.06112524177819106, 42.07337631880069, 0], [0.2603969286567665, 0.0705445348396509, 0.012053924588593458, 0.05811242596946024, 0.16849235241662155, 0.056783784653745485, 0.022056664827798056, 0.1218359722036224, 0.22972341184374143, 26, 0, 1, 1, 1, 0, 72, 0.02565048671884278, 22.941524366408963, 0], [0.11200882216246992, 0.03718932093218612, 0.0796523229492588, 0.03144408840560135, 0.13318516724854307, 0.10730386829075243, 0.022300376920233008, 0, 0.47691603309095537, 25, 0, 1, 1, 1, 0, 45, 0.028049643287062793, 16.548879105974425, 0], [0.1518005469932755, 0, 0.04611520161528677, 0.039576977207353585, 0.08709607424546481, 0, 0, 0.04666786034885204, 0.6261559324119422, 32, 1, 1, 1, 1, 0, 30, 0.06963112249929612, 27.147932249241055, 0], [0.15115510046487896, 0.04656881831998315, 0, 0.15350294115035437, 0.079893751870707, 0.0904024190591688, 0, 0.12626730674622177, 0.34460448630666973, 25, 1, 1, 1, 0, 1, 43, 0.06951548211272905, 23.20359977342312, 0], [0.3109557690484203, 0.025913822211923943, 0.018581728825525924, 0.04552411355044512, 0, 0.09309059142114513, 0, 0, 0.4910022728380678, 23, 0, 1, 1, 1, 0, 94, 0.02726666721070673, 16.139421552750363, 0], [0.2690833244051846, 0.13909169211731137, 0.07226731472472582, 0.02568338205030867, 0.14338609364396301, 0.09393827111050088, 0.01397601804445315, 0.03989413606451741, 0.2026797678390351, 25, 0, 1, 1, 1, 0, 95, 0.028864645837079178, 19.910828630969426, 0], [0.13731028080647464, 0, 0.08832774079007177, 0.09037460166117099, 0.09395754329875614, 0.15805735955144173, 0.025856400938419066, 0.03169531403157725, 0.3744207589220884, 28, 0, 1, 1, 1, 1, 75, 0.018234822411539616, 26.872567487706792, 0], [0, 0.4144853639419585, 0, 0.20247484141152938, 0.09342923991374558, 0.02395569636577215, 0.012124208093005056, 0.1208761770019472, 0.12685562842571585, 26, 1, 0, 1, 0, 0, 47, 0.05687618171755198, 40.856770248123354, 0], [0.03491424075001855, 0.5715807635482077, 0.03537521629192251, 0, 0.1575235753001678, 0.03167300353398292, 0.03929780971732323, 0, 0.12719692256116194, 21, 0, 0, 0, 1, 0, 76, 0.025213715012841528, 21.982305938196237, 0], [0.06079718678965697, 0.12076518736748029, 0.042859344926207424, 0.06659147601702657, 0.13070384836416216, 0.2828815748526963, 0.019693665261730896, 0.19484616645064035, 0.08086154997039917, 20, 0, 1, 0, 1, 1, 111, 0.047805984082680796, 10.03877457847671, 0], [0.24179615535302884, 0.25877927480072604, 0.014820691844876928, 0, 0.15832174526028378, 0.04691725391892066, 0.02777877010727125, 0, 0.2515861087148924, 26, 0, 1, 1, 1, 0, 91, 0.028014435217873997, 27.14035356818026, 0], [0.1888413365959418, 0.2141387098938834, 0, 0.030257905893081753, 0.13999376145684408, 0.2484588896605596, 0.018492536320019196, 0, 0.15016320973605818, 20, 0, 1, 1, 1, 0, 19, 0.07976915102905811, 7.509609085313165, 0], [0.2921193437209957, 0.15041100054754036, 0.013405180819004782, 0, 0.07832077304849397, 0.1020683380020485, 0.04059857854891436, 0.18210696530984333, 0.14096982000315894, 31, 1, 1, 0, 1, 1, 6, 0.08478921916083872, 24.263302530112142, 0], [0.4783050945370415, 0, 0.017188098064123122, 0.0162286219671033, 0.09235223281596723, 0.07107177232630822, 0.016359434613226226, 0.08028336309746503, 0.2281784539277298, 27, 0, 1, 1, 1, 0, 78, 0.056331287752765136, 26.644967083465193, 0], [0.2732104840704842, 0.050312631266876684, 0.025064757668281028, 0.03887100855793242, 0.140768015957551, 0.1590462786304207, 0.01150928465894333, 0.04010563942059936, 0.2611118997689112, 28, 1, 1, 1, 0, 1, 68, 0.05632547729148834, 23.21037368538042, 0], [0.1845353930507475, 0.08202665523721009, 0.05059827649516026, 0.02059246576858974, 0.14023777526003428, 0.13737861030280243, 0.02168541612890467, 0.0770971395415294, 0.28584826821502163, 29, 1, 1, 1, 0, 1, 10, 0.026697194238399877, 26.741969854191524, 0], [0.2688810142203121, 0.01998005252121516, 0.01913375440846553, 0, 0.1404904228857283, 0.25210380774426705, 0, 0, 0.299410948220012, 28, 1, 1, 1, 0, 1, 12, 0.0753065894720415, 28.313414505967327, 0]], [[0.7401299663305263, 0, 0, 0, 0, 0, 0, 0, 0.2598700336694741, 32, 0, 1, 0, 1, 0, 128, 0.0, 47.99999999999998, 0], [0.28832992549705305, 0, 0, 0, 0, 0, 0, 0, 0.7116700745029486, 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 0], [0.21364662607116325, 0, 0, 0, 0, 0, 0, 0.4171956599144815, 0.3691577140143562, 29, 0, 0, 1, 1, 1, 128, 8.673617379884005e-20, 47.999999999999986, 0], [0.3090520142432652, 0, 0, 0, 0.15914841291545415, 0.2309197574735759, 0, 0, 0.3008798153677035, 32, 1, 0, 0, 1, 1, 128, 2.3592239273284567e-17, 22.53707608439276, 0], [0.327584311408983, 0.09256081639926735, 0, 0, 0, 0.011553444268137843, 0.05304003014701074, 0.05749489568536397, 0.45776650209123726, 31, 0, 1, 1, 1, 0, 80, 0.054537139688529634, 27.358594500814252, 0], [0.38111628817713455, 0, 0.06133138716820026, 0, 0.03411085893121368, 0, 0, 0.073040075033785, 0.45040139068966645, 32, 1, 1, 1, 1, 0, 128, 0.0, 48.0, 0], [0.47156108549615056, 0.15547448548182796, 0, 0.018332000692114634, 0.11043580067254727, 0.07206806949905704, 0.010660273802280622, 0, 0.1614682843560219, 20, 0, 1, 1, 1, 0, 105, 0.0375205533911588, 22.8886409676621, 0], [0.24060821129151935, 0.1122029732340159, 0, 0, 0.14006195386471124, 0.17091874559342574, 0.012777779765451509, 0, 0.31576633994814385, 29, 0, 1, 1, 0, 0, 48, 0.0, 30.87623086263716, 0], [0.14351096319217463, 0.14136411767043375, 0.08511431044446209, 0, 0.10952277612608234, 0.10097668747781378, 0, 0, 0.41951114508903364, 29, 0, 1, 1, 1, 0, 46, 0.062078816198449016, 28.184796008208647, 0], [0.22933992438509518, 0, 0, 0, 0.21045383106178137, 0, 0.0424308186642838, 0, 0.5177754258888396, 19, 1, 0, 0, 1, 1, 25, 0.024107270189119283, 29.3441185561731, 1], [0.12587119922541543, 0.05678137962496864, 0.018039565483185013, 0.013557449271478025, 0.09616730405062363, 0.10609425111064304, 0.015556826041170795, 0.1119059067946758, 0.4560261183978397, 28, 0, 1, 1, 1, 0, 79, 0.02542816838099518, 26.7801172475475, 0], [0.07358791323286702, 0, 0.07437164484197321, 0.09767220304894361, 0.2683081022959224, 0, 0, 0, 0.4717087034889414, 18, 1, 1, 0, 1, 1, 79, 0.016039293956993113, 20.24325026056652, 0], [0.26049900602097664, 0.10387359994157346, 0.0738564451324377, 0.0314301667769465, 0.17962326125201575, 0.15425608904478672, 0.021744077205264816, 0.06955212905337772, 0.10516522557262055, 29, 0, 1, 1, 1, 0, 92, 0.072817089845866, 26.26312643423737, 0], [0.3092514955931313, 0, 0.09527908730329955, 0.014313217767709104, 0.028154843228189112, 0.04603761650781422, 0, 0, 0.5069637395998569, 26, 0, 1, 1, 1, 1, 90, 0.025776919894303137, 21.534762447005512, 0], [0.12086922237066114, 0.20251592737371713, 0.01095479253742851, 0.09053388008452153, 0.12603216149929733, 0.18639624520919623, 0.03273933975891967, 0.06323874023628907, 0.16671969092996938, 30, 1, 1, 1, 0, 1, 53, 0.06908466535580444, 32.924057223706086, 0], [0.254051212886874, 0.02002433388779299, 0, 0.11966075486123634, 0.2553983960793289, 0.16910535262286347, 0, 0, 0.17853855950793765, 20, 0, 1, 1, 1, 0, 103, 0.03067025029176279, 24.890203149902185, 0], [0.3233356205210876, 0.05832634707244166, 0.06598059446309, 0.022243797250613265, 0.09667886150005432, 0.064004380332378, 0.012216411313349166, 0, 0.3543482143521564, 32, 0, 1, 1, 1, 0, 29, 0.011977913276068813, 22.773041631722986, 0], [0.20840404537268953, 0.036124061453278046, 0.06817528899530644, 0, 0.061143071857057904, 0.07024160029708303, 0.05245478607810237, 0, 0.4963857381702718, 21, 0, 1, 1, 1, 0, 94, 2.1639116771163212e-19, 13.280565128719214, 0], [0.22325274347010376, 0.24661238597615073, 0.022999649118634363, 0, 0.06576550546236748, 0.05519196577567155, 0.013503915196522782, 0.08621472362075962, 0.2864591113797898, 29, 1, 1, 1, 0, 1, 17, 0.041637887961377594, 25.471841049248305, 0], [0.14064990140912526, 0.16427539938421923, 0.06744470524011813, 0, 0.13562602385180123, 0.11029199236978879, 0.04579948604182454, 0, 0.3284647029100229, 23, 0, 1, 1, 1, 0, 77, 0.03175110524279667, 34.31481369003796, 0], [0.3400645763869028, 0.054806308898202746, 0.055964884676256474, 0.062276567968355456, 0.10827769645134053, 0, 0.012080767374214565, 0.05221326788683493, 0.3083622109272999, 30, 0, 1, 1, 1, 0, 12, 0.0815183629370597, 13.528202502597676, 0], [0.25459639204590373, 0, 0.010497519959184238, 0.032976052626168255, 0.09875437218291909, 0.02382395525391044, 0, 0, 0.5785920899263937, 27, 0, 1, 1, 1, 0, 38, 0.0762605739383984, 24.59820174443584, 0], [0.16178669821027475, 0.07005339360581668, 0.013536071194817522, 0.058419851914172295, 0.17190580077949255, 0.1523380820156142, 0, 0.07707663293263038, 0.2944630160031521, 31, 1, 1, 1, 0, 1, 26, 0.07732271774220917, 19.03655897334575, 0], [0.32653540529619485, 0, 0.03685129113617712, 0.11016744020579909, 0.12772913191608018, 0, 0, 0, 0.3968513253659269, 27, 0, 1, 1, 1, 0, 44, 0.026190721199171065, 19.517203497739562, 0], [0.25074415963754926, 0.017092437724441405, 0.02374830775054052, 0.08238414188797887, 0.057686159961843175, 0.08927542655360975, 0.012989983339345144, 0.047986369480364485, 0.4180930136643274, 30, 1, 1, 1, 0, 1, 38, 0.06506273815184536, 29.7933962262941, 0], [0.21153753704207187, 0, 0.04896564917424103, 0.10376173484679199, 0.08675728524230933, 0.14063072063999496, 0.021762857433756944, 0.08993027590461886, 0.29665393971621506, 27, 1, 1, 1, 0, 1, 74, 0.06512548986251684, 25.462670447928538, 0], [0.13835170485938392, 0.0732893499301599, 0.036629478576843726, 0, 0.205202069439569, 0.08914654599201254, 0, 0, 0.44421436143795545, 28, 0, 1, 1, 1, 0, 91, 0.0823522268851013, 5.705668760349102, 0], [0.107311846843097, 0.16787807065671825, 0.042032274005147356, 0, 0.20098162028507222, 0.03217950656242254, 0, 0.15375191064000615, 0.29586477100753655, 32, 1, 1, 1, 0, 1, 16, 0.07602175225199254, 18.056245332364714, 0], [0.22766493148676137, 0.18637436690708084, 0, 0.11222136257780278, 0, 0.2681110195963128, 0, 0.20562831943204227, 0, 13, 0, 0, 0, 1, 0, 77, 1.5612511283791162e-17, 31.7135323065686, 0], [0.2610135886434425, 0, 0.06687758606834901, 0.028330236690664545, 0.14900678821689278, 0.026461579024512214, 0.033830188145398246, 0, 0.42907602783958076, 27, 0, 1, 1, 1, 0, 39, 0.017633400910146618, 13.766332433033838, 0]], [[0.7399629160510698, 0, 0, 0, 0, 0, 0, 0, 0.26003708394893027, 32, 0, 1, 0, 1, 0, 128, 1.517883041479706e-18, 48.0, 0], [0.2898155259308516, 0, 0, 0, 0, 0, 0, 0, 0.7101844740691465, 1, 1, 0, 1, 0, 0, 2, 0.1, 48.0, 0], [0.21347422324593468, 0, 0, 0, 0, 0, 0, 0.41453037773001067, 0.37199539902405465, 29, 0, 0, 1, 1, 1, 128, 8.673617379884035e-19, 48.0, 0], [0.31507025835112645, 0, 0, 0, 0.16167007745740777, 0.22344669690370272, 0, 0, 0.29981296728776274, 32, 1, 0, 0, 1, 1, 128, 0.0, 22.507652426013916, 0], [0.32538344708822997, 0.08798648084079161, 0, 0, 0, 0.013540389075487524, 0.057395213422666035, 0.055953440562241455, 0.4597410290105835, 31, 0, 1, 1, 1, 0, 78, 0.05440247231379241, 27.52955732320367, 0], [0.38040995475658274, 0, 0.061361958838340114, 0, 0.032587306743078574, 0, 0, 0.07322978639890491, 0.4524109932630937, 32, 1, 1, 1, 1, 0, 128, 3.0357660829594127e-19, 48.0, 0], [0.4299696977442723, 0.12582093181852355, 0, 0.029446639157695242, 0.09610372408642309, 0.07190039769560237, 0, 0, 0.24675860949748346, 21, 0, 1, 1, 0, 0, 63, 0.004572883543628167, 23.826856873198565, 0], [0.23764706444051817, 0.11196474585214754, 0.02411727060137197, 0, 0.19498479609813602, 0.12336814516670357, 0.017232435140870746, 0.03114234453134015, 0.25954319816891175, 30, 0, 1, 1, 1, 0, 45, 0.0065405152620106825, 30.517398635615372, 0], [0.08154221220211783, 0, 0, 0, 0.26870525398091266, 0.13096098346653465, 0.0287700790577814, 0, 0.4900214712926535, 26, 0, 1, 1, 1, 0, 18, 0.03084186213832493, 33.04038074131623, 0], [0.17620020276923742, 0.1604311558655159, 0.056560641848404355, 0.03400413338680903, 0.11279184685566496, 0, 0.027871912907978212, 0, 0.43214010636639, 26, 0, 1, 1, 1, 1, 125, 0.05415440840859075, 30.56758872637405, 1], [0.30775366599073023, 0, 0.05563987590353033, 0, 0.07636980967769055, 0.09763272988364248, 0, 0.10230433603974229, 0.36029958250466404, 26, 0, 1, 1, 1, 0, 54, 0.027334173050309458, 21.135216047584723, 0], [0.0641029903088202, 0, 0.07000994608566934, 0.01113993138446206, 0.13844138047965682, 0.2630521269951029, 0, 0.018599730691665383, 0.4346538940546234, 22, 0, 1, 1, 1, 0, 90, 0.009696145966038586, 19.30509234853406, 0], [0.5059136408063534, 0.029076651113469583, 0.018800190549195364, 0.07221770752076317, 0.12852724187443568, 0.0702243489416434, 0.02771100325147201, 0.09865101395395359, 0.04887820198871381, 29, 1, 0, 1, 0, 1, 25, 0.06979459890740512, 39.057867234943785, 0], [0.10191963319634617, 0.4335460498465621, 0, 0, 0.11642281351489021, 0.05494869927611654, 0, 0, 0.29104051375385964, 28, 0, 1, 1, 1, 0, 80, 0.046776489419539775, 5.242818810543914, 0], [0.08105067454912904, 0.3604078876758696, 0.023500026813944974, 0, 0.11959163061059994, 0.05851717672932071, 0.016509747267391107, 0.11556880313551161, 0.21541145846841545, 28, 1, 0, 0, 1, 0, 46, 0.03934633542388128, 27.6177717216438, 0], [0.25893613869491805, 0.016825048806244233, 0, 0.04506822521707407, 0.0811739183346659, 0.11328696445747911, 0, 0, 0.48136441768256505, 20, 1, 0, 0, 1, 1, 82, 0.04543252200055806, 20.218405490277977, 1], [0.2220578209937544, 0.0207206513163935, 0, 0.011469114823354687, 0.1428922869705705, 0.0410736075923853, 0.02074093569601453, 0.10969443307592287, 0.4216474549872496, 25, 0, 1, 1, 1, 0, 91, 0.02562866465287938, 26.839051350240034, 0], [0.19739085439623974, 0.20364993027399156, 0.07751645970654678, 0.0711315679616493, 0.12841983327792925, 0.0420672153099314, 0.022414502250670272, 0, 0.25740963682304174, 28, 0, 1, 1, 1, 0, 78, 0.04691643249230856, 26.3742571270078, 0], [0.25912614648143584, 0.144485317261946, 0.05749333199160797, 0.034560170448277035, 0.07206946502190076, 0.0603954244261858, 0, 0, 0.3718701443686466, 24, 0, 1, 1, 1, 0, 77, 0.05892758114716593, 14.580875458317513, 0], [0.27439058830514657, 0.1883340090316896, 0.045191400182136104, 0.08169938496894052, 0.09842399569725606, 0.10978005380851871, 0.012490231910509074, 0, 0.18823886369882853, 25, 0, 1, 1, 1, 0, 77, 0.025950364994338754, 24.962792049159418, 0], [0.14207412537024364, 0.04401626903617818, 0, 0.02067650330467506, 0.09589178122945692, 0.08917725693664706, 0, 0.1119730780543002, 0.49229571322401827, 24, 0, 1, 1, 1, 0, 56, 0.036037062711555316, 13.794392719019536, 0], [0.2277564644666549, 0, 0.08546785095648687, 0.017753488913714033, 0.03786214405322362, 0.04842153425964593, 0, 0.09175390582112003, 0.48934752360073913, 28, 0, 1, 1, 1, 0, 74, 0.04176684502004917, 18.207778788621034, 0], [0.23351847763606187, 0.04433918926539996, 0.03735309061150259, 0.07584039302776992, 0.15350366583160485, 0.0858822645688419, 0, 0.12657816134355498, 0.2429847577152639, 30, 1, 1, 1, 0, 1, 48, 0.038470029502677694, 20.56859309327719, 0], [0.24760246952844273, 0.061850226730378606, 0.023330456650404896, 0, 0.07717319183728616, 0.18054785523103417, 0, 0.14726760351733087, 0.2622281965051227, 29, 1, 1, 1, 0, 1, 46, 0.08978898264472278, 18.08998952854078, 0], [0.08950262128809854, 0.3794969482613048, 0, 0, 0.02952306604776449, 0.120768680778494, 0, 0.140171028631419, 0.24053765499291926, 28, 1, 1, 1, 0, 1, 19, 0.060295594692041335, 11.556082610737949, 0], [0.14662662954615252, 0.276014506977034, 0, 0.05794555555065428, 0.12889806482971544, 0.062560770187327, 0.022200979707622023, 0.08284042973375513, 0.21859330661157464, 25, 0, 0, 1, 1, 1, 69, 0.05578677306194388, 23.755155306654864, 1], [0.10977747361569619, 0.2028557609110231, 0, 0, 0.07512414135680076, 0.37901769245064965, 0, 0.1306153347314215, 0.10260959693440885, 30, 0, 1, 0, 1, 1, 28, 0.07556832638185083, 36.54846065229333, 0], [0.11315810015473805, 0.45339203026357705, 0, 0, 0.1664118910128755, 0.10691822075511544, 0.043588026341089665, 0.10060393115959994, 0.015927800313004378, 19, 1, 1, 1, 0, 1, 61, 0.02710204532086204, 14.98103139747407, 0], [0.14728653157492588, 0.24366569164639973, 0, 0, 0.18336239158926432, 0, 0, 0.13555442466511186, 0.28095359777004236, 32, 1, 1, 0, 1, 1, 18, 0.08408381007967397, 4.189410363253511, 0], [0.1517487242677402, 0.3810764299608995, 0.015097362147931837, 0, 0, 0, 0, 0.1341007314049, 0.31797675221852845, 32, 1, 1, 0, 1, 1, 12, 0.08355638090181036, 17.846375297193827, 0]]], 'random_full_train_performance': [-0.8283027410507202, -1.0663707256317139, -0.8693589568138123, -0.8668621778488159, -0.8583806753158569, -0.8432464003562927, -0.8721480965614319, -0.868810772895813, -0.9140738248825073, -0.8810084462165833, -0.8707413077354431, -0.9481663107872009, -0.83283531665802, -0.9257520437240601, -0.9291384816169739, -0.8885197639465332, -0.8824106454849243, -0.8834204077720642, -0.8870855569839478, -0.875321626663208, -0.913427472114563, -0.8854158520698547, -0.8742722272872925, -0.875694215297699, -0.9216276407241821, -0.8933585286140442, -0.8949460387229919, -0.928623378276825, -0.9070004820823669, -0.889920711517334]}
Traceback (most recent call last):
  File "/home/alfred/Data-Mixing/BO_runs_LLM_joint_optimization.py", line 277, in <module>
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/home/chenzhil/results'
wandb: - 0.054 MB of 0.054 MB uploadedwandb: \ 0.054 MB of 0.054 MB uploadedwandb: | 0.054 MB of 0.054 MB uploadedwandb: / 0.054 MB of 0.054 MB uploadedwandb: - 0.054 MB of 0.081 MB uploadedwandb: \ 1.262 MB of 1.262 MB uploadedwandb: | 1.262 MB of 1.262 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:            eval/runtime ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ
wandb: eval/samples_per_second ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ
wandb:   eval/steps_per_second ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ
wandb:             train/epoch ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá
wandb:       train/global_step ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá
wandb:         train/grad_norm ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:     train/learning_rate ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñá‚ñÉ‚ñÜ
wandb:              train/loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.88992
wandb:             eval/runtime 5.4137
wandb:  eval/samples_per_second 184.715
wandb:    eval/steps_per_second 11.637
wandb:               total_flos 1.1200620940650086e+17
wandb:              train/epoch 1.0
wandb:        train/global_step 625
wandb:          train/grad_norm 0.67192
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.784
wandb:               train_loss 0.94651
wandb:            train_runtime 384.5557
wandb: train_samples_per_second 25.996
wandb:   train_steps_per_second 1.625
wandb: 
wandb: üöÄ View run trainer_output at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface/runs/917b72te
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/alfred-leong-national-university-of-singapore/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260102_051345-917b72te/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
